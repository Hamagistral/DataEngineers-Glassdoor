company,company_rating,location,job_title,job_description,salary_estimate,company_size,company_type,company_sector,company_industry,company_founded,company_revenue
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,,Unknown / Non-Applicable
Futuretech Consultants LLC,,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),,,,,,
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,,Unknown / Non-Applicable
"Apple
4.2",4.2,"Cupertino, CA",Data Engineer,"Summary
Posted: Dec 22, 2021
Weekly Hours: 40
Role Number:200327520
As part of our Video Engineering group, you’ll help deliver creative solutions to various problems that could impact the people all over the world. This Data Engineer will work closely with other members of the Video Engineering group to mine data, implement model evaluation pipeline, analyze large scale data, visualize data, and ensure the delivery is of the highest quality. This position will also require strong coding skills, presentation skills, and collaborating with multiple teams (ex: machine learning, cloud infrastructure support).
Key Qualifications
A curious mind
An obsession for quality
Background in Data science, Data mining, Multivariate statistics, Computer vision, Machine learning
Experience working with large scale data sets
Solid programming skills including:
Python
C/C++
Experience with data visualization and presentation, familiar with data analysis tools such as Tableau
Excellent problem solving and communication skills
Description
The responsibilities of this position includes the following for current and future products: - Implement algorithm evaluation methods - Analyze data and build data analysis tools - Deep-dive failure analysis - Discover new perspectives for old data - Produce / Present meaningful data visualization to higher-ups and across various involved teams
Education & Experience
Masters in Computer Science or relevant experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $50.72 and $76.44/hr, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1976.0,$10+ billion (USD)
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,,,,Unknown / Non-Applicable
"Jane Street
4.4",4.4,"New York, NY",Data Engineer,"About the Position
We are looking for a Data Engineer who can help us understand, clean, manage, and share the data that guides our trading. At Jane Street, having a thorough and accurate understanding of data is at the core of the work we do.
Using our mix of in-house and open-source software, you will analyze datasets gathered from a variety of sources, checking for anomalies, matching formats and symbologies, automating ETL processes, and generally making it easier for our traders to generate valuable insights.
You should be excited about digging deep into datasets and explaining your findings to different types of colleagues, working collaboratively with traders and software engineers.
While prior experience with financial data would be nice, we don’t expect you to have a finance background. We’re happy to hire talented engineers and teach them what they need to know.
About You
Top-notch programming skills in any language (Python a plus)
Experience with using SQL and relational databases
Experience with generating data visualizations
Knowledge of statistical techniques, including multivariate regression and time series analysis
Clear and concise communication skills; able to efficiently analyze and deconstruct technical problems
Fluency in English required
Base salary is $175,000 - $300,000. Base salary is only one part of Jane Street total compensation, which includes an annual discretionary bonus.
Jane Street is an Equal Opportunity Employer","$237,500 /yr (est.)",1001 to 5000 Employees,Company - Private,Management & Consulting,Research & Development,2000.0,Unknown / Non-Applicable
"Steward Health Care
2.7",2.7,"Westwood, MA",Data Engineer,"Position Purpose:
Reporting to the Manager of the Data Warehouse team, part of the larger Health Informatics group, the data engineer applies their technical expertise to meet the needs of the department and Steward Health Care Network (SHCN).

Key Responsibilities:
ETL/Automation
Design configurable data process flows with full automation
Develop ETL processes for data loading and data extraction
Schedule ETL processes for full process automation
Data Engineering
Responsible for data analysis to support building data processes and reporting
Design useful and accurate data marts that meet requirements
Apply SQL skills when designing and building data marts and data flows
Quality
Establish and utilize QC processes to ensure data integrity
Incorporate standard error logging and alerts to ensure data is loaded as expected
Documentation
Create and maintain clear documentation

Education / Experience / Other Requirements


Education:
Bachelor's degree in Computer Science, Mathematics, Statistics or related experience


Years of Experience:
5+ years of database related work
2+ years of focus on healthcare data

Specialized Knowledge:
Knowledge of healthcare data
Experience using relational databases, SQL Server experience preferred
Experience using ETL tools (SSIS, Informatica, etc.)
Strong SQL programming skills
Experience with scripting languages (PowerShell, R, Python, etc.)
Experience automating data flows
Experience with Health Catalyst tools preferred, but not required
Deep understanding of database structures and data design.
Creative, flexible, and self-motivated with sound judgment
Strong communication skills




Location: Steward Health Care Network · 1301.72330 Steward Health Care Network
Schedule: Full Time, Day Shift, 40 hours","$94,536 /yr (est.)",10000+ Employees,Hospital,Healthcare,Health Care Services & Hospitals,1998.0,Unknown / Non-Applicable
"Twitch Interactive, Inc.
3.8",3.8,"San Francisco, CA",Data Engineer,"3+ years of experience in data engineering, software engineering, or other related roles. 3+ years in relational database concepts with a solid knowledge of star schema, SQL, SQL Tuning, OLAP, Big Data technologies 3+ years of experience in generating and maintaining data pipelines from various data sources, in collaboration with diverse stakeholders. 3+ years of experience working with Amazon Webservices, S3, EMR, Redshift etc. Experience with best practices for development including query optimization, version control, code reviews, and documentation. Experience with coding languages like Python/Java/Scala
About Us: Twitch is the world's biggest live streaming service, with global communities built around gaming, entertainment, music, sports, cooking, and more. It's where millions of people come together to chat, interact, and make their own entertainment. We're about community, inside and out. You'll find coworkers who are eager to team up, collaborate, and smash (or elegantly solve) problems together. We're on a quest to empower live communities, so if this sounds good to you, see what we're up to on LinkedIn and Twitter, get interviewing tips on Instagram, and discover projects we're solving on our Blog. About the Role: Data is central to Twitch's decision-making process, and data engineers operate at the forefront of this by creating authoritative datasets that drives analysis and decision-making across all of Twitch. In this role you will be shaping the way that business performance is measured, defining how we transform our data, and scaling analytics methods and tools to support our growing business, leading the way for high quality, high velocity decisions. For this role, we're looking for an experienced data engineer to join our Content Data Science team, which is focused on empowering staff throughout Twitch to use and trust our business data. Your responsibilities may range from developing and enhancing our data warehouse which act as authoritative sources of truth across the company, driving data quality and trustworthiness across product verticals and business areas, building self-service business intelligence infrastructure for analysts, as well as connecting into data interfaces that enable everyone in Twitch to discover and analyze the data. In the process, you will have the opportunity to interact with technical and non-technical staff members throughout the company, and will report to the Director of Content Data Science. This position can be located in San Francisco, CA; Irvine, CA; Seattle, WA; New York, NY; and Salt Lake City, UT. You Will: Define and own team level data architecture for trusted, governed, dimensionally-modeled repository of data that enables Twitch staff to quickly and reliably answer their business questions. Keep existing data sources fresh against data quality issues, design, develop and maintain data quality assurance framework and continuously improve the processes for developing new ones raising the level of quality expected from our work. Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost. Improve search, discovery and literacy: Create exploration and visualization interfaces in our BI tools and evangelize the adoption of these sources across the company through education and training programs. Improve business and engineering team processes via data architecture, engineering, test, and operational excellence best practices. Make enhancements that improve data processes.
Bonus Points
A passion for data science and interest in growing / learning data science, machine learning at scale.
A passion for games and the gaming industry
Perks
Medical, Dental, Vision & Disability Insurance
401(k)
Maternity & Parental Leave
Flexible PTO
Amazon Employee Discount
Monthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages, etc.)
We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
Pursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $105,700/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. Applicants should apply via our internal or external career site.","$105,700 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Internet & Web Services,1994.0,$10+ billion (USD)
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018.0,$5 to $25 million (USD)
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,,,2019.0,Less than $1 million (USD)
"Nike
4.1",4.1,"Boston, MA",Data Engineer,"Become part of the Converse Team

Converse is a place to explore potential, break barriers and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Converse, it’s about each person bringing skills and passion to a challenging and constantly evolving world to make things better as a team.
Converse, Inc. Boston, MA. Work closely with Project Management and Business teams to completely define specifications to ensure the project acceptance. Involved in preparation of functional and technical specifications with different cross teams. Lead team, defining solution options, providing estimates on effort and risk, and evaluating technical feasibility in Agile development process, including Scrum and Kanban. Work on troubleshooting data and analytics issues and perform root cause analysis to proactively resolve issues. Develop data extracts and feeds from the full spectrum of systems in the Converse ecosystem, including transactional ERP systems, POS data, product and merchandising systems. Engineer data products for a variety of Operations analytics use cases, ranging from reporting and data visualization to advanced analytics/machine learning use cases. Support designing technical specifications and data transformation models for junior developers. Ensure development is on track and meets specifications as defined by product management and the business. Responsible for data integrity of current platform and QA of new releases. Support the development and maintenance of backlog items and solution feature. Participate in sprint planning activities from a development perspective. Responsible for designing cloud-based data architecture using AWS stacks. Design and develop Python data science and data engineering libraries dealing with structured and unstructured data. Work with a variety of database types (SQL/NoSQL, columnar, object-oriented) and diverse data formats. Responsible for ETL with Spark and building data pipelines/orchestrations in Airflow and working on ETL tools like Matillion. Responsible for DevOps toolchain and Continuous Development, Continuous Integration and Automated Testing using Jenkins. Ensure and use data engineering for advanced analytics/data science and Software development skills.
Applicant must have a Bachelor’s degree in Computer Science, Information Systems, or Information Technology and 5 years of progressive post-baccalaureate experience in the job offered or a related occupation. Experience must include:
Data warehousing;
ETL or ELT;
Amazon Web Service (AWS) Cloud Services, including AWS S3, AWS Lambda, AWS EC2, AWS EMR or AWS DynamoDB;
Relational Database Management Systems (RDBMS), such as Oracle, Teradata, SQL Server or Snowflake;
Database Development with writing stored procedures, functions, triggers, cursors or SQL queries;
Hadoop, HDFS, Hive or Spark;
Programming languages, including Java or Python;
Business Intelligence Tools, such as Tableau;
Unix Shell scripting; and
Version control systems, such as Git, Bitbucket or Github
#LI-DNP
Converse is more than a company; it’s a worldwide advocate for self-expression. This belief motivates our employees, permeates our working environment and inspires our products. No two of us look or think exactly alike. We are each one-of-a-kind. Individually and as a culture, we have the freedom to create and grow professionally. Generous benefits packages only sweeten the experience. From Boston to Shanghai, from Brand Design to Finance, Converse is a brand that celebrates the unique and creative people of the world. Together, we’re different.","$115,797 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1972.0,$10+ billion (USD)
"AgileEngine
5.0",5.0,Remote,Big Data SDET Engineer,"Test with the best! At AgileEngine, you're free to build your own team of A-players and choose out of 70+ available projects the one you'd like to work on. Join the squad of top-3% software experts who make a difference through off-the-wall solutions.
Must haves
Strong Scala and Spark (hands on). If not Scala, strong experience in either Java or Python
Big Data concepts
API testing experience
Understanding qa process to create test cases and test strategies
Independently able to contribute to automation framework as well interact with the stake holders as needed
SQL experience
AWS experience
Strong test mindset
Experience of deploying or using Terraform and Gitlabs would be a bonus
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
The company behind the world’s leading OS virtualization software leverages AgileEngine as a provider of external QA and testing automation expertise. We’ve developed our client’s test strategy complete with test plans, ETAs, analysis, and risk assessment. Our teams have also optimized the company’s test suites, halving the time it takes for the tests to run. Our other deliverables include the implementation of new tests and the development of documentation, how-to guides, and test instructions on an internal Wiki.
Our team is also working to establish a strong QA/AQA process for an industry-leading cybersecurity platform. We’ve developed our client’s test strategy, manual testing cases, and automated tests from scratch. Our deliverables include backend, API, and UI testing, as well as BDD integrated with a streamlined CI process.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010.0,Unknown / Non-Applicable
"Zartico
4.5",4.5,"Salt Lake City, UT",Jr. Data Engineer,"Meet Zartico, the world’s first Destination Operating System.
Zartico’s mission is to empower DMOs to be better stewards of the world’s tourist destinations through improved data intelligence and decision-making. Makers of the first Destination Operating System, Zartico harnesses and streamlines complex data to provide a full spectrum of data science, benchmarking, and analytical services for use in marketing, community development, and sustainability efforts. Based in Salt Lake City, Utah, Zartico has over thirty years of experience in technology, tourism, and destination, travel and tourism marketing.
Zartico is looking to add a Junior Data Engineer to the team. In this role, Primary Responsibilities include but are not limited to the following:
Be part of Data Engineering Team to develop data infrastructure that is able to ingest and transform data at scale coming from many different sources, different customers, and in many different varieties. You will build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources and build robust data pipelines that collect, process and compute business metrics from activity data. You will be responsible for creating critical datasets for machine learning, growth funnels, business forecasting, and many other strategic initiatives.
To be successful in this role, you will need a strong background in Data Engineering, specifically, Python, SQL, Google Cloud Platform (BigQuery, GoogleCloud Storage, Airflow). In addition to needing an ability to problem-solve effectively, you will need to have strong written and oral communication skills.
What You’ll Do:
Effectively use, optimize, and automate processing pipelines, as well as explore new technologies to meet the expanding needs of our products.
Experience working as part of a Scrum team and familiarity with Scrum team ceremonies such as daily stand-up, backlog refinement, sprint retrospectives, sprint reviews, and sprint planning.
Collaborate with the product team to define high-level requirements for product development.
What you’ll bring :
You will need 1-2 years of experience as a Data Engineer, with fluency in SQL and programming languages, specifically Python. Experience with Ruby and scripting languages will also be important. You will also need a successful history of manipulating, processing, and extracting value from large, disconnected datasets. You will be expected to be an expert in coding, with an ability to promote solid design and coding standards.
Education and Certifications: BA/BS in a quantitative or computer science field.
Why Zartico?
We believe in a growth mindset. We are a learning organization.
We emphasize focus because we know that to achieve big dreams, you have to execute and get the small stuff right.
We lead with inclusion and value diversity. We believe in diversity of thought, perspective, and experience. Diversity of experience and perspectives creates a more robust product and a more beautiful world.
We dream in color and code.
We hustle.
We are humble and know that the sum of our parts is greater than any one of us as individuals.
Above all else, we do the right thing. We believe in transparency, honesty, and integrity.
We believe travel and tourism are a force for good because it builds connection, understanding, and appreciation of our world’s cultures, history, and natural resources. We believe data and the right metrics allow us to make better decisions because transparent data helps focus on the right issues, problems, and therefore, solutions, to be better stewards of our world's most precious destinations.
We’re building a global community—one that’s safe for people of all backgrounds. We are an equal-opportunity employer where our diversity and inclusion are central pillars of our company strategy. We look for applicants who understand, embrace and thrive in a multicultural and increasingly globalized world. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. When you join our team, you join the Zartico family.","$81,338 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2019.0,Unknown / Non-Applicable
"Aretec Inc
1.0",1.0,Remote,Junior Data Engineer,"POSITION TITLE: Junior Data Engineer YEARS OF EXPERIENCE: 1-3
LOCATON: 100% remote
*****Please Note: Aretec, Inc. does not offer Corp - 2 - Corp (C2C) employment. *****
Aretec is looking for a Junior Data Engineer. The Junior Data Engineer will be primarily responsible for design, development, support and enhancement of the data pipelines developed in AWS.

RESPONSIBILITIES:
You'll write clean and functional code on the front- and back-end
You'll write reusable and maintainable code
Coordinate with data migration plans
Ability to communication and collaborate with various teams and vendors.
Participates in functional and technical design.
Participation in Agile activities Scrum, Kanban.
Ensure coding, testing, debugging and implementation activities completed as required.
Flexible and adaptable with the ability to align to changing priorities
The developer should have great communication skills and be able to discuss and develop requirements with multiple levels of staff from corporate and field locations
An interest in and ability to understand financial reporting, accounting concepts and related accounting data
Participate in data flow diagramming and/or process modeling (code architecture)
Documents work and steps to completion as required
Follows AWS best practices to integrate with ecosystem and infrastructure
Ability to partner with domain architects to implement the defined solution architecture including application, infrastructure, data, integration, and security domains

REQUIRED SKILLS:
1-3 years of software engineering experience
1+ years of real industry experience
Experience with website development, web services and API development
Hands-on experience performing data engineering and transformation tasks using Python
Experience implementing backend in Python using frameworks such as Django or Flask
Knowledge of web technologies - both back and front-end development including, but not limited to JavaScript, React, CSS, HTML, T-SQL, and Python
Understand log monitoring and analytics
Experience Meeting both technical and consumer needs
Experience testing software to ensure responsiveness and efficiency
A general knowledge of index migrations, debugging and researching concepts are major pluses
Must be aware of CI/CD pipelines and well-versed in using GitLab for creating required pipelines for CI/CD

EDUCATION: Bachelors Degree in Mathematics/Statistics/Technology/Science/Engineering/Applied Mathematics or related field

CERTIFICATIONS: N/A","$102,500 /yr (est.)",51 to 200 Employees,Contract,,,,Unknown / Non-Applicable
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,,,,Unknown / Non-Applicable
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982.0,Unknown / Non-Applicable
"Adobe
4.4",4.4,"New York, NY",Data Engineer,"Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

Job Description
Adobe Customer Solutions is looking for a full time Data Engineer with experience in building data integrations using AWS technology stack as part of the team's Data as a Service portfolio for Adobe’s Digital Experience enterprise customers.
Customer facing Engineers who enjoy tackling complex technical challenges, have a passion for delighting customers and who are self-motivated to push themselves in a team oriented culture will thrive in our environment
What you'll Do
Collaborate with Data architects, Enterprise architects, Solution consultants and Product engineering teams to gather customer data integration requirements, conceptualize solutions & build required technology stack
Collaborate with enterprise customer's engineering team to identify data sources, profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating customer data sources and third party data sources with Adobe solutions
Develop new features and improve existing data integrations with customer data ecosystem
Encourage team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Collaborate with a Project Manager to bill and forecast time for customer solutions
What you need to succeed

Proven experience in building/operating/maintaining fault tolerant and scalable data processing integrations using AWS
Proven track record in Python programming language
Software development experience working with Apache Airflow, Spark, MongoDB, MySQL
Experience using Docker or Kubernetes is a plus
BS/MS degree in Computer Science or equivalent proven experience
Ability to identify and resolve problems associated with production grade large scale data processing workflows
Excellent interpersonal skills
Experience crafting and maintaining unit tests and continuous integration.
Passion for crafting I ntelligent data pipelines that customers love to use
Strong capacity to handle numerous projects are a must
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists. You will also be surrounded by colleagues who are committed to helping each other grow through our outstanding Check-In approach where feedback flows freely.
If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the significant benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age, sexual orientation, gender identity, disability or veteran status.

Our compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $101,500 -- $194,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.","$147,900 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1982.0,$5 to $10 billion (USD)
"Glow Networks
3.5",3.5,"Dallas, TX",Data Engineer,"Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.",$73.00 /hr (est.),51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003.0,$5 to $25 million (USD)
"Enterprise Knowledge LLC
4.0",4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”","$89,080 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013.0,$5 to $25 million (USD)
"Bose
3.8",3.8,"Framingham, MA",Data Analytics Engineer,"Job Description
Bose is about better sound, but better sound is just the beginning. We’re about inventing new technologies that truly benefit people and creating a culture where innovation and teamwork are highly valued. Working at Bose, you’re encouraged to question conventional thinking in the relentless quest to create products and experiences that change people's lives. Data analytics and data science are a crucial part of our mission, fueling the creation of new and innovative products, helping us to bring the right products to the right customers, and allowing us to astonish customers with carefully crafted and personalized experiences.

We are looking for a Data Analytics Engineer within our Data and Analytics Center of Excellence. The mission of this team is to develop world-class data science, machine learning, and statistical solutions to extract insights and enable data driven decisions that improve our business and enhance customer experiences.
This Data Analytics Engineer role will be critical to help drive analysis, data science, and reporting across many diverse data sets to make sure we deliver impact from our data products and services. This includes working with raw data sets that leads to development of logical and physical data models, data wrangling and design / building of semantic layer that helps to create dashboards and models that drive insights and actions. This work will directly inform and influence multiple division’s strategies. This is a unique opportunity to shape the experiences, technologies, and products that millions of people will use.
The ideal candidate has a deep understanding of data modeling, SQL, data warehouses / data lake and data engineering skills. You will often serve as an internal expert about the data, what it means and how it can be used to solve business problems. You can demonstrate that you have experience deeply understanding the data structure and transformations required to develop a semantic layer for data analysts and scientists. We are seeking a highly motivated, detail-oriented engineer who is passionate about data and enabling data-driven decisions.
Primary Responsibilities:
Leverage DBT to mine , analyze, and transform large structured and unstructured datasets, collaborating with analysts, data scientists, and business users to create actionable metrics and datasets that solve business questions.
Provide expertise in query building and data modeling to ensure data is made available in a timely, efficient, and comprehendible means.
Help define the strategy and standards for analytical engineering.
Help define testing and data quality programs leveraging dbt Cloud
Partner with our other teams to ensure documentation, privacy, and best in class coding practices are implemented within Analytical engineering.
Act as SME for specific data domains by enabling analytical data sets for ad-hoc analysis
Engage with key stakeholders in the business, product and software teams to clearly understand and scope analytical requests.
Qualifications:
Extensive background and demonstrated track record in building analytical models in SQL.
Strong technical skills, including experience with analytics, query and data visualization tools.
Experience with Snowflake, dbt, and Python is preferred.
Excellent communication skills, and the ability to explain deep technical results to diverse groups of stakeholders.
A life-long learner who is curious, has a passion for solving hard, ill-defined problems, has comfort taking initiative and who continuously seeks to improve their skills and understanding.
Enjoys working in a team environment as well as independently.
Education:
Bachelor’s or Master’s degree in math, physics, computer science, engineering, business, finance, marketing, economics or related quantitative or computational field
Work Experience:
3+ years of related data analytics experience in relevant consulting, finance, data science or market intelligence functions
Previous experience in a consumer electronics environment, particularly in a technical field, is a definite plus
Previous experience with mobile app data is a plus


What's in it for you:
Be a part of and work with a top notch, multidisciplinary, transparent, and agile team.
Collaborate with people like you who want to solve problems and have fun together.
Work on developing innovative Wellness products that improve people's lives.
Excellent work life balance and a continuous learning environment.
Highly competitive package as well as a comprehensive benefits program.
We strive to help our employees and customers reach their fullest potential.
Compensation for this role for a candidate based in Colorado is expected to be between $101,800 and $130,300 and for a candidate based in NYC to be between $101,800 and $130,300 . Actual pay may be higher or lower depending on geographic locations, skills, experience, and other factors permitted by law. This role is also eligible to receive a bonus, which is not guaranteed and may be based on individual and company performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.","$98,163 /yr (est.)",5001 to 10000 Employees,Company - Private,Manufacturing,Consumer Product Manufacturing,1964.0,$1 to $5 billion (USD)
"Empower Federal Credit Union
4.0",4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000","$99,876 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,,$25 to $100 million (USD)
"APLOMB Technologies
4.4",4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$72,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,,$1 to $5 million (USD)
"Small Batch Standard
4.1",4.1,Remote,Junior Data Engineer,"We're the premier, remote accounting, tax, and consulting firm built exclusively to serve the craft brewing industry.
Our mission is to help craft breweries grow profits and build deep successful relationships. And our team is filled with expert, autonomous, adaptable, technology-driven high performers.
Are you up for the challenge?
We're looking for a full-time, remote Junior Data Engineer to join our specialized team. The main objective of this role is to design, develop, implement, and improve both internal and external applications to support our brewery clients and team in accordance with the SBS Core Values.
About The Role
This role will report to our Technology/Product Manager and is accountable for fulfilling the following responsibilities:
Building our data pipeline and analysis applications. A key aspect of the consulting service we provide to clients involves the collection, aggregation, analysis, modeling, and usage of financial data and benchmarks. We use this data both internally to develop and inform strategy, as well as externally through our Benchmarks Assessment (https://sbstandard.com/assessment/) and Compass analysis product (https://sbstandard.com/levelup-compass/). You'll be responsible for working with our team to build out our data pipeline for these tools, and progressively increasing our ability to aggregate, analyze, query, and feed back this data into our reporting, analysis, and consulting work. Platforms we're building with include: SQL, Airflow, Excel Visual Basic for Applications (VBA), Google Apps Script, Intuit/QuickBooks Online.
New process and technology R&D. We're always looking for new opportunities to provide both our team and our clients access to additional tools that give them leverage, automate and streamline processes, and overall make work more efficient. Part of your time will be dedicated to researching, testing, and prototyping new tech and application options.
Participate and contribute to the overall success of our team. Each week the team meets to share wins, progress, and knowledge, as well as identify and solve issues at multiple levels (company, team, individual). Your full participation in this process is critical to ensure that we are operating as a cohesive, high-performance unit.
About You
We're looking for an individual who:
Is a problem solver through-and-through. Everywhere you look, you both (a) see problems to solve, and (b) see solutions and new ways of doing things that just haven't been done yet. You know how to think outside of the box, are willing to “go there” with new ideas and solutions that haven't been done before, and have the confidence to start building, testing, iterating, and making sh*t work.
Is a systems thinker. You understand both the big picture and how the functional components fit together, and have the ability to take a specific analysis outcome and generalize it to fit a wide range of scenarios through structure and sound system design.
Can fail fast, iterate, and learn. You're an independent, self-directed, learner who isn't afraid to “move fast and break stuff” knowing that failure is a prerequisite to success, ESPECIALLY in product development. You may not have traditional credentials, but what you do have is the ability to rapidly learn, adopt, test, and understand new languages, platforms, tools, and solutions.
Is a manager of one. Unlike working within a traditional firm, in this role you'll be in the driver's seat, managing your workflow and workload in order to meet the standard set of deliverables required for each client.
About Our Culture
We're fully remote, with team members and clients located all across the U.S. and have developed our own unique culture we call The SBS Way, within which we operate, evaluate performance, and make decisions using our core values as a guide:
Be Antifragile. Everything we do, good or bad, makes us better. And every experience is an opportunity for learning and continuous improvement.
Play The Long Game. We make decisions, to the best of our ability, in the long-term interest of our firm, our team, our clients, and our broader industry and community.
Embrace Technology. We welcome new technologies with open arms, and are always exploring, testing, and implementing them in the interest of enhancing both our internal capabilities and our client's outcomes.
Build and Trust The Process. Each member of the team is committed to building, following, and improving the processes we use to deliver exceptional results for our clients.
Act as A Team of Expert Knowledge Workers. We openly and willingly collaborate, communicate, and provide rapid, direct feedback in the interest of learning, improving and developing ourselves.
Working At SBS
What it's like working at our firm:
High flexibility. We believe in the ability of our team to determine the best way to complete their work. We measure outputs, not inputs. We don't have time sheets. We don't track hours. We don't pay attention to when and where our team works. Your schedule is yours to make.
High accountability. What we care about most is that we deliver on what we promise to our clients. In this respect, we measure and manage to our deliverable performance metrics and ensure each team member takes ownership over their accomplishment with a high level of quality that aligns with our core values
Great pay for great work. We pay based on the characteristics that matter: position (and its market value), level of mastery, and longevity with the firm. All of which aim to ensure each member of the team feels they are compensated well and can focus on great work.
Merit-based career progression. We have clearly established career tracks, performance benchmarks, and mastery levels set for all of our core positions. How quickly you progress is entirely under your control, with a quarterly review and bi-annual promotion consideration cycle in place to evaluate your progress.
Generous benefits. We offer a generous benefits package that includes medical, dental, and vision insurance enrollment; as well as an IRA match, tech stipend, 3 weeks of paid time off, and entry into our profit share bonus program after two years of service.
Personal and and team development. In addition to our overall continuous learning focus, we also provide support for personal development in the form of expense coverage for continuing education (books, courses, training, certifications, etc.) as well as experiential learning (brewery visits, industry events and conferences, etc.). Each year we also meet in person for an all-expenses-paid annual retreat as a team. No work. Lots of fun. Lots of client beer.
Job Requirements
The following basic requirements must be met:
Previous experience in SQL development and database management.
Previous experience building useful applications in scripting languages like VBA, Google Apps Script, Python, PHP, etc.
Can do effective cross-functional work in a remote environment.
Have crystal clear professional written and verbal communication skills.
Have exacting organizational standards and a calm and friendly attitude.
Available and responsive during normal business hours (9am-5pm Eastern Time, Monday-Friday).
Have a strong, consistent internet connection and a work environment conducive to video calls.
Preferred qualifications include:
Direct previous experience building data pipelines.
Direct previous experience building Airflow workflows and applications.
Experience building out and managing API connections.
Experience working with Quickbooks Online or similar accounting or finance platforms.
Experience using Podio or similar remote project management tools (e.g. Trello, Asana, etc.).
Next Steps
If the position, culture, values, and mission at Small Batch Standard sound like they're the right fit for you, please apply here.","$64,000 /yr (est.)",1 to 50 Employees,Company - Private,Financial Services,Accounting & Tax,2010.0,Unknown / Non-Applicable
"Buchanan & Edwards
3.8",3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location","$138,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998.0,$25 to $100 million (USD)
"Tripoint Solutions
4.5",4.5,Remote,Data Engineer (Remote),"Tripoint Solutions is seeking a Data Engineer to join our team.
The Data Engineer will be part of a team responsible for ensuring the success of a highly visible, results-driven federal client through the development of a cloud-based next generation system.
This position requires the applicant to parse disparate data sources, including structured and unstructured elements, to find the patterns and meaning in large quantities of data. The successful candidate will leverage machine learning as well as best of breed pipeline technology to process and store a variety of data elements.
Location: This position is eligible for fully remote work. Selected candidates living within a 25 miles radius of the NITAAC office in Rockville, MD will be required to come into the office once a week. The selected candidate must be currently located in, or willing to relocate to, a state supported by Tripoint Solutions corporate offices (AL, DC, FL, IL, LA, MD, MI, MN, MS, NJ, NC, PA, TN, TX, or VA).
The successful candidate will be accountable to:
Creating and maintaining optimal data pipeline architecture.
Assembling large, complex data sets that meet functional / non-functional business requirements.
Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Building the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Keeping data separated and secure across national boundaries through multiple data centers and AWS regions.
Strong interest to learn and stay up to date on relevant technologies, trends, industry standards and identify new ones to implement.
What you bring
Experience, Education & Training:
Bachelor's degree in computer science, Math, Analytics, Statistics, Informatics, Information Technology or equivalent quantitative field.
5 years of experience working in a Data Engineer or Data Scientist role.
Experience with cloud data services (AWS preferred).
Experience solutioning and applying Natural Language Processing (NLP) and or Machine Learning (ML) technologies
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience with Microsoft SQL, database development and design.
Experience building processes supporting data integration, transformation, data structures, metadata, dependency and workload management.
Demonstrated success in manipulating, processing and extracting value from large disconnected data sets.
Demonstrated accomplishments in designing, coding, testing and supporting data analytics and reporting solutions in a cloud environment.
Experience with object-oriented/object function scripting languages: Python, Java
Concept experience; information retrieval, search engine, document data extraction
Preferred experience with AWS cloud services: Textract, Comprehend, GlueMaker, Athena, Notebook
Working knowledge of message queueing, stream processing, and highly scalable ‘big data’ data stores.
Clearance Requirements:
Applicants selected may be subject to a government security investigation and must meet eligibility requirements for potential access to classified information. Accordingly, US Citizenship or Green Card is required.
What we offer
About Tripoint Solutions
We are technology innovators, partnered with state-of-the-art providers, such as AWS, ServiceNow, and UiPath, to drive digital transformation in the federal space. TPS teams are bringing automation and data science into areas of the government that are crying out for fresh tech—making positive impacts felt by tens of thousands of users, countless citizens, and all six branches of the military each day. Our Agile teams are responsible for envisioning, launching, and operating the massive data systems and analytics platforms used to manage $14.5B in government procurements and $200B in military real estate assets globally. At TPS, we apply the power of cloud technologies to help the government think smarter and function better—for everyone.
TPS Company Values
We value and respect each employee's dedicated work and unique contributions; as they directly impact who we are and what we do.
Your talent and innovative thinking bring leading-edge solutions to our customers.
Our success is driven by the dedication of our employees.
Employee-generated solutions have sustained our continued success and customer satisfaction
Benefit Offerings
Tripoint Solutions builds flexibility into health benefit plan choices, covers most of the monthly premiums, and helps employees build a career with impact through our generous professional development program.
We offer all full-time employees:
Medical, Dental, Vision benefits with a national provider network (company pays 100% of Vision and Dental premiums)
Flexible Spending and Health Savings Accounts (FSA & HSA)
Company-paid Life and Disability insurance including Short-Term, Long-Term, and Accidental
Paid-time off (PTO), accruing with each year of service, up to 20 days, plus 11 paid holidays
401(k) Retirement Plan - No waiting period to contribute and company makes 3% contribution of eligible pay in addition to annual profit-sharing contribution option
Eligibility to receive impact bonuses each quarter
Referral Program
Professional Development Reimbursement Program to pursue undergraduate, graduate, training, and certifications
Monthly transportation, parking, and cell phone service reimbursement
COVID-19 Related Information
Tripoint Solutions does not have a vaccination mandate applicable to all employees. However, to protect the health and safety of its employees and to comply with customer requirements, Tripoint Solutions may require employees in certain positions to be fully vaccinated against COVID-19. Vaccination requirements will depend on the status of the federal contractor mandate and customer site requirements. Furthermore, remote work arrangements are subject to change based on customer site requirements.
Tripoint Solutions is an Equal Opportunity Employer/Veterans/Disabled
Job Type: Full-time
Pay: $145,000.00 - $155,000.00 per year
Benefits:
401(k)
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Retirement plan
Tuition reimbursement
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
What cloud services have you worked with?
Do you have experience with Machine Learning or NLP?
Does the advertised salary align to your expectations?
US citizenship or green card is required. Do you meet this requirement?
This is a remote position (See description for details and requirements). Where are you located?
Are you willing to undergo a federal background check?
Education:
Bachelor's (Required)
Experience:
data scientist or data engineer role: 5 years (Required)
cloud services: 2 years (Required)
Microsoft SQL (development and design)?: 2 years (Required)
optimizing ‘big data’ pipelines, architectures and data sets: 2 years (Required)
AWS: 1 year (Preferred)
Python: 1 year (Required)
Java: 1 year (Required)
Work Location: Remote","$150,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2013.0,Unknown / Non-Applicable
"Angle Health
4.0",4.0,Remote,Data Engineer,"Changing Healthcare For Good
At Angle Health, we believe the healthcare system should be accessible, transparent, and easy to navigate. As a digital-first, data-driven health plan, we are replacing legacy systems with modern infrastructure to deliver our members the care they need when they need it. If you want to build the future of healthcare, we'd love for you to join us.

The Role
As a Data Engineer on the Strategy team at Angle Health, you will be part of an elite team of problem solvers tackling some of the hardest operational challenges across the business. You will be working closely with Technical Product Managers and other engineers in small teams embedded within functions across the company—from sales to operations to finance and more.

In partnership with operational stakeholders, your job will be to quickly understand business workflows, and design and implement technical, data-driven solutions to achieve the intended business outcomes. That may include architecting backend micro-services, building and maintaining robust data pipelines, standing up health metrics and automated alerts, and developing scalable technical solutions to support Angle Health's day-to-day operations.

Every day will be a new learning experience in this role and may span discussing architecture with fellow engineers, wrangling large-scale data, coding a custom web app or micro-service, or speaking with customers and executives.

This role is ideal for entrepreneurial engineers, applied data scientists, and creative, intellectually curious thinkers that want to solve high-value problems in healthcare.

This position may be based in San Francisco, New York City, Salt Lake City, or Remote.

We are currently trialing various titles for the same role. Please consider the following posted roles as the same position: Deployed Engineer, Software Engineer, Business Operations, and Data Engineer. Please note these are the same, so if you have already applied for one position, there is no need to reapply for the others.
What We Value
A strong engineering background in computer science, software engineering, data science, mathematics, or similar technical field is required for this role
Proficiency in programming languages (e.g. Python, SQL, Java, TypeScript/JavaScript, or similar) and data engineering frameworks
A highly analytical mindset and an eagerness to build technical solutions to complex business problems
High attention to detail and intellectual curiosity—you're not satisfied with surface-level answers. You want to dive into the data, the ""how,"" and the ""why"" because ""the way it's always been done"" is not always the way it should be done
Low ego—the outcome matters more than who gets the credit
Demonstrated ability to collaborate effectively in teams of technical and non-technical individuals
Highly organized with an ability to multitask, problem solve, and balance competing priorities in a rapidly changing environment
Because We Value You:

Competitive compensation and stock options
100% company paid comprehensive health, vision & dental insurance for you and your dependents
Supplemental Life, AD&D and Short Term Disability coverage options
Discretionary time off
Opportunity for rapid career progression
Relocation assistance (if relocation is required)
3 months of paid parental leave and flexible return to work policy (after 10 months of employment)
Work-from-home stipend for remote employees
Company provided lunch for in-office employees
401(k) account
Other benefits coming soon!

Backed by a team of world class investors, we are a healthcare startup on a mission to make our health system more effective, accessible, and affordable to everyone. From running large hospitals and health plans to serving on federal healthcare advisory boards to solving the world's hardest problems at Palantir, our team has done it all. As part of this core group at Angle Health, you will have the right balance of support and autonomy to grow both personally and professionally and the opportunity to own large parts of the business and scale with the company.

Angle Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. Angle Health is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities.",,1 to 50 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2019.0,Unknown / Non-Applicable
Data Crunch Corp,,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),,,,,,
LOVEFOODIES INC,,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",$30.00 /hr (est.),Unknown,Company - Public,,,,Unknown / Non-Applicable
"ESRI, Inc.
4.0",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1","$98,800 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969.0,$1 to $5 billion (USD)
"Double Line, Inc.
4.2",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w","$85,882 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009.0,$5 to $25 million (USD)
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018.0,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015.0,Unknown / Non-Applicable
Kaizen Dynamics,,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,,,,Unknown / Non-Applicable
"Fiserv, Inc.
3.2",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.","$108,451 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984.0,$10+ billion (USD)
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986.0,$5 to $25 million (USD)
"AgileEngine
5.0",5.0,Remote,Senior/Lead Data Engineer,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Work collaboratively with other engineers, architects, data scientists, analytics teams, and business product owners in an agile environment
Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools
Design and build robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Must haves
Bachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience
6 years of experience with Python or Java
4+ years of experience in building data lake, cloud data platform leveraging cloud (GCP/AWS) cloud native architecture, ETL/ELT, and data integration
3 years of development experience with cloud services ( AWS,GCP,AZURE) utilizing various support tools (e.g. GCS, Dataproc, Cloud Data flow, Airflow(Composer), Kafka , Cloud Pub/Sub)
Expertise in developing distributed data processing and Streaming frameworks and architectures (Apache Spark, Apache Beam, Apache Flink, )
In-depth knowledge of NoSQL database technologies (e.g. MongoDB, BigTable, DynamoDB)
Expertise in build and deployment tools – (Visual Studio, PyCharm, Git/Bitbucket/Bamboo, Maven, Jenkins, Nexus, )
5 years of experience and expertise in database design techniques and philosophies (e.g. RDBMS, Document, Star Schema, Kimball Model)
5 years of experience with integration and service frameworks (e.g API Gateways, Apache Camel, Swagger API, Zookeeper, Kafka, messaging tools, microservices)
Expertise with containerized Microservices and REST/GraphQL based API development
Experience leveraging continuous integration/development tools (e.g. Jenkins, Docker, Containers, OpenShift, Kubernetes, and container automation) in a Ci/CD pipeline
Advanced understanding of software development and research tools
Attention to detail and results-oriented, with a strong customer focus
Ability to work as part of a team and independently
Problem-solving and technical communication skills
Ability to prioritize workload to meet tight deadlines
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
Our team of data experts and engineers has helped create a supply chain management platform used by Forbes 500 enterprises and featured by Gartner. We’ve accompanied the platform from an early prototype to a full-fledged solution based on big data, AI, and machine learning. Data quality is integral to our current engagement with this project as we’re expanding the platform functionality.
AgileEngine has also been working on data and ML solutions in multiple industries and verticals like telecom, FinTech, energy efficiency, and more. Our work with our clients’ data has contributed to patented software products, Nasdaq-grade fraud detection, and products used by Deloitte and T-Mobile.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010.0,Unknown / Non-Applicable
"Dovenmuehle Mortgage, Inc.
2.6",2.6,"San Francisco, CA",Data Engineer,"Data Engineer
DMI Software, the San Francisco branch of Dovenmuehle Mortgage, Inc, the leading sub-servicer of mortgage loans in the United States, is looking for a talented and enthusiastic data engineer. We work exclusively in Software Development. Our growing office offers the feel of a startup with the backing and security of a long-established company. We aspire to create elegantly scalable products while fostering the continued growth of each team member. The ideal candidate will have 5+ years relevant experience, including Hadoop Ecosystem or similar, and with a scripting language.

Here we believe that the best software is created by an eclectic set of voices, and we strive to nurture an environment rich in differing opinion, belief, and background. Only in this way can we develop revolutionary products capable of meeting the varied needs of an increasingly interconnected world.

What You’ll Be Doing:
Design, implement, automate, and maintain large-scale enterprise ETL processes
Evolve data model and schema based on business and engineering needs
Oversee systems tracking data quality and consistency
Collaborate with data analysts to bridge business goals with data delivery

Requirements:
5+ years data engineering experience
Highly experienced using Python, SQL and and Hadoop
Excellent communication, analytical and problem-solving skills
Keen attention to detail while keeping an eye toward the big picture
You are comfortable with the nuts and bolts of systems programming in the Linux environment (shell/bash scripting)
Experience working in an Agile environment
Excellent presentation and communication skills
Experience profiling, debugging, tracing, and or parallelizing/optimizing Python code
Ideal candidate is one who can adapt and adopt to our existing architectures while also making impactful improvements and suggestions.

Job Type: Full-time","$135,927 /yr (est.)",1001 to 5000 Employees,Company - Private,Financial Services,Banking & Lending,1844.0,Unknown / Non-Applicable
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012.0,Unknown / Non-Applicable
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,,,,Unknown / Non-Applicable
BOTG LLC,,"Chicago, IL",Data Engineer,"We are looking for a Data Engineer in Chicago, IL (Hybrid) for a direct-hire position.
Job Description:
Position: Data Engineer - Centralized Data Science and Analytics (CDSA)
Location: Chicago, IL (Hybrid)
Duration: Direct-hire position
Client: Direct Client
Note: This is a W2 direct-hire role. Looking for candidates who are open to work independently on W2.
Requirements:
· Experience building and optimizing ""big data"" data pipelines, architectures and data sets.
· Working knowledge of message queuing, stream processing and highly scalable ""big data"" data stores.
· Advanced working SQL knowledge and experience working with cloud and relational databases.
· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
· Experience building processes supporting data transformation, data structures, metadata, dependency and workload management.
· A successful history of manipulating, processing and extracting value from large, disconnected datasets.
· Experience using the following software/tools:
· Relational SQL and NoSQL databases, including Postgres.
· Data pipeline and workflow management tools.
· Azure cloud services.
· Object-oriented/object function scripting languages: Python, PySpark Java, C++, R/RStudio/RSpark.
· CI/CD systems.
· Strong understanding across cloud and infrastructure components (server, storage, network, data, and applications) and ability to deliver end to end cloud infrastructure, architectures, and designs.
· Knowledge and implementation of enterprise scale cloud security platforms and tooling.
· Experience with enterprise applications, solutions, and data center infrastructures.
· Bachelor's degree in computer science or similar field; master's degree a plus.
· Exceptional product, project and client management skills.
· Azure, AWS or any other cloud/data engineering certifications are preferred.
Job Type: Full-time
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Big data: 5 years (Required)
Advanced SQL: 5 years (Required)
Cloud: 3 years (Required)
CI/CD: 3 years (Preferred)
NoSQL: 2 years (Required)
Work Location: Hybrid remote in Chicago, IL 60606","$85,894 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
TheHive,,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",$35.00 /hr (est.),,,,,,
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986.0,Unknown / Non-Applicable
"CapitalTech Solutions
4.5",4.5,Remote,Data Engineer,"Job Description:
12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
Complete Description:
Require the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals:
(1) Establish a data governance program,
(2) Perform a comprehensive data gap analysis,
(3) Design a master data architecture,
(4) Create a data warehouse for all data assets,
(5) Develop a front-end for program staff to quickly access workforce information and visualize program status,
(6) Create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities
(7) Foster relations with other agencies and improve inter-agency data integration.
The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff.
Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.
Develop and maintain an understanding of the data landscape including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.
Support the Data Management Project team to develop and maintain data quality controls.
Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.
Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.
Support the data stewards to troubleshoot and resolve data issues.
Support business users to obtain requirements for enhancements and/or new analytic assets.
Assist in the Development of data asset training and documentation.
Participate in the development and implementation of a data standard.
Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.
Job Types: Full-time, Contract
Pay: $66.00 - $74.00 per hour
Schedule:
8 hour shift
Experience:
in SQL, Python, R, JavaScript, JSON: 10 years (Preferred)
Agile Testing, Automation Testing, Black-box Testing: 10 years (Preferred)
Windows and Linux: 10 years (Preferred)
of BI tool architecture, Tableau: 9 years (Required)
Work Location: Remote",$70.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1999.0,$25 to $100 million (USD)
"etrailer.com
3.9",3.9,Remote,Data Engineer/Data Scientist,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow","$140,000 /yr (est.)",501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946.0,$100 to $500 million (USD)
"PrizePicks
4.9",4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1","$101,755 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015.0,Unknown / Non-Applicable
Invictus Data,,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location","$125,500 /yr (est.)",,,,,,
"ITExpertUS
2.8",2.8,"Chicago, IL",Data Engineer,"Client: Agilisium
Role: Data Engineer
Location: Chicago, IL – Hybrid role – Need to be onsite once/twice in a month.
Type: Long term contract
Job Description
Title: Data Engineer
We are looking for a savvy Data Engineer to join our growing team. The hire will be responsible for expanding and optimizing our data and pipeline architecture and data flow and collection. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. In addition, the Data Engineer will collaborate and support software developers, implementation architecture, and data engineers on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. In addition, they must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
Responsibilities:
Because we work on a programmatic leading edge of many technologies, we need someone who is a creative problem solver, resourceful in getting things done, and can shift productively to or from working independently and collaboratively. This person would also take on the following responsibilities:
Process unstructured data into a form suitable for analysis.
Support the business with ad hoc data analysis and build reliable data pipelines.
Implementation of best practices and IT operations in mission-critical tighter SLA data
pipelines using Airflow
Query Engine Migration from Dremio to Redshift.
We leverage: Multiple AWS Data & Analytic Services(e.g.,Glue, Kinesis, S3) , SQL (e.g.,
PostgreSQL, Redshift, Athena); NoSQL (e.g., DocumentDB, MongoDB); Kafka, Docker, Spark(AWS EMR and DataBricks), Airflow, Dremio, Qubole, etc.
We use AWS extensively, so experience with AWS cloud and AWS Data & Analytics certification will help you hit the ground running.
Business Skills for Data Engineers:
Creative Problem-Solving: Approaching data organization challenges with a clear eye on what is important; employing the suitable approach/methods to make the maximum use of time and human resources.
Effective Collaboration: Carefully listening to management, data engineers, and architects to establish their needs.
Intellectual Curiosity: Exploring new territories and finding creative and pragmatic ways to solve data problems.
Industry Knowledge: Understanding and perspective of how industry functions and how data can be collected, analyzed, and utilized for value creation; maintaining flexibility in the face of big data developments.
Skills and Qualifications:
3 to 5 years of real-world Data Engineering experience.
Programming experience, ideally in Python and other data engineering languages like Scala
Programming knowledge to clean structure and semi-structure datasets.
Experience processing large amounts of structured and unstructured data. Streaming data experience is a plus.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Background in Linux
Build the infrastructure required for optimal extraction, transformation, and loading of data
from various data sources using SQL and other cloud ‘big data’ technologies like DataBricks, Snowflake, Dremio, and Qubole.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience creating a platform on which complex data pipelines are built using orchestration tools like Airflow, and Astronomer.
Experience with real-time sync between OLTP and OLAP using AWS technologies like real- time sync between AWS Aurora and AWS Redshift.
Job Type: Full-time
Salary: Up to $68.00 per hour
Schedule:
8 hour shift
Work Location: On the road
Speak with the employer
+91 6305048577",$68.00 /hr (est.),501 to 1000 Employees,Company - Private,,,,Unknown / Non-Applicable
"E-Logic INC
4.4",4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.","$84,277 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007.0,$5 to $25 million (USD)
"Fiserv, Inc.
3.2",3.2,"Berkeley Heights, NJ","Software Engineer, Senior – Data and Analytics","What does a Senior Software Engineer – Data and Analytics do?
As an experienced member of our Data Engineering Platform Group you will be responsible for building and taking ownership over the successful design and development of data engineering projects within Fiserv’s Enterprise Data Analytics division. You will be required to apply your depth of knowledge and expertise to all aspects of the data engineering lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. You'll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally. You will Lead large-scale data engineering, integration and warehousing projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL. Additional responsibilities include, but are not limited to Architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years' experience with building java applications
Bachelors in Computer Science or related field
Provide technical leadership in developing data solutions and building frameworks
Hands-on experience with Springboot framework & Java
Java experience with OOPS concepts, multithreading
Hands on experience with leveraging CI/CD to rapidly build & test application code
Conduct code reviews and strive for improvement in software engineering quality
Review data architecture and develop detailed implementation design
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Experience working on SQL Databases
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance","$101,076 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984.0,$10+ billion (USD)
Ascent Solutions,,Remote,Hadoop Big Data Engineer,"Hadoop Big Data Engineer
Remote
Must Haves • Strong experience in Big Data, Cloudera Distribution 7.x, Cloud migration, RDBMS • Strong Experience with Amazon EMR/Databricks/Cloudera CDP • 4-5 experience building data pipelines using Hadoop components Sqoop, Hive, Solr, MR, Impala, Spark, Spark SQL., HBase. • 4-5 years of programming experience in Python, Java and Scala is must. • Strong experience with REST API development using Python frameworks (Django, Flask etc.) • Micro Services/Web service development experience using Spring framework is highly desirable.
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",$60.00 /hr (est.),,,,,,
"IntelliBridge LLC
3.9",3.9,"McLean, VA",Data Engineer,"Title: Data Engineer
Location: Permanent remote role
Clearance: Not required: Start date not contingent on a having or completion of a clearance, however one could be offered upon starting for future programs
Overview:
IntelliBridge is seeking a Data Engineer to collaborate with technical and non-technical data and development team members to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of analytics that provide help ensure national security. You'll be able to gain experience in designing cloud architectures while providing critical support to the client's mission. You will be responsible for designing and building smart data pipelines that are secure, robust, and alerting. You will also create innovative ways to combine disparate data sources and build integrated datasets for advanced analytics.
As a direct employee of IntelliBridge, you would receive a benefit package that includes health/dental/vision insurance coverage, 401K with company match, PTO & paid holidays, and annual tuition/training assistance. For more information, please visit our website.
Responsibilities/Duties:
Build and maintain the infrastructure to support integration, extraction, transformation, and loading (ETL) of data from a wide variety of data sources, such as relational SQL and NoSQL databases, and other platform APIs
Design data pipelines that are robust and secure including pipeline monitoring and alerting mechanisms
Create innovative ways to orchestrate data ETL processes
Guide and support the implementation of new data engineering solutions to enable adoption and growth
Integrate disparate data sources into powerful datasets for advanced analytics
Recommend tools and capabilities based on understanding the current environment and knowledge of various on-premises, cloud based, and hybrid capabilities/technologies
Monitor existing metrics, analyze data, and lead partnership with other Data and Analytics personnel to identify and implement system and process improvements
Develop processes to convert aggregated data from teams, collection tools, and dashboards
Configure and manage data analytic frameworks and pipelines using databases and tools
Develop Python packages to improve application capabilities
Apply distributed systems concepts and principles such as consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms
Administrate cloud computing and CI/CD pipelines to include Amazon Web Service (AWS)
Investigate legacy code to determine areas of improvement and automation
Required Qualifications:
Excellent verbal and written communications
Bachelor’s Degree in a STEM filed or Master’s Degree in Operations Research, Industrial Engineering, Applied Mathematics, Statistics, Physics, Computer Science, or related fields
5+ years of experience with Python, SQL, Unix(Linux), and handling semi-structured data (JSON)
3+ years of experience with Elasticsearch, Logstash, and Kibana (ELK stack)
3+ years of experience with Amazon Web Services (AWS) or other cloud provider
Proficient in Docker
Proficient in Agile Development
Proficient in Git Operations
Experience understanding requirements, analyzing data, discovering opportunities, addressing gaps and communicating them to multiple individuals and stakeholders
Demonstrated expertise in technical data engineering on integrating complex applications, systems, software, and project activities and integrating them into cloud-based resources
General knowledge in machine learning for building efficient and accurate data pipelines that occur for downstream users, such as for data scientists to create the models and analytics that produce insight
Preferred Qualifications:
Organizational skills and a love of documentation
Experienced in Airflow
Experience with demonstrated strength in data lake/warehouse technical architecture, infrastructure components, and ETL/ELT pipelines
Experience with geo-spatial data
Experience with deployments via Kubernetes
Experience with configuring and aggregating logs for data analysis using Splunk or ELK solutions
Experience with developing and managing machine images or templates to automate cloud deployments
About Us:
IntelliBridge delivers IT strategy, cloud, cybersecurity, application, data and analytics, enterprise IT, intelligence analysis, and mission operation support services to accelerate technical performance and efficiency for Defense, Civilian, and National Security & Federal Law Enforcement clients.","$92,898 /yr (est.)",501 to 1000 Employees,Company - Private,Government & Public Administration,National Agencies,,Unknown / Non-Applicable
"Oddball
4.6",4.6,Remote,Data Engineer,"Oddball believes that the best products are built when teams understand and value the things they are working on. We value learning and growth and the ability to make a big impact at a small company. We believe that we can make big changes happen and improve the daily lives of millions of people by bringing quality software to the federal space.
As a Database Engineer you will be responsible for helping to develop production-quality analytics tools, utilizing existing data sources to empower stakeholders to inform research and development. Act as a liaison between engineers and analysts. Implement and enforce standards and procedures to ensure data is managed consistently and properly integrated.
Experience Requirements
5+ years of proven data and performance engineering experience.
A proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs.
Experience with system analysis, data analysis or programming, using a variety of computer languages and procedures.
Experience working in Agile environments.
Experience in developing, analyzing, and presenting data models.
Knowledge of data management, data standardization, and data governance.
Ability to analyze source data for potential data quality issues.
Expert in SQL and/or SQL based languages.
Create supporting documentation, such as metadata and diagrams of entity relationships, business processes, and process flow.
Create plans, test files, and scripts for data warehouse testing, ranging from unit to integration testing.
This is a Remote position. This is a salaried role. Oddball offers both a tech, and a continuing education stipend.
Must be located within the United States.
Clearances
Ability to obtain low-level federal clearance is required.
Oddball is an Equal Opportunity Employer and does not discriminate against applicants based on race, religion, color, disability, medical condition, legally protected genetic information, national origin, gender, sexual orientation, marital status, gender identity or expression, sex (including pregnancy, childbirth or related medical conditions), age, veteran status or other legally protected characteristics. Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Work Location: Remote",,1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"MARVEL TECHNOLOGIES INC
3.7",3.7,Remote,Data Engineer,"Primary Skills
SCALA, SPARK, SQL, HADOOP, AWS, HIVE, Spark SQL, HIVE QL, CICD (Continuous Integration/Continuous Delivery), VCS (GIT HUB)
Coding in Scala
Designing in of HADOOP ecosystem
Hands-on experience on AWS tools like EMR, EC2
Hands-on experience of SQL in Big Data: SQL, Spark SQL, Hive QL
Proficient in working with large data sets and pipelines
Proficient with workflow scheduling / orchestration tools
Well versed with CICD process and VCS
Databricks is a big PLUS
Job Types: Full-time, Contract
Pay: $50.00 - $58.00 per hour
Schedule:
8 hour shift
Monday to Friday
Experience:
Spark: 4 years (Required)
Scala: 4 years (Required)
Hadoop: 3 years (Required)
Aws: 3 years (Required)
Hive: 3 years (Required)
CI/CD, VCS: 3 years (Required)
Databricks: 1 year (Required)
Work Location: Remote",$54.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,,$5 to $25 million (USD)
Grid,,"Oakland, CA",Data Engineer,"Today’s financial system is built to favor those with money. Grid’s mission is to level that playing field by building financial products that help users better manage their financial future. The Grid app lets users access cash, build credit, spend money, file their taxes and lots, lots more.

Grid is a fast growing team that’s deeply passionate about making a difference in the lives of millions. We’re solving huge problems and believe that a merit driven culture allows every team member to play a big role. Join our growing team in our Bay Area headquarters or additional offices across the US.

Our Data Engineers are responsible for converting the data surrounding a customer's financial life into tools that make our customers smarter and better equipped financially for the future. We value making intelligent decisions backed by good data and tools. We're looking for people who share our values, particularly if you have experience analyzing, processing, and learning from large data sets.

We’re looking for a seasoned data engineer who can help us lay the foundation of an exceptional data engineering practice. The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions. You will be writing code, setting style guides and collaborating cross-functionally with product, engineering and leadership.
Problems we work on:
Analytics: Collect all the data for a user into tools that help our customers
Machine Learning: Using a variety of techniques to reach better insights
Data Processing: Managing data & statistics using scalable and efficient technologies
Visualization: Envision our data as beautiful graphs and tools that allow customers to explore their data & ask their own questions
Risk: Analyze data for anomalous patterns and build tools that allow us to find bad actors quickly
We Practice:
Open collaboration
Code reviews
Testing
Agile development
We Use:
Go
Python
MySQL
Google Cloud Platform
Kubernetes
Kubeflow
Docker
Google Pubsub
BigQuery
Firebase
We’re looking for Engineers to:
Design and implement platform services, frameworks and ecosystems
Build a scalable, reliable, operable and performant big data workflow platform for data scientists/engineers, AI/ML engineers, and product/operation team members
Drive efficiency and reliability improvements through design and automation: performance, scaling, observability, and monitoring
Requirements:
Strong programming skills with Python
Strong programming skills with a typed programming language, such as Java, Scala, Go, etc.
Disciplined approach to development, testing, and quality assurance
Excellent communication skills, capable of explaining highly technical problems in English
Understand data processing and ETL, hands on experience building pipelines and using frameworks such as Hive hdfs, Presto, Spark, etc.
Really strong candidates may have:
Really strong candidates may have:
Actively contribute to open source software
Worked with a strong, lean-based development environment
Previous work experience in a start-up environment
Ability to recognize the right tool for the right situation/problem.
Will have strong programming skill in Go","$112,815 /yr (est.)",,,,,,
"Agiles Enterprise
3.4",3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road","$136,537 /yr (est.)",1 to 50 Employees,Company - Private,,,,Less than $1 million (USD)
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"Iyka Enterprises, Inc.
3.8",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115","$85,831 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,,Less than $1 million (USD)
"Second Wave Delivery Systems, LLC",,Remote,Data Engineer,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker",,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020.0,Unknown / Non-Applicable
"FlexIT Inc
4.0",4.0,"Beaverton, OR",Data Engineer,"FlexIT client is looking for a Data Engineer 12 months contract in Beaverton, Oregon.
Looking for local candidates to work on site.
Top skills: Python, SQL , AWS, Spark","$106,334 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"FlexIT Inc
4.0",4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.","$100,714 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
Metrohm Spectro,,"Plainsboro, NJ",Data Engineer,"Metrohm Spectro is an advanced mobile spectroscopic instrumentation leader, developing, manufacturing, and servicing state-of-the-art analytical devices, including portable and handheld Raman analyzers.We provide solutions for the pharmaceutical, biomedical, safety and security, chemical, and academic research industries. We are constantly growing with new products and new opportunities, and are always looking for talented, dedicated employees to join us and grow together as a team.
With the fast growth of our business, we have an immediate vacancy for a full-time Data Engineer for our Plainsboro, NJ location.
Job Description
In this role, you will take responsibility for developing and maintaining databases within software products. You will be required to have hands-on problem solving, from the upkeep and generation of database, to data validation as well as the capability of data processing and analysis, and will be able to perform data processing algorithm validation with the knowledge of data science. To excel in this role, you need to be very organized with a fine eye for detail, and openness to learn new skills to meet growing business needs.
Education
· Bachelor’s of Science degree from an accredited university or college in chemistry, physics, mathematics or computer science.
Experience:
· High-level proficiency in Microsoft Excel or other automated data management tool.
· Experienced in database programming and familiar with all popular database types. Good understanding of MySQL is a plus;
· Knowledge in MATLAB, R, Python or SAS tools for data processing and analysis;
· Knowledge in AI/machine learning and data mining basics;
· Knowledge in C/C++ programming for data processing algorithm;
· High-level proficiency in Microsoft Excel or other automated data management tool;
· Knowledge in Network/Cloud infrastructure will be a plus.
ROLE AND JOB RESPONSIBILITIES
· Develop and maintain database for cross-platform software implementation on all BWTEK spectroscopic products.
· Assist in data process and analysis algorithm design and validation.
· Collaboration with entire software team for product enhancement and new product.
Job Type: Full-time
Application Question(s):
What are your salary expectations?
Work Location: Plainsboro, NJ - on site
Can you commute to this location?
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
5x8
8 hour shift
Monday to Friday
Ability to commute/relocate:
Plainsboro, NJ 08536: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Will you need sponsorship to work in US?
Work Location: In person",,,,,,,
"Manufacturers Bank
3.2",3.2,"Los Angeles, CA",Data Engineer,"GENERAL SUMMARY:
Manufacturers bank is looking for a Data Engineer with strong software engineering backgrounds, a ""platform engineering mindset"" and deep passion for data. This Engineer will help to shape the technical direction of the data domain within the organization. The Engineer will also partner closely with internal business units and external vendors to develop a high performance, reliable and scalable data environment for the bank.

PRINCIPAL DUTIES & RESPONSIBILITIES:
Develop and automate high-performance data processing systems and visualization to ensure reliability and meet critical business requirements.
Design, develop, implement, document and support Data & Analytics systems which adhere to the industry best practices.
Lead data engineering projects, and overall strategy for data governance, security, privacy, quality, and retention.
Triage and resolve issues reported by Risk, Operations, and other business stakeholders.
Mentor junior engineers and continue promoting data engineering and analytics tooling & standards.","$115,908 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,,$25 to $100 million (USD)
Edrstaffing,,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker","$120,000 /yr (est.)",,,,,,
"Govini
3.7",3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.","$88,151 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011.0,Unknown / Non-Applicable
"Airbus Americas
4.0",4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.","$92,046 /yr (est.)",10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970.0,$10+ billion (USD)
"EMONICS LLC
3.8",3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location","$150,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016.0,$100 to $500 million (USD)
"Arthur Grand Technologies Inc
4.8",4.8,"Atlanta, GA",AWS Data Engineer,"Role: AWS Data Engineer
Location: Atlanta, GA (Meet customer Monthly once post client selection & onboarding)
JD for AWS Data Engineer
Experience with the core AWS services, plus the specifics mentioned in this job description.
Experience with Docker . Having used ECS , SNS/SQS, Athena, Lambda,S3, RDS and layers in the past is a plus.
Proficiency in at least in Python, Java
Strong notions of security best practices (e.g. using IAM Roles, KMS, etc.).
Experience with monitoring solutions such as CloudWatch, Cloud Trail.
Previous exposure to large-scale systems design.
Knowledge of writing infrastructure as code (IaC) using CloudFormation or Terraform.
Experience with building or maintaining cloud-native applications.
Past experience with the serverless approaches using AWS Lambda is a plus. For example, the Serverless Application Model (AWS SAM).
Job Types: Full-time, Contract
Schedule:
10 hour shift
8 hour shift
Work Location: Remote","$94,994 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2012.0,Unknown / Non-Applicable
"Nursa
4.3",4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!","$89,485 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018.0,Unknown / Non-Applicable
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,,Less than $1 million (USD)
"IBR (Imagine Believe Realize)
4.5",4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007.0,Unknown / Non-Applicable
"Globaleur
4.5",4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information
Deploy sophisticated analytics programs, machine learning, and statistical methods
Ensure compliance with data governance and security policies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java
Have working experiences in e-commence, travel, marketing domain is a plus
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.","$119,136 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017.0,Unknown / Non-Applicable
Lendem Solutions,,"Plano, TX",Data Engineer,"LENDEM Solutions is looking at add a Data Engineer to our business!
CORE COMPETENCIES
Ability to thrive in a dynamic and fast-paced environment, drive change, and collaborate effectively with a variety of individuals and teams
Strong analytical and problem-solving skills
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
PRINCIPAL DUTIES
Shredding and parsing data to extract meaningful information
Preparing historical and live data for data studies to identify trends and patterns
Performing adhoc analysis to answer specific business questions and provide insights
Working with relational databases to model and query complex data relationships
Understanding and working with MySQL data in several different data environments
Mining consumer loan data utilizing tools such as SQL, Python, R, or other comparable data mining tools.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Engineering, or a related field Master’s degree preferred
3+ years of experience as a data engineer or similar role
REQUIRED SKILLS, ABILITIES, SOFT SKILL FACTORS
· Strong experience in ETL processes, data modeling, and data warehousing
Experience working with graph databases, such as Neo4j or Apache Cassandra
Expertise in programming languages such as SQL, Python
Familiarity with big data technologies, such as Hadoop, Spark, or Kafka
Ability to analyze and manipulate large and complex data sets
Strong problem-solving skills and the ability to work independently and as part of a team
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
If you are a data engineer with a passion for problem-solving and a strong background in ETL processes, data modeling, and graph databases, we encourage you to apply for this exciting opportunity. We offer a competitive salary, comprehensive benefits, and a dynamic work environment where you can continue to grow and develop your skills.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Plano, TX 75024: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 5 years (Preferred)
SQL: 5 years (Preferred)
Work Location: Hybrid remote in Plano, TX 75024",,,,,,,
Okaya Corp,,"Mahwah, NJ",Azure Data Engineer,"Job Title: Azure Data Engineer
Location: Mahwah, NJ
Duration: Full Time
Skills Required:
Azure data factory, data bricks, data lake, automation, and performance optimization of ETL
Strong Hands-on experience in ADF, data bricks, data lake, power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end-to-end CI/CD implementation or Devops process in Data & Analytics context.
Design, plan, develop and update technical docs & BI solutions.
Understand the requirements and define the data load strategy for data refresh.
Create, debug, troubleshoot and deploy solutions.
Work on ETL design
Designing and optimization of ETL Process using ADF
Implementing end-to-end automated ETL processes and monitoring of those processes using various options using Azure
Experience in Azure data factory, data bricks, data lake, automation, and performance optimization of ETL
Job Type: Full-time
Work Location: One location",,,,,,,
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,,Unknown / Non-Applicable
"PSRTEK
4.6",4.6,"Mount Laurel, NJ",Lead Data Engineer,"AR# 226054
Role: Lead Data Engineer /Databricks (On-site)
Location: Mt. Laurel, NJ
Full-time
Visa status: GC/USC
Must have skills:
Databricks, Python, RDBMS, PowerShell scripting, data warehouse
Detailed JD:
Experience in ETL/Pipeline Development using tools such as Azure Databricks/Apache Spark and Azure
Data Factory with development expertise on batch and real-time data integration
Experience in programming using Python
RDBMS knowledge and experience in writing the Store Procedures
Experience in writing bash and Power shell scripting.
Experience in data ingestion, preparation, integration, and operationalization techniques in optimally addressing the data requirements
Experience in Cloud data warehouse like Azure Synapse, Snowflake analytical warehouse
Experience with Orchestration tools, Azure DevOps, and GitHub
Experience in building end to end architecture for Data Lakes, Data Warehouses and Data Marts
Experience in relational data processing technology like MS SQL, Delta Lake, Spark SQL, SQL Server
Experience to own end-to-end development, including coding, testing, debugging and deployment
Extensive knowledge of ETL and Data Warehousing concepts, strategies, methodologies
Experience working with structured and unstructured data
Familiarity with Azure services like Azure functions, Azure Data Lake Store, Azure Cosmos
Ability to provide solutions that are forward-thinking in data and analytics
Job Type: Full-time
Salary: $120.00 - $130.00 per year
Schedule:
8 hour shift
Experience:
Data Warehouse: 10 years (Required)
Python: 10 years (Required)
PowerShell: 10 years (Required)
Data Bricks: 10 years (Required)
RDBMS: 10 years (Required)
Work Location: On the road
Speak with the employer
+91 609-917-9952","$121,211 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,,Unknown / Non-Applicable
"Pomeroy Technologies, LLC.
3.0",3.0,"Cleveland, OH",Data Engineer,"The Data Engineer is responsible for the development, integration, and implementation of ETL Feeds in support of various modern architecture projects. This role requires hands-on knowledge of modern Integrations Platform solutions, file, and database ETL, and XML transformation. The incumbent will act as a technical resource, participating in the successful design, development, unit testing, implementation, and support of new and existing ETL integrations.
Essential Functions
Must be able to acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design.
Working with manager, gathering business requirements for the development of new applications.
Perform programming assignments based on established standards, methods, and best practices.
Develop ETL solutions as assigned in supporting new or updated business solutions and requirements.
Help business users develop functional requirements for integration.
Provide support during user acceptance testing.
Prepare documentation for code changes.
Code ETL feeds, and conduct tests to verify correct program functionality, reliability, and data integrity.
Maintain and modify ETL feeds; make approved changes by amending application documentation, developing detailed programming logic, and coding changes.
Provide support and resolution for application problems and issues.
Provide user and system documentation for operational and technical support.
Assist in the discovery and investigation of production problems as required.
Support ongoing daily use of ETL environments and Integrations platforms.
Provide on-call support for critical application problems and issues.
Analyze program performance and take action to correct deficiencies based on consultation with clients and approval of supervisor.
required.Work outside the standard office 7.5 hour workday may be
Experience Requirements
2+ years of IT experience.
1+ years of experience building ETL feeds using any ETL tool (Talend, Informatica, etc.)
Knowledge of the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment, and post-production support.
Understanding of XML, JSON, XSD, WSDL, REST, and SOAP APIs.
Experience in developing SQL programs.
Job Type: Full-time
Pay: From $80,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Work Location: Hybrid remote in Cleveland, OH 44115","$80,000 /yr (est.)",1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982.0,Unknown / Non-Applicable
TY Software,,"Dallas, TX",Data Engineer,"Job Title Data Engineer,
Location: Dallas, TX
Type of work- Onsite , C2C
Job Description
Experience 3-5 years
At least 3+ years of enterprise experience in working with data bricks and highly proficient in SQL, Spark, Scala/Python.
Skilled in Big Data Technologies like Spark, Spark SQL, PySpark
Experience with one or more of the major cloud platforms & cloud services such as - Azure/AWS/GCP, Databricks
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Strong analytic skills related to working with unstructured datasets
Working knowledge of highly scalable ‘big data’ data stores
A successful history of manipulating, processing and extracting value from large, disconnected datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Experience developing enterprise software products
Experience with at least one of these object-oriented/object function scripting languages: PySpark/Python, Scala, Java
Build monitoring and automated testing to ensure data consistency and availability
Experience supporting and working with cross-functional teams in a dynamic environment
Experience working in an AGILE environment
Job Types: Full-time, Contract
Salary: $42.15 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75201: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",$51.08 /hr (est.),,,,,,
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012.0,Unknown / Non-Applicable
Sky Consulting Inc,,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.","$103,312 /yr (est.)",1 to 50 Employees,Company - Public,,,,Unknown / Non-Applicable
"Orange County's Credit Union
3.9",3.9,"Santa Ana, CA",Data Engineer,"Great Opportunity At Orange County's Credit Union
Must reside in the state of CA, AZ, NV or TX.
Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.
Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.
Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.
Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!
More about Orange County's Credit Union:
Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.
As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.
Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!
Overview:
Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.
Essential Functions:
Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).
Technical Must Haves for this Role:
5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.","$90,612 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1938.0,$25 to $100 million (USD)
"BCVS Group INC
5.0",5.0,"Oakland, CA",Data engineer lead,"Role: Data Engineer Lead
Location: Oakland, CA
Description
Qualitest is looking for a Data Architect to join our team.
Must Have-
Seasoned data professional with experience implementing solutions across Data Quality, Data Assurance,
Data Governance and Test Data Management
Ability to assess current state, recommend solutions and build a rollout strategy
Analyze existing data landscape, business processes and create knowledge articles (architectures, data flows, tools)Design, Build and Deliver quality gates across data lifecycle for automated and continuous data quality assurance.
Integrate data testing solutions with Automation, CI/CD Pipelines for Continuous Testing
Working experience on various database technologies such as Oracle, SQL Server, MongoDB, Snowflake, Teradata
Good understanding about QA Test Management, Defect Management and Testing methodologies. Evaluate tools, build proof of concepts, and implement NextGen solutions
Work with customer executives and cross functional teams for seamless project implementation and delivery
Mentor onshore/offshore teams to deliver projects in time and assist in case of delays/clarifications
Experience working on data quality validation tools such as RightData, Icedq, Informatica, Tosca DI, Datagaps
Experience working on TDM tools such as Informatica TDM , IBM Optim, Delphix, CA TDM/Broadcom TDM
Proficient in AWS Cloud Services such as Glue, Lambda, RedShift, Dynamo DB and Databricks
Experience working on Data Warehouse, Data Lake, Big Data platforms
Requirements
Good to Have-
Knowledge about Enterprise Applications such as Salesforce, SAP, Veeva CRM
AI/ML knowledge on building Intelligent data validations and eliminate data anomalies
Job Types: Full-time, Contract
Salary: From $41.80 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Oakland, CA 94619: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 +12019071788",$41.80 /hr (est.),Unknown,Company - Public,,,,Unknown / Non-Applicable
"ASCENDING
4.2",4.2,"Rockville, MD",Data Engineer,"Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.
This role is only available for W2 or individual contracts. Please no C2C.
100% Remote Work.

Responsibilities:
Analyze system requirements and design responsive algorithms and solutions.
Use big data and cloud technologies to produce production quality code.
Engage in performance tuning and scalability engineering.
Work with team, peers and management to identify objectives and set priorities.
Perform related SDLC engineering activities like sprint planning and estimation.
Work effectively in small agile teams.
Provide creative solutions to problems.
Identify opportunities for improvement and execute.

Requirements:
Minimum 5 years of proven professional experience working in the IT industry.
Degree in Computer Science or related domains.
Experience with cloud based Big Data technologies.
Experience with big data technologies like Hadoop, Spark and Hive.
AWS experience is a big plus.
Proficiency in Hive / Spark SQL / SQL. Experience with Spark.
Experience with one or more programming languages like Scala & Python & Java.
Ability to push the frontier of technology and independently pursue better alternatives.
Kubernetes or AWS EKS experience will be a plus.

Thanks for applying!
U3GJMKlbkr","$96,611 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"FalconSmartIT
4.5",4.5,"Dover, DE",Big Data Engineer,"Job Title: Big Data Engineer
Location: Toronto, Ontario, Canada
Job Type: Full time


Job Description:


Qualifications :
8+ years of software development experience in Big Data technologies (Spark/Hive/Hadoop)
Experience in working on Hadoop Distribution, good understanding of core concepts and best practices
Good experience in building/tuning Spark pipelines in Scala/Python
Good experience in writing complex Hive queries to derive business critical insights
Good Programming experience with Java/Python/Scala
Experience with AWS Cloud, exposure to Lambda/EMR/Kinesis will be good to have
Experience in NoSQL Technologies - MongoDB, Dynamo DB
Roles and Responsibilities :
Design and implement solutions for problems arising out of large-scale data processing
Attend/drive various architectural, design and status calls with multiple stakeholders
Ensure end-to-end ownership of all tasks being aligned
Design, build & maintain efficient, reusable & reliable code
Test implementation, troubleshoot & correct problems
Capable of working as an individual contributor and within team too
Ensure high quality software development with complete documentation and traceability
Fulfil organizational responsibilities (sharing knowledge & experience with other teams/ groups)
Conduct technical training(s)/session(s), write whitepapers/case studies/blogs etc.",,1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
Monogram Health Renal Services,,"Brentwood, TN",Azure Data Engineer,"Position: Azure Data Engineer

The Azure Data Engineer will work with our business operations and technology teams to build out an enterprise-wide Data Warehouse and related systems in the cloud. Will collaborate with our internal teams and 3rd party vendor in building our future state, business process, system interfaces, governance and technology platforms needed to enable a single source of truth for product and customer data, drive a more sophisticated data environment and be positioned to scale for future business growth.

Roles and Responsibilities
In collaboration with our 3rd party vendor, implement strategic roadmap and operational framework for Enterprise Data Management, in partnership with key stakeholders
Collaborate on the strategy on new cloud data stores and migration of existing data in Azure
Work collaboratively with teams to design, document, and ensure continuous improvement of SDLC best practices for data in the cloud
Be an advocate for quality and security through data quality and accuracy validation and auditing of security best practices
Focus on understanding business problems and determine what aspects require optimization; articulate those aspects in a clear and concise manner
Work collaboratively with cross-functional teams to support Data Models in alignment with business needs
Partner with IT, on design and oversight of APIs and other data sharing capabilities in collaboration with short / long term business needs
Partner with IT to integrate new data management technologies and software engineering tools (automation projects, process improvements, etc.) into existing systems to enable single source of truth for product and customer data
Adhere to identified key performance indicators for quality and compliance metrics to ensure data related policies and standards are followed
Acts as a SME to explain all facets of EDM data exchange and processing, business user involvement, and overall architecture
Support Monogram as necessary with any tasks required to deliver excellent personalized kidney care and perform all other duties as assigned
Uphold the mission and values of Monogram Health in all aspects of your role and activities
Position Requirements
Nashville, TN
BS or equivalent experience in crucial duties and responsibilities
3-5 years’ experience in cloud computing
Solid understanding of enterprise data management in Healthcare Data
Solid understanding of data, databases, data tools (e.g. SQL) and data processing experience
Solid understanding of standard ETL tools and techniques, sourcing, maintaining, and updating data, data warehousing, data cleansing, and other analytical techniques required for data usage.
Experience with database systems, Azure cloud storage, and significant exposure to or experience with modern data and BI platforms (such as Databricks, MSSQL Data Warehouse (Synapse), Snowflake, and Tableau).
Previous experience with data migration to cloud-based environment
Interest and passion in designing and implementing data management, querying, and storage, with a particular focus on purpose-built data stores.
Functional knowledge of data visualization tools such as PowerBI and Tableau.
Strong intuition for business and analytical skills
Team player with excellent communication skills

Benefits

Opportunity to work in a dynamic, fast-paced and innovative value-based provider organization that is transforming the delivery of kidney care
Competitive salary and opportunity to participate in the company’s bonus program
Comprehensive medical, dental, vision and life insurance
Flexible paid leave and vacation policy
401(k) plan with matching contributions

About Monogram Health
Monogram Health is a next-generation, value-based chronic condition risk provider serving patients living with chronic kidney and end-stage renal disease and their related metabolic disorders. Monogram seeks to fill systemic gaps and transform the way nephrology, primary care and chronic condition treatment are delivered. Monogram’s innovative, in-home approach utilizes a national nephrology practice powered by a suite of technology-enabled clinical services, including case and disease management, utilization management and review, and medication therapy management services that improve health outcomes while lowering medical costs across the healthcare continuum. By focusing on increasing access to evidence-based care pathways and addressing social determinants of health, Monogram has emerged as an industry leader in championing greater health equity and improving health outcomes for individuals with chronic kidney and end-stage renal disease.

At Monogram Health we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.
Experience
Required
SQL
healthcare data
cloud computing
Azure
Preferred
Tableau
Snowflake
Data Warehouse
Databricks
Power BI",,,,,,,
Savvy Technology Solutions,,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location","$107,339 /yr (est.)",,,,,,
"Infinity Quest
4.0",4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote","$105,128 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006.0,Unknown / Non-Applicable
"Business Integra Inc
3.5",3.5,"San Francisco, CA",Data Engineer,"Position can be 100% remote but preferred to have candidates who can periodically (2 x month) work at headquarters.
(Data Engineer)
Job Description:
Assigned Personnel to provide data analytic support to the Data Analytics/Data Integration Project for Judicial Branch Statistical Information System (JBSIS) data reporting.
This position will perform high level data engineering and data analytics on a variety of agency data sources, but primarily on the Judicial Branch Statistical Information System (JBSIS).
Partnering with IT staff, this position will reengineer JBSIS to create new technical documentation for JBSIS; create mappings for the Court Statistics Report and other JBSIS products, make policy recommendations, create and/or implement new governance standards, enhance data auditing and data quality controls, and create data visualizations.
These same tasks may be performed with additional agency datasets.
Specific Skills/Qualifications Required
Technical project management and documentation skills.
Ability to analyze issues from system documentation and recommend solutions.
Experience managing technical projects, including conflict resolution, issue escalations, status reporting and resource management.
Experience creating and executing data mappings and scripts to clean, compile and analyze data
Ability to assess and maintain data pipeline, data quality in the database, and address data reporting issues.
Experience developing and implementing testing protocols for data and system quality
Experience in R and Stata.
Experience with data visualization and software such as Tableau and Power BI.
Excellent oral, written, analytical and communication skills with the ability to lead a technical discussion to both technical and non-technical staff.
Excellent analytical, verbal and conflict resolution skills.
Additional Skills/Qualifications Desired:
General:
Understanding of courtroom operations and workflow.
Experience in government (State) setting
Excellent presentation skills for both technical and non-technical audiences, including creating and presenting executive summaries to management and technical committees.
Technical:
Exposure and experience with Cloud computing.
Conceptual understanding of Amazon Web Services, Microsoft Azure, Google Cloud, IBM and Oracle Cloud Platforms.
Prior experience using Snowflake
Experience using Python or other database query languages.
Job Types: Full-time, Contract
Pay: $112,604.02 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Vision insurance
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco, CA 94102: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data science: 9 years (Required)
Work Location: Hybrid remote in San Francisco, CA 94102","$131,302 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2001.0,$25 to $100 million (USD)
"Softcrylic
4.0",4.0,Remote,Senior Data Engineer,"Who We Are
For more than 20 years, we have been working with organizations large and small to help solve business challenges through technology. We bring a unique combination of engineering and strategy to Make Data Work for organizations.
Our clients range from the travel and leisure industry to publishing, retail and banking. The common thread between our clients is their commitment to making data work as seen through their investment in those efforts.
In our quest to solve data challenges for our clients, we work with large enterprise, cloud based and marketing technology suites. We have a deep understanding of these solutions so we can help our clients make the most of their investment in an efficient way to have a data driven business.
Why Work at Softcrylic?
Softcrylic provides an engaging, team-focused, and rewarding work environment where people are excited about the work they do and passionate about delivering creative solutions to our clients.
We are looking to add a Senior Data Engineer to our team! This is a 100% Remote role and preference will be given to candidates from Atlanta, NJ or Texas regions.
Job Description:
Softcrylic is looking for a Senior Data Engineerwith strong design, development, and team leadership skills. The person should be working with Clients / Customer and with our internal (onshore and offshore) members to design, develop and rollout data projects. The person to be hands on and have strong leadership/communication and interpersonal skills.
Requirement:
· 5 to 7 years of experience in working as a Data Engineer.
· Strong experience in Python.
· Experience in working on GCP.
· Good experience in Airflow.
· Should have good experience in ETL pipeline design and development.
· Very good experience in SQL
· Experience in working on Redshift.
· Excellent designing and documentation (diagrams) and presentation skills.
· Data Quality Concepts are must have.
· Must know design and development of any of industry leading graph databases.
· Good communication skills.
· Independent thinker, good team player with Data Engineering Design skills.
· Work with minimum guidelines.
Plus:
GCP - Big Query
Agile background
Microsoft Power BI
Graph Database
Job Types: Full-time, Contract
Pay: $130,000.00 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Compensation package:
Performance bonus
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
GCP: 3 years (Preferred)
Python: 4 years (Preferred)
Work Location: Remote
Speak with the employer
+91 609.241.9641","$135,000 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000.0,$5 to $25 million (USD)
"MOBE, LLC
3.7",3.7,"Minneapolis, MN",Data Engineer (ETL),"MOBE
MOBE guides people to better health and more happiness. Behind our innovative health solutions is uniquely human philosophy. We believe that person-to-person connections and understanding can make a difference in a world where self-care can be complicated, and health care is ever-evolving and complex.
MOBE works with health plans and large employers to identify individuals who are frequent users of health care but aren?t finding resolutions for their underlying health issues. We use a whole-person approach and guidance to impact health outcomes positively.
Supporting people is at the core of our business, employees included. MOBE is a high-growth organization with a culture built on trust and collaboration. Consistent across our teams and offerings is a belief in the power of people doing good together. We genuinely care about people and consider our workforce the most significant asset.
Your Role at MOBE
This is an exciting time at MOBE and we are growing fast. At MOBE, we have a lot of data: eligibility, medical and pharmacy claims, marketing campaign impressions, transcripts from participant interactions, etc.
This position is responsible for providing technical and project expertise to enable MOBE analytics and operations with structured and unstructured data. Responsibilities includes executing and/or leading user story development, data design and architecture, data pipeline development, testing and deployment in the Analytic Data Framework. This role will partner with internal and external business and technology teams to drive project deliverables and ensure high quality delivery of data architecture and integration.
Responsibilities
The Data Engineer ensures the following capabilities and functions:
Translate high level business processes into logical data processing steps
Design data structures and pipelines that are flexible and scalable for MOBE analytics and operational requirements
Support Analytic partners through collaborative and transparent development, information delivery, problem resolution, shared insights, and training
Data processing definition, execution, and documentation, in a time appropriate way, to meet business priorities and requirements
Data quality and maintenance consistent within the Analytic Data Framework
Lead small to moderate sized projects and initiatives, following through on execution of chosen strategies and demonstrating the ability to work through obstacles and changing priorities.
Demonstrate ability and willingness to play multiple roles for different projects (e.g. planning/architecture, project development, hands-on technical resource/support for others, analysis and resolution of data issues)
Identify and constructively communicate the need for improvements or enhancements in MOBE technology assets
All other duties as assigned to help fulfill our Mission and abide by MOBE?s Guiding Principles","$97,357 /yr (est.)",51 to 200 Employees,Company - Private,Personal Consumer Services,Beauty & Wellness,2014.0,Unknown / Non-Applicable
"Boston Globe Media Partners
4.1",4.1,"Boston, MA",Data Engineer,"Boston Globe Media is New England's largest newsgathering organization - and much more. We are committed to being an indispensable, trusted, reliable source of round-the-clock information. Through the powerful journalism from our newsroom, engaging content from our content marketing studio, or through targeted advertising solutions, brands and marketers rely on us to reach highly engaged, educated and influential audiences through a variety of media and experiences.
Responsibilities:
Collect, organize, and document often-used data resources (maps, APIs, etc).
Create scripts to scrape data from websites for stakeholders.
With guidance, start creation of a data style guide.
Technology:
Basic knowledge of HTML, CSS, and JavaScript.
Basic familiarity with PHP, Groovy, or another server side scripting language.
Basic familiarity of build tools such as Grunt, Gulp, or Webpack.
Basic familiarity with version control systems such as SVN or Git.
Qualifications:
Understands and follows the team’s agile process.
Adheres to defined coding standards.
Participates in code reviews.
A willingness to adapt and be audience focused, with a curious mindset and a commitment to creating an inclusive work environment
Vaccination Statement:
We require that all BGMP employees (including temporary employees, co-ops, interns, and independent contractors) be vaccinated from COVID-19, unless an exemption from this policy has been granted as an accommodation or otherwise. All BGMP employees, regardless of vaccination status or work location, must provide proof of vaccination status as instructed by the employee's designated Human Resources contact. Employees may request a reasonable accommodation or other exemption from this policy by contacting their designated Human Resources contact. Failure to comply with or enforce any part of this policy, or misrepresentation of compliance with this policy, may result in discipline, up to and including termination of employment, subject to reasonable accommodation and other requirements of applicable federal, state, and local law.
EEO Statement:
Boston Globe Media Partners is an equal employment opportunity employer, and does not discriminate on the basis of race, color, religion, gender, sexual orientation, gender identity or expression, age, disability, national origin, ancestry, genetic information, military or veteran status, pregnancy or pregnancy-related condition or any other protected characteristic. Boston Globe Media Partners is committed to diversity in its most inclusive sense.
wcZyZ7QvrB","$110,394 /yr (est.)",1001 to 5000 Employees,Company - Private,Media & Communication,Publishing,,$100 to $500 million (USD)
"NIVID Technologies
3.7",3.7,Remote,Sr. Data Engineer,"We hope this Job meets your skills and expectations. If you are available and interested, please contact me at your earliest convenience. You will be working with a highly skilled team of IT Professionals in a high pace corporate environment. This opportunity will move quickly and candidates will be interviewed in the order they apply. Job Description/ Required Skills: (i) Strong hands-on programming experience in Python (ii) Hands-on experience of API development (from application / software engineering perspective) (iii) AWS Lambda and data streaming ingestion (Kinesis) (iv) AWS tech stack from data engineering stand-point
Job Types: Full-time, Contract, Permanent
Salary: $39.76 - $86.23 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$63.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2012.0,$5 to $10 billion (USD)
"Plaxonic Technologies
4.6",4.6,"New York, NY",GCP Data Engineer,"Bachelor’s Degree in Computer Science or a related discipline
5+ years of applicable engineering experience
Strong proficiency in Python with an emphasis in building data pipelines
Ability to write complex SQL to perform common types of analysis and aggregations
Experience with Apache Airflow or Google Composer
Detail-oriented and document all the work
Ability to work with others from diverse skill-sets and backgrounds
GCP solution architect - certified
Experience in GCP, Big Query
Working experience in Databricks, Spark is expected
Job Types: Full-time, Contract
Benefits:
401(k)
Health insurance
Paid time off
Schedule:
8 hour shift
Work Location: One location
Speak with the employer
+91 (727) 216-7989","$117,952 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013.0,$1 to $5 million (USD)
EZOPs Inc,,"New York, NY",Python Data Engineer,"Responsibility:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies.
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems.
Utilize programming languages like Python, ReactJs, JavaScript and Open Source RDBMS and Cloud based data warehousing services such as Snowflake.
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community.
Collaborate with product managers and deliver robust cloud-based solutions.
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply).
At least 1 year of experience in data technologies.
Hands on Experience in application development with Python, Pandas, NumPy, SQL, Docker.
Preferred Qualifications:
2+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud).
1+ year experience working on real-time data and streaming applications like Kafka is big plus.
1+ years of data warehousing experience (Redshift or Snowflake)
1+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
Job Type: Full-time
Benefits:
401(k)
Health insurance
Paid time off
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
New York, NY: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location
Speak with the employer
+91 9599382735",,,,,,,
"PepsiCo
4.0",4.0,"Plano, TX",Azure Data Engineer,"As a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems.
Responsibilities:
Active contributor to code development in projects and services.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.
Responsible for implementing best practices around systems integration, security, performance and data management.
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.
Develop and optimize procedures to “productionalize” data science models.
Define and manage SLA’s for data products and processes running in production.
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale.
Research in state-of-the-art methodologies.
Create documentation for learnings and knowledge transfer.
Create and audit reusable packages or libraries.
Requirements:
2+ years of overall technology experience that includes at least 2+ years of hands-on software development, data engineering, and systems architecture.
2+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
1+ years in cloud data engineering experience in Azure Certification is a plus.
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.
Experience with version control systems like Github and deployment & CI tools.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus
Understanding of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI)
Covid-19 vaccination may be a condition of employment dependent on role and location. For specific information, please discuss role requirements with the recruiter
Education
BA/BS in Computer Science, Math, Physics, or other technical fields
Skills, Abilities, Knowledge
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.
Proven track record of leading, mentoring data teams.
Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.
Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.
Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.
Foster a team culture of accountability, communication, and self-management.
Proactively drives impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals
Ability to lead others without direct authority in a matrixed environment.
Job Type: Full-time
Pay: $85,000.00 - $90,000.00 per year
Schedule:
Monday to Friday
Work Location: Hybrid remote in Plano, TX 75024","$87,500 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Food & Beverage Manufacturing,1965.0,$10+ billion (USD)
"Ascendion
4.5",4.5,Remote,Senior Data Engineer,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote",$65.00 /hr (est.),1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022.0,Unknown / Non-Applicable
"Arthur Grand Technologies Inc
4.8",4.8,"Jersey City, NJ",Azure Tech Lead/ Sr Data Engineer,"Role: Azure Tech Lead/ Sr Data Engineer – Onsite role – Preferred locals
Location: Jersey City, New Jersey / Fort Mill, South Carolina.
Full-time
Mandatory Skills: MS Azure using Azure Data Factory, MS Synapse, Scala, Spark, Data Warehousing
Skills:
Over all 12 to 15 years of experience with Data Management, Data Warehousing and Analytics.
At least 4 to 5 years of experience in Architecting and Implementing Data Solutions.
At least 3 years of experience in implementing the data solutions on MS Azure using Azure Data Factory, MS Synapse.
One to two years of experience in Azure Synapse Analytics is plus.
Installing and configuring ADF integration runtimes and linked services.
At least one hands on experience with Big data platform tool selection POC.
Two years of experience in data migrations to Azure by using data box or Data migration Services.
Apache Spark experience using Scala or PySpark or pre-packaged tools like Databrick is must.
Extensive hands-on experience in data warehousing design, tuning and ETL/ELT process development by using cloud native technologies.
At least one year experience with unified data governance solution using MS Purview.
Developing the CICD pipeline for Azure Infrastructure, version control strategy and Integrate source control ( Azure repos)
In-depth understanding of various storage services offered by Azure.
Experience with implementation of data security, encryption, PII/PSI legislation, identity and access management across sources and environments.
Experience with data process Orchestration, end-to-end design and build process of Near-Real Time and Batch Data Pipelines.
Certification in Azure data engineering and solution architecture Azure is must.
Strong client-facing communication and facilitation skills.
Job Type: Full-time
Salary: $81,075.29 - $186,473.81 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Experience:
Azure: 4 years (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road","$133,775 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2012.0,Unknown / Non-Applicable
"ConnectiveRx
3.0",3.0,"Hanover, NJ",Sr. Data Engineer,"ConnectiveRx is a leading, technology-enabled healthcare services company. We work strategically with hundreds of biopharmaceutical manufacturers to help commercialize and maximize the benefits of specialty and branded medications. Our mission is to simplify how patients get on and stay on therapy. We fulfill our mission by providing our customers with innovative services such as patient and provider messaging, the design and operation of copay, vouchers and patient affordability programs, and hub services, all of which accelerate speed-to-therapy and help improve outcomes for manufacturers, healthcare providers and patients.

ConnectiveRx was formed in 2015 by bringing together the industry-leading business of PSKW, PDR/LDM, Careform (2017) and The Macaluso Group (2018) to advance our technology-driven expertise in providing state-of-the-art commercialization solutions. To learn more about our company, visit ConnectiveRx.com

Job Description

What you will do:
Looking for a seasoned Senior Data Engineer to help us continue to build out our new Enterprise Data Platform. This person must have a strong understanding and demonstrated experience with data streaming architectures that leverage microservice & message-oriented integration patterns and practices within AWS cloud native technologies. This person will help to scale our data ingestion pipelines which are at the core of our Enterprise Data Platform which supports our client reporting as well as our internal analytics & operational teams.

The successful candidate will:
Work with senior leadership, architects, engineers, data analysts, product managers and cloud infrastructure teams to deliver a new features and capabilities.
Write clean, robust, and well-thought-out code with an emphasis on quality, performance, scalability, and maintainability.
Demonstrate strong end to end ownership & craftsmanship - analysis, design, code, test, debug, and deploy
Your ability to traverse the full stack within AWS server-less technologies will be an asset to us as we evaluate the tradeoffs inherent in software engineering. You have the product driven development mindset and can work closely with BA’s and Product teams to breakdown requirements and translate business workflows into scalable technical solutions.

What we’d like from you:
Strong Python & strong SQL
Extensive relational DB experience (Redshift, SQL Server, PostgresSQL) with exposure to document DBs such as DynamoDB. ElasticSearch.
Experience with designing solutions that run in AWS cloud technologies (Lambda, ECS, DynamoDB etc), docker containers
Message oriented architectures, patterns and tools, CQRS, event streaming, Kafka, SQS
Change data capture concepts, Database Triggers, AWS DMS
Data lake concepts, data catalogs, meta data etc
CICD Pipelines
Event store processing, data validation, operational logging via AWS Cloud Watch
Why work with us?
Excellent company culture, fun events, and volunteer opportunities
Competitive benefits (medical, dental, vision & more)
401k package with dollar-for-dollar match-up
Generous PTO and paid holidays days offered
Opportunities to grow professionally and personally
Team-oriented atmosphere
#LI-BJ1

Equal Opportunity Employer: This employer (hereafter the Company) is an equal opportunity employer and does not discriminate in recruitment, hiring, training, promotion, or other employment policies on the basis of age, race, sex, color, religion, national origin, disability, veteran status, genetic information, or any other basis that is prohibited by federal, state, or local law. No question in this application is intended to secure information to be used for such discrimination. In addition, the Company makes reasonable accommodation to the needs of disabled applicants and employees, so long as this does not create an undue hardship on the Company or threaten the health or safety of others at work. This application will be given every consideration, but its receipt does not imply that the applicant will be employed.","$115,021 /yr (est.)",1001 to 5000 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2015.0,Unknown / Non-Applicable
"BNY Mellon
3.7",3.7,"Pittsburgh, PA",Big Data Sytems Engineer(Hadoop),"Overview
Big Data Engineer
Bring your ideas. Make history.
BNY Mellon offers an exciting array of future-forward careers at the intersection of business, finance, and technology. We are one of the world's top asset management and banking firms that manages trillions of dollars in assets, custody and/or administration. Known as the “bank of banks” - 97% of the world’s top banks work with us as we lead and serve our customers into the new era of digital.
With over 238 years of rich history and industry firsts, BNY Mellon has been built upon our proven ability to evolve, lead, and drive new ideas at every turn. Today, we’re approximately 50,000 employees across 35 countries with a culture that empowers you to grow, take risks, experiment and be yourself. This is what #LifeAtBNYMellon is all about.
We’re seeking a future team member in the role of Big Data Engineer to join our TSG Data Management team. This role is located in Pittsburgh, PA and Nashville, TN .
In this role, you’ll make an impact in the following ways:
Work as a senior system engineer for the Hadoop enterprise data platform platforms and create highly complex big data/Hadoop platforms. You will have expert knowledge in distributed datalake systems.
You’ll configure and tune service components to optimize platform performance. You will design and implement backup and restore strategies and disaster recovery solutions and will support and troubleshoot performance related issues.
You’ll consult with storage and host engineering services to create optimized big data ecosystems and assist in cross-team collaboration and design discussions.
Provide sandbox functionality to users of the datasets to assist application analytics and storage infrastructure system management including capacity planning, performance monitoring and tuning, security management etc.
Responsible for infrastructure Support and projects including system design, planning, installations, configurations, upgrades and planning and Manage Tier-3 support following ITIL practice and incident management
Manage system configurations and solutions, and monitor the full lifecycles of storage area network (SAN) administration, architecture, and support and Data storage tasks such as management, provisioning, troubleshooting and debugging
Lead technical assessments of hardware, software, tools, applications, firmware, and operating systems to support business operations. You’ll develop trusted corporations with application tenants. You’ll advise applications with sounds data layer resiliencies and provide input into the selection of tools and any necessary migration into the company's environment.
To be successful in this role, we’re seeking the following:
Bachelor's degree in computer science or a related discipline, or equivalent work experience required.
Advanced degree preferred10-12 years of related experience required; experience in the securities or financial services industry is a plus.
5-10 years of experience in Hadoop technologies, data lake design, experience in the securities or financial services industry is a plus.
Excellent knowledge with Hadoop components for big data platforms related to data ingestion, storage, transformations and analytics. Excellent DevOps skillsets and SDLC practices. Excellent knowledge of automation tools such as Ansible, Python programming skills, knowledge of Docker containers
8+ years of experience on BigData Platform on Hadoop, Spark, Hive, HBase and 3+ years’ experience on Python and DevOps tools such as Ansible
Strong Agile/Scrum development experience and Working knowledge with PostgreSQL or Mongo DB databases a plus
At BNY Mellon, our inclusive culture speaks for itself. Here’s a few of our awards:
Fortune World’s Most Admired Companies & Top 20 for Diversity and Inclusion
Bloomberg’s Gender Equality Index (GEI)
Best Places to Work for Disability Inclusion , Disability: IN – 100% score
100 Best Workplaces for Innovators, Fast Company
Human Rights Campaign Foundation, 100% score Corporate Equality Index
CDP’s Climate Change ‘A List’
Our Benefits:
BNY Mellon offers highly competitive compensation, benefits, and wellbeing programs rooted in a strong culture of excellence and our pay-for-performance philosophy. We provide access to flexible global resources and tools for your life’s journey. Focus on your health, foster your personal resilience, and reach your financial goals as a valued member of our team, along with generous paid leaves that can support you and your family through moments that matter.
BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer - Underrepresented racial and ethnic groups/Females/Individuals with Disabilities/Protected Veterans.
This person is a subject matter expert in several of the tools/technologies used in the space.Leads the development infrastructure engineering growth strategies and initiatives. Leads initiatives to analyze complex infrastructure problems to be solved with advanced design.Leads the evaluation of the effectiveness of the organization's existing infrastructure technology and tools. Analyzes trends to develop strategy for the implementation of upgrades that will enhance the reliability, Resiliency and efficiency of the IT infrastructure.Provides leadership to execute project plans and performance requirements for all stages/phases through the management of human capital resources.This person is a subject matter expert in at least one of the tools/techologies used in the space.Participates in or leads initiatives to analyze infrastructure problems to be solved with advanced design. Utilizes standard procedures and policies when selecting methods, techniques, and evaluation criteria for obtaining results.Participates in or leads initiatives to analyze infrastructure problems to be solved with advanced design. Utilizes standard procedures and policies when selecting methods, techniques, and evaluation criteria for obtaining results.Manages the processes for ensuring that all systems/applications/software/hardware are compliant with Corporate policy/procedures.Monitors project plans and budgets.Works closely with external vendors, internal partners and busienss teams to provide infrastructure/tool needs.Works with Application Development and Quality Assurance, Testing and Business teams to understand infrastructure needs during the development, testing and production BAU processes. Ensures these needs are taken into account when developing infrastructure.Acts as escalation point for major incidents.Leads strategy to increase automation across the organization.Contributes to the achievement of multiple teams' objectivesBachelor's degree in computer science or a related discipline, or equivalent work experience required; advanced degree preferred10-12 years of related experience required; experience in the securities or financial services industry is a plus. BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer. Minorities/Females/Individuals with Disabilities/Protected Veterans. Our ambition is to build the best global team – one that is representative and inclusive of the diverse talent, clients and communities we work with and serve – and to empower our team to do their best work. We support wellbeing and a balanced life, and offer a range of family-friendly, inclusive employment policies and employee forums.
Employer Description:
For over 230 years, the people of BNY Mellon have been at the forefront of finance, expanding the financial markets while supporting investors throughout the investment lifecycle. BNY Mellon can act as a single point of contact for clients looking to create, trade, hold, manage, service, distribute or restructure investments and safeguards nearly one-fifth of the world's financial assets. BNY Mellon remains one of the safest, most trusted and admired companies. Every day our employees make their mark by helping clients better manage and service their financial assets around the world. Whether providing financial services for institutions, corporations or individual investors, clients count on the people of BNY Mellon across time zones and in 35 countries and more than 100 markets. It's the collective ambition, innovative thinking and exceptionally focused client service paired with a commitment to doing what is right that continues to set us apart. Make your mark: bnymellon.com/careers.
EEO Statement:
BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer. Minorities/Females/Individuals With Disabilities/Protected Veterans. Our ambition is to build the best global team – one that is representative and inclusive of the diverse talent, clients and communities we work with and serve – and to empower our team to do their best work. We support wellbeing and a balanced life, and offer a range of family-friendly, inclusive employment policies and employee forums.
Job Type: Full-time","$80,404 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Investment & Asset Management,1784.0,$10+ billion (USD)
Talent-One,,"Imperial, CA",Data Engineer,"Responsibilities:
Develop predictive models
Develop optimization models
Develop re-activation and retention models
Advanced analytics to drive incremental revenue
Identify performance metrics definition, algorithm development and automation
Reporting and visualization
Complex data analysis tasks
Data anomaly detection and correction modeling
Conversion of data into stories for internal and external consumption
Cross-team support for CRM and Database Marketing Teams.
Qualifications:
Bachelor’s degree in an Analytical field (Business, Marketing) required.
At least three (3) years casino database experience required, or the equivalent combination of education and experience in data analysis.
SAS programming level 1 or higher certification required.
Ability to use data to solve complex business problems.
Advanced SQL skills
Strong industry experience of Microsoft Office Suite, including Excel, Word, Access, required
Job Type: Full-time
Salary: $1.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",,,,,,,
"BluePath Labs
4.8",4.8,"West Point, NY",Junior Data Scientist or Data Engineer -ONSITE,"Junior Data Scientist or Data Engineer - ONSITE

Location: Army Cyber Institute, West Point, NY - ONSITE
BluePath Labs is looking for a Junior Data Scientist or Data Engineer to support our team at the Army Cyber Institute (ACI) in West Point, NY. This is a critical and exciting opportunity at the intersection of advanced cyber research and the latest cloud technologies. This effort will have direct operational relevance. The successful candidate will demonstrate a track record of successfully working in a collaborative environment. They will be working with leading Army researchers and programs and partners throughout the US Government. This position will start immediately.
Primary Responsibilities
Responsible for designing, developing, testing, and maintaining data pipelines and data models that support advanced research and analysis for the Army Cyber community.
Work both alone and as part of a team with other developers, designers, and stakeholders to deliver high‐quality data‐driven insights, analytics, and models.
Perform exploratory data analysis, cleaning, and labeling on both structured and unstructured data.
Implement and train machine learning models on various data types, including text, tabular, time series and image data.
Comfortable working with on‐premises and cloud‐native environments for the discovery, extraction, and analysis of patterns and trends to gather, process, and derive valuable insights needed to build AI‐enabled capabilities.
Required Qualifications
Minimum of a bachelor's degree in data science (DS), data engineering (DE), computer science (CS), software engineering (SE), computer engineering (CE), electrical engineering (EE), mathematics, information technology (IT), information systems (ISYS) or a related field.
Primary programming proficiency in Python and R.
Additional experience designing and querying with relational and non‐relational databases (SQL, NoSQL, or time‐series).
Familiarity with Debian-based Linux OSes, such as Ubuntu.
Knowledge of computer and network security principles, such as network segmentation and VLANs.
Desired Qualifications
U.S. or Five Eyes national citizenship.
Ability to obtain DoD Secret clearance.
About BluePath Labs
BluePath Labs is a fast-growing research and consulting company committed to solving complex problems for federal, state, and local government clients. We offer a range of professional, scientific, and technology services. Our specific areas of expertise include business consulting, research and data science, and technology integration.
BluePath Labs combines mission and business insights with advanced technologies to deliver measurable performance improvements for our clients. BluePath is dedicated to surpassing client expectations by always living by our core values of integrity, professionalism, and resilience. BluePath's extensive experience in Government, Military, Commercial, and Academic environments is unique among small businesses and a core differentiator of our solutions. Our multidisciplinary background allows us to solve diverse and complex problems. Most importantly, we work closely with our clients to frame problems correctly, optimize processes, leverage technologies, and implement enduring solutions. Labs are where ideas are born, experiments occur, and breakthroughs happen. It is the hallmark of BluePath's culture.
BluePathLabs.com
BluePath Labs is an equal opportunity employer.","$119,189 /yr (est.)",1 to 50 Employees,Company - Private,Government & Public Administration,National Agencies,2016.0,Unknown / Non-Applicable
"PLAXONIC
4.6",4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642","$117,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013.0,$1 to $5 million (USD)
"Xiar tech inc
3.3",3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$51.90 /hr (est.),1 to 50 Employees,Company - Public,,,,Unknown / Non-Applicable
Kaizen Dynamics,,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.",$85.00 /hr (est.),Unknown,Company - Public,,,,Unknown / Non-Applicable
"YT Global Network
5.0",5.0,Remote,Data Engineer- Remote,"Data Engineer- Remote
Role: Data and Analytics is an evolving space which includes more software engineering, distributed systems, and cloud skills.
WIll develop, maintain, and enhance the data platform capabilities in an open and collaborative environment to build the central platform.
Will collaborate with internal data customers across IT and the Business to minimize the time from idea inception to analytical insight.
Job responsibilities will include: contributing to data infrastructure design efforts and collaborating with other platforms to integrate infrastructure into the client's systems and testing the feasibility and effectiveness of various technology options; supporting complex tools and solutions to manage orchestration, data pipelines, and infrastructure as code solutions the Data Engineering team builds.
Required skills:
Proven experience in designing, building, and supporting complex data pipelines using a variety of traditional and non-traditional data sources.
Version Control and associated best practices
Advanced programming experience in programming languages used in analytics and data science (e.g. Python, Java, Scala). Comfortable with Linux environments and shell scripting.
Experience with Cloud-based infrastructures (AWS)
Experience working with SQL/NoSQL
Experience utilizing data pipeline orchestration frameworks.
Verbal Communication
Preferred skills and experiences:
Analysis
API Development
CI/CD
Creating Real Time or Streaming Systems
Data Governance
Data Lineage
Data Metadata
Data Testing
Distributed Databases
Domain Knowledge
Schema
Snowflake
Visual Communication
EDUCATION AND/OR EXPERIENCE REQUIRED:
Education and/or experiences listed below are the minimum requirements for job entry.
Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of two years of work experience designing, programming, and supporting software programs or applications.
In lieu of degree, minimum of four years related work experience designing, programming, and supporting software programs or applications may be accepted.
Job Types: Full-time, Contract
Pay: $90.00 - $120.00 per hour
Benefits:
Health insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote",$105.00 /hr (est.),Unknown,Company - Public,,,,Unknown / Non-Applicable
"Teamware Solutions (quantum leap consulting).
4.6",4.6,"South San Francisco, CA",Data Engineer - Onsite,"Hi,
Data Engineer
Bay Area, CA – Onsite(Hybrid)
Client: Decision Minds/PANW
Duration: Contract
Exp Level: 10+ Years
Must have skill: Google cloud exp
Job Responsibilities:
Expert in data engineering and GCP data technologies.
Work with client teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
Work with Agile and DevOps techniques and implementation approaches in the delivery
Key responsibilities: Architecture, Design and Development
Required Skills:
10+ Year experience in BI and Analytics
Hands on and deep experience ( at least 2 years) working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, ]etc.).
Experience in Spark (Scala/Python/Java) and Kafka, Airflow
Data Engineering and Lifecycle (including non-functional requirements and operations) management.
E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.
Experience with SQL and NoSQL modern data stores.
Thanks & Regards
Jagadeesh
Teamware Solutions Inc |2838 E. Long Lake Road,Suite# 210, TROY, MI 48085
Job Type: Full-time
Salary: $60.00 - $65.00 per hour
Experience level:
10 years
11+ years
9 years
Schedule:
On call
Ability to commute/relocate:
South San Francisco, CA 94080: Reliably commute or planning to relocate before starting work (Required)
Experience:
Google Cloud Platform: 4 years (Preferred)
Data Engineer: 9 years (Preferred)
Spark: 4 years (Preferred)
Work Location: One location",$62.50 /hr (est.),1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2003.0,Unknown / Non-Applicable
Excellerent Solutions Inc,,"Coppell, TX",Data Integration Engineer,"Role: Data Integration Engineer
Location: Irving, TX(5 days a week in Office, 100% onsite role)
Must have experience working with SnapLogic to perform ELT processes.
Must have good written and verbal skills
Must be capable of knowledge transfer to team of ~10 ETL developers on use of SnapLogic
Must be able to work with a larger team to complete sizable migration of on-prem processes to Snowflake.
Must be able to follow design patterns set out by our architecture team.
Job Types: Full-time, Contract
Pay: $48.18 - $60.00 per hour
Schedule:
8 hour shift
Work Location: In person",$54.09 /hr (est.),,,,,,
"Arthur Grand Technologies Inc
4.8",4.8,"Mount Laurel, NJ",Lead Informatica / Data Engineer,"Role: Lead Informatica / Data Engineer – On Prem –ETL (Onsite role) / Senior Informatica / Mid-Level Informatica
Location: Mount Laurel, NJ / Charlotte, NC
Duration : FTE
Client :: Hexaware / TD Bank
Key Skills: Informatica Power Centre, Autosys, Unix
Must Have
More than 12+ years of IT experience in Datawarehouse and ETL
Hands-on Experience on ETL Informatica Power Centre
Experience on Autosys, Unix and scripting knowledge on Python, Shell Scripts
Experience on Oracle Database
Ability to understand ETL Design, Source to target mapping (STTM) and create ETL specifications documents
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have
Any cloud experience on Azure or AWS or Informatica cloud connector
Any relevant certifications
Job Type: Full-time
Salary: $69,919.38 - $166,922.18 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 3 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road","$118,421 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2012.0,Unknown / Non-Applicable
Predict Health,,"Arlington, VA",Data Engineer,"Overview of the Role
We are looking for an exceptional Data Engineer who is interested in joining a small and rapidly growing Product and Services team at our company. We view our data-driven products and services as strategic assets, and you will be a key part of our team and company. The successful candidate will have 5+ years of experience designing and implementing data solutions and services. This role will report to our VP, Product Engineering and will be an instrumental member of our product development team.
The Data Engineer will participate in the design of new information intensive product and service offerings. They will be responsible for the development and implementation of the data pipelines and services underlying these new products. They will deepen their understanding of the political, policy, financial and business aspects of the health plan industry so as to be able to provide insight into the best manner to collect and manage information related to these businesses.
The Data Engineer will work closely with the Product Management function to understand customer needs and help identify capabilities for new offerings. They will also partner with the Data Science team to understand the analytical needs of the business and assist in the design and implementation of data solutions to support these efforts.
About Predict Health, Inc.
Predict Health is transforming how health plans, providers and their partners increase retention, quality, and lifetime value of their customer or patient relationships. The core competency of the organization is the curation, analysis, and presentation of information to support targeted operational improvements in healthcare enterprises. The company is an early-stage enterprise based in Washington, D.C. which focuses on three solution categories: Member Acquisition, Quality Management, and Retention Management.
Our company, Predict Health was founded in 2019 and is led by an experienced team of healthcare experts, data scientists and healthcare entrepreneurs. We are focused on building services and solutions designed to improve the healthcare experience of our nation’s seniors. We founded our company after seeing first-hand how challenging it is for the seniors in our lives - including our parents, grandparents and friends - to deal with the complexities of Medicare with little or no real help.
Detailed Job Description
The Data Engineer will initially support the deployment of an Azure solution to help collect, aggregate, transform, and distribute information related to Predict Health’s current service offerings. The Data Engineer is responsible for designing the solution, building the data integration and associated pipelines, design and deploying data structures required to support current products and analytical efforts, and automating the operational aspects of the data platform. The Data Engineer responsibilities will include:
· Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as, but not limited to, Azure Synapse, Azure Data Factory and Azure pipelines.
· Translate business requirements from product owners and analysts into technical code.
· Identify, fix and document bugs, bottlenecks in workflows and pipelines.
· Enforce security compliance, fine tune performance and promote code quality standards.
· Identify and curate new data sources required to support business requirements
· Explore, learn the latest Azure data management technologies to add to current competencies.
· Support development of data intensive solutions in a fast-paced, dynamic environment
· Support Business Intelligence Applications (e.g. Power BI) as required.
Qualifications
Requires minimum of five years relevant experience in design, development, implementation, and testing of data services
· Experience in data structures, data modeling, data warehousing and data pipeline development on Azure
· 3+ years of hands-on experience in the design, implementation and performance tuning of Azure Data Factory, Synapse, or other Azure data services
· Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure
· Experience developing software code in one or more programming languages (Java, Python, etc.)
· Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform
· Experience with data migration
· Experience defining system architectures and exploring technical feasibility trade-offs.
· Ability to prototype and evaluate applications and interaction methodologies.
Other Knowledge, Skills, and Abilities Required
· Demonstrated capability for problem solving, decision making, sound judgement and assertiveness.
· Ability to change direction to meet demand in a high-paced work environment
· Ability to focus on details and technicalities while simultaneously retaining a comprehensive vision of organizational goals
· Excellent collaborator and able to influence and direct work contributions from others to meet critical program deliverables and timelines
· Advanced interpersonal, written, and oral communication skills.
· Proficiency in Microsoft Office including Excel and PowerPoint
What we Offer
This is a full-time position and compensation, and benefits will be highly competitive.
Predict Health is committed to bringing together people from different backgrounds and perspectives to deliver real results for our clients and end-users. We recruit, employ, compensate, and promote regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.
This position will be based in our Arlington, VA office. We offer a flexible work schedule with a minimum of three days in the office each week.
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Employee discount
Flexible schedule
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Arlington, VA: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location","$115,000 /yr (est.)",,,,,,
The Sunwater Institute,,"Rockville, MD",Data Engineer,"The Sunwater Institute is a nonprofit, nonpartisan public policy organization focused on improving the foundations of our social systems. The Sunwater Institute’s current programs focus on improving congressional performance, preserving and advancing constitutional rights and liberties, strengthening and enlarging property rights, and exploring the foundations of group decision-making.
The Sunwater Institute’s interdisciplinary, long-term orientation and focus on the building blocks of social systems sets it apart as a haven for bold, fresh ideas and free, constructive dialogue. The Sunwater Institute is located in North Bethesda and maintains a presence on Capitol Hill.
POSITION
The Sunwater Institute is seeking a Data Engineer who has a background in engineering or computer science and is highly skilled in SQL and database services technologies. The ideal candidate will possess strong problem-solving skills, excellent communication, and a great technical aptitude. The individual in this position will provide project support and help drive team initiatives. The candidate will be creative, and comfortable working in small teams and coordinating with US and non-US-based developers.
Responsibilities
Develop data models to store and manage large amounts of structured and unstructured data
Design efficient ETL processes to move and integrate data from different sources into the enterprise warehouse
Write complex queries using SQL Server etc. to analyze large datasets
Identify trends, patterns, and correlations within datasets by performing exploratory analysis
Ensure high quality of data by implementing best practices on storage, maintenance, indexing techniques, etc
Coordinate project status updates between teams and communicate deliverables and timelines
REQUIREMENTS
Bachelor’s degree in Computer Science or Engineering related field
4+ years of relevant experience in a similar role
Strong knowledge of relational databases including SQL Server
Proficient skills in writing complex queries using SQL language; developing stored procs, views, etc.
Experience with end-to-end ETL (Extract Transform Load) Processes
Familiarity with Big Data platforms such as AWS etc.
Ability to develop effective reports that help drive business decisions
Must be able to work effectively with teams but be able to work independently on tasks to meet tight deadlines
Exceptional communication skills verbal, written, and active listening
Extreme attention to detail and accuracy
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Vision insurance
Compensation package:
Yearly bonus
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Rockville, MD 20852: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",,,,,,,
"Plaxonic Technologies
4.6",4.6,"Dallas, TX",Data Engineer,"Job Description – Sr. Data Engineer:
Minimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark
Ability to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark
Hands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies
Strong skill in understanding database architecture, data models and writing complex SQL queries/code
Hands on experience with integration of different data sources (Files, DBs, APIs)
Ability to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation
Knowledge of various ETL techniques
Experience with data pipeline and workflow management tools [Azure DevOps]
Strong analytic skill to work with unstructured data
Experience working with Agile teams scrums.
Strong customer handling skills and communication skills
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75287: Reliably commute or planning to relocate before starting work (Required)
Experience:
Microsoft Excel: 1 year (Preferred)
Microsoft Office: 1 year (Preferred)
Compliance management: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: One location
Speak with the employer
+91 7272167989","$98,641 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013.0,$1 to $5 million (USD)
Ascent Technologies,,Remote,Data Engineer,"Job Description: Involved in resource / volume ramp-up plan/ backfilling resources planned for rotation and resignation cases Tracking Performance of individual Projects within the delivery/SIPs. Participate / presenting SD performance during client visit Publish Daily, Weekly and Monthly reports on team productivity Reviewing the weekly / monthly dashboards Publishing dashboards to customer People Management Drive the team to reach the defined targets Ensure training and development of the team members Mentor team members on technical and communication aspects. onsite anni data enginneer-pheonix,az python,charlottee,nc ba-houston tx
Job Types: Full-time, Contract, Permanent
Salary: $39.87 - $86.24 per hour
Benefits:
Dental insurance
Compensation package:
1099 contract
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91 7842516676",$63.05 /hr (est.),,,,,,
"Cloudbc Labs
3.9",3.9,Remote,Anaplan Data Engineer,"Job Title :: Anaplan Data Engineer
Location :: Remote
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Remote",$65.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015.0,Unknown / Non-Applicable
"Adobe
4.4",4.4,"Lehi, UT",Data Warehouse Engineer,"Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

The Opportunity
As a Data Warehouse Engineer for Adobe's Cloud Engineering team you are a key contributor in developing and supporting our next generation financial analysis platform that enables deep insights for business leaders across Adobe's diverse and globally distributed technology portfolio.
What you'll Do
Design, scale, and maintain a multi-petabyte Data Warehouse comprised of product telemetry and spend.
Build and operate an extensive data transformation pipeline capable of efficiently ingesting and allocating billions of data points to sophisticated business hierarchies and financial models daily.
Enable performant and insightful reporting for business analytics through efficient schema design and ETL processes.
Partner with Product Management to understand emerging requirements from business leaders and finance leads as you shape our next gen data platform for product metrics and spend data.
Engage with Sr. Engineers globally to integrate internal and external platform cost and telemetry.
What you need to succeed
Engineering Experience - have completed a Computer Science degree and/or equivalent experience supporting and building Data Warehouse systems.
Databases - Expertise in RDBMS schema design, data warehousing strategies, BI cubes, ETL procedures, indexing, performance tuning, and maintenance of large datasets.
Data Analysis - Experienced with BI tooling, extracts, and reporting. Capable of using data to identify insights and tell compelling stories.
Software Development - Familiarity with Agile development methodologies, CI/CD pipelines, version control strategies, containerization, microservices, unit tests, and API creation and interaction.
Programming Languages - Capable with one or more programming languages such as Python, Java, and R.
Financial Competence - you understand corporate expenses (CAPEX, OPEX) and how changes impact margins.
Excellent Communicator - well written and comfortable communicating data visually.

Our compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $119,000 -- $232,700 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.","$175,850 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1982.0,$5 to $10 billion (USD)
"Decision Point Healthcare
5.0",5.0,"Boston, MA",Junior Data Engineer,"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a data engineer, you will be responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, management, and operation of our client data hubs, including data intake, data quality assessment/evaluation, data curation and enrichment processes. Our client data hubs consist of various health related data sources supporting Decision Point services including our AI/ML platform, analytics platform, and OPUS application suite. If this interests you, read on.
The position:
Develop and apply scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Support various components of the data pipelines, including ingestion, validation, cleansing and curation
Manage and ensure the success of ongoing data pipeline routines
Collaborate with our implementation team to assist with the identification and reconciliation of data anomalies
Create and maintain documentation on data pipelines
Engage with our software engineering team to ensure precise data points per application specifications
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering, or applicable experience
Expertise with SQL, database design and data manipulation methodologies
Expertise with ETL/ELT and the development of automated validation and data pipelines
Strong data profiling and analytic skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW, master data management and other database design principles
Comfortable working with high volume data in a variety of formats
Experience with Pandas/Numpy in a production environment
Experience with CI/CD and version control tools: Git preferred
Experience working within hybrid cloud environment; AWS experience is a plus
Excellent verbal and written communication
Excellent listener and collaborator with senior leaders, peers, and staff
Familiarity with data engineering and workflow management frameworks such as Airflow and dbt
Familiarity with healthcare data is a plus
A little bit about Decision Point:
We are a rapidly growing healthcare technology company changing the fundamentals of patient and provider engagement. For years, health plans have relied on descriptive data and reactive engagement. We empower our clients to understand and predict the whole member journey, enabling sustained improvements in member health outcomes and plan performance. We combine the latest, most practical technologies and a deep understanding of healthcare, bringing innovative, pragmatic solutions to an industry that touches us all.","$92,443 /yr (est.)",1 to 50 Employees,Company - Public,,,2013.0,Less than $1 million (USD)
"Schneider Electric
4.2",4.2,"Columbia, SC",Manufacturing Data Engineer,"Job Description:
Schneider Electric has an opportunity for a Manufacturing Data Engineer in our Columbia, South Carolina location. The Manufacturing Data Engineer is a key contributor to the data strategies for the Columbia plant, specifically, in the areas of system design and performance.

What will you do?
Specializes in several areas of knowledge regarding production / manufacturing processes: process design, ergonomics, capacity, simulation tools, investment, and cost analysis
Supports, maintains, upgrades and troubleshoots applicable system infrastructure
Develops a variety of applications and reports, leveraging large data sets to facilitate business operations and boost organizational efficiency.
Analyzes business processes and develops reports, tools, scripts and procedures to bridge the gap between legacy systems
Provides end-user support, including researching user complaints/issues, answering technical questions, and/or assisting with application revisions.
Manages Enterprise Resource Planning (ERP) by updating material, Bill of Materials (BOMs), and routings when modifications are required by Engineering
What qualifications will make you successful?
Bachelor’s degree in a field of Engineering ME or EE
Good communication skills
Relevant experience preferred 3 plus years
Experience in Excel, Macros, SQL, and Tableau

Qualifications:
What's in it for me?
Schneider Electric offers a robust benefits package to support our employees such as flexible work arrangements, paid family leave, 401(k)+ match, and more.
Who will you report to?
Manufacturing Engineering Manager

Let us learn about you! Apply today.

About Our Company:
Why us?
Schneider Electric is leading the digital transformation of energy management and automation. Our technologies enable the world to use energy in a safe, efficient and sustainable manner. We strive to promote a global economy that is both ecologically viable and highly productive.

€25.7bn global revenue
137 000+ employees in 100+ countries
45% of revenue from IoT
5% of revenue devoted for R&D

You must submit an online application to be considered for any position with us. This position will be posted until filled

It is the policy of Schneider Electric to provide equal employment and advancement opportunities in the areas of recruiting, hiring, training, transferring, and promoting all qualified individuals regardless of race, religion, color, gender, disability, national origin, ancestry, age, military status, sexual orientation, marital status, or any other legally protected characteristic or conduct. Concerning agencies: Schneider Electric does not accept unsolicited resumes and will not be responsible for fees related to such.","$90,065 /yr (est.)",10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1836.0,$10+ billion (USD)
"Archon Resources
3.4",3.4,"Tulsa, OK",Data Engineer,"EDI/Data Engineer:
6-month contract to hire. Must be in Tulsa/OKC and able to be on site weekly.
Needed: Azure Data Factory; Microsoft SQL; Oracle; Experience with big data
Nice to Haves: Experience monitoring data in and out; data warehouse provisioning to feed analytical requirements
________________________________________________________________
JOB SUMMARY:
The Data Engineer will be responsible for expanding, optimizing and monitoring our data and data pipeline architecture, as well as optimizing data flow and collection across organizational teams. The Data Engineer will support our software engineers, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
KEY RESPONSIBILITIES:
Create and maintain optimal data pipeline architecture to support our next generation of products and data initiatives.
Assemble large, complex data sets that meet functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.
Experience in the development of SSIS, ETL and other standardized data management tools.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Performs other duties as required.
QUALIFICATIONS:
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong project management and organizational skills.
Ability to work independently, handle multiple tasks and projects simultaneously.
EDUCATION/EXPERIENCE:
Bachelors degree or equivalent experience required.
Project management skills preferred.
Willingness to work in a high-tech, continually evolving, innovative environment.
Job Types: Full-time, Contract
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Health insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Tulsa, OK 74105: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Tulsa, OK 74105",$60.00 /hr (est.),1 to 50 Employees,Company - Public,,,,Unknown / Non-Applicable
"Chevron
4.1",4.1,"Denver, CO",Wells Data Engineer,"Chevron is accepting online applications for the position Wells Data Engineer through 03/08/2023 at 11:59 p.m. (CST).
Chevron’s strategy is straight-forward: be a leader in efficient and lower carbon production of traditional energy, in high demand today and for decades to come, while growing lower carbon businesses that will be a bigger part of the future. To achieve these goals, we’ll build on the assets, experience, capabilities, and relationships we’ve developed over 140 years to incubate and grow new business.

Technology will play a crucial role in unlocking ever cleaner and more affordable sources of energy. Chevron is seeking innovative, technology professionals with a desire to thrive in the global digital environment and help us lead the global energy transition. An IT career at Chevron offers you the opportunity to work in a technical environment with a global reach. You’ll find that we make a business of investing in our people and encouraging your professional development through a learning culture and challenging on-the-job opportunities. We differentiate ourselves through the application of cutting-edge technology, and by taking a collaborative approach that includes in-house expertise, proprietary solutions, and strategic partnerships. We also offer flexible work schedules and very competitive benefits.

Join Chevron IT. Lend us your skills and enjoy a great career with Chevron.
Wells Data Engineer responsible for multiple aspects of RBU Engineering Team data management and quality, automation, system connectivity, visualizations. This role is expanded through supporting performance related processes, benchmarking, and cost/metrics tracking in our Competitive Performance framework. The Data Engineer is also responsible for providing technical and analytical support in the area of data entry and standard templates for drilling, workover, intervention, and abandonments project management and execution.
The role

Wells Data Engineer responsible for multiple aspects of RBU Engineering Team data management and quality, automation, system connectivity, visualizations. This role is expanded through supporting performance related processes, benchmarking, and cost/metrics tracking in our Competitive Performance framework. The Data Engineer is also responsible for providing technical and analytical support in the area of data entry and standard templates for drilling, workover, intervention, and abandonments project management and execution.
Responsibilities for this position may include but are not limited to:

Understanding data systems architecture and management
Data sources, data quality, and QA/QC mechanisms needed for systems of record
Administrator level ownership of multiple data systems such as WellView, SharePoint, WellSafe systems, reporting systems, and data systems
Working with multiple data systems to provide clear and impactful data visualizations and tracking for the Wells leadership team
Working with the Wells Performance Engineer to support cost, metrics, and performance tracking processes
Support WellView well/job creation, data quality review and support, field training, and regulatory data
Utilizing PowerBI, Spotfire, or Excel to collate cost tracking information in Wellview and Siteview
Required to have strong written communication & excellent organizational skills.
Required Qualifications:

Data science or engineering related training and related systems
Advanced skills in Excel and other Microsoft Office Suite applications
Good communicator and able to collaborate effectively with multiple stakeholders (i.e. engineers, regulatory, operations, etc…)
Preferred Qualifications:

Experience working with WellView, PowerBI and ArcGIS applications
Experience in supporting oil and gas well drilling, completion, workover, and asset retirement operations.
Flexible Working
Chevron offers a complete package and provides career development opportunities to all employees. We do this through on-boarding, training and development, mentoring, volunteering opportunities and employee networking groups. We advocate work-life balance and offer employees access to various health and wellness programs.
What type of flex work does the position offer?
We offer alternative work schedules including 9/80 (work 9-hour days, with every other Friday off)
We offer a hybrid work model - work remotely from home 2-3 days a week

Relocation & International Considerations
Relocation [ may / will not be] considered.
Expatriate assignments [ may / will not be ] considered.

Chevron regrets that it is unable to sponsor employment Visas or consider individuals on time-limited Visa status for this position.
Working with us
Chevron is one of the world’s leading integrated energy companies. We believe affordable, reliable and ever-cleaner energy is essential to achieving a more prosperous and sustainable world. Chevron produces crude oil and natural gas; manufactures transportation fuels, lubricants, petrochemicals and additives; and develops technologies that enhance our business and the industry. We are focused on lowering the carbon intensity in our operations and seeking to grow lower carbon businesses along with our traditional business lines. More information about Chevron is available at
www.chevron.com
.
Benefits
Chevron offers competitive compensation and benefits programs which includes, but is not limited to, variable pay, healthcare coverage, retirement plan, insurance, time off programs, training and development opportunities and a range of allowances connected to specific work situations. Details of such benefits and allowances are available at
https://hr2.chevron.com/
.

The compensation and reference to benefits for this role is listed on this posting in compliance with Colorado law. The selected candidate’s salary will be determined based on his or her skills, experience, and qualifications.

Regulatory Disclosure for US Positions

The compensation and reference to benefits for this role is listed on this posting in compliance with applicable law. Please note that the compensation and benefits listed below are only applicable for U.S. payroll offers.
The anticipated salary range for this position is $ 112,200 – $ 221,900 The selected candidate’s compensation will be determined based on their skills, experience, and qualifications.
Chevron offers competitive compensation and benefits programs which includes, but is not limited to, variable pay, health care coverage, retirement plan, protection coverage, time off and leave programs, training and development opportunities and a range of allowances connected to specific work situations. Details are available at
http://hr2.chevron.com/
.
Regulatory Disclosure for US Positions:
Chevron is an Equal Opportunity / Affirmative Action employer. Qualified applicants will receive consideration for employment without regard to race, color, religious creed, sex (including pregnancy, childbirth, breast-feeding and related medical conditions), sexual orientation, gender identity, gender expression, national origin or ancestry, age, mental or physical disability (including medical condition), military or veteran status, political preference, marital status, citizenship, genetic information or other status protected by law or regulation.
We are committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or an accommodation, please email us at
emplymnt@chevron.com
.
Chevron participates in E-Verify in certain locations as required by law.","$167,050 /yr (est.)",10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1879.0,$10+ billion (USD)
"Crisp Inc
3.6",3.6,Remote,Data Engineer,"Here at Crisp, we value the strength in teamwork, and strongly believe that it’s the key to Crisp’s success. By bringing together bright, motivated creators wherever they live and work, we leverage diverse experiences and backgrounds to understand the challenges facing our food system and solve them together. Come join us, and help build the type of business you’d like to be a part of.
Crisp is a socially conscious, distributed team. We give you the opportunity to solve challenges in the global food industry while living where you’re most comfortable and working in areas where you can help foster and grow the community that you are a part of. We believe in transparency, diversity, and merit, and foster a culture of empowerment, personal impact and career growth.
As a Data Engineer at Crisp, you will help unlock the potential of our customers’ data by highlighting and elevating the semantic context. Your responsibilities will include data cleansing, semantic labeling, normalization, and using modern BI Technology to efficiently convey insights to our customers. Being part of the engineering team, you will not only help clients leverage our data platform, you will also help evolve the platform itself by being a subject matter expert involved in product development.
This is an evolving role with ample opportunity for growth. Whether you are coming from a startup or corporate background, you appreciate the significant impact to be had in smaller organizations and you relish the ability to shape your own role and the future of the company.
Signs of a great candidate for Crisp:
Collaborative: You know that your colleagues’ perspectives will make our customers successful. Similarly, you use your strengths to help us grow together. You propose ways for us to be more valuable to our customers.
Customer focused: Our customers are at the forefront of your day. You prioritize our customers’ voices to ensure their needs are met.
Ambitious, curious, and resourceful: You are innately curious, and you aren’t afraid to work hard. You are self-driven and able to find creative results on your own, but you also take direction well. You are driven to succeed because your hard work and results make you proud.
Disciplined and reliable: You enjoy the benefits of working on a distributed team while consistently delivering what you have committed to. When you hit a snag, you communicate and reset expectations early.
Appreciative of honest feedback: You know that the best way to learn and grow is through constructive feedback delivered kindly. You view feedback given to you as an opportunity to get better and strive to do the same for others.
Work smarter and harder: You often identify a problem, create a solution, and bring it to a state of completion - with others, or even on your own. You find ways of eliminating or automating stuff that is uninteresting or wasteful.
Signs of a great candidate for a Data Engineer:
Data oriented: You think about data in a rigorously structured manner. You live by the mantra “garbage in, garbage out” and are deeply experienced in the art of data cleansing.
Focus on the business problem: You are passionate about using visualizations to tell stories and glean actionable insights. In order to tell the story the right way, you need to understand how the business works and how to communicate with different stakeholders.
Brings business context to data engineering: You are the bridge between the business problem and the data pipeline. You are experienced in codifying business context via a semantic definition layer. You enable automation of labor intensive workflows.
Strong sense of aesthetics and user experience: You feel strongly about not just making Business Intelligence visually appealing, but also ensuring that it’s easy to learn and a pleasure to use.
Deep tooling expertise: Many BI tools have a point and click design layer, but you have a deep understanding of the modeling layer and how it interacts with the underlying data store. You are familiar with one or more tools that facilitate data exploration for the purposes of data cleansing or normalization. You hold strong opinions, born from experience, on the features that make a great semantic definition metadata capture tool. You are adept at transforming data in analytical databases using SQL.
We are building a team of people with a breadth of combined experiences so that we can collaboratively enable our customers to be successful. There are no hard requirements on specific background, experience or geographical location. Instead we’re looking for individuals that are capable, reliable, and hoping to grow along with us. Do you have strengths you can share? If so, we’d love to hear from you!",,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2018.0,Unknown / Non-Applicable
Horizon Health Alliance,,"Getzville, NY",Data Engineer,"Are you seeking a rewarding and fulfilling career in the Mental Health and Addictions field?
Apply to be a Data Engineer today!
What will your day look like?
At Horizon, you will enjoy a supportive, team-based work environment. Have a question? There is always someone there to help! We offer a seamless onboarding experience that will ensure your success in your new role.
As a Data Engineer at Horizon you will...
Acquire and assemble datasets that align with business needs
Identifying, designing, and implementing internal process improvements including optimizing data delivery, and automating manual processes
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency
Working with stakeholders including executive teams and assisting them with data-related technical issues
Working with stakeholders to support their data infrastructure needs and understand company objectives
Build, test, and maintain database pipeline architectures
Create new data validation methods and data analysis tools
Assist and support development of data governance policies
Create and distribute corporate reports using industry standard reporting tools and software such as SQL, Power BI, and Python.
Develop and maintain the workflows and security controls related to data extract, transform, and load (ETL) of corporate data
Providing technical expertise in data storage structures, data mining, and data cleansing.
Develop and maintain databases, data marts, models, data sets, and other key technical solutions
Why choose Horizon to build your career?
Besides the fact that we’ve been named a Best Place to Work for 14 (yes, 14!) years in a row? At Horizon, you can be assured that you will make difference in the lives of others. Even better, your teammates will be just as motivated to make a difference!
What we offer that you’ll love…
Company Culture: At Horizon, we pride ourselves on cultivating an atmosphere of teamwork where all employees feel heard and valued.
Diversity & Inclusion: We are committed to equity, racial justice, and equal opportunity for all, and strive toward this goal through the work of our Diversity, Equity, Inclusion and Belonging Council, frequent trainings, ongoing conversations, affinity groups, and more.
Trainings, Trainings, and More Trainings: We have an entire team dedicated to your personal development and professional growth.
Team Building, Connection, and Relationships: At Horizon, we’re more than co-workers, we’re a community. We support each other, celebrate our achievements and milestones together, and have fun together!
Retirement: We know you want to retire comfortably and we’re here to help! Horizon offers 401(k) AND profit-sharing programs to make sure you’re set for the future.
Student Loan Assistance: We help pay off our team members' student loans every month. One year after joining, you’ll have been able to pay off an extra $600!
PTO & Holidays: We believe self-care is essential. Combined with holidays, you’ll earn up to 22.5 paid days in your first year. By your 3rd anniversary, that's almost doubled with up to 37.5 paid days off!
What makes you a great candidate?
We can’t wait to learn more about you! Here are a few specifics of what you’ll need for the job:
Bachelor's degree in Computer Science, Information Systems, Engineering or equivalent
1+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL and reporting/analytic tools
Strong experience in coding languages like SQL and Python
Fluent in relational based systems and writing complex SQL
Strong analytical and problem-solving skills
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Thorough knowledge of Microsoft Office
Ability to work with multidisciplinary teams
Ability to multitask and manage competing deadlines
Ability to communicate appropriately with all levels of management
Excellent understanding of Microsoft Office suite
Ability to build and optimize data sets
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytic skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Location:
55 Dodge Road, Getzville, NY 14068
Hours:
Full-time position, Monday - Friday; 8:00am-5:00pm
Disclaimer:
Horizon endorses public health measures including vaccinations. We encourage all applicants to be mindful of the fact that Horizon is a healthcare agency proving in person services throughout our community.
This information is intended to provide a general overview of the position; it is not a full job description.

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)","$84,681 /yr (est.)",1 to 50 Employees,Nonprofit Organization,,,,Unknown / Non-Applicable
"Epsilon
3.9",3.9,"Irving, TX",AWS Data Engineer,"Job Description

As a data engineer, you will design and maintain data platform road maps and data structures that support business and technology objectives. Naturally inquisitive and open to the deep exploration of underlying data, finding actionable insights, and working with functional competencies to drive identified actions. You also enjoy working both freely and as part of a team and have the confidence to influence and communicate with stakeholders at all levels, and to work in a fast-paced complex environment with conflicting priorities.
About the role:
Reporting into the delivery leader, you will deliver consumable, contemporary, and immediate data content to support and drive business decisions. The key focus of the role is to deliver a custom solution to support various business critical requirements. You will be involved in all aspects of data engineering from delivery planning, estimating and analysis, all the way through to data architecture and pipeline design, delivery, and production implementation. From the beginning you will be involved in the design and implementation of complex data solutions ranging from batch to streaming and event-driven architectures, across cloud, on-premise, and hybrid client technology landscapes.
Brief Description of Role:
We are looking for 3+ years of experience in data engineering in a customer or business facing capacity and experience in the following:
Ability to understand and articulate requirements to technical and non-technical audiences
Stakeholder management and communication skills, including prioritizing, problem solving and interpersonal relationship building
Strong experience in SDLC delivery, including waterfall, hybrid and Agile methodologies
Experience of implementing and delivering data solutions and pipelines on AWS Cloud Platform.
A strong understanding of data modelling, data structures, databases, and ETL processes
An in-depth understanding of large-scale data sets, including both structured and unstructured data
Knowledge and experience of delivering CI/CD and DevOps capabilities in a data environment
Develop new inbound data source ingestions required within the multi-tiered data platform to support analytics and marketing automation solutions
Supports data pipelines – Builds the required dimensions, rules, segments, and aggregates
Support all database operations: performance monitoring, pipeline ingestion, maintenance, etc.
Monitor platform health - data loads, extracts, failures, performance tuning
Create/modify data structures/pipelines
Leveraging capabilities of Databricks Lakehouse functionality as needed to build Common/Conformed layers within the data lake
Develop, document, and test software and environment setup to ensure that the outcome meets the needs of end-users and achieves business goals
Qualifications
The following skills are required:
Tech Stack: AWS pipeline, Glue, Databricks, Python, SQL, Spark, etc.
Building the Data Lake using AWS technologies like S3, EKS, ECS, AWS Glue, AWS KMS, EMR
Extensive experience in ETL and audience segmentation
Developing sustainable, scalable, and adaptable data pipelines
Attention to detail in design, documentation, and test coverage of delivered tasks
Strong written and verbal communication skills, team player
In addition, the candidate should have strong business acumen, interpersonal skills, and communication skills, yet also be able to work independently.
At least 3 years of experience with designing and developing Data Pipelines for Data Ingestion or Transformation using AWS technologies
At least 2 years of experience in the following Big Data frameworks: File Format (Parquet, etc.), Resource Management, Distributed Processing
At least 3 years of experience developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 1-2 years of experience with Spark programming (PySpark)
At least 2 years of experience with Databricks implementations
Familiarity with the concepts of “delta lake” and “lakehouse” technologies
The following skills are nice to have, and expertise is not required:
Adobe (Campaign, Audience Manager, Analytics)
MLFlow
Microsoft Power BI
SAP Business Objects

Additional Information

When you’re one of us, you get to run with the best. For decades, we’ve been helping marketers from the world’s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon’s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC:
Culture: https://www.epsilon.com/us/about-us/our-culture-epsilon
DE&I: https://www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR: https://www.epsilon.com/us/about-us/corporate-social-responsibility
Life at Epsilon: https://www.epsilon.com/us/about-us/epic-blog
Great People Deserve Great Benefits
We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.
#LI-SJ1
REF186919L","$88,775 /yr (est.)",5001 to 10000 Employees,Subsidiary or Business Segment,Media & Communication,Advertising & Public Relations,1969.0,$1 to $5 billion (USD)
"CINQCARE
4.0",4.0,"Washington, DC",Data Engineer,"Overview
The Data Engineer is a critical member of our growing data science team. In this role, you will have the chance to define and develop a core data asset which provides a representative and culturally aware view of the individuals and the communities that CINQCARE serves. You will work to evolve this asset over time using a product roadmap that includes identification and closure of gaps in existing data, introduction of new data sources, generation of proprietary data while quantifying and eliminating areas of structural biases. CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations and to so, we must also seek to fix gaps in data that ignore and marginalizes the Black and Brown communities that we serve.

An idea candidate for this role will embody CINQCARE’s core values, including, Trusted, Empathetic, Committed, Humble, Creative and Community-Minded. At CINQCARE, we don’t have patients or customers – we have Family Members.
Job Responsibilities
The Data Engineer will have the following responsibilities:
The Data Engineer will be responsible for the design, development and delivery of data pipelines and value-added data assets, leveraging a variety of data warehousing methodologies and disciplines to ingest the data from heterogeneous sources into cloud-based Data-Lake Environment in AWS.
Manage initiatives & projects of significant complexity and risk. Excellent business and communication skills to be able to work with business owners providing input to prioritized roadmaps, develop work estimates, and ensure successful delivery to support strategic planning and initiatives, improve organizational performance, and advance progress towards CINQCARE’s goals.
Assist in the overall architecture of the ETL Design and proactively provide inputs in designing, implementing, and automating the data pipelines.
Investigate and mine data to identify potential issues within the data pipelines, notify end-users and propose adequate solutions.
Ensure data quality and integrity within the data lake in the AWS environment with a focus on compliance to HIPAA and state level compliance requirements.
Oversee user permissions and configurations for adherence to documented access management standards and policies.
Independently (with minimal oversight) develop and maintain trusted advisor relationships with business, clinical, and operations leaders at the senior leadership level and with external partners, that include guidance for optimizing use of analytic capabilities and deliverables, and prioritization based on strategic vision.
Use coding/scripting pipelines and APIs to uncover and turn data into assets that are analysis-friendly using AWS services like Athena Data Catalogue, Quick Sight or any other big data tool on AWS.
Create a high-quality catalog of all pertinent data with the primary goal of establishing a single source of truth and significantly increasing productivity by reducing the time required for data search and discovery.
Crossing team boundaries, educate/advise on data projects, on how to combine and aggregate client data across platforms or technologies, and how to make the greatest use of data.
Lead consistent adherence to the Software Development Life Cycle framework and governance processes including, but not limited to leading planning sessions, collecting, and documenting requirements, identifying design patterns, create and define custom transformations, aggregations, and other data manipulations, developing data pipelines, creating documentation, developing test plans, performing unit testing, conducting peer review sessions.
Use knowledge of healthcare industry, market environment, and clinical and business workflows and activities, to inform solution design and development to execute high-quality or differentiated solutions in an established problem space.
Perform other job-related duties as assigned.

General Duties
The Data Engineer should have the following duties:
Leadership: The Data Engineer will lead the continued build out of the data asset to create business value, including collaborating with their team to design, develop, and execute those strategies and solutions to deliver desired outcomes.
Strategy: The Data Engineer will contribute to the business strategy and roadmap: (1) improve outcomes for CINQCARE Family Members; (2) enhance the efficacy of other CINQCARE. business divisions; and (3) develop and deliver external market opportunities for CINQCARE products and services. In establishing the business strategy, the Data Engineer will define and innovate sustainable revenue models to drive profitability of the Company.
Collaboration: The Data Engineer will ensure that AI capabilities form a cohesive offering, including by working closely with other business divisions to learn their needs, internalize their knowledge, and define solutions to achieve the business objectives of CINQCARE.
Knowledge: The Data Engineer will provide subject matter expertise in the AI solutions, including determining and recommended approaches for designing and building elegant data structures in support of existing reporting tools and custom visualization platforms.
Culture: The Data Engineer is accountable for creating a productive, collaborative, safe and inclusive work environment for their team and as part of the larger Company.
Qualifications
The Data Engineer should have the following qualifications:
Education: Bachelor’s degree in Computer Science, Engineering, Software Engineering, or related field; Master’s degree preferred.
Experience: The ideal candidate should have at least 3+ years of experience in healthcare data engineering. Experience with a variety of data projects and environments, whether on-prem or in-cloud (5+ years in SQL Server, ETL Tools, Business Intelligence & Analysis, Architecture). Familiarity with the Microsoft Stack; experience with other platforms is a plus. Mastery of Python and SQL. Strong foundational knowledge of data lakes and AWS products such as AWS Glue. Experience with healthcare eligibility and claims, implementing APIs, HL7/FHIR standards, ETL scheduling solutions, SQL, and healthcare data security.
Entrepreneurial: CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations. This position is accountable for ensuring CINQCARE is positioned to innovatively deliver on its promise.
Communication: Strong analytical and collaboration skills is required. Excellent verbal, written communication and presentation skills; ability to clearly articulate and present concepts and models in an accessible manner to CINQCARE’s team, investors, partners, and other stakeholders.
Relationships: Ability to build and effectively manage relationships with business leaders and external constituents.
Culture. Good judgement, impeccable ethics, and a strong team player; desire to succeed and grow in a fast-paced, demanding, and entrepreneurial Company.

Location: New York, NY
Compensation: $100,000-$120,000
My3ehHtP4z","$110,000 /yr (est.)",201 to 500 Employees,Company - Public,,,,Unknown / Non-Applicable
"Liberty Source
3.0",3.0,"Hampton, VA",Data Engineer,"Company Background:

Liberty Source PBC combines state-of-the-art technology with a human overlay, enabling our clients to realize greater returns from their investments in artificial intelligence, machine learning, business intelligence and deep analytics platforms. We work with our clients’ Data Science teams, Data Operations staff, Data Quality functions, and other key stakeholders to refine and enhance the data that is vital to the success of their advanced technology initiatives. We are the Data Fitness experts.

Our specialized recruiting mission focuses on the talents of veterans and families of active-duty military personnel to fulfill our brand promise of 100% U.S.-based operations and staff.

Founded in 2014, Liberty Source PBC is based in Hampton, Virginia and is a Certified B Corporation.

Position Summary:

Are you ready to put your data wrangling skills to the test and take your career to the next level? Liberty Source is seeking a Data Engineer to join our rapidly growing team. The right individual will expand and optimize our data and data pipeline architecture and build the systems that collect, manage, and convert raw – and often unstructured - data into usable information for our clients.

The ideal candidate can demonstrate experience in optimizing data flow, has collected and distributed data across teams and across companies, has built data systems (both large and small) from the ground up, and isn’t afraid to wade into the lake for some data cleansing and transformation. We’re looking for an individual who is self-directed and comfortable supporting the data needs of multiple teams, systems, and products, and excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. Beyond technical prowess, our new Data Engineer will have the soft skills for clearly communicating highly complex data ideas and issues to the leadership team.

Position Responsibilities:

Create and maintain optimal data pipeline architecture
Build the infrastructure required for efficient ETL (extraction, transformation, and loading) of data from a wide variety of data sources using SQL, Hadoop and AWS ‘big data’ technologies.
Build analytic views that utilize the data pipeline to provide actionable insights into operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Leverage data to solve business problems, building and maintaining the infrastructure to answer questions and improve processes
Help streamline our data science customer workflows, adding value to our product offerings and building out the customer lifecycle and retention models
Work closely with customer data science and business intelligence teams to develop data models and pipelines for research, reporting, and machine learning
Be an advocate for best practices and continued learning

Requirements:

Bachelor’s degree in computer science, information technology, engineering, or related discipline.
Five or more years of experience in a Data Engineer role utilizing Python and data visualization/exploration tools.
Advanced SQL capabilities and experience working with relational databases, query authoring (SQL) as well as hands-on experience with a variety of data constructs.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Demonstrated ability to work with unstructured data.
Proven ability to build processes supporting data transformation, data structures, metadata, dependency.
Professional certifications such as Cloudera Certified Professional (CCP), IBM Certified Data Engineer or Google’s Certified Professional is a plus.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong project management and organizational skills.
Experience supporting and working with cross-functional, innovation-oriented teams in a dynamic environment.
Great communication skills, especially for explaining technical concepts to nontechnical business leaders.

Benefits:

Paid Time Off (PTO) and 10 paid holidays
Medical, dental, vision, life insurance, and other ancillary benefits
401k Plan

A pre-employment background check is required.

To learn more about our business, please visit our website at https://liberty-source.com/","$91,094 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2014.0,$5 to $25 million (USD)
"Lenovo
4.0",4.0,"Morrisville, NC",Data Engineer,"General Information
Req #
WD00037891
Career area:
Hardware Engineering
Country/Region:
United States of America
State:
North Carolina
City:
Morrisville
Date:
Wednesday, January 18, 2023
Working time:
Full-time
Additional Locations:
Morrisville - North Carolina - United States of America
Why Work at Lenovo
Here at Lenovo, we believe in smarter technology that builds a brighter, more sustainable and inclusive future for our customers, colleagues, communities, and the planet.

And we go big. No, not big—huge.

We’re not just a US$70 billion revenue Fortune Global 500 company, we’re one of Fortune’s Most Admired. We’re transforming the world through intelligent transformation, offering the world’s most complete portfolio of smart devices, infrastructure, and solutions. With more than 71,500 employees doing business in 180 markets, we help millions—not just the select few—experience our version of a smarter future.

The one thing that’s missing? Well… you...
Description and Requirements
Lenovo’s Infrastructure Solutions Group (ISG) is seeking a qualified candidate to join our global tools, data, and automation development team. This team designs, develops, deploys, and maintains software applications for productivity and automation solutions.
What You'll Do
The position will develop automation to drive efficiency and effectiveness, automate manual tasks, and extend existing automation platforms for others to build on. Additionally, this role will provide Technical Leadership to a worldwide team and utilize problem solving skills to provide solutions based on data for worldwide product assurance issues. Responsibilities Include:
Work with team to develop data strategies: data model, tools, storage, parsing, etc.
Define the physical components in data.
Define the configuration of a collection of components over time in data.
Define characteristics of a specific configuration over time in data.
Integrate with existing business data
Coordinate with functional teams within Lenovo to develop and automate data pipelines to continuously supply our analysts with clean and reliable data.
Work with to existing team to integrate, adapt, or identify new tools to efficiently collect, clean, prepare, and store data for analysis


Basic Qualifications:
Bachelor’s degree in Computer Science, Mathematics, Engineering, or in a related field
4+ years’ experience with object-oriented/object function scripting languages: Python, Scala, etc
4+ years’ experience building processes supporting data transformation, data structures, metadata, dependency and workload management
4+ years’ experience in data schema, data pipeline design and database management
4+ years’ experience in optimizing data pipelines, architectures and data sets Fluency in structured and unstructured data and management through modern data transformation methodologies


Preferred Qualifications:
4+ years’ experience with designing and managing data in modern ETL architect like Spark, Kafka, Hadoop, Snowflakes
Experience with cloud service environment
Working experience with NoSQL
Experience with Power BI ETL pipeline
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Strong analytical, problem solving, verbal and written communication skills



This position must sit in Morrisville, NC.
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any federal, state, or local protected class.
Lenovo adopted a COVID-19 Vaccination Policy for US-based employees. As a condition of employment, employees must adhere to Lenovo’s US Vaccination Policy and be fully vaccinated against COVID-19, subject to any applicable accommodations. To be fully vaccinated means individuals must receive the full series of a vaccine either approved by the FDA or WHO and listed by the CDC (e.g. two dose of the Moderna, AstraZeneca or Pfizer-BioNTech vaccines; or one dose of the Johnson & Johnson vaccine). This applies to all US-based employees, contractors and interns, regardless of work location. As a condition of employment, you must provide proof that you are fully vaccinated or follow Lenovo’s accommodation process.
TO BE DELETED - Multiple Cities (OLD)
Morrisville - North Carolina - United States of America
Multiple Countries (Posting Locations)
United States of America
Multiple States (Posting Locations)
North Carolina
Multiple Cities (Posting Locations)
Morrisville - North Carolina - United States of America","$102,019 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1984.0,$10+ billion (USD)
"i6 Group, Ltd.
4.2",4.2,Remote,Junior Data Engineer,"Junior Data Engineer
Job Location: Fully Remote – UK/EU
Job Type: Full-time, Permanent
Relocation: No
We’re pioneering the use of data to improve efficiency, transparency, and sustainability. Today, our aviation fuel management technology connects business functions, drives operational efficiencies, and enables environmental accountability across the globe. Our ambition is big, we’re transforming the aviation industry, and we need great people to make that happen.
Bring Your Skills and Passion to Life at i6:
As our company grows and matures from a start-up into a scale-up with an increase in customers and multi-tenancy applications, the opportunity for a curious and inspired Junior Data Engineer has opened up in our Data Team. This is your chance to join a team of innovative and like-minded engineers, analysts, and innovators pushing the boundaries of their products and themselves, working with our high-profile global customers to craft truly outstanding SaaS solutions. You will be responsible for creating, owning and driving the data solutions, including transforming data into critical assets for the business while ensuring security, reliability, privacy and performance by design.

Your Career at i6:
In your first month you’ll…
Ask lots of questions and begin to explore and really get to know our products, our clients and gain deeper insight into the Aviation industry
Start to build an idea of the challenges and requirements of our clients, think about what it really means to connect our biosphere to highly varied client architectures managing multiple interactions a second and dealing in near real time
Begin to translate business issues and requirements into data solutions
Maintain monitoring and alerting across the data processing systems
Start to learn and develop your skills in implementing data processes, ensuring consistency & re-usability
Start to work closely with data analysts ensuring data and its required structure & availability is accessible for reporting

Within 6 mths you’ll…
Start to develop the knowledge, expertise and confidence to identify pain points , challenge the status quo, then pioneer new ideas, develop designs and implement solutions helping us scale faster and drive the business forward
Participate fully in, and contribute to, sprint planning, retrospectives, standups and other ceremonies in a constructive and can-do manner
Begin to gather data requirements, working closely with business stakeholders and analysts and design & build the required integrations into the data pipeline following industry best practices on ETL, ELT processes & data warehousing to support project delivery
Working closely with the infrastructure team ensuring business data applications are well served and prioritise when required
Working closely with Solutions Architects, Developers, Product Owners & Managers to ensure adherence to data models when developing new products or applications
Start implementing data analysts reporting requirements into the data warehouse or pipeline

Within 1 year, you’ll…
Be able to proactively identify gaps & solutions in the data functionality requirements of the business
Effectively manage challenges associated with handling large volumes of data while working towards tight deadlines
Be proposing, researching & supporting the implementation of technical solutions that help the business achieve its commercial objectives in a cost-efficient and scalable manner
Feeling proud of what you have achieved, the influence you have had and how you have made positive changes for yourself, your team, i6, your clients and the planet
What you’ll bring to the role:
Bright, ambitious, humble and most of all inquisitive, you love telling stories with data, building reports and propositions that give real insight and impact, ensuring our products remain the best in the market. You love data and are a true evangelist, able to use your outstanding analytical and architecture skills to power internal insight and supercharge our data focused products. Data is the secret sauce to your superpower, and you are ready to share your expertise to help launch us to the next level.
What you will be working on: (You don't need all of this, but outstanding SQL is an essential)
Enterprise Systems
Near Real-Time Applications
Good understanding and experience using the following technologies: SQL, Python, Airflow, DBT, BigQuery, GitHub, GitHub Actions
Nice to have understanding of the following technologies: BI Tools (e.g. Tableau), Docker, Kubernetes, Google Cloud Platform, AWS, Azure Cloud
Bonus knowledge: Node.js, Typescript, MongoDB
SaaS
A dedicated, creative, and highly collaborative team of developers, engineers and analysts
A massive variety of systems and products for our major clients across the globe

Our Interview Process:

Stage 1: Initial screening interview with Talent Manager
Stage 2: In depth technical interview our Senior Data Engineer
Stage 3: Cultural interview with our CTO
Stage 4: Offer
Stage 5: Hand in notice and join our awesome team at i6

A Career With i6 Group

Our Values, The Six I’s That Make i6:
Improve - we’re open to new ideas and deliver an amazing customer experience.
Influence - we build strong relationships and make great decisions.
Impress - we work hard and smart to deliver great work on time.
Innovation - we share ideas, experiment and find new ways to solve challenges.
Intelligence - we continue to develop our skills, knowledge and behaviours.
Integrity - we respect and embrace differences in people.

Our Benefits

Subsidised subscriptions to Headspace, gym membership and health insurance (UK Only)
Various discounts and perks at high street shops, supermarkets and online vendors
Fully remote – work from anywhere within the UK/ Europe (with occasional travel to Farnborough Airport)
Clear goals, targets, and progression plan so you can maximise your career
Cross training opportunities
Sustainable workplace - Carbon footprint offset

Diversity & Inclusion: Equal opportunities for everyone.
We’re embracing diversity in all its forms and fostering an inclusive environment for all people to do their best work. This is integral to our mission of driving operational efficiency and environmental accountability across the world.

QTPX6EGOKQ",,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013.0,Unknown / Non-Applicable
Kaizen Dynamics,,Remote,Data Engineer,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and system administrator for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Support the System Administration of associated tools and software of data and analytics landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, and analytic products. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
ETL processes development: 10 years (Preferred)
Work Location: Remote",$85.00 /hr (est.),Unknown,Company - Public,,,,Unknown / Non-Applicable
"GTA (Global Technology Associates)
4.4",4.4,"Plano, TX",Data Logging Engineer,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",$50.00 /hr (est.),Unknown,Company - Private,,,,Unknown / Non-Applicable
"Raphael and Associates
3.3",3.3,"Rutherford, NJ",Data Analytics Engineer,"We are looking for a passionate and highly skilled Data Analyst. The successful candidate will turn data into information, information into insight and insight into business decisions.
RESPONSIBILITIES:
Set and deliver the vision and agenda for Business Analytics for the organization
Work with stakeholders to understand business needs, establishing a portfolio that will drive cost savings, revenue growth and continuous improvement
Develop and implement data analyses, data collection systems and other strategies that optimize statistical efficiency and quality
Acquire data from primary or secondary data sources and maintain databases
Lead development of analytic models to derive actionable insights that will have high business impact
Collaborate with IT to define the enterprise data strategy encompassing data collection architecture, data governance and data reporting infrastructure
Identify, recruit and develop top analytic talent
Locate and define new process improvement opportunities
Perpetuate a data-driven culture across the enterprise
KEY CAPABILITIES
Strategic thinker, with ability to make recommendations and influence senior leadership
Leadership experience in an analytics/BI environment
Leadership experience deploying new analytical approaches
In-depth understanding of insurance claims administration
Experience designing, building, testing and implementing predictive models
2-3 years people leadership experience
REQUIREMENTS
Proven working experience as a data analyst or business data analyst
Technical expertise regarding data models, database design development, data mining and segmentation techniques
Strong knowledge of and experience with Tableau and/or other reporting packages, databases (SQL etc), programming (XML, Javascript, or ETL frameworks)
Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SPSS, SAS etc)
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Adept at queries, report writing and presenting findings
BS in Mathematics, Economics, Computer Science or Statistics
eNdkCsQAff","$91,886 /yr (est.)",51 to 200 Employees,Company - Private,Insurance,Insurance Carriers,,$5 to $25 million (USD)
"Sanametrix
3.8",3.8,Remote,AWS Data Engineer,"Candidates Must be able to obtain/maintain a Public Trust (US Citizenship is required). This position is full-time w/benefits and 100% remote!
Sanametrix, Inc. is a fast-growing small business headquartered in Arlington, VA. We are dedicated to providing federal agencies with legendary customer service and focused solutions for their business and technology needs. This role is responsible for building data pipelines for transferring data from source systems (virtual machines, Microsoft SQL Server) into AWS Cloud using AWS Native Tools. This resource has strong data modeling and scripting experience and has a strong knowledge of AWS Data Services.
Responsibilities:
Perform data processing, algorithm / structures, pipeline orchestration, data quality, governance, discovery.
Work with structured and unstructured data, blob data
Develop and work with APIs
Collect and organize data using data warehousing technique and file storage technologies
Perform ELT and ETL processes
Gather data requirements
Develop and maintain scalable data pipelines and build out new API integrations to support continuing increases in data volume and complexity.
Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the organization.
Implement processes and systems to monitor data quality, to ensure production data accuracy, and ensure key stakeholder and business process access.
Write unit/integration tests, contribute to engineering wiki, and documents.
Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Work closely with a team of front-end and back-end engineers, product managers, and analysts.
Design data integrations and dataquality framework based on established requirements.
Must be able to obtain a Public Trust (US Citizenship required to obtain Public Trust)
Qualifications & Skills:
Scripting
SQL & Scripting
Python
Spark
Linux / shell scripting
Services / Tools (six or more)
S3 Lambda
Redshift
Lake Formation
Glue ETL
Kinesis
DMS
Glue catalog/Crawlers
Git
Jira
Airflow /Orchestration
Education, Experience, and Licensing Requirements:
BS or MS degree in Computer Science or a related technical field
4+ years of Python or Java development experience
4+ years of SQL or NoSQL experience
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining data processing systems
AWS Certified is preferred
Job Type: Full-time
Pay: $110,000.00 - $120,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Work Location: Remote","$115,000 /yr (est.)",Unknown,Company - Private,,,,Unknown / Non-Applicable
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"Epsilon
3.9",3.9,"Irving, TX",AWS Data Engineer,"Job Description

As a data engineer, you will design and maintain data platform road maps and data structures that support business and technology objectives. Naturally inquisitive and open to the deep exploration of underlying data, finding actionable insights, and working with functional competencies to drive identified actions. You also enjoy working both freely and as part of a team and have the confidence to influence and communicate with stakeholders at all levels, and to work in a fast-paced complex environment with conflicting priorities.

About the role:

Reporting into the delivery leader, you will deliver consumable, contemporary, and immediate data content to support and drive business decisions. The key focus of the role is to deliver a custom solution to support various business critical requirements. You will be involved in all aspects of data engineering from delivery planning, estimating and analysis, all the way through to data architecture and pipeline design, delivery, and production implementation. From the beginning you will be involved in the design and implementation of complex data solutions ranging from batch to streaming and event-driven architectures, across cloud, on-premise, and hybrid client technology landscapes.

Brief Description of Role:

We are looking for 3+ years of experience in data engineering in a customer or business facing capacity and experience in the following:
Ability to understand and articulate requirements to technical and non-technical audiences
Stakeholder management and communication skills, including prioritizing, problem solving and interpersonal relationship building
Strong experience in SDLC delivery, including waterfall, hybrid and Agile methodologies
Experience of implementing and delivering data solutions and pipelines on AWS Cloud Platform.
A strong understanding of data modelling, data structures, databases, and ETL processes
An in-depth understanding of large-scale data sets, including both structured and unstructured data
Knowledge and experience of delivering CI/CD and DevOps capabilities in a data environment
Develop new inbound data source ingestions required within the multi-tiered data platform to support analytics and marketing automation solutions
Supports data pipelines - Builds the required dimensions, rules, segments, and aggregates
Support all database operations: performance monitoring, pipeline ingestion, maintenance, etc.
Monitor platform health - data loads, extracts, failures, performance tuning
Create/modify data structures/pipelines
Leveraging capabilities of Databricks Lakehouse functionality as needed to build Common/Conformed layers within the data lake
Develop, document, and test software and environment setup to ensure that the outcome meets the needs of end-users and achieves business goals

Qualifications

The following skills are required:
Tech Stack: AWS pipeline, Glue, Databricks, Python, SQL, Spark, etc.
Building the Data Lake using AWS technologies like S3, EKS, ECS, AWS Glue, AWS KMS, EMR
Extensive experience in ETL and audience segmentation
Developing sustainable, scalable, and adaptable data pipelines
Attention to detail in design, documentation, and test coverage of delivered tasks
Strong written and verbal communication skills, team player
In addition, the candidate should have strong business acumen, interpersonal skills, and communication skills, yet also be able to work independently.
At least 3 years of experience with designing and developing Data Pipelines for Data Ingestion or Transformation using AWS technologies
At least 2 years of experience in the following Big Data frameworks: File Format (Parquet, etc.), Resource Management, Distributed Processing
At least 3 years of experience developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 1-2 years of experience with Spark programming (PySpark)
At least 2 years of experience with Databricks implementations
Familiarity with the concepts of ""delta lake"" and ""lakehouse"" technologies

The following skills are nice to have, and expertise is not required:
Adobe (Campaign, Audience Manager, Analytics)
MLFlow
Microsoft Power BI
SAP Business Objects

Additional Information

When you're one of us, you get to run with the best. For decades, we've been helping marketers from the world's top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon's best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC:
Culture: https://www.epsilon.com/us/about-us/our-culture-epsilon
DE&I: https://www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR: https://www.epsilon.com/us/about-us/corporate-social-responsibility
Life at Epsilon: https://www.epsilon.com/us/about-us/epic-blog

Great People Deserve Great Benefits

We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.

#LI-SJ1

REF186919L","$90,830 /yr (est.)",10000+ Employees,Company - Public,Media & Communication,Advertising & Public Relations,1926.0,$5 to $10 billion (USD)
"AGM Tech Solutions, LLC
4.8",4.8,"Alpharetta, GA","Senior Data Engineer (Must be local to Atlanta, GA)","Sr. Data Engineer
Location: Must be local to Atlanta Metro area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $75.00 - $85.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Application Question(s):
How many years of experience do you have as a Data Engineer?
How many years of experience do you have building data pipelines with Spark or Databricks?
Can you work on W2 without sponsorship? No C2C engagements for this position.
Are you currently located in Atlanta Metropolitan area?
Experience:
Python: 4 years (Preferred)
Spark: 3 years (Preferred)
AWS: 3 years (Preferred)
Work Location: One location",$80.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018.0,$5 to $25 million (USD)
"OPENMIND TECHNOLOGIES INC
4.2",4.2,"King of Prussia, PA","Data Engineer - Hybrid Onsite - King of Prussia, PA","Details:
JOB DESCRIPTION:
We are working with a client in need of a talented Data Engineer to support our client located in King of Prussia, PA. This contractor will be working on a high-level corporate initiative to build an enhanced customer service model for a leading utilities organization. This contractor will be working with our client on Data Engineering topics, including creating relevant data models, developing powerful data pipelines, exposing them through various mechanisms including APIs, and using data visualization tools to efficiently present data.
Responsibilities:
· Partner with our client’s leadership teams, engineers, program managers and data analysts to understand data needs.
· Design, build and launch efficient and reliable data pipelines transforming data into useful report ready datasets.
· Use your data and analytics experience to ‘see what’s missing,’ identifying and addressing data gaps, build monitors to detect data quality issues and partner to establish a self-serve environment.
· Broad range of partners equates to a broad range of projects and deliverables, including ML Models, datasets, measurements, services, tools and process.
· Leverage data and business principles to automate data flow, detect business exceptions, build diagnostic capabilities, and improve both business and data knowledge base.
· Build data expertise and own data quality for your areas.
QUALIFICATIONS:
· At least 4+ years' of advanced SQL experience (including at least one SQL DBMS and one no SQL).
· 4+ years' of Python development experience.
· 3+ years' experience with Data Modeling.
· Experience analyzing data to discover opportunities and address gaps.
· Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. SnowFlake, Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
· BSc/BA in Data Science, Computer Science, Engineering.
· Familiarity with SAP order generation and invoicing modules
Job Types: Full-time, Contract
Salary: $75.00 - $85.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
advanced SQL (including at least one SQL DBMS, one no SQL: 4 years (Preferred)
Python development: 4 years (Preferred)
Data Modeling: 3 years (Preferred)
analyzing data to discover opportunities and address gaps: 1 year (Preferred)
cloud or on-prem Big Data/MPP analytics platform: 1 year (Preferred)
SnowFlake, Netezza, Teradata, AWS Redshift, Google BigQuery,: 1 year (Preferred)
Azure Data Warehouse: 1 year (Preferred)
SAP order generation and invoicing modules: 1 year (Preferred)
Work Location: One location",$80.00 /hr (est.),Unknown,Private Practice / Firm,Information Technology,Enterprise Software & Network Solutions,,Unknown / Non-Applicable
"FlexIT Inc
4.0",4.0,"Beaverton, OR",Full Stack Data Engineer,"APLA is building capabilities around the company's data foundation to build data sources that are needed for reporting and analytics
The type of engineer were looking for is a Full Stack Data Engineer
Knowledge of data visualization engineering as well as consumption and view build engineering","$106,106 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986.0,$5 to $25 million (USD)
ITEOM,,Remote,Data Engineer - Remote,"Our client is seeking a Data Engineer who would be responsible for design of data warehouse schemas as well as end-to-end design and implementation of fault tolerant and scalable data processing pipelines using a variety of technologies for orchestrating data movement, primarily Snowflake , DBT and Airflow . You'll work with a team of data engineers to design and implement solutions that provide business-critical insights.
Salary : Mid-Level - 130K to160K+ bonuses and equity options
Location: REMOTE WORK 100% is an option: (locations limited to CO, AZ, TX, VA, CA, GA, HI, MN, MI, NJ, NY, NC, OR, PA, WA)
How You’ll Do It
Working collaboratively with a team of other data engineers and developers of varying experience levels and areas of expertise, you'll be responsible for:
Responsibilities
Design and implement data warehouse schemas to store large data sets that can be easily and efficiently queried and joined together for reporting and analytical purposes.
Design and develop ETL/ELT pipelines to efficiently move and aggregate data, while optimizing for maximum code and data reuse.
Collaborate with other team members on improvements to existing systems
Investigate data anomalies and provide quick resolutions.
Provide technical support to business users and analysts.
What We’re Looking For
TOP SKILLS: ETL, Data Warehousing, SQL, Jinga Scripting, Python and Snowflake.
3+ years of working experience with large scale data warehouse systems.
Very strong knowledge of SQL and data manipulation best practices
Experience in building efficient and fault tolerant ELT/ETL data pipelines
Very strong knowledge of working with large scale datasets and data modeling and data warehouse design
Experience with Snowflake is preferable
Knowledge of DBT, Jinja scripting and Airflow is a big plus
Nice to Have
Experience working cross functionally with product and engineering teams
Desire to wear many hats and work your tail off for a bit - great earning potential and equity available.
Benefits: A diverse benefit package, sign on bonus, options, etc. is offered through our client.
About ITEOM: ITEOM Digital Technology Talent Partners collaborates with top technologists and top employers to align purpose and passion and re-incorporate belonging to make the job search and work experience more human. We are an equal opportunity employer; however, this role is not eligible for sponsorship at this time. Employees eligible for full time W2 employment are encouraged to apply.","$145,000 /yr (est.)",1 to 50 Employees,Company - Public,,,,Unknown / Non-Applicable
"Globaleur
4.5",4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes.
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders.
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information.
Deploy sophisticated analytics programs, machine learning, and statistical methods.
Ensure compliance with data governance and security policies.
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition.
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences.
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields.
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing.
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java.
Have working experiences in e-commence, travel, marketing domain is a plus.
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies.
Excellent problem solving and troubleshooting skills.
Process oriented with great documentation skills.
Excellent oral and written communication skills with a keen sense of customer service.


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.","$119,183 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017.0,Unknown / Non-Applicable
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,,Less than $1 million (USD)
"plaxonic
4.6",4.6,"Hunt Valley, MD",AWS python data engineer,"AWS Python Data Engineer
Must Have: AWS Databricks Python, Spark, PySpark
Location Hartford, CT or St. Paul, MN or Hunt Valley, MD
Roles & responsibilities:
Acts as a single point of contact for data migration to AWS projects for customer
Provides innovative and cost-effective solution using AWS, Spark, python & customer suggested toolset
Optimizes the use of all available resources
Develops solutions to meet business needs that reflect a clear understanding of the objectives, practices and procedures of the corporation, department and business unit
As a leader in the Cloud Engineering you will be responsible for the overseeing development
Learn/adapt quickly to new Technologies as per the business need
Develop a team of Operations Excellence, building tools and capabilities that the Development teams leverage to maintain high levels of performance, scalability, security and availability
Skills:
The Candidate must have 3-5 yrs of experience in PySpark & Python
Hands on experience on AWS Cloud platform especially S3, lamda, EC2, EMR
Experience on spark scripting
Has working knowledge on migrating relational and dimensional databases on AWS Cloud platform
Relevant experience with ETL methods and with retrieving data from dimensional data models and data warehouses.
Strong experience with relational databases and data access methods, especially SQL.
Knowledge of Amazon AWS architecture and design
Job Type: Full-time
Schedule:
8 hour shift
Experience:
Python: 3 years (Preferred)
Work Location: On the road","$81,908 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013.0,$1 to $5 million (USD)
"WeVideo, Inc.
4.4",4.4,"Mountain View, CA","Senior Data Engineer, Analytics","Job type: Full-time position. Location: Mountain View, CA

What inspires you? This is the question that drives most career decisions.

Is it working with a fantastic team that is dedicated to a common goal? Is it the ability to make a significant impact on the success of a product and company? Perhaps it is to contribute to a product that ignites the creativity of content creators, small businesses, educators, and schoolchildren worldwide?

We have an immediate opening for a Senior Data Engineer. You will be working within the Analytics team at WeVideo to help build out and grow our data infrastructure. The team plays a central role in shaping the data ecosystem in our company, and enables business performance by providing users across the company with the insights, tools, infrastructure, and consulting to make data-driven decisions. Our org is hungry for data and needs you to help us get to the next level.

What you will do in this role:
Be the lead engineer that builds and automates data pipelines for our Product data, as well as enable Data Integration projects.
Develop and improve the technical architecture of current data warehouse (BigQuery)
Design data models optimized for aggregation, visualization and advanced analytics (machine learning)
Design, implement and maintain data pipelines from beginning to end. Our Product data pipeline is high velocity and we need to re-architect it using Apache Kafka. Our Business data pipes are standard fare, and we use ‘modern data stack’ tooling (ELT, reverse ETL, DBT, self-serve BI)
Facilitate data integrations and write optimized data transformations in SQL/Python
Implement automated QA and data quality checking systems for pipelines and warehouse
Leverage automation using Apache Airflow where possible to help the team scale
Skills and knowledge you possess:
2+ years in data engineering with a Bachelor's degree in CS, Data Science, or similar technical field, or equivalent professional experience
Demonstrated success building and automating batch and streaming data pipelines, as well as end-to-end Monitoring and Alerting solutions
High fluency with advanced SQL in BigQuery (or Redshift) environment
Proficient in creating high quality, fast services and projects in Python
Experience building data pipes using Apache Kafka or Pub/Sub, Apache Airflow, GCP/AWS
Strong communication skills and interest in working with Marketing & Sales teams
Benefits & perks :
Potential remote opportunity
Medical, dental, vision and 401(k)
Generous PTO policy
Free lunch and snacks
Enough free caffeine to keep you up for 2 weeks straight
Employee development resources
Why you might like working here:
We’re a small, close-knit team that enjoys working and learning from each other.
People stick around. Some of your future colleagues have been for over 8+ years.
Our users love us; just take a look at the tweets shared by teachers.
About WeVideo:
WeVideo is a powerful, easy to use, cloud-based video creation platform that is the digital editing and storytelling choice of more than 22 million consumers, students, businesses, and third-party media solutions. WeVideo is available from virtually any computer or device at home, school, work, or on-the-go to capture, edit, view, and share videos. Built for the future in HTML5, WeVideo brings maximum speed, responsiveness, security, and expandability to browser-based video editing. WeVideo is a Google Play Editors' Choice selection with more than 9 million downloads to date. WeVideo is also the exclusive digital storytelling solution of Google’s Education Creative Bundle for Chromebooks and a Microsoft Education Partner. More than 6,500 schools use WeVideo to enhance classroom learning.","$150,960 /yr (est.)",Unknown,Company - Private,,,,Unknown / Non-Applicable
"Quirch Foods
3.0",3.0,"Coral Gables, FL",Data Engineer,"About Quirch Foods
In business for over 50 years, Quirch Foods is a global distributor and exporter of fresh and frozen food products with a unique focus on ethnic cultures. We service our customers from 5 distribution centers strategically located in the United States. We have consistently been ranked among the top 50 exporters in the US and are one of the largest importers of seafood. Our customers include independent grocers, chain supermarkets, foodservice distributors, cruise lines, restaurants and food processors/manufacturers. We have an extensive product list with over 8,000 SKU's and carry niche brands such as Panamei, Mambo and Chiquita along with national brands like Tyson, IBP, Excel, JBS, Iowa Premium, Smithfield, Dietz & Watson, along with many others. We are headquartered in Coral Gables, FL.
Quirch is a portfolio company of Palladium Equity Partners, a leading mid-market private equity fund based in New York City, which partnered with the Executive Team to capitalize on Quirch Foods’ record of excellence and to grow it decisively beyond its current business.
Position Summary
At Quirch Foods we believe in leveraging technology as a source of competitive advantage. This includes customized ERP Solutions, Business Intelligence, EDI, Web and Mobile Solutions. We are currently looking for a Data Engineer to join our development efforts.
Essential Duties and Responsibilities:
Develop, test and maintain custom business solutions by leveraging state of the art software development practices using an agile methodology
Evaluate business requirement and help define problems and develop solutions
Participate in design and architecture discussions with business leaders, end users and IT team members
Document business requirements and solution, document code and provide support for creation of end user documentation
Update business knowledge, technical skills and soft skills, leverage educational opportunities, participate in professional organizations
Experience:
MS SQL Programming
Visual Studio\C#\WPS Experience
Microsoft SSAS, SSIS, SSRS desired but not required
Biztalk experience desired but not required
Skills and Requirements:
Bachelor’s degree in Computer Science, Engineering, Math or equivalent and\or related experience and training preferred
Strong analytical and problem-solving skills
Strong collaborator and team player
Great organizational skills, attention to detail and follow thru
Effective oral and written communication skills
Extensive familiarity with data management principles
Benefits:
Professional growth and developmental opportunities to grow your skills using state of the art technology
Comprehensive benefits package that includes: Medical, Dental, Prescription Drug Plan, Disability Plan, Life insurance Plan
401K savings Plan
Paid Holidays
Personal Time off
Employee Discounts
Quirch Foods is an Equal Opportunity Employer (EOE). Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. All applicants must be eligible to work in the United States.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Coral Gables, FL 33134: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What is your desired compensation?
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Data Engineering: 1 year (Preferred)
Work Location: In person","$89,340 /yr (est.)",201 to 500 Employees,Company - Private,Retail & Wholesale,Wholesale,1967.0,$500 million to $1 billion (USD)
"ECI - Sacramento
4.4",4.4,Remote,Healthcare Data Quality/Engineer/SAS SME (FT),"Healthcare Data Quality / Data Engineer / SAS SME

Senior Healthcare SAS Data Quality Management SME
Location: remote
24-months.
Must have 10-years working in Information Technology or related field, and those years must include the following experiences:
Minimum of three (3) years of experience interpreting, analyzing and applying HIPAA health care transaction requirements.
Minimum of three (3) years of functional experience within an organization with a large, complex data set.
Minimum of three (3) years of experience gathering, documenting, and designing and applying data quality requirements and data standards.
Minimum of three (3) years of experience with implementing electronic tools/software to support data quality requirements.
Minimum of three (3) years of experience establishing a framework for enterprise data quality.

Others:
Experience working on Medi-Cal, or another Medicaid, as a Data Quality Analyst, Business Analyst, or Business Intelligence Analyst.
Experience with tools focused on business intelligence dashboards and reports, such as Statistical Analysis Software (SAS), Microsoft PowerBI or Tableau.
Experience applying data quality management concepts and the Data Management Body of Knowledge (DMBOK) or similar methodology.
Experience in the creation and implementation of data quality service level agreements.
A bachelor's degree from an accredited college or university.



About ECI - Sacramento:

Estrada Consulting, Inc. (ECI) delivers technology-enabled services and solutions to clients all over the USA and British Columbia. We provide system integration, custom application development, data warehouse and business intelligence, project management, custom reporting solutions and consulting services to mid-size and large enterprises in all major industries. The Company headquarters is in Sacramento, California, and was established in year 2000. Visit http://www.estradaci.com/ to learn about our projects, managed services, awards and certifications delivering value for a range of businesses and government agencies.",$70.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2000.0,Unknown / Non-Applicable
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983.0,$500 million to $1 billion (USD)
TEKletics,,"Scottsdale, AZ",Azure Data Engineer,"Azure Data Engineer
Tekletics is an information technology company focused on providing quality resources to support our customers needs. Successful candidates will work with world class organizations to deliver projects. Each candidate is required to have a strong work ethic and the ability to handle high pressure situations.
A little about this role:
As the Azure Data Engineer, you are primarily responsible for the collection and transformation of data across a multitude of data sources. This individual is also responsible for the optimization of the environment, structure, and processes associated with said data.
A day in the life:
Data Warehouse - As the Azure Data Engineer, you are responsible for the data warehouse design, development, testing, support, and configuration. You will review business requests for data warehouse data and data warehouse usage. You will also research data sources for new and better data feeds ensuring consistency and integration with existing warehouse structure.
Data Collection – You will be responsible for developing automated data pipelines and/or data integrations within the Azure Synapse environment. You will use SQL, Python scripts and Azure Functions to automate data collection from a wide variety of sources.
o API utilization – ability to leverage REST APIs as needed.
Data Transformation – You will create BI (Business Intelligence) and Data Warehousing cube design. You will create and manage ETL/ELT processes to transform and load data into data warehouse for reporting and analytics.
Data Optimization – As the Azure Data Engineer, you will create and maintain standards and policies. You will identify, design, and implement internal process improvements, including automation of manual processes and optimization of data delivery. You will continuously improve data reliability, efficiency, and quality.
Training – Identify and demonstrate techniques to optimize reporting for Data Visualization Analyst, provide and participate in internal and external training sessions, and produce documentation to support understanding and learnings around the Presence data/reporting environment.
This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Projects and responsibilities may change at any time with or without notice due to our business, industry, and/or market changes.
What we are looking for:
Previous experience using SQL, Python scripts and Azure Functions
Experienced in Azure Synapse environment
Experience designing, building, and maintaining data processing system
Dependable, extroverted, diplomatic person, able to problem-solve successfully with a wide variety of people and issues
Attention to detail and strong organizational skills, self-motivated
Ability to work independently while being a strong team player
Ability to mentor junior level developers
Passion for innovation and “can do” attitude to thrive in a fast-paced environment
Proficient in time management and adhering to deadlines
Knowledge and interest of the natural products/brands and retail landscape is a plus
Proficient computer (MS Office applications) and data-mining skills
Flexibility to successfully multi-task in a fast-paced environment with a positive attitude
Regular and predictable attendance is required
Ability to manage time and deadlines
Job Type: Full-time
Pay: $79,947.00 - $142,509.31 per year
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Scottsdale, AZ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 3 years (Required)
SQL: 3 years (Required)
Data warehouse: 3 years (Required)
Work Location: In person","$111,228 /yr (est.)",,,,,,
"Pfizer
4.1",4.1,"Tampa, FL","Senior Associate, Regulatory Quality Assurance Data Engineer","Pfizer has established a chief digital office which will lead the transformation of Pfizer into a digital powerhouse that will generate patient superior patient experiences that will result in better health outcomes.
As a member of the Digital Health Medicines and AI, Corporate Functions Data Solutions & Engineering team, you will help to realize Pfizer Digital strategy on the cloud by designing and deploying analytic solutions, work with high performing teams, and building lasting relationships with our business colleagues. This digital program will provide you with solid foundational understanding of the data types that drives business insights through data science and enterprise BI reporting solutions.
This role requires you to provide solution development and operations support for our Regulatory Quality Assurance (RQA) Domains (Audit Planning, Audits Made Easy, RQA Metrics, etc.). It will require you to maintain, enhance and provide operations support of our RQA solutions while leveraging our Data Hub, Dataiku, Snowflake, and data visualization product (Spotfire).
As the Senior Associate RQA Data Engineer, you will help in the pursuit of the Pfizer Digital strategy on the cloud by designing data products that drive business insights. You will be hands on and have opportunities to lead a group of vendors resources. You will be expected to partner with Digital Colleagues, (Enterprise Architecture, Client Partners, and Data Scientists, etc.) and Business Unit stakeholders to develop and sustain the RQA analytics strategy and data architecture. Data engineers also partner with solution development teams to ensure use case delivery goals are met while adhering to data architecture principles, guidelines, and standards.
Role Responsibilities
Reporting to the Senior Manager, Finance Sales Procurement Solution Delivery & Engineering, and as the Senior Associate RQA Data Engineer you will be responsible for data modeling, building/enhancing data interfaces, web applications, and visualizations that deliver insights that drive impactful business outcomes.
High level responsibilities may include (but are not limited to):
Leading the gathering, analysis and documentation of business and technical requirements
Create test plans, test scripts, and perform data validation
Enhance the design of the existing RQA Cloud Data Lake, Dataiku workflows, interface APIs, Web Apps, and our Spotfire dashboards.
Design automated solutions for building, testing, monitoring, and deploying ETL tools.
Develop internal APIs and DSS data solutions for thousands of end-users to power Audit applications and promote connectivity.
Opportunities to lead development teams that are driven to build impactful data products and visualizations
Perform root cause analysis and resolve Level 3 production and data issues
Coordinate with backend engineering team to analyze data to improve the efficiency, and speed of the application as well maintain elevated levels of data quality and data consistency.
Tune SQL queries, reports and ETL pipelines
Build and maintain data dictionary and process documentation
Present solutions to leadership, management, architects, and developers.
Work in conjunction with our cloud engineering staff, and partner with project managers, and analysts to deliver insights to the business
Professional Experience and Educational Requirements
Required:
Applicant must have a Bachelor’s degree with three years of relevant experience; OR Master’s degree with one year of relevant experience; OR Associate's degree with six years of relevant experience; OR eight years of relevant experience with a high school diploma or equivalent
At least 2 years of experience in data engineering, and/or reporting & analytics
Fundamental understanding of Data warehousing, data modeling, and data transformation
Exposure to web application development platforms such as Angular/Spring Boot; this is a must have.
Fundamental understanding of Cloud data warehouse solutions (Snowflake, Redshift, Spark, etc).
Working experience with one or more general purpose programming languages, including but not limited to: SQL, Java, Scala, Python, or JavaScript
Nice to Have:
Prior experience with data preparation and ETL: Dataiku, Informatica, Talend, Alteryx, etc is a major plus
Understanding of compliance and audit processes for GxP, SOX etc. is a plus
Cloud computing, machine learning, text analysis, NLP & Web development experience is a plus
Knowledge of Graph Databases (Neo4j, Titan, etc.) and query syntax is a plus
Experience with Semantic technologies and approaches is a plus
Full stack development using infrastructure cloud services (AWS preferred) and cloud-native tools and design patterns (Containers, Serverless, Docker, etc) is a plus and Open-Source technologies is a plus
Experience with sourcing and modeling data from application APIs
Prior experience with AWS Cloud stack (ECC, S3, Redshift) or Google Cloud Platform
Experience designing complex and inter - dependent data models for analytic, Machine learning use cases
Experience with Software engineering best-practices, including but not limited to version control (Git, TFS, Subversion, etc.), CI/CD (Jenkins, Maven, Gradle, etc.), automated unit testing, Dev Ops
Professional and Leadership Characteristics
Creative: Able to bring forth innovative ideas to improve our existing practices and takes calculated risks to innovate new capabilities within Pfizer Digital Business Analytics, with a focus on data products and analytics solutions
Analytical Thinker: Understands how to synthesize facts and information from varied data sources, both new and pre-existing, into discernable insights and perspectives; takes a problem-solving approach by connecting analytical thinking with an understanding of business drivers and how BA can provide value to the organization
Adaptable: Demonstrates flexibility in the face of shifting targets, thrives in new situations
Pioneering: Pushes self and others to think about new innovation and digital frontiers and ways to conquer them
Ambiguity Tolerant: Successfully navigates ambiguity to keep the organization on target and deliver against established timelines
Strong Data and Information Manager: Understands and uses analytical skills/tools to produce data in a clean, organized way to drive objective insights
Exceptional Communicator: Can understand, translate, and distill the complex, technical findings of the team into commentary that facilitates effective decision making by senior leaders; can readily align interpersonal style with the individual needs of customers
Highly Collaborative: Manages projects with and through others; shares responsibility and credit; develops self and others through teamwork; comfortable providing guidance and sharing expertise with others to help them develop their skills and perform at their best; helps others take appropriate risks; communicates frequently with team members earning respect and trust of the team
Proactive Self-Starter: Takes an active role in one’s own professional development; stays abreast of analytical trends, and cutting-edge applications of data

Work Location Assignment: Flexible
The annual base salary for this position ranges from $72,700.00 to $121,200.00. In addition, this position offers an annual bonus with a target of 7.5% of the base salary. Benefits offered include a retirement savings plan, paid vacation, holiday and personal days, paid caregiver/parental and medical leave, and health benefits to include medical, prescription drug, dental and vision coverage in accordance with the terms and conditions of the applicable plans. Salary range does not apply to the Tampa, FL location.
Relocation assistance may be available based on business needs and/or eligibility.
Sunshine Act
Pfizer reports payments and other transfers of value to health care providers as required by federal and state transparency laws and implementing regulations. These laws and regulations require Pfizer to provide government agencies with information such as a health care provider’s name, address and the type of payments or other value received, generally for public disclosure. Subject to further legal review and statutory or regulatory clarification, which Pfizer intends to pursue, reimbursement of recruiting expenses for licensed physicians may constitute a reportable transfer of value under the federal transparency law commonly known as the Sunshine Act. Therefore, if you are a licensed physician who incurs recruiting expenses as a result of interviewing with Pfizer that we pay or reimburse, your name, address and the amount of payments made currently will be reported to the government. If you have questions regarding this matter, please do not hesitate to contact your Talent Acquisition representative.
EEO & Employment Eligibility
Pfizer is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Pfizer also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Pfizer is an E-Verify employer.
Information & Business Tech
#LI-PFE","$96,950 /yr (est.)",10000+ Employees,Company - Public,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,1849.0,$10+ billion (USD)
"Solect
3.4",3.4,"San Francisco, CA",Senior Data Engineer,"Job Locations US-CA-San Francisco | US-TX-Houston
Posted Date 1 month ago(4/7/2023 5:15 PM)
# of Openings
1
Category
Enterprise Technology
Job ID
2023-2250
Overview
Company Overview
Pattern Energy is a leading renewable energy company that develops, constructs, owns, and operates high-quality wind and solar generation, transmission, and energy storage facilities. Our mission is to transition the world to renewable energy through the sustainable development and responsible operation of facilities with respect for the environment, communities, and cultures where we have a presence.

Our approach begins and ends with establishing trust, accountability, and transparency. Our company values of creative spirit, pride of ownership, follow-through, and a team-first attitude drive us to pursue our mission every day. Our culture supports our values by fostering innovative and critical thinking and a deep belief in living up to our promises.

Headquartered in the United States, Pattern has a global portfolio of more than 35 power facilities and transmission assets, serving various customers that provide low-cost clean energy to millions of consumers.
Responsibilities
Job Purpose
Pattern Energy is embarking on a major business transformation to scale processes and systems to enable the significant growth of our business. One critical aspect of this transformation is building an enterprise data architecture capable of supporting strong enterprise analytical capabilities and interconnected data across functions for improved and real-time decision making. This new role of Senior Data Engineer will be instrumental in building this future.

The Senior Data Engineer will help guide Pattern’s journey by building novel and modern tools, pipelines, and data systems. Through lived experience this role will act in a key strategic resource in our transition from siloed, disparate data assets to a grand, unified, data estate. The senior data engineer will lead development of data systems, namely data lakehouse pipeline, and analytics environments to support data lifecycles across the business. These tools will help drive our efforts toward democratization of data, reducing duplication of effort, and adding value to the data stream.

Key Accountabilities
Work with the Data team to understand and interpret use cases around the business and develop tools/systems to support data lifecycle from the point of production to the point of consumption.
Implement modern technology and concepts to enable Pattern teams’ design of enterprise-grade systems capable of achieving data goals.
Develop pipelines and analytics tools, including automations wherever possible
Templatize these solutions to help our adoption of infrastructure-as-code
Qualifications
Experience/Qualifications/Education Required
Previous experience in building enterprise data lifecycle and cloud solutions.
5-7+ years experience, specific to data management, analytics, or data reporting.
B.S. in a technical field with M.S. preferred.
Significant experience delivering Data solutions using the Azure cloud stack
Experience with database, data lake, lakehouse design concepts and the use of SQL.
Experience with transformations and reporting solutions, namely Power BI
Experience with environment management, release management, code versioning, deployment methodologies, and CI/CD tools
Additional Requirements

Demonstrated excellence and ability to learn new programming skills and languages.
Strong understanding of security and governance principles, including access policies
Proven record of excellent communication and cooperation with varied stakeholders.
Renewable energy industry experience is strongly preferred.
Demonstrated experience building scalable data models & ingestion pipelines from a variety of systems including Enterprise Applications (e.g. ERP, CRM), Operational Assets (e.g. SCADA), Big Data (e.g. Weather or product configuration simulators).
Technical Skills
Experience throughout the Microsoft Integrated Data Platform (IDP) and related stacks including, but not limited to:
Azure, AAD, Synapse, Purview, Data Lake, Delta Lake, and Lakehouse design
Azure Data Factory, Data Explorer, Logic Apps, Power Automate
Databricks; PySpark, and optimized resource consumption, therein
The expected starting pay range for this role is $95,000 - $129,000 USD. This range is an estimate and base pay may be above or below the ranges based on several factors including but not limited to location, work experience, certifications, and education. In addition to base pay, Pattern’s compensation program includes a bonus structure for full-time employees of all levels. We also provide a comprehensive benefits package which includes medical, dental, vision, short and long-term disability, life insurance, voluntary benefits, family care benefits, employee assistance program, paid time off and bonding leave, paid holidays, 401(k)/RRSP retirement savings plan with employer contribution, and employee referral bonuses.

Pattern Energy Group is an Equal Opportunity Employer.","$112,000 /yr (est.)",51 to 200 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,2009.0,$25 to $100 million (USD)
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,,Unknown / Non-Applicable
"Nisum
4.0",4.0,Remote,Data Engineer GB4790,"Location: Remote, USA
Team: Data Science & Analytics
Work Type: Full Time
Nisum is a leading global digital commerce firm headquartered in California, with services spanning digital strategy and transformation, insights and analytics, blockchain, business agility, and custom software development. Founded in 2000 with the customer-centric motto “Building Success Together®,” Nisum has grown to over 1,800 professionals across the United States, Chile,Colombia, India, Pakistan and Canada. A preferred advisor to leading Fortune 500 brands, Nisum enables clients to achieve direct business growth by building the advanced technology they need to reach end customers in today’s world, with immersive and seamless experiences across digital and physical channels.

What You'll Do
Defines, designs, develops, and test software components/applications using Spark, Sql, and PySpark.
Building solutions using a variety of open-source tools Microsoft Azure services and a proven track record in delivering high-quality work to tight deadlines.
Ability to work with the customer as part of the Agile model of delivery.
Design and Build Modern Data Pipelines and maintain data warehouse schematics, layouts, architectures, and relational/non-relational databases for data access and Advanced Analytics
What You Know
Hands-on distributed computing development experience using PySpark, Spark SQL, and Databricks delta tables.
Should have experience in structured streaming, stateless, and state full.
Familiarity with Azure resources (AKV, Managed identity, SPN, ADLS, etc..).
Must have good craftsmanship skills including unit testing, and code quality, and be a creative thinker/problem solver.
Experience in CICD and operation tooling integrations is a plus.
#Li-Remote
Education
Bachelor’s degree in Computer Science, Information Systems, Engineering, Computer Applications, or related field
Benefits
In addition to competitive salaries and benefits packages, Nisum US offers its employees some unique and fun extras:
Professional Development - We offer in-house technical training and professional learning programs aimed at developing skills across a broad spectrum of topics such as technology, leadership, role-based training, and process expertise. We also offer an annual stipend for employees to attend external courses in order to maintain professional certifications
Health & Wellness Benefits - We believe that your health and welfare are important, and we strive to ensure that you have affordable options available to you, including some plans that are subsidized for employees and their families by up to 90%. We also have dental and vision plans in the US where Nisum pays 100% of premiums for employees
Volunteerism Pay - We believe in giving back and in the US, our employees are eligible for up to 40 hours of paid time off each year to volunteer towards the causes that they are most passionate about. This is in addition to personal PTO and paid holidays
Additional Benefits - We offer all the other important benefits to keep employees and their families healthy and financially secure, such as 401(k) retirement savings with a company match, pre-tax parking and transit programs, disability insurance, and Basic Life/AD&D, alongside exclusive employee discounts on a wide variety of products and services
Compensation Band
$120-125k per year
Nisum is an Equal Opportunity Employer and we are proud of our ongoing efforts to foster diversity and inclusion in the workplace.","$122,500 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2000.0,Unknown / Non-Applicable
"CEDENT
5.0",5.0,"Phoenix, AZ",Big Data Support Engineer for cloud migration project-Remote,"Title: Big Data Support Engineer for cloud migration project @ Phoenix, AZ or Remote
Terms of Hire: 12 months.
Salary: $ Open K/ YR + Benefits.
Job Summary

Years of Experience: 5 – 7
Bachelor’s Degree: Computer Science or engineering
Start Date: ASAP
Interview Type: Video

Job duties:
Big Data Engineer position for cloud migration project
Writing pyspark & hive code to build use case.
Design solution
Troubleshoot production issue
Discussing business requirement with product team
Write and Execute test case.
Skill Requirements:
5 to 7 years of experience in big data space
Should be good in Hadoop concepts and technology
Hands on working experience on Pig, Hive, Sqoop, Spark, Python, Shell scripting.
Good understanding of Data processing using big data tools.
Experience in optimizing Hadoop jobs and spark jobs
Experience and knowledge of building REST web services.
AWS cloud experience is must
Programming experience in UNIX and Python
Good communication skill
SHOULD HAVE EXPERIENCE WITH OOZIE SCHEDULING
SHOULD BE FAMILIAR WITH AGILE METHODOLOGY
SHOULD HAVE STRONG HOLD ON SQL QUERIES
SHOULD BE AWARE OF USING GIT REPO AN D RESPECTIVE COMMANDS

You Will Enjoy:
An opportunity to be a part of a great culture, an awesome team, a challenging work environment, and some fun along the way!
Apply today to learn more and be part of our Growth story.
All applications will be kept strictly confidential and once shortlisted, our team will be in touch with you for further discussions.","$81,771 /yr (est.)",1 to 50 Employees,Contract,Information Technology,Computer Hardware Development,,Less than $1 million (USD)
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
WellTrust Medical Group,,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",,,,,,,
"Oak Ridge National Laboratory
3.6",3.6,"Oak Ridge, TN",Geospatial Data Engineer,"Requisition Id 7635
Overview:
The Geospatial Science and Human Security Division (GSHSD) is seeking a geospatial data engineer within the Location Intelligence group. This position is within the Division’s Human Dynamics section and part of the National Security Science Directorate. The Location Intelligence Group performs ground-breaking research into place-based knowledge extraction, narrative analysis, and geosocial data discovery. Leveraging primarily “non-traditional” geospatial data sources such as volunteered geographic information (VGI), internet of things (IoT) data, and telemetered sensor sources, and harmonizing with commercial and open data sources, the Location Intelligence group delivers novel approaches and technologies for describing places, points of interest, events, activities, and populations, as well as the patterns of interaction among the three. The group’s dynamic content portfolio addresses the timely need to broaden research in spatial data curation, multi-scale land-use modeling, generating higher-level features from ground photo imagery, disinformation detection, network spatialization, narrative analysis, and transformation to describe the landscape and patterns of activity within it, using novel and non-traditional data and theoretical techniques.

This position involves developing and deploying scalable geospatial applications supporting the group’s R&D portfolio. The position’s focus includes architecting geospatial services, working with the CI/CD deployment teams, performing stress testing of deployed services and applications, cloud-to-premise interfacing, and distributed computing to deploy real-time geospatial intelligence applications. The individual will collaborate with other scientists in developing transformative technologies that address these challenges through teaming and collaborative efforts. The work will involve harvesting and processing several hundred disparate data sources from all around the world. These sources contain text, sensors, ground-level imagery, social media, trajectory, and travel data, among others. Enabling this data for purposeful use requires designing and architecting RESTful services and distributed failover backend services. To confidently run them, individuals are required to have a good background in the following initiatives: Architect and design Spring boot and Java Application, open-source exploitation; geospatial OGC services architecture and development, Java and object-oriented concepts, application server failover and load balancing, and geospatial formats such as GeoJson and GeoPackage; and a working knowledge of GIS methodologies.

Our commitment to diversity:
As we strive to become the world’s premier research institution in the sciences and technologies that underpin critical national security missions, we are committed to creating an inclusive environment that highly values a diverse workforce. We recognize that breadth of perspectives, insights, and experiences is necessary to drive innovation and discovery mission-critical to national security sciences. Our commitment extends beyond our workforce to the next generation of researchers with STEM education outreach that seeks to engage a diverse range of students.

Major Duties and Responsibilities:
Architect services on NoSQL data (ElasticSearch), OGC GeoServer configuration, Java and Object-oriented application architecture fluency, Maven built Spring Boot, Docker, Kubernetes, CI/CD pipeline design and deployment.
Design and develop scalable and distributed data and backend services for GEOINT purposes.
Deployment and CI/CD and perform benchmark analysis and examination of results and logs from failover and robust service deployment.
Performing post-deployment logistics.
Examine new and developing technologies and tools to see if they can be used on the job.
Manage and update existing data format and perform regular improvements.
Augment existing research and development activities in location intelligence and association analysis.
Develop and support the plan for project data analysis and management.
As needed, provide technical assistance to other employees and customers.

Basic Requirements
Bachelor’s degree or higher in computer science, mathematics, or a related field and 2+ years of related experience.
Prior experience with J2EE application development, microservices, distributed Java Caching, regular expressions, web-based technologies.
Prior expertise with REST data services, APIs, and microservices as part of a service-oriented architecture (SOA).
Proficiency in open-source or commercial geospatial tools such as ArcGIS, QGIS, and PostGIS.
Prior expertise with relational (MS SQL Server, MySQL, PostgreSQL, and so on), document (JSON, ElasticSearch, and so on), spatial, graph, and other unstructured databases is required.
Experience with Python, R, and other similar languages for data manipulation, exploration, graph analysis, and statistical analysis.
Hands-on experience using cloud-enabled technologies and platforms that includes Google Cloud and AWS.
Excellent written and verbal communication and demonstrated ability to work in interdisciplinary teams.

Preferred Qualifications:
Agile software development approaches are a plus.
Working knowledge of both structured and unstructured data.
Working knowledge of version control systems such as Git.
Working knowledge of automated deployment and testing environments is a plus.
UML, Object-oriented programming, Linux, shell scripting, and other related skills are required.
Containerization technologies such as Docker and Conda have been used to design and manage computing environments.
Experience with geospatial ETL process design and implementation
Experience with database technologies such as MySQL and ElasticSearch to store, analyze, and manipulate data.
Experience in statistics, computational sciences, activity and event data, and working with social media data.

This position will remain open for a minimum of 5 days after which it will close when a qualified candidate is identified and/or hired.
We accept Word (.doc, .docx), Adobe (unsecured .pdf), Rich Text Format (.rtf), and HTML (.htm, .html) up to 5MB in size. Resumes from third party vendors will not be accepted; these resumes will be deleted and the candidates submitted will not be considered for employment.

If you have trouble applying for a position, please email ORNLRecruiting@ornl.gov.

ORNL is an equal opportunity employer. All qualified applicants, including individuals with disabilities and protected veterans, are encouraged to apply. UT-Battelle is an E-Verify employer.","$77,071 /yr (est.)",5001 to 10000 Employees,Government,Government & Public Administration,National Agencies,1943.0,Unknown / Non-Applicable
"Macy’s
3.4",3.4,"Johns Creek, GA",Lead Data Engineer,"Macy's, Inc is building an Enterprise Data & Analytics team to further grow our capabilities in support of our mission to be a data - led, customer centric company. This team will focus on accelerating impact from analytics, coordinating an enterprise-wide roadmap, and ensuring proper data governance and management. As a member of this team, the engineer will help lead the charge to execute on our vision to build profitable lifetime customer relationships by embedding data & analytics at the heart of everything we do.
Position Overview:
The Lead Data Engineer is responsible for development and support of data products on a modern cloud-based data lake, leveraging expertise and knowledge of multiple technologies & data domains to help build a robust, scalable, and reliable data engineering platform.
The Lead Data Engineer is responsible for providing data services for enterprise-grade analytical environments, utilizing automated data pipelines at scale, and streamlining efficient data transformations for priority use cases, be involved hands on in development of the codebase and partner closely with business units and peer technology groups to support analytics execution.
Essential Functions
Solution Design & Implementation:

Work closely with business stakeholders, implement scalable solutions to meet requirements
Follow and improve existing processes and procedures
Lead a pod of data engineers, providing both technical oversight and supporting their growth
Build, maintain and simplify enterprise data pipelines with emphasis on reusability & data quality
Work with Legal and Privacy teams to adhere to data privacy and security requirements

Culture:
Train and mentor fellow engineers on both technical stack and data domain specifics
Establish a pro-active approach to data management, ensuring business stakeholders & platforms can access required data within the SLA window
Drive change management to increase user adoption of enterprise data repositories and leverage standardized data pipelines across use cases
Increase agility in identifying data issues and taking action to remediate

Qualifications
Education/ Experience:

Data engineering experience with:
3+ years of experience in designing and implementing cloud-based data solutions
3+ years of experience integrating with analytics reporting solutions (e.g. Tablaeu, PowerBI)
3+ experience with big data processing technologies such as Hadoop, Spark, etc.
5+ years of experience building & automating ETL data pipelines using enterprise grade tools
5+ years of experience building enterprise-grade data warehouses (either on-prem or on cloud)
8+ years of overall programming experience, including recent experience with Python & SQL
Ability to effectively share technical information, communicate technical issues and solutions to all levels of business stakeholders
Customer-centric and experienced with cross-functional collaboration
Excellent written and verbal communication skills

What we can offer you:

Exciting, challenging problems to solve - you'll never have a boring day at work
A refreshingly fun work environment where you will collaborate with a smart and talented team
Unique freedom to build and lead a team in next gen thinking
A chance to learn and participate in the growth of NYC’s largest retailer

This job description is not all inclusive. Macy’s Inc. reserves the right to amend this job description at any time. Macy's Inc. is an Equal Opportunity Employer, committed to a diverse and inclusive work environment.","$161,600 /yr (est.)",10000+ Employees,Company - Public,Retail & Wholesale,"Department, Clothing & Shoe Stores",1858.0,$10+ billion (USD)
"Fathom Management LLC
2.0",2.0,Remote,Sr. Data Engineer Remote Opportunity,"Sr. Data Engineer

seeking a Senior Data Engineer who possesses expert level knowledge of appropriate data sources to address the specific requirements of projects for data modeling. Understand business requirements and translating into technical work. Design and implement features in collaboration with team engineers, product owners, data analysts, business partners using Agile/SCRUM Methodology.

This is a full- time position / 100% Remote.
The salary range of $140,000 - $160,000 will be based on technical experience and technical interview.

Responsibilities:

Ability to build programs or systems that can take data and turn it into meaningful information that can be studied.
Build ETL/ELT jobs and workflows to combine data from disparate sources.
Install continuous pipelines of huge pools of filtered information so that data analyst/scientists can pull relevant data sets for their analyses.
Build data workflows using SQL Server Integration Services (SSIS)
Build data workflows using Microsoft Azure (Azure Data Factory, Storage Accounts, Synapse)
Build data workflows using Databricks
Collaborate with business stakeholders, business operations, and product engineering teams in analyzing business problems, in building and testing solutions and data models.
Experience implementing and operating analytic models and services.
Document the current-state and target-state software architecture and create roadmap plans for success on various software components.
Assist in the design, implementation, and maintenance of complex solutions.
Build systems that collect, manage, and convert raw data into usable information for business analysts to interpret.
Make data accessible for evaluation and optimization
Collaborate with business stakeholders, business operations, and product engineering teams.
Coordinate activities with other technical personnel as appropriate.
Works with back-end data and develops tables using SQL scripts, SSIS, and SSMS.
Experience with Azure cloud platforms and Data bricks

Required Experience and Education:
Master's degree in computer science, systems engineering, or related technical discipline is preferred with 7-10 years of experience as a Data Engineer/Administrator or similar role. OR , B.S. in Computer Science with 15 years of relevant experience.

Benefits Overview: Full-time employees are offered comprehensive and competitive benefits package including paid vacation, sick leave, holidays, health insurance, life insurance, military leave, training, tuition reimbursement, a wellness program, short- and long-term disability, 401(k) retirement plan with company matches/immediate vesting, commuter benefits, and more.
COVID Policy: In accordance with the Federal Executive Order on Ensuring Adequate COVID Safety Protocols for Federal Contractors, this position requires that you are fully vaccinated at least 2 weeks before your start date. You will be required to provide proof of vaccination before you begin employment.
EEO Policy: It is our policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits, and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability, or any other characteristic protected by applicable federal, state or local law.","$150,000 /yr (est.)",1 to 50 Employees,Self-employed,,,,Unknown / Non-Applicable
"Farm Credit East
3.9",3.9,"Enfield, CT",Data Engineer,"Be part of a team focused on the success of our customers, the success of our communities, and the success of each other. Farm Credit East is the leading provider of loans and farm advisory services to farm, forest product, fishing, and other agricultural business owners across the northeast. We are One Team Working Together with a focus on our five pillars: Outstanding Customer and Employee Experience, Quality Growth, Operational Excellence, Commitment to our Communities, and Protecting Customer Information.

Position Summary
The Data Engineer is responsible for cleaning, managing, and sharing data that guides business decisions. Using ETL tools you will gather data from a variety of sources, checking for anomalies, automating processes, and generally making it easier for business stakeholders to generate valuable insights. This position will collaborate with internal and external organization to capture requirements, design, create, document, manage, and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.

Duties and Responsibilities
Work with product stakeholders to implement, maintain, and enhance data models and solutions used to define and measure quality of data domains.
Design data models to meet requirements
Perform ETL (Extract, Transform, and Load) on data to meet stakeholder specifications.
Design and develop data access methods, datasets, views etc.
Develops data modeling and is responsible for data acquisition, access analysis, archive, recovery, load design and implementation.
Coordinates new data developments to ensure consistency with existing warehouse structure.
Collaborates with internal customers to capture requirements, design, create, document, manage and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.
Assists with the development, implementation, and maintenance of front-end presentation (dashboards), automated report solutions and other BI solutions to support tactical and strategic reporting needs of the organization.
Assists in identification of data integrity problems and recommends solutions.
Work collaboratively with key stakeholders both internally and externally, including but not limited to Senior Management, Business Unit Leaders, Knowledge Exchange, and Farm Credit Financial Partners (FPI).

Job Qualifications/Requirements
Bachelor’s Degree in Computer Science, Business, Finance, or other related field from an accredited University.
Experience with MSFT SQL Server
Microsoft Azure (Data Bricks, Data Factory, Logic Apps, Functions, etc.)
2 plus years of experience in Finance related informatics, performance measurement, or analysis with strong relational database SQL skills.
1 + years of experience using Microsoft Azure product to perform ETL
Familiar with Databricks Unity catalog

Farm Credit East is an Equal Opportunity Employer. As an Equal Opportunity Employer, we do not discriminate on the basis of race, color, religion, national origin, sex, sexual orientation, gender identity or expression, age, marital status, parental status, political affiliation, disability status, protected veteran status, genetic information or any other status protected by federal, state or local law. It is our goal to make employment decisions that further the principle of equal employment opportunity by utilizing objective standards based upon an individual's qualifications for a specific job opening. In compliance with the Americans with Disabilities Act (“ADA”), if you have a disability and would like a reasonable accommodation in order to apply for a position with Farm Credit East, please call 1-800-562-2235 or e-mail FarmCreditCareers@farmcrediteast.com .","$86,685 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Banking & Lending,1916.0,$100 to $500 million (USD)
United Digestive,,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time",,Unknown,Company - Public,,,,Unknown / Non-Applicable
"YSI
3.7",3.7,Remote,Senior Data Engineer,"Position Title: Senior Data Engineer/ Oracle Apex Developer
Job Id: 202301002
Location: Herndon, VA (Remote)
Yakshna Solutions, Inc., (YSI) is a CMMI Level 3 assessed, ISO 9001, 20000:1, 27001 certified, woman-owned small business enterprises, headquartered in Herndon, Virginia, USA. YSI provides professional IT solutions and services to business corporations and government organizations. YSI is committed to serve its business communities as a leading IT vendor providing innovative, quality, and cost-effective IT business solutions and services.
Our benefits are very competitive that include 401(k), health, dental, and vision insurance, Life insurance, short-term and long-term disability insurance, paid time off, training, and professional development assistance.
YSI is seeking a highly qualified Senior Data Engineer. The selected candidate will be able to communicate effectively (written/verbal), possess strong interpersonal skills, be self-motivated, and be innovative in a fast-paced environment.
Responsibilities:
The Data Engineer will be the senior technical expert on work associated with data management, data quality and data structures, coordinating with the ADA as needed to ensure development and data structures are synchronized.
The Data Engineer will design the approach for data tasks and will contribute to completion of data tasks and oversee execution, providing advice and guidance as necessary to junior staff.
Required Qualifications and Skills:
Bachelors or master’s in relevant filed.
Good Data Engineering/Management experience
· Should be familiar with the the Civil Works missions for Hydropower, Recreation, Environmental Stewardship, and Water Supply.
· Extensive technical knowledge of programming in a software stack that includes an Oracle Relational database, SQL, PL/SQL, Oracle Spatial, JavaScript, Oracle REST Data Services (ORDS), and Oracle APEX to make the necessary revisions to the relevant systems. In addition to these general skills and experience, also possess the following:
· Knowledge and experience in developing and maintaining a relational database operated in Amazon Web Services (AWS cloud).
· Knowledge and experience of data entry and options to increase automation and efficiency.
· Knowledge and experience with documenting data systems and processes and creating reports for increased efficiencies.
· Knowledge and experience in optimizing the performance of existing Oracle based applications, including procedures, functions, etc.
· Knowledge and experience with creating Representational State Transfer (RESTful) services utilizing common data dissemination formats.
· Knowledge and experience with making websites 508 compliant.
· Knowledge and experience with authentication and authorization procedures
· Knowledge and experience with maintaining geospatial data in oracle relational database.
· Knowledge and experience with geospatial web services (OGC & ESRI REST)
· Knowledge and experience with cloud native development methodologies
Job Types: Full-time, Contract
Pay: $110,000.00 - $120,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Herndon, VA 20170: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
Oracle Apex: 5 years (Preferred)
AWS CLOUD: 5 years (Preferred)
Erwin: 8 years (Preferred)
Data modeling: 8 years (Preferred)
Metadata: 5 years (Preferred)
Work Location: Remote","$115,000 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,2011.0,$1 to $5 billion (USD)
"Shutterfly
3.3",3.3,"Eden Prairie, MN",Senior Data Engineer,"Description
At Shutterfly, we make life’s experiences unforgettable. We believe there is extraordinary power in the self-expression. That’s why our family of brands helps customers create products and capture moments that reflect who they uniquely are.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.
What You'll Do as a Sr. Data Engineer II
Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines am improve the operation and performance of the Data Warehouse
The Skills You'll Bring
Expert knowledge Python, Spark, SQL scripting and Java
7+ years of hands-on experience with building data ingress frameworks and experience with dealing with a wide variety of source data formats
Hands-on experience in building data applications, including design, implementation, debugging, and support.
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it’s the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.","$132,365 /yr (est.)",10000+ Employees,Company - Private,Information Technology,Internet & Web Services,1999.0,$1 to $5 billion (USD)
Seamless Migration LLC,,Virginia,Data Engineer (Full Scope Poly),"ABOUT US
Seamless Migration is a Service-Disabled Veteran-Owned Small Business (SDVOSB) started in 2021 with the purpose of enabling businesses and organizations through automation. Our goal is to help organizations discover, implement, and maintain solutions which evolve and mature with their ever-changing business needs. We believe in applying agile methodologies in all aspects of our business practices and use these methods to ensure effective results for our clients.
US Citizenship (Required)
CLEARANCE
TS/SCI W/ Full Scope Poly
LOCATION
Northern, VA
OVERVIEW
The Sponsor seeks a Data Engineer who will be responsible for designing, developing, testing, and maintaining data infrastructure projects such as data warehouses, data lakes, ETL pipelines, and data governance frameworks.
The goal of this position is to ensure that our organization processes and manages data efficiently and accurately to support data-driven decision-making.
Key Responsibilities include but are not limited to:
Design, develop and maintain data architectures, pipelines, and data integration solutions.
Collaborate with data analysts, data scientists, and other stakeholders to understand their data requirements.
Implement and manage data quality and data governance initiatives to improve data accuracy, completeness and consistency.
Work with our DevOps and IT teams to ensure sufficient infrastructure and access to required data sources.
Monitor data infrastructure and perform necessary fixes or performance tuning.
Ensure data security and privacy are maintained.
Stay up to date with industry trends and technologies related to data engineering and recommend new approaches to improve data processing capabilities.
REQUIRED SKILLS
Proven experience as a Data Engineer or similar role.
Deep knowledge of data modeling, ETL pipelines, and data warehousing concepts.
Proficiency in big data technologies such as Hadoop or Spark.
Proficiency in programming languages such as Python or Java.
Familiarity with data visualization tools such as Tableau or Power BI.
Experience with building and managing data pipelines using cloud services such as AWS or Azure.
Knowledge of relational database systems and SQL.
DESIRED SKILLS
Bachelor's degree in computer science, computer engineering, or related field.
2+ years of experience in a data engineering or related role.
Experience with data modeling, ETL, and data warehousing.
Excellent analytical and problem-solving skills.
Strong communication and collaboration skills.
BENEFITS
6% 401k match (100% vested)
100% paid Medical, Dental, Vision
$3,000 HSA contribution
28 days' vacation (PTO, Federal Holidays, and Sick Leave)
Flexible Hours
Tuition and certification reimbursement
Growth opportunities within an emerging defense company
All your information will be kept confidential according to EEO guidelines","$187,500 /yr (est.)",Unknown,Company - Public,,,,Unknown / Non-Applicable
"Apple
4.2",4.2,"Cupertino, CA",Biomedical Data Engineer - Health Technologies,"Summary
Posted: Aug 8, 2022
Weekly Hours: 40
Role Number:200402289
The Health Technologies Team conceives and proves out innovative technology for Apple’s future products and features in health. We are seeking a highly capable Biomedical Data Engineer to join a multi-disciplinary team. Successful candidates will be able to integrate with our research study leads, data scientists and engineers to develop and support effective data analysis and machine learning workflows.
Key Qualifications
Experience with software engineering frameworks
Excellent coding skills in Python (e.g.,Pandas, Spark, Jupyter)
Workflow orchestrations (e.g., Airflow, Luigi)
Designing and maintaining (non-)relational databases (e.g. Postgres, Cassandra, MongoDB) and file systems (e.g. Parquet, CSV, JSON)
Great understanding of infrastructure designs
Linux, MacOS based development frameworks
iOS/ watchOS development (e.g., Swift, Objective-C)
Web Service APIs (e.g., AWS, REDCap, XNAT)
Version control frameworks (Git, virtualenv)
Familiarity with best practices for information security, including safe harbor privacy principles for sensitive data
Experience with biomedical sensors/platforms for measuring physiological signals in the health, wellness and/or fitness realms
Description
- Work closely with team members and study staff to design, build, launch and maintain systems for storing, aggregating and analyzing large amounts of data - Process, troubleshoot, and clean incoming data from human studies - Automate and monitor data ingestion and transformation pipelines, with hooks for QA, auditing, redaction and compliance checks per data management specifications - Create and maintain databases with existing and incoming clinical data - Architect data models and create tools to harmonize disparate data sources - Incorporate and comply with regulations as they pertain to electronic and clinical data and databases
Education & Experience
BS/MS in Computer Science, Engineering, Informatics, or equivalent with relevant 4+ years industry experience with biomedical, health or sensitive data.
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $104,000 and $190,000 annualized, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1976.0,$10+ billion (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Atlanta, GA",Lead Software Engineer (Data),"We have an opportunity to impact your career and provide an adventure where you can push the limits of what's possible.

Job summary

As a Lead Software Engineer at JPMorgan Chase, you are an integral part of an agile team that works to enhance, build, and deliver trusted market-leading technology products in a secure, stable, and scalable way. As a core technical contributor, you are responsible for conducting critical technology solutions across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Executes creative data solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems
Develops secure and high-quality production code, and reviews and debugs code written by others
Identifies opportunities to eliminate or automate remediation of recurring issues to improve overall operational stability of software applications and systems
Leads evaluation sessions with external vendors, startups, and internal teams to drive outcomes-oriented probing of architectural designs, technical credentials, and applicability for use within existing systems and information architecture
Leads communities of practice across Software Engineering to drive awareness and use of new and leading-edge technologies
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Formal training or certification on software engineering concepts and 5+ years applied experience
Hands-on practical experience delivering system design, application development, testing, and operational stability
Advanced in both hands on development and solutioning
Proficiency in AWS, Hadoop, Snowflake, Glue, Lake formation, etc.
Proficient in all aspects of data (migration, modernization, building platforms, etc.)
Advanced understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security
In-depth knowledge of the financial services industry and their IT systems
Practical cloud native experience

Preferred qualifications, capabilities, and skills
Experience in financial services industry, specifically Wealth Management
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans","$117,975 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799.0,$10+ billion (USD)
"Introba Inc.
3.0",3.0,"New Haven, CT","Physical Security Engineer, Data Centers (Embedded)","WHERE PASSION + PURPOSE ALIGN
Introba is the union of Integral Group, a global network of deep green engineers and consultants and Ross & Baruzzini, a technology, consulting, and engineering firm.

Together as Introba, the organization provides world-class building engineering design, analytic, and consulting services at all scales, specializing in net-zero first thinking. Through the cultivation of thought leadership, we deliver sustainable and forward-thinking solutions to the most complex design challenges facing the world’s leading clients and partners.
Job Summary
Introba is the union of Integral Group, a global network of deep green engineers and consultants and Ross & Baruzzini, a technology, consulting, and engineering firm.

Together as Introba, the organization provides world-class building engineering design, analytic, and consulting services at all scales, specializing in net-zero first thinking. Through the cultivation of thought leadership, we deliver sustainable and forward-thinking solutions to the most complex design challenges facing the world’s leading clients and partners.

We are currently seeking an embedded Physical Security Engineer (remote position) to join our team.

Our security engineers are responsible for the design and engineering of electronic security systems, product research, development of design guidelines, and creation of design documents. The embedded security engineer role is a full-time position dedicated to one of our most prestigious clients.

While the successful candidate will be an Introba employee, the entirety of their daily responsibilities will be to provide engineering services for data center related security tasks for our client. These data centers help support the way we work, live, and communicate, and are located all over the world.
Responsibilities & Qualifications
The embedded security engineer will work with our client security team to perform the following:
Review existing security system design standards and develop recommendations for improvements to the current security program
Evaluate new equipment and technologies related to security.
Develop proof-of-concept documents for new equipment and technologies.
Manage master security detail libraries in partnership with outside security consulting firms
Manage system wiring diagrams and develop new standards for security design packages
Coordinate new technology design requirements with existing design document standards
Provide routine updates and improve upon master security specifications (CSI formatted Division 28 specifications)
Minimum of 10 years of experience in the design or installation of electronic security systems
Understanding of construction documentation
Understanding of computer networks and telecommunications systems
Good documentation and technical writing skills
Familiarity with Microsoft Office products, including Microsoft Teams
Proficiency in the use of computer-aided design software platforms (Revit and AutoCAD) considered strongly beneficial
Experience designing, installing, or coordinating security systems specifically for data centers considered strongly beneficial
Education Requirements

BS in electrical engineering, computer engineering, or computer science extremely beneficial, though strong technical experience in the security industry may be considered to meet this requirement

Additional Information
Equal Opportunity Employer

Introba is an equal opportunity employer, and we prohibit discrimination and harassment of any type as protected by federal, state, or local laws. We celebrate diversity and are committed to creating an inclusive environment for all employees.

Introba is an equal opportunity employer, and we prohibit discrimination and harassment of any type as protected by federal, state, or local laws. We celebrate diversity and are committed to creating an inclusive environment for all employees. The Company and its employees are required to comply with all local health authority, legal or lawful client requirements. You should ask your manager about this prior to starting, should you be subject to the current or potential future requirements.
We offer a competitive remuneration package, wellbeing benefits and an Employee Assistance Program to provide mental wellbeing support, guidance and advice. Our hybrid working patterns and flexible hours help our people to achieve a healthy work-life balance.","$111,898 /yr (est.)",1001 to 5000 Employees,Company - Private,"Construction, Repair & Maintenance Services",Architectural & Engineering Services,,Unknown / Non-Applicable
"Koch Ag & Energy Solutions
3.8",3.8,"Wichita, KS",Project Data Engineer / Analyst,"Description
Koch Ag & Energy Solutions (KAES) is looking for a Project Data Engineer/Analyst who will be a primary point of contact for project cost forecasting, change management, and risk management on small to large capital construction projects and turnarounds. This opportunity utilizes data from multiple sources to extract insights and perform analysis.
Our Team

We support cross site, cross functional teams throughout the project lifecycle on projects and turnarounds across (up to) 6 plants throughout the US and a site in Canada.
This position works a Monday – Friday, 8-hour day, and is based out of Wichita, KS with expected travel about 25% of the time.
What You Will Do
Use critical thinking, analysis, curiosity, and collaboration throughout the following to enable cost competitive project delivery (but not limited to) functions:
Analyze per project actual cost, schedule performance, and estimate details to forecast final cost per project at the cost breakdown structure level.
Track project progress against the baselines (scope, cost, and schedule).
Partner with project teams to identify issues and risks early and develop proactive resolutions and mitigations.
Monitor purchase order commitments, manage change orders, and adjust forecasts to align with reality.
Prepare various charts, tables and reports with insights to understand and communicate the forecast including what has changed and why.
Works with team members in assessing data store solutions that supports dashboards, insights, and other analytical solutions.
Demonstrate initiative to improve project outcomes, data integrations, processes, and forecasting quality.
Share knowledge and solve problems across teams to improve project results across KAES.
Work around the field construction sites without assistance.


Who You Are (Basic Qualifications)
Minimum of 2 years of experience in Projects within construction/oil & gas/manufacturing industry OR Bachelor’s Degree in a Business Administration or Engineering field
Demonstrated ability to analyze trends/variances and determine root causes
Experience creating and maintaining automated reporting processes
What Will Put You Ahead
Experience on projects and/or turnarounds in an industrial or construction environment.
Experience with reporting and analytics software such as Alteryx, Tableau, PowerBI, etc.
Experience with project cost management (including forecasting, change management, WBS/CBS buildup, risk management, reporting, etc.)
Familiarity with Project Management and Project Controls tools (e.g. EcoSys, Maximo, P6 or similar project portfolio or scheduling tools).
Experience performing risk based contingency assessments and identifying key drivers of risks.
Experience influencing change across an organization.


Relocation may apply based on candidate.

At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate’s knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.
Hiring Philosophy
All Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.
Who We Are

As a Koch company, Koch Ag & Energy Solutions (KAES) is a global provider of value-added solutions for the agriculture, turf and ornamental, energy and chemical markets. From agriculture to energy, KAES makes things grow better with plant nutrient and biological technologies. Our team of innovators unleash their potential while developing the technologies that feed and power the world.
At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.
Our Benefits
Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.
Equal Opportunities
Equal Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please visit the following website for additional information: http://www.kochcareers.com/doc/Everify.pdf","$81,180 /yr (est.)",10000+ Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,1940.0,$10+ billion (USD)
"Capital One
4.2",4.2,"Richmond, VA",Senior Data Engineer - Principal Associate,"West Creek 2 (12072), United States of America, Richmond, Virginia
Senior Data Engineer - Principal Associate
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance

Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Spark, SQL, Python, Scala or Java
2+ years of experience with a public cloud (AWS)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
No agencies please. Capital One is an Equal Opportunity Employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, physical and mental disability, genetic information, marital status, sexual orientation, gender identity/assignment, citizenship, pregnancy or maternity, protected veteran status, or any other status prohibited by applicable national, federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",,10000+ Employees,Company - Public,Financial Services,Banking & Lending,1994.0,$10+ billion (USD)
Infosoft,,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,,,,Unknown / Non-Applicable
"CDNetworks Inc.
2.9",2.9,"Monrovia, CA",Jr. Data Center Engineer,"CDNetworks is one of the top leading CDN & Edge Service Providers with global offices in Korea, Japan, Singapore, Malaysia, China, Russia, London, and Canada. We focus on delivering integrated cloud and edge computing solutions with unparalleled speed, ultra-low latency, rigorous security, and reliability so that our clients can focus on what’s most important – growing their business.
The Jr. Data Center Engineer is a Full-Time position based in Monrovia, CA. Job is performed indoors in a Data Center or Warehouse environment. 75% Travel required for this position.
Job Responsibilities
Ensure all incidents are logged and resolved, gather all relevant data, and ensure all incidents and tasks follow the appropriate procedures.
Support data center activities and work closely with our system and network team to complete tasks/projects.
First responder to all alerts and problem reports while managing communications between departments and handling crisis documentation and dissemination after the fact.
Utilize internal systems such as JIRA/Wiki to manage project plans and progress.
Performing general system administration duties including OS patching and upgrades, batch job monitoring, system and hardware diagnostics, and other activities to ensure optimal health and performance of all systems as required.
Resolve complex problems related to Server and H/W areas.
Assisting/working closely with Network, System Engineers to configure customer requirements.
Physical deployment of network devices, servers, cables, etc.
Assembling/dissembling server hardware for deployment and OS installation and network equipment testing.
Maintain existing department and system documentation (update workflow, process, training documentation).
Other duties as assigned.
Abilities Required
Good verbal and written communication skills, and ability to work independently with minimum instruction.
Basic degree of mentorship, training, and direction team members skills.
Knowledge of IDC industry.
Intermediate degree of analytical and project management skills.
Other Features of Job
Job is performed indoors in a Data Center or Warehouse environment.
Language Skills
Excellent communication skills (English) – written and verbal. Bilingual Chinese or Korean is a plus!!
Job Type: Full-time
Salary: $20.00 - $25.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Schedule:
10 hour shift
8 hour shift
Evening shift
Monday to Friday
Night shift
On call
Overtime
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Monrovia, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Computer networking: 1 year (Preferred)
Shift availability:
Overnight Shift (Required)
Night Shift (Required)
Day Shift (Required)
Willingness to travel:
75% (Preferred)
Work Location: In person",$22.50 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000.0,$10+ billion (USD)
"Merkle
3.7",3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971.0,$500 million to $1 billion (USD)
Redzara.us,,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),,,,,,
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,,Unknown / Non-Applicable
Diamondpick,,"Alpharetta, GA",Java Software Engineer with Big-Data,"About us
There is someone out there who fits your requirements like a glove! We are an innovative Talent Solutions company with a vision to build talent supply chain models without compromise for the technology industry.
We are a new-age talent solutions company with a vision to build an enterprise recruitment model without compromise. We love to solve complex talent challenges and strive to be an enabler of business success for our clients using data, technology and our team’s vast experience in the technology market.
Senior Java Developer with Big data
Alpharetta, GA (Hybrid Work from Day1)
Job Description:
The ideal candidate must possess strong background on frontend and backend development technologies.
The candidate must possess excellent written and verbal communication skills with the ability to collaborate effectively with domain experts and technical experts in the team.
Responsibilities:
As a Java Senior Developer, you will
Maintain active relationships with Product Owner to understand business requirements, lead requirement gathering meetings and review designs with the product owner
Own his backlog items and coordinate with other team members to develop the features planned for each sprint
Perform technical design reviews and code reviews
Be Responsible for prototyping, developing, and troubleshooting software in the user interface or service layers
Perform peer reviews on source code to ensure reuse, scalability and the use of best practices
Participate in collaborative technical discussions that focus on software user experience, design, architecture, and development
Perform demonstrations for client stakeholders on project features and sub features, which utilizes the latest Front end and Backend development technologies
Requirements:
6+ years of experience in Java/JEE development
Skills in developing applications using multi-tier architecture
Knowledge of google/AWS cloud
Java/JEE, Spring, Spring boot, REST/SOAP web services, Hibernate, SQL, Tomcat, Application servers (WebSphere), SONAR, Agile, AJAX, Jenkins..etc
Skills in UML, application designing/architecture, Design Patterns..etc
Skills in Unit testing application using Junit or similar technologies
Good communication skills
Leadership skills
Provide overlap coverage with onsite/customer teams
Capability to support QA teams with test plans, root cause analysis and defect fixing
Strong experience in Responsive design, cross browser web applications
Strong knowledge of web service models
Strong knowledge in creating and working with APIs
Experience with Cloud services, specifically on Google cloud
Strong exposure in Agile, Scaled Agile based development models
Familiar with Interfaces such as REST web services, swagger profiles, JSON payloads.
Familiar with tools/utilities such as Bitbucket / Jira / Confluence
Job Types: Full-time, Contract, Temporary
Pay: From $60.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Day shift
Work Location: Hybrid remote in Alpharetta, GA 30005",$60.00 /hr (est.),,,,,,
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
39U0b7kJu8","$103,213 /yr (est.)",1 to 50 Employees,Company - Public,,,,Unknown / Non-Applicable
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015.0,$5 to $25 million (USD)
"Longevity Holdings Inc.
3.9",3.9,"Minneapolis, MN",Associate Data Engineer (Temporary),"As a Associate Data Engineer, you will treat data as an asset to design, build, and execute high performance and data centric solutions by using the comprehensive big data capabilities for the company's data platform environment. In this role, you will build and optimize data products to bring data and analytics products and solutions to businesses.
Essential Job Responsibilities:
· Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions
· Leading data collection efforts and performing trend analysis to identify common performance challenges that require further attention
· Partner closely with our data scientists to ensure the right data is made available in a timely manner to deliver compelling and insightful solutions
· Building out scalable data pipelines and choosing the right tools for the right job. Manage, optimize, and monitor data pipelines
· Incorporate core data management competencies including data governance, data security, and data quality
Required Skills:
· Bachelors in a quantitative field
· 1+ years of data engineering or equivalent experience
· Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices
· Demonstrated knowledge of relational data sets, structures, and SQL
· Familiarity with big data platforms such as Apache Spark, Hadoop, Kafka, etc.
· Experience leading data organization, dashboarding, & visualization efforts
· Inquisitive, proactive, and interested in learning new tools and techniques
· Excellent communication skills
Longevity Holdings Inc prohibits discrimination and harassment and will take affirmative action to employ and
advance in employment qualified individuals based on their status as protected veterans or individuals with
disabilities, race, color, religion, sex, national origin, sexual orientation, and gender identity.
Our privacy notice is available at https://longevity.inc/employment-privacy-notice
Job Type: Full-time
Pay: $20.00 - $30.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Will you now or in the future require sponsorship for employment visa status (e.g., H-1B visa status)?
Work Location: Hybrid remote in Minneapolis, MN 55402",$25.00 /hr (est.),51 to 200 Employees,Company - Private,Management & Consulting,Research & Development,1998.0,$25 to $100 million (USD)
"FreightWaves, Inc.
3.5",3.5,Remote,Senior Data Engineer,"Are you smart, driven, curious, resourceful, and not afraid to fail? Then we want to meet you! Our team of bold, innovative, and creative teammates is what makes us a top startup to work for. FreightWaves delivers news and commentary as well as data and analytics which empower risk management and actionable market insights in the logistics and supply chain industry. If you are ready to join our team, it is time for YOU to apply!
FreightWaves is on the hunt for a curious, tenacious, and team-oriented Senior Data Engineer to join our fast paced engineering team. The ideal candidate is inquisitive, versatile, team oriented, thrives on change, and has a positive attitude. If you are ready to be challenged, learn new and exciting technologies, and have the unique opportunity to work with some of the most talented developers in the country, we want you to apply!
**This position is fully remote.**
**Must RESIDE in the United States and be eligible to work.**
What you will be doing:
Implementing ingestion pipelines, using Airflow as the orchestration platform, for consuming data from a wide variety of sources (API, SFTP, Cloud Storage Bucket, etc.).
Implementing transformation pipelines using software engineering best practices and tools (DBT)
Working closely with Software Engineering and DevOps to maintain reproducible infrastructure and data that serves both API-only customers and in-house SaaS products
Defining and implementing data ingestion/transformation quality control processes using established frameworks (Pytest, DBT)
Building pipelines that use multiple technologies and cloud environments (for example, an Airflow pipeline pulling a file from an S3 bucket and loading the data into BigQuery)
Create and ensure data automation stability with associated monitoring tools.
Review existing and proposed infrastructure for architectural enhancements that follow both software engineering and data analytics best practices.
Working closely with Data Science and facilitating advanced data analysis (like Machine Learning)
What you bring to the table:
Strong working knowledge of Apache Airflow
Experience supporting a SaaS or DaaS product, bonus points if you were creating new data products/features
Strong in Linux environments and experience in scripting languages
Python Expert
Strong understanding of software best practices and associated tools.
Experience in any major RDBMS (MySQL, Postgres, SQL Server, etc.).
Strong SQL Skills, bonus points for having used both T-SQL and Standard SQL
Experience with NoSQL (Elasticsearch, MongoDB, etc.)
Multi-cloud and/or hybrid-cloud experience
Strong interpersonal skills
Comfortable working directly with data providers, including non-technical individuals
Experience with the following (or transitioning from equivalent platform services):
Cloud Storage
Cloud Pubsub
BigQuery
Apache Airflow
dbt
DataFlow
Bonus knowledge/experience:
Experience implementing cloud architecture changes
Working knowledge of how to build and maintain APIs using Python/FastAPI
Transforming similar data from disparate sources to create canonical data structures
Surfacing data to BI platforms such as Looker Studio
Data Migration experience, especially from one cloud platform to another
Certification: Professional Google Cloud Certified Data Engineer
Our Benefits:
An excellent work environment, flat hierarchies, and short decision paths.
Work from home
A generous benefits package including 100% employer-paid health, dental, vision and Life insurance, STD, LTD
Stock options
Appealing 401k matching plan
Career Mentorship Opportunities
Personal Development Credit (Can be used toward Student loans or relevant PD Courses)
Annual life achievement bonus of $2000 for having a baby, buying a house, or getting married (max one per year)
No set days off Vacation policy (our team takes time off as needed with supervisor approval)
Up to $50 for Gym or Virtual Gym membership.
Audible or Kindle Unlimited subscription
Discount on Ford vehicles
oYXkhYiWkU",,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2017.0,Unknown / Non-Applicable
"NexTek, LLC",,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",,,,,,,
"Argo Data
2.7",2.7,"Richardson, TX",Senior Data Engineer,"ARGO is a leading provider of software and analytics solutions for both the financial services and healthcare industries. ARGO transforms business processes for financial service providers and healthcare organizations using proven business models and software innovation informed by real customer challenges, breakthrough technology, and rich analytics.
ARGO- Data Engineer II (Markham, ON; Memphis, TN; ...) (theapplicantmanager.com)
JOB SUMMARY
The Data Engineer II is responsible for designing and implementing data solutions to support business needs. The successful candidate will have a strong background in software engineering and database design, as well as experience with working with cloud-based tools and technologies.
EXPECTED WORK AND PERFORMANCE
This section is intended to provide a more detailed overview of what the expectations of the job are to include: work to be performed, expected performance results and how they will be measured, as well as connection to corporate strategy, values, and mission. Job duties and responsibilities include the following. Other duties may be assigned as needed.
Design, build, and maintain data pipelines for data extraction, transformation, and loading (ETL) from various sources.
Ensure data security and privacy protocols are implemented and enforced.
Monitor and optimize data storage and processing performance to ensure high performance and cost-effectiveness.
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and translate them into technical solutions.
Maintain documentation of data systems, processes, and policies.
Stay current with the latest data tools, technologies, and trends and recommend improvements.
Mentor and train junior data engineers as needed.
QUALIFICATIONS
Bachelor's degree in Computer Science, Software Engineering, or related field.
3-5 years of experience in software engineering and SQL database design.
Proficiency in programming languages such as Python.
Strong understanding of data warehousing, ETL processes, and data modeling.
Experience with cloud infrastructure technologies such as Microsoft Azure.
Excellent problem-solving and analytical skills.
Ability to work well in a team-oriented environment and communicate effectively with both technical and non-technical stakeholders.
PHYSICAL DEMANDS
The candidate will work at our corporate headquarters in Richardson, TX. While performing the duties of this job, the employee is regularly required to stand; walk; sit; use hands to finger, handle, or feel; reach with hands and arms; talk or hear. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.
ADDITIONAL REQUIREMENTS:
Applicants for U.S. or Canadian based positions with ARGO must be legally authorized to work in the United States or Canada. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available at this time for ARGO positions.
Pre-employment background screening will be conducted.
Job Type: Full-time
Pay: $70,745.42 - $141,624.04 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richardson, TX 75081: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Experience:
Informatica: 3 years (Required)
SQL: 5 years (Required)
Data warehouse: 3 years (Required)
Python: 3 years (Required)
Azure: 3 years (Required)
Work Location: In person","$106,185 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,1980.0,$25 to $100 million (USD)
"Leadstack Inc
4.3",4.3,Remote,Lead Data Engineer with Sql/Python,"LeadStack Inc. is an award winning, one of the nation's fastest growing, certified minority owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Title: Lead Data Engineer with Sql/Python
Location: Remote
Duration: 6+months
Direct Client
Immediate interview
Top Skills: Azure, SQL/Python Developer.
Job Description:
Design and develop data system integrations and data models for reporting and alerting.
Essential Job Functions:
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state
Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions
Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Interpreting data, analyzing results using statistical techniques
Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality
Acquiring data from primary or secondary data sources and maintaining databases
Promote the reuse of data assets, including the management of the data catalog for reference
Key Responsibilities:
Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy
Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Mentor team members in data principles, patterns, processes and practices
Promote the reuse of data assets, including the management of the data catalog for referenceDraft and review architectural diagrams, interface specifications and other design documents
To know more about current opportunities at LeadStack , please visit us on https://leadstackinc.com/careers/
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Azure: 5 years (Preferred)
Data warehouse: 1 year (Preferred)
SQL developer: 6 years (Preferred)
Work Location: Remote",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016.0,Unknown / Non-Applicable
"Ikigai Labs, Inc.
4.9",4.9,"Cambridge, MA","Software Engineer, Data Engineering","Ikigai Labs is a fast growing startup founded out of MIT to empower data operators. We are building an easy to use AI augmented data processing and analytics platform on the cloud. Our users depend on us to automate, maintain, and enhance day-to-day mission critical operations. We are a team of talented, hardworking and fun-loving engineers, data scientists, and data analysts working towards the goal of building the next generation of data tools.
Job Description
JOB TITLE: Software Engineer, Data Engineer [Full-time]
LOCATION: Cambridge, MA
SUMMARY:
Ikigai Labs is seeking a dynamic and passionate engineer with strong software fundamentals to join a high-performing data platform development team. We are looking for a team player who is a quick learner, performs in a rapid development cycle, has a drive to surpass expectations, and an eagerness to share their work and knowledge.
We encourage applicants from all backgrounds and communities. We are committed to having a team that is made up of diverse skills, experiences, and abilities.
Technologies
Languages: Python3, SQL
Databases: Postgres, Elasticsearch, DynamoDB, RDS
Cloud: Kubernetes, Helm, EKS, Terraform, AWS
Data Engineering: Apache Arrow, Dremio, Ray
Misc.: Apache Superset, Plotly Dash, Metabase, Jupyterhub, Stripe, Fivetran
The Position
Design and develop scalable data integration (ETL/ELT) processes
Design and develop an on-demand predictive modeling platform with gRPC
Utilize Kubernetes to orchestrate the deployment, scaling and management of Docker containers
Utilize and learn various AWS services to solve cloud-native problems
Implement a testing platform which performs sanity check, load test, scale test, heartbeat test, and performance test
Provide periodic support to our customer success team
Qualifications
0-3 years of experience with a bachelor's degree in Computer Science, Math, or Engineering; or a master's degree
Experience with Python, AWS services, and/or ETL/ELT pipeline experiences
Experience with Kubernetes and/or EKS (optional)
Understanding of the fundamentals of design patterns and testing best practices
The ability to learn quickly in a fast-paced environment
Excellent organizational, time management, and communication skills
The desire to work in an AGILE environment with a focus on pair programming
Willingness to discuss obstacles, find creative solutions, and take initiative
The ability to receive and give both constructive and encouragement feedback","$102,419 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"Clinical Ink
4.4",4.4,Remote,Data Engineer,"Company Information
Clinical ink is the global life science company that brings data, technology, and patient science together to unlock clinical discovery. Our deep therapeutic-area expertise, coupled with Direct Data Capture, eCOA, eConsent, telehealth, neurocognitive testing, and digital biomarkers advancement, drive the industry standard for data precision and usher in a new generation of clinical trials. With offices in Philadelphia, PA, Winston Salem, NC, and Iowa City, IA, Clinical ink is rewriting the clinical development experience.

Job Description
Clinical ink is seeking a Data Engineer to join our Data Team based remotely across the United States! The Data Engineer will work to develop solutions used in applications for clinical trials. The ideal candidate will be a minimum of two years of experience as a software engineer and prior experience working with a variety of tools and frameworks. The Data Engineer's responsibilities include:
Develop data engineering solutions used in applications for clinical trial data collection that both make data available for further use and generate value out of data
Contribute to the methodology by which advanced analytics projects are delivered to clients and codify the tooling needed to support them
Build and support tools that allow data analysts and data scientists to work in complex projects
Implement quality, availability, and integrity of code, solutions, and respective systems and follow best practices related to data integrity, security, scalability, etc.
Participate in code inspections, reviews, and other activities to ensure quality
Qualifications
Bachelors in Mathematics, Statistics, Computer Engineering, Computer Science, or related field of study
2-5 years of experience in software engineering, working on multi-discipline teams
Experience with a variety of tools and frameworks such as Snowflake, Airflow, Spark, Kafka, RedShift, Sage Maker, Kubernetes, etc., AWS ecosystem (Lambda, Glue, S3, E2C, etc.), programming tools and querying languages (i.e., Python, C++, SQL, Scala, Java, etc.)
At least 2+ years of experience with Python
Data modelling and database development experience required
Data visualization experience preferred in Tableau and/or AWS QuickSight
Nice to have experience with issue tracking tools such as JIRA and Confluence
Ability to think creatively and take initiative; ability to learn new technical topics and develop new technical skills quickly
Willingness to learn and explore bleeding-edge/cutting-edge technologies
Additional Information
Clinical ink is an equal opportunity employer and does not discriminate against otherwise qualified applicants on the basis of race, color, creed, religion, ancestry, age, sex, marital status, national origin, disability or handicap, or veteran status.
www.clinicalink.com",,201 to 500 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2007.0,Unknown / Non-Applicable
"CliftonLarsonAllen
3.9",3.9,"Minneapolis, MN",Data Engineer,"The Data Engineer collaborates with internal team members to develop solutions that enable data, insights, analytics and actionable triggers and solutions. S/he utilizes a full range of data technologies including data modeling techniques, data architecture, engineering, data analysis, and reporting for a rapidly growing data analytics platform on Azure.
Essential Job Functions
Pipeline Building: Builds data pipelines utilizing Azure Databricks and Azure Data Factory to build a scalable solutions, following an existing framework and agreed upon design. Operationally monitors and maintains the systems to ensure pipelines continue to run successfully.
Data Solutions: Collaborates with other members of the Data Engineering team to analyze existing software programs and technology processes to ensure effectiveness and efficiency. Troubleshoots data-related problems as needed. Drives strategic initiatives and provides technology solutions for complex business problems. Assists in gathering and analyzing data for business projects, as well as mapping underlying processes and data flows.
New Technology Implementation: Assists with implementing new technology for firm use, as well as creating roadmaps and tools to guide and monitor usage.
Reporting: Routinely provides reports and data extraction as requested by other teams for use within the firm and on behalf of clients.

Requirements
Bachelor’s degree in computer science, mathematics, or related field required. (3 or more years’ experience in data engineering may be considered in lieu of Bachelor’s degree.)
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities.

Wellness at CLA
To support our CLA family members, we focus on their physical, financial, social, and emotional well-being and offer comprehensive benefit options that include health, dental, vision, 401k and much more.
To view a complete list of benefits click
here
.","$92,921 /yr (est.)",5001 to 10000 Employees,Company - Private,Financial Services,Accounting & Tax,1953.0,$500 million to $1 billion (USD)
"Syngenta Group
4.0",4.0,"Downers Grove, IL",Senior Master Data Engineer,"Company Description

Syngenta Group is one of the world’s leading sustainable agriculture innovation companies, with roots going back more than 250 years. Our 53,000 people across more than 100 countries strive every day to transform agriculture through tailor-made solutions for the benefit of farmers, society and our planet –making us the world’s most local agricultural technology and innovation partner.
Syngenta Group is committed to operating at the highest standards of ethics and integrity. This is a commitment that we are making to investors, customers, society and employees. Syngenta Group is also committed to maintaining a workplace environment free from discrimination and harassment.

Job Description

The Senior Master Data Engineer will be responsible for ensuring Syngenta has the right identity data capabilities to support the current and future Syngenta production and commercial needs.
We have the responsibility to think beyond our past needs and help unlock future opportunities with one of our most valuable data assets. Robust, accurate and trusted identity data will unlock opportunities to improve customer experience, simplify vendor interactions and allow us to explore new ways of marketing our products and services.
The investment in the MDG platform has been the first phase or our journey to support our current and future business needs. The Senior Master Data Engineer will be responsible to build on this first phase and help to define our vision, strategy, operating model, and roadmap for the future of Party data capabilities. This may include supplementing the MDG platform with additional technology and services
Responsibilities
Contributes to creating a breakthrough transformation that shapes the Party data (including Identity and Reference Data) capability in line with Syngenta's strategic vision
Help define & deliver the strategy and roadmap for Identity data and reference data (including operating model, data products, technology, data quality & process analytics/health) ensuring solutions and technologies are maintainable and scalable
Enroll and align with stakeholders to experiment and leverage the Identity data capabilities within relevant domains (Employee, Sales/Customers, Legal Entities, Intercompany, Vendors). Including simplification, automation, rationalization, and harmonization.
Create and advocate Identity data offers (data as a product) that add value and solve business problems that improve integration and adoption.
Provide technical leadership in leveraging and experimenting with appropriate technologies including existing platforms as well as new opportunities (e.g. Microservices, API’s, AI, ML, etc.) to create Identity data products and services to meet business needs.
Seamlessly embed themselves in a cross-functional teams as a subject matter expert and participate in Identity data design authority.
Contribute to the creation and implementation of a reference data capability the compliments Identity master data.
The preferred candidate will be near an established Syngenta location The right candidate will be considered for a remote setting IN THE UNITED STATES.
We are unable to provide Visa Sponsorship for this position at this time.

Qualifications

Required Skills / Experience
Bachelor’s degree with 8 or more years of relevant experience
MUST HAVE hands on experience with a popular MDM tool
Extensive experience in customer master data management
Must have thought leadership and the ability to influence the business with best practices
Experience in reference data principles and practices.
Understand relevant technologies (Master Data Management tools, Microservices, etc)
Stakeholder management / influencing: able to engage with different functions, leadership levels and cultures

Additional Information

What we Offer
A culture that celebrates diversity & inclusion, promotes professional development, and strives for a work-life balance that supports the team members
We offer flexible work options to support your work and personal needs
Full Benefit Package (Medical, Dental & Vision) that starts your first day
401k plan with company match, Profit Sharing & Retirement Savings Contribution
Paid Vacation, 9 Paid Holidays, Maternity and Paternity Leave, Education Assistance, Wellness Programs, Corporate Discounts, among other benefits
Syngenta is an Equal Opportunity Employer and does not discriminate in recruitment, hiring, training, promotion or any other employment practices for reasons of race, color, religion, gender, national origin, age, sexual orientation, marital or veteran status, disability, or any other legally protected status
Family and Medical Leave Act (FMLA)
(http://www.dol.gov/whd/regs/compliance/posters/fmla.htm)
Equal Employment Opportunity Commission's (EEOC)
(http://webapps.dol.gov/elaws/firststep/poster_direct.htm)
Employee Polygraph Protection Act (EPPA)
(http://www.dol.gov/whd/regs/compliance/posters/eppa.htm)

#LI-SB2","$100,820 /yr (est.)",10000+ Employees,Company - Private,Agriculture,Crop Production,2000.0,$100 to $500 million (USD)
"Axos Bank
3.6",3.6,"San Diego, CA",Junior Business & Technology Analyst - Data Engineer,"Job Summary and Opportunity:
This is an exciting opportunity to join a unique and immersive rotational program as a first step in your career in technology. This full-time rotational program is geared toward providing multi-software platform exposure that focuses on the expansion of knowledge and real-life application within each. We are seeking innovative and energetic individuals who are excited about expanding their skillsets and accelerating their career path with immediate exposure to software applications.
For this position, you will be in the Data Engineer Rotational Program where you will be joining the Axos' Center of ExcellenceTeam. You will get to be a part of a team responsible for the implementation of cutting-edge software driven solutions. As you progress through the program, you will rotate into different complimentary areas within the Data program where roles and responsibilities will change. The final goal of the program is permanent placement within your area of focus. For those looking to make an impact this is where it begins.
In this role you will be focused on SQL related software or software built on direct interactions with SQL. Through the different rotations completed, you will gain the knowledge and skills database development, data quality, and business intelligence reporting to provide enterprise level solutions.
This position is on-site and will be located at our HQ in San Diego, California.
Responsibilities:
Define, prepare, execute and implement data validation and unit and integration testing methods to ensure data quality
Create SSIS packages for data transformation, cleansing, caching, aggregation, staging, and transfer
Analyze and define data flow requirements and prepare applicable system documentation and operation manuals as needed
Code, test and maintain new and existing SQL jobs, stored procedures and functions
Performance tune existing stored procedures, tables and indexes
Troubleshoot problems that may come up with database environments: performance issues, replication issues, or operational issues
Review SQL code written by other developers to ensure compliance to coding standards and best practices as well as maximum performance
Perform data analysis and data profiling tasks to provide support and recommendations for development and design decisions
Develop standardized reporting dashboards to meet the needs of the multiple business units across the Bank
Apply advanced modeling, data mining, machine learning and/or statistical techniques to data and dashboards to generate actionable insights enabling informed decision-making for optimized business and operational performance
Create mock-ups of reporting products, scorecards, dashboards, etc. to provide visualization to the end user
Work with teams within the organization to gather and document reporting requirements
Join client meetings to communicate status, give demos, provide timelines and offer insights
Participate in daily meetings that go over testing, and code reviews
Work with IT, Enterprise Data Management, Project Managers, Business Analysts, stakeholders across multiple business units to systematically plan the launch of new or enhanced dashboards, prepare launch collateral/documentation and work closely with users during through the different phases of a project
Develop deep understanding of the Bank's databases, identify appropriate data sources, relationships and logic needed to produce consistently reliable reports
Contribute to the overall strategy and quality of dashboarding
Document process steps of repetitive tasks performed
Partner with IT and other Infrastructure teams to tackle software upgrades, and coordinate testing
Perform any additional duties as assigned
Requirements:
Bachelor's degree in Information Technology, Computer Science, Business Administration, Mathematics or a related discipline
Customer Obsession: ""Good enough"" isn't good enough for you. You're obsessed with perfecting the customer experience
Leadership: A confident person with the ability to connect and inspire others to achieve success, whether or not they directly report to you
Results Oriented: A driver who possess the ability to take actions and implement effective solutions in a timely manner. Excuses aren’t in your vocabulary because you always find alternative solutions when issues arise
Ethics: Highest level of professional integrity and honesty as well as personal credibility. Your reputation for precedes you in this regard
Innovation: Dedication to maintaining cutting edge talent with the courage to implement new ideas, technology, and aggressively challenge the status quo. You don’t accept responses to new ideas like “That’s the way it’s always been done” because you use facts, data, and people skills to implement meaningful change
Immersion: A propensity to rapidly master the understanding and application of new technology
Excellent verbal and written communication skills, including ability to simplify complex concepts for technical and non-technical audience
Preferred:
Basic to intermediate knowledge in SQL server database development and testing
Working knowledge of Tableau
1+ year's working in an office environment or recent college graduate
APPLY DIRECTLY FOR CONSIDERATION:
Born digital, Axos Bank has reinvented the banking model and grown to over $18.4 billion in assets since our founding in 2000. With a broad and ever-growing range of financial products, Axos Bank is rated among the top 5 digital banks in the country! Axos Financial is our holding company and publicly traded on the New York Stock Exchange under the symbol ""AX"" (NYSE: AX).

We bring together human insight and digital expertise to anticipate the needs of our customers. Our team members are innovative, technologically sophisticated, and motivated to achieve.

Learn more about working here!

A targeted annual base salary range of USD $24/HR - $30/HR, based on the experience, skills, and education/certification required for this position. Eligibility for a discretionary semi-annual incentive compensation plan, based upon performance, payable in cash and/or share grants (RSU’s) that may vest over time. The annual discretionary target bonus percentage is up to 20%.

Axos benefits and perks include:
3 weeks’ Vacation, Sick leave, and Holidays (about 11 a year); Medical, Dental, Vision, Life insurance and more
HSA or FSA account and other voluntary benefits
401(k) Retirement Saving Plan with Employer Match Program and 529 Savings Plan
Employee Mortgage Loan Program and free access to Self-Directed Trading

Pre-Employment Drug Test:

All offers are contingent upon the candidate successfully passing a credit check, criminal background check, and pre-employment drug screening, which includes screening for marijuana. Axos Bank is a federally regulated banking institution. At the federal level, marijuana is an illegal schedule 1 drug; therefore, we will not employ any person who tests positive for marijuana, regardless of state legalization.

Equal Employment Opportunity:

Axos Bank is an Equal Opportunity employer. We are committed to providing equal employment opportunities to all employees and applicants without regard to race, religious creed, color, sex (including pregnancy, breast feeding and related medical conditions), gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship status, military and veteran status, marital status, age, protected medical condition, genetic information, physical disability, mental disability, or any other protected status in accordance with all applicable federal, state and local laws.

Job Functions and Work Environment:

While performing the duties of this position, the employee is required to sit for extended periods of time. Manual dexterity and coordination are required while operating standard office equipment such as computer keyboard and mouse, calculator, telephone, copiers, etc.

The work environment characteristics described here are representative of those an employee may encounter while performing the essential functions of this position. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this position.

#LI-Onsite",$27.00 /hr (est.),1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,2000.0,$500 million to $1 billion (USD)
"Opensignal
4.5",4.5,"Boston, MA",Senior Data Engineer,"Department: Product/Technology
Location: Boston / US East Coast or Canada

Purpose of Role

We’re looking for a Senior Data Engineer to join our Marketing Performance Group in
transforming our real-world raw data into valuable and credible industry-leading metrics that
provide insights to our analysts and our customers.

What you will be doing
The creation and implementation of a framework to assist in building complex statistical
models. Working closely with our data scientists and our data engineers to create and
evolve products that measure market dynamics in the Telecommunication space that drive
our customers short-term marketing campaign tactics and their longer-term customer
acquisition and retention strategies. This role reports to our Engineering Manager.

We expect our Lead Data Engineer to do:
Own and improve our data pipeline. Assemble large, complex data sets that
meet business requirements, with engineering best practices in mind.
Champion building scalable and resilient data infrastructure, as well as tools to
extract and transform data used by stakeholders and customers.
Be security conscious and sensitive to privacy concerns and legislation related
to the data within the platform.
Have a continuous improvement mindset when it comes to both the platform
and the process.
Work efficiently, automate manual processes where possible, and take a test-
driven approach to engineering.
Take a keen interest in improving the platform’s scalability while understanding
the cost.
Be a good team player, with an agile approach and a can-do attitude.
Keep yourself current and make sure we follow best practices and engineering
standards.
Be an advocate for the platform and its health. Take ownership of your work
from conception through to support.
Be detail orientated and understand the importance of the credibility of our
metrics. Document and communicate with stakeholders in a language
understood by all.
Can work in a fast-paced environment with an ability to shift priorities and focus
on changing requirements and market demands.
Able to coach and mentor Data Engineers in best practices.
Cross-collaborate with the wider team to drive and maintain high standards in
our data pipeline builds.
Comfortable and effective at working in a remote capacity, collaborating with
team members across different locations through digital channels.

What we need from you:
As a Senior Data Engineer, we would expect you to have previous experience in
manipulating, processing, storing, and extracting value from big data.
Advanced with hands-on experience in architecting, crafting, documenting, and
developing highly scalable distributed data processing systems.
Advanced with big data tools, specifically Apache Spark.
Advanced with relational SQL databases. Prior experience with MSSQL,
Postgres, AWS Athena (Presto).
Advanced with SQL query authoring including DBT.
Experience with data pipeline / workflow tools i.e. Apache Airflow.
Experience with AWS cloud services like EC2, S3, managed Kubernetes, AWS
ECS, and Aurora.
Experience with object-oriented/object function scripting languages: Python and
Scala.
Experience in implementing complex clustering and classification models on
large datasets to support new product development.
Experience in writing tests, especially in BDD style and working with Git.
Strong analytic skills in working with unstructured datasets.
Experience in root cause analysis of data when asked to answer specific
business questions.
Experience building and optimizing & ""big data"" data pipelines.
Experience supporting and working with cross-functional teams.
Strong self-organizational skills
Experience being part of a cross-functional team, using agile methodologies.
Bachelor’s degree

For US applicants only

At this time, the company will not sponsor a new applicant for employment sponsorship for this position.
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

About Us
Opensignal is the leading global provider of independent insight and data into network
experience and market performance. Our user-centric approach allows communication
providers to constantly improve their network and maximize commercial performance.
Leading analysts, investors and financial institutions place a high value on our independent
analysis and we are regular contributors to their reports.
Real network experience is our focus and ultimately that’s what influences customer choice.
Our mission is to advance connectivity for all and here at Opensignal, the team is leading
the industry in enabling operators to link their network experience and market performance
in a way that has never before been possible.
With offices in London, Boston and Victoria, British Columbia, we are truly global, with
employees working across four continents and representing over 25 nationalities. We are
an equal opportunity employer dedicated to building an inclusive and diverse workforce.

Benefits:
We believe we are stronger when we not only celebrate our many differences, values, and
voices but include them in everyday practice. Having a diverse and inclusive culture is
essential, which is why we offer a flexible approach to work-life balance, operating in a
remote-hybrid way. We’ll help you get set up with the essentials you need to work from
home or the office. We also offer an attractive range of additional benefits, including:
Competitive compensation packages including a long-term equity program.
Comprehensive group benefits package and company-sponsored retirement
savings plan (details depend on your country of work).
Professional development opportunities: education reimbursement, learning
allowance, company-sponsored workshops, and more!
Generous holiday allowance, sick leave, parental leave, flexibility including Flex
Fridays, and the opportunity to work from abroad.
Charity matching and time off for community volunteering and DE&I
program/committees.
Regular virtual and in-person events and socials.","$148,405 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2010.0,Unknown / Non-Applicable
"Johns Hopkins University
4.0",4.0,"Baltimore, MD",PARADIM Data Engineer,"The Platform for the Accelerated Realization, Analysis, and Discovery of Interface Materials (PARADIM), a National Science Foundation Materials Innovation Platform developing the next generation of electronic and quantum materials, is looking for a full-time PARADIM Data Engineer to join the platforms materials data science team.

The successful candidate must be capable of designing and implementing software solutions related to data acquisition, streaming, processing, and storage, and should have a keen desire to partner with scientists developing a versatile, real-time, streaming data infrastructure to enable discovery and development of novel interface materials that will drive the quantum computing revolution.

The ideal candidate will also have technical skills that complement a vision and creativity to find solutions to connecting Big Data from diverse experimental and computational laboratories as well as the rigor and experience to develop high-quality, production software and data pipelines to address these challenges. The ability to travel by car to the campuses of both Cornell University in Ithaca, NY, and the Johns Hopkins University in Baltimore, MD, is strongly preferred.

Specific Duties & Responsibilities

With the chief data officer and PARADIM leadership team, design and implement software solutions related to data acquisition, streaming, processing, and storage, both centrally and at end stations at both PARADIM Institutions (JHU and Cornell).
Work with PARADIM scientists to design and implement a versatile, real-time, streaming data infrastructure to enable discovery and development of novel interface materials that will drive the quantum computing revolution.
Implement software tools for data analysis and interpretation as needed by the PARADIM platform.
Participate in the design of back-end and front-end systems to implement PARADIMs data vision.
Research and implement new technologies that could be beneficial to PARADIM.
Evaluate, test and vet new technology in support of PARADIM efforts.
Work with vendors to procure prototypes and demo units.
Attend department and University-sponsored training to increase knowledge, improve skills, and learn new skills.
May substitute University training for supervisor approved commercial job-related course offerings.

This position may be primarily remote (90%). Occasional in-person trips to Cornell University and/or Johns Hopkins University will be needed for work directly involving software on new hardware. This requirement may best suit candidates located between Baltimore, MD and Ithaca, NY.
Minimum Qualifications

Bachelor's Degree
Five years related experience
Additional education may substitute for required experience and additional related experience may substitute for required education, to the extent permitted by the JHU equivalency formula

Preferred Qualifications

The ideal candidate will also have technical skills that complement a vision and creativity to find solutions to connecting Big Data from diverse experimental and computational laboratories as well as the rigor and experience to develop high-quality, production software and data pipelines to address these challenges. The ability to travel by car to the campuses of both Cornell University in Ithaca, NY, and the Johns Hopkins University in Baltimore, MD, is strongly preferred.

Proficiency in at least one major object-oriented language such as Java, C++, or C#
Proficiency in at least one major software versioning and tracking platform (e.g. git, github, gitlab, svn)
Experience with python and pydata packages such as jupyterlab, cython, numpy, tensorflow
Experience working with instrumental data
Contributions towards open-source software and commitment to open-source development
Experience with Apache Kafka or Confluent Platform
Level of Independent Decision Making
Works independently

Classified Title: Systems Engineer
Role/Level/Range: ATP/04/PE
Starting Salary Range: $71,230 - $97,880 - $124,510 annually (Commensurate with experience)
Employee group: Full Time
Schedule: Monday-Friday, 8:30 am - 5:00pm
Exempt Status: Exempt
Location: Homewood Campus (Hybrid)
Department name: Chemistry
Personnel area: School of Arts & Sciences

Total Rewards
The referenced salary range is based on Johns Hopkins University's good faith belief at the time of posting. Actual compensation may vary based on factors such as geographic location, work experience, market conditions, education/training and skill level. Johns Hopkins offers a total rewards package that supports our employees' health, life, career and retirement. More information can be found here: https://hr.jhu.edu/benefits-worklife/

Please refer to the job description above to see which forms of equivalency are permitted for this position. If permitted, equivalencies will follow these guidelines:

JHU Equivalency Formula: 30 undergraduate degree credits (semester hours) or 18 graduate degree credits may substitute for one year of experience. Additional related experience may substitute for required education on the same basis. For jobs where equivalency is permitted, up to two years of non-related college course work may be applied towards the total minimum education/experience required for the respective job.

**Applicants who do not meet the posted requirements but are completing their final academic semester/quarter will be considered eligible for employment and may be asked to provide additional information confirming their academic completion date.

The successful candidate(s) for this position will be subject to a pre-employment background check. Johns Hopkins is committed to hiring individuals with a justice-involved background, consistent with applicable policies and current practice. A prior criminal history does not automatically preclude candidates from employment at Johns Hopkins University. In accordance with applicable law, the university will review, on an individual basis, the date of a candidate's conviction, the nature of the conviction and how the conviction relates to an essential job-related qualification or function.

The Johns Hopkins University values diversity, equity and inclusion and advances these through our key strategic framework, the JHU Roadmap on Diversity and Inclusion .

Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

EEO is the Law

Learn more:
https://www.eeoc.gov/sites/default/files/migrated_files/employers/poster_screen_reader_optimized.pdf

Accommodation Information

If you are interested in applying for employment with The Johns Hopkins University and require special assistance or accommodation during any part of the pre-employment process, please contact the Talent Acquisition Office at jhurecruitment@jhu.edu . For TTY users, call via Maryland Relay or dial 711. For more information about workplace accommodations or accessibility at Johns Hopkins University, please visit accessibility.jhu.edu .

Johns Hopkins has mandated COVID-19 and influenza vaccines, as applicable. Exceptions to the COVID and flu vaccine requirements may be provided to individuals for religious beliefs or medical reasons. Requests for an exception must be submitted to the JHU vaccination registry. For additional information, applicants for SOM positions should visit https://www.hopkinsmedicine.org/coronavirus/covid-19-vaccine/ and all other JHU applicants should visit https://covidinfo.jhu.edu/health-safety/covid-vaccination-information/ .

The following additional provisions may apply, depending on campus. Your recruiter will advise accordingly.

The pre-employment physical for positions in clinical areas, laboratories, working with research subjects, or involving community contact requires documentation of immune status against Rubella (German measles), Rubeola (Measles), Mumps, Varicella (chickenpox), Hepatitis B and documentation of having received the Tdap (Tetanus, diphtheria, pertussis) vaccination. This may include documentation of having two (2) MMR vaccines; two (2) Varicella vaccines; or antibody status to these diseases from laboratory testing. Blood tests for immunities to these diseases are ordinarily included in the pre-employment physical exam except for those employees who provide results of blood tests or immunization documentation from their own health care providers. Any vaccinations required for these diseases will be given at no cost in our Occupational Health office.

Note: Job Postings are updated daily and remain online until filled.

To apply, visit https://jobs.jhu.edu/job/Baltimore-PARADIM-Data-Engineer-MD-21218/1027795600/

jeid-8befc9e383ba7d4394f407df6f0f70f2","$124,510 /yr (est.)",10000+ Employees,College / University,Education,Colleges & Universities,1876.0,$1 to $5 billion (USD)
"Boston Dynamics AI Institute
4.7",4.7,"Cambridge, MA",Data Engineer,"Our Mission
Our mission is to solve the most important and fundamental challenges in AI and Robotics to enable future generations of intelligent machines that will help us all live better lives.

Data Engineers will work cross-functionally, creating new technology to improve software development for robots. If you have a passion for developing technology for robots and use it to advance their capabilities and usefulness, you will want to join us! We are onsite in our new Cambridge, MA office where we are building a collaborative and exciting new organization.
Responsibilities
Work collaboratively with research scientists and software engineers on software development for a range of different robotic platforms
Develop and maintain our data warehouses and data pipelines in cloud and on-premise infrastructureBuild event and batch driven ingestion systems for machine learning and R&D as needed
Develop and administer databases, knowledge bases, and distributed data stores
Create and use systems to clean, integrate, or fuse datasets to produce data products
Establish and monitor data integrity and value through visualization, profiling, and statistical tools
Perform updates, migrations, and administration tasks for data systems
Develop and implement a data governance, data retention strategyUse Python and SQL to develop, maintain and scale our data stores
Requirements
BS/MS in computer science, robotics, or a related field
5+ years of experience in a data engineering or similar role
Demonstrated experience with a variety of relational database and data warehousing technology such as AWS Redshift, Athena, RDS, BigQuery
Demonstrated experience with big data processing systems and distributed computing technology such as Databricks, Spark, Sagemaker, Kafka, etc
Strong experience with ETL design and implementations in the context of large, multimodal, and distributed datasets
Bonus (Not Required)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)2+ years of experience with Airflow
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.","$112,287 /yr (est.)",201 to 500 Employees,Subsidiary or Business Segment,Information Technology,Computer Hardware Development,1992.0,$5 to $25 million (USD)
"TECKpert
4.9",4.9,"Miami, FL",Data Engineer,"We are looking for a Data Engineer to support our client based in Miami, Florida.
US BASED CANDIDATES ONLY.
This is an hybrid position. Candidates must be located in or near Miami, Florida.
Who we are
Founded in 2009 and headquartered in beautiful Miami, FL, TECKpert is a tech consulting and staff augmentation firm. At TECKpert, we offer a contingent workforce built for any size digital transformation project. Experts in design, development, IT, analytics and marketing, provide innovative digital solutions to achieve success in our new economy. Our leaders identify the technical talent best suited to bolster our client’s capabilities, across all industries, including, healthcare, government, finance, legal, real estate, and startups.
The project
TECKpert seeks to hire a Data Engineer based in Miami, FL to support our client, a large healthcare system based in Miami, FL with centers throughout the United States.
The Data Engineer leads data integration and analytics projects that support data collection, automation, transformation, storage, delivery, and reporting processes. Optimizes data retrieval and processing, including performance tuning, delivery design for down-stream analytics, machine learning modeling (including feature engineering), and reporting.
Responsibilities
Lead data engineering projects and collaborate with stakeholders to develop end-to-end solutions, including designing data structures for downstream analytics, machine learning modeling, feature engineering, prototype development, and reporting.
Assist in all stages of data orchestration, including working with diverse data sources, data cleaning, data transformation, ETL/ELT processes, and data visualization.
Create analytics solutions using Azure Cloud tools, leveraging the capabilities of the platform.
Design and engineer efficient data pipelines for data collection, processing, and distribution using appropriate data platform infrastructure.
Build visualizations to extract meaningful business insights and construct cloud data warehouses/data marts utilizing Azure Data Lake.
Extract data from relational and structured/unstructured sources, perform data analysis, identify correlations, patterns, and other relevant insights.
Identify gaps in master data and transactional data through data analysis.
Assist in defining and maintaining reporting and dashboard standards, guidelines, and processes to ensure high-quality data.
Compensation and Term
This opportunity is for a full-time, ongoing need and pay commensurate with experience up to $96,000 to $122,000 per year. Medical, dental & vision insurance, employee mental health program, paid time off, paid holidays, 401(k) with employer match, employee stock purchase program, tuition reimbursement and much more.
Qualifications
Bachelor's degree in Computer Science, Math, Statistics, Economics, Accounting, Business, or a related field.
Minimum of 2 years of hands-on development experience in building analytics solutions using Microsoft Azure Cloud, with knowledge of Cloud Security, DevOps, Governance, and Data privacy.
Proficiency in programming languages such as Python, Java, Scala, and SQL. Familiarity with database systems, distributed computing systems, and big data technologies (e.g., Hadoop, Spark, Kafka).
Experience in developing and supporting database systems for medium to large organizations, including database structure systems, data management resources, data mining, and data modeling.
Implement data pipelines for Azure Analysis Services reporting data models.
Conduct complex analyses of business data and processes.
Provide strategic and analytic models to address key business questions.
Collect, organize, manipulate, and analyze diverse datasets.
Track and report on the performance of deployed models.
Assist in developing dashboards to facilitate strategic decision-making by executives.
Perform data studies and product experiments related to new data sources or novel applications of existing data sources, interpreting the results effectively.

crqHwoeq0Q","$109,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Internet & Web Services,2009.0,$1 to $5 million (USD)
"Abile Group, Inc.
5.0",5.0,"Saint Louis, MO",Data Cloud Engineer - Master,"Overview:
Abile Group has an exciting and challenging opportunity for a Data Cloud Engineer-Master on a 10 year contract providing User Facing and Data Center Services supporting an Intelligence Community customer. All the personnel on the team will work together to support innovative design, engineering, procurement, implementation, operations, sustainment and disposal of user facing and data center information technology (IT) services on multiple networks and security domains, at multiple locations worldwide, to support the IC mission.

The right candidate will possess the below skills and qualifications and be ready to handle all responsibilities independently and professionally.
Responsibilities:
Provides technical/management leadership on major tasks or technology assignments.
Establishes goals and plans that meet project objectives. Has domain and expert technical knowledge.
Directs and controls activities for a client, having overall responsibility for financial management, methods, and staffing to ensure that technical requirements are met.
Interactions involve client negotiations and interfacing with senior management.
Decision making and domain knowledge may have a critical impact on overall project implementation.
May supervise others.
Qualifications:
Clearance Required: TS/SCI

Degree and Years of Experience: BS/BA and 10 -15 years of relevant experience

Required Skills:
Experience in the various aspects of hybrid cloud activities.
Supports procurement and deployment of Platform Services to enable application portability across the private and public cloud environments offered by NGA.
Readies NGA's Hybrid Cloud Environment for system migration to IC ITE and oversee the future expansion of NGA Hybrid Cloud to additional public clouds.
In concert with DCS Government, supports standardization of DCS operations in a NGA Hybrid Cloud Management environment.
Transforms Government cloud requirements into appropriate technological alternatives and provides expertise in hybrid virtualization and cloud environments.
Experience developing systems, products, and/or processes based on a total systems perspective.
Consults, plans, analyzes designs, develops tests, assures quality, configures, installs, implements, integrates, maintains, and manages systems.
Has and maintains a diverse set of skills across multiple technical disciplines with recognized expertise in multiple disciplines and possess advanced knowledge of multiple mature and emerging technologies.
Works across organizational boundaries, both internally and externally and helps to drive the relationship between technical solutions and business needs of customers. Analyzes, defines and documents customer needs and required functionality.
Designs, develops and tests theoretical and/or physical models and develops the system design, considering operational impacts, performance, testing, manufacturing, cost and schedule, training, maintenance, and support.
Performs system level design trade analysis, reviews and approves system specifications and description documents, determines how a system is to be built, tested, and implemented, plans the system development execution and ensures adherence to appropriate standards, policies, principles, and practices.
Analyzes system capacity and performance to support problem resolution and system enhancements and monitors systems tests.
Responds to inquiries from a variety of sources for the purpose of providing technical assistance, consultation, advice and support, and regularly provides advice and recommends actions and solutions involving complex issues.

About Abile Group, Inc.:
Abile Group, Inc. was formed in July 2004 to partner with the Intelligence Community and their Contractors in the areas of Enterprise Analytics & Performance Management, IT & Systems Engineering and Program & Project Management. We have significant experience with the Federal Government and are an EDWOSB dedicated to our employees and clients. We are looking for high performing employees who enjoy providing advice and guidance along with solutions development and implementation support, crafted by combining industry best practices with the clients’ subject matter experience and Abile’s breadth of expertise.
EEO Statement:
Abile Group, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. Anyone requiring reasonable accommodations should email careers@abilegroup.com with requested details. A member of the HR team will respond to your request within 2 business days.

Please review our current job openings and apply for the positions you believe may be a fit. If you are not an immediate fit, we will also keep your resume in our database for future opportunities.",,51 to 200 Employees,Company - Public,,,,Unknown / Non-Applicable
"Financial Information Technologies LLC
3.7",3.7,"Tampa, FL",Senior Data Engineer,"Join Fintech as a Senior Data Engineer!
Fintech is the leading business solutions provider for the beverage alcohol industry, empowering alcohol suppliers, distributors, and retailers with smart solutions that simplify beverage alcohol management. Our unique, thriving company culture promotes collaboration and growth at every level, and our comprehensive employee benefits have earned Fintech the title of a Tampa Bay Times Top 100 Workplaces for 2020 and 2021.
Fintech’s Senior Data Engineer brings a depth of relational database modeling and an understanding of transactional processing across a myriad of database types. They can analyze and assess new data sets to understand nuances of content in the context of purpose with an ability to conceptualize cleansing, harmonization, and modeling efforts. Working under the direction of the principal process architect the senior data engineer will lead a small team of experienced data wranglers to tackle a myriad of ad-hoc custom projects as well as service the development needs within our warehouse and app abstraction layers.
Essential Functions:
Collaborates with ELT/process automation, data insights, and data science teams
Builds data models in accordance with prescribed methodologies
Serves as knowledgeable backstop for level III ticket resolution
Guides and instructs junior developers and engineers on how to implement directives in accordance with project needs within adopted framework
Gains a familiarity with and contributes to the core meta-data driven data processing engine
Advising on data model consumption in analytics layers
Contributes to knowledge base
Qualifications:
8 + years of experience with SQL in multiple database flavors (SQL Server, Oracle, Snowflake, Postgres, Greenplum)
5 + years of experience with data ingest transformations and harmonization
5 + years of experience with database object creation and modeling
Analytical thinker that can adapt and problem solve in a fast-paced environment
Team oriented
Must be able to consume, understand, and implement a complicated but flexible processing back-end in a short time frame
Our Benefits:
Employer Matched 401K (Up to 10% of Employee Salary)
Company Paid Medical Insurance Option (Employee and Dependent Children)
Company Paid Dental Insurance Option (Employee only)
Company Paid Vision Insurance Option (Employee only)
Company Paid Long and Short-Term Disability
Company Paid Life and AD&D Insurance
Employee Recognition Program
18 PTO Days a Year
Six Paid Holidays
Business Casual Dress Code
Check out www.fintech.com for more information!
We E-Verify.
Fintech is an Equal Opportunity Employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth and pregnancy-related conditions), gender identity or expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, genetic information, or any other characteristic protected by applicable federal, state, or local laws and ordinances. Fintech’s management team is dedicated to this policy with respect to recruitment, hiring, placement, promotion, transfer, training, compensation, benefits, employee activities, access to facilities and programs and general treatment during employment.
Fintech is a Drug-Free Workplace.","$109,213 /yr (est.)",51 to 200 Employees,Company - Private,Financial Services,Financial Transaction Processing,1991.0,$25 to $100 million (USD)
"Chmura Economics and Analytics
3.6",3.6,"Cleveland, OH",Data Engineer,"Description

Founded in 1998, Chmura Economics & Analytics is headquartered in Richmond, Virginia’s historic Shockoe Slip with a regional office in Cleveland, Ohio. We provide labor market software, consulting, and data so our clients can make informed decisions that grow their communities and organizations.

For example, our technology:
Helps economic developers understand their local industries and labor market
Allows workforce development practitioners to guide workers to high-wage, high-demand jobs
Assists educators in training their students for well-paying, in-demand careers
Helps site selectors choose the best site for their clients’ expansion or relocation by understanding the talent availability in competing locations
Allows staffing and recruitment firms to determine competitive wages
We’re more than a technology company – we help our clients win.
Our employees are encouraged to think differently, ask challenging questions, and pursue what’s best for our clients.

We want our clients to make confident decisions. If you want to help communities and organizations thrive, you’ve come to the right place.

Responsibilities
Implement, maintain, and continuously optimize data processing solutions for a wide variety of complex big data sets.
Architect, design, develop, and maintain data integration solutions as they relate to all stages of extract, transform, and load (ETL) pipelines.
Build internal tooling that serves to improve upon the ability to create, test, build, serve, compare, and optimize complex data models and their related data sets.
Troubleshoot data issues and effectively triage timely solutions.
Contribute to and maintain documentation as it relates to new and existing data models, processes, storage, and optimization techniques.
Requirements
Experience in a relevant programming language: C#, Python, Java, etc.
Experience with Elasticsearch
Experience with containerization platforms (Docker, K8S, etc)
Experience with schema design and writing queries for SQL Server, Postgres or similar
Azure experience
Kanban/Agile experience
Familiarity with machine learning and NLP is nice to have but not required
At least 2 years. This is not an “junior” position.

We offer a comprehensive compensation and benefits package.
Salary is commensurate with experience.
Chmura is not able to provide sponsorship for this role.

Chmura is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, physical and mental disability, genetic information, marital status, sexual orientation, gender identity/assignment, citizenship, pregnancy or maternity, protected veteran status, or any other status prohibited by applicable national, federal, state or local law. Chmura promotes a drug-free workplace. Chmura will consider for employment, qualified applicants with a criminal history in a manner consistent with the requirements of applicable federal, state, and local laws and regulations regarding criminal background inquiries, including, to the extent applicable, following applicable federal, state, and local laws and regulations regarding criminal background inquiries.","$90,299 /yr (est.)",1 to 50 Employees,Company - Public,Management & Consulting,Research & Development,1998.0,Unknown / Non-Applicable
"VISUAL SOFT, INC
4.1",4.1,"Washington, DC",Data Engineer - Active TOP SECRET - REMOTE-ONSITE,"Visual Soft, Inc is seeking qualified candidates to work on our efforts with a Prime for their end customer, a federal agency.

Position: Data Engineer - (50% REMOTE and 50% ONSITE)
Location: Washington, DC or Crystal City, Arlington, VA
Shift time: 8 am to 5 pm

JOB DESCRIPTION:
As a Data Engineer, you’ll implement data engineering activities on some of the most mission-driven projects in the industry. You’ll deploy and develop pipelines and platforms that organize and make disparate data meaningful. You will collaborate and work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You’ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients.
**Desirable skills include, Spark, Databricks, Data Lakes, Bigdata Tools and Technologies and AWS
Years of Experience:: 5+ years of experience
Education Requirement: BS degree preferred
Clearance requirement: Top SECRET is a MUST

Standard Benefits:
Our standard benefits include: Our standard benefits include 3 weeks of Paid time off (PTO that includes sick leave). Any unused PTO will be issued as a check at the end of an employee's anniversary with us. we also provide 2 floating and 8 public holidays. Floating and holidays expire at the end of every year of service of an employee. In addition, company will cover 50% of health and dental insurances only for all full time employees, however, dependents can be added at extra cost. Employee's health and dental coverage becomes effective after 30 days or first of the month after an employee completes initial 30 working days, we cover 50% for the employee's health and dental insurances. Dependents coverage for health and dental insurances is available as an out of pocket expense for employees. An employee has to finish all of your paper work for health and dental in the first 30 days of your employment with us. We provide STD, LTD and one time salary equivalent of life insurance at NO cost to all full time employees. All full time employees or w-2 employees with no benefits will be eligible to participate in company's 401k program after 90 days of employment with a company match of 4%, immediate vesting. In addition, all w-2 employees are eligible to be part of company's profit sharing, no employee contributions required. No commuting and/or parking expenses provided.","$98,574 /yr (est.)",1 to 50 Employees,Company - Public,,,,$1 to $5 million (USD)
Wingsoft Consulting LLC,,Remote,AWS Data Engineer – W2 Role,"Greetings,
This is Deepak Sharma Technical recruiter from Wingsoft consulting LLC. I have urgent opening for AWS Data Engineer for Charlotte, NC Locationwith one of our direct client. Please let me know if you or anyone interested and available for this role!!
Title: AWS Data Engineer – W2 Role
Location: Charlotte, NC (onsite would be preferred) will consider remote
Duration:1 year contract +
Job Description
AWS Data Engineer Lead, Core Technical Skills
1) 5+ years of AWS experience
2) Experience in a regulated environment
3) AWS services - S3, EMR, Glue Jobs, Lambda, Athena, CloudTrail, SNS, SQS, CloudWatch, Step Functions
4) Experience with Kafka/Messaging preferably Confluent Kafka
5) Experience with databases such as Document DB, MySQL, Postgres, Glue Catalog, Lake Formation, Redshift, DynamoDB and Aurora and SQL
6) Tools and Languages – Python, Spark, PySpark
7) Experience with Secrets Management Platform like Vault and AWS Secrets manager
8) Experience with Event Driven Architecture
9) Experience with Rest APIs and API gateway
10) Experience with AWS workflow orchestration tool like Airflow or Step Functions
AWS Data Engineer Lead Additional Technical Skills (nice to have, but not required for the role)
11) Experience with native AWS technologies for data and analytics such as Kinesis, OpenSearch
12) Databases - Document DB, MongoDB Atlas
13) Data Lake platform (Hive, Druid, Apachi Hudi/Apache Iceberg/Databricks Delta)
14) Java, Scala, Node JS, Pandas
15) Workflow Automation
16) Experience transitioning on premise big data platforms into cloud-based platforms such as AWS
17) Strong Background in Kubernetes, Distributed Systems, Microservice architecture and containers
18) Day to Day Responsibilities/project specifics:
a. Provides technical direction, guides the team on key technical aspects and responsible for product tech delivery
b. Lead the Design, Build, Test and Deployment of components
i. Where applicable in collaboration with Lead Developers (Data Engineer, Software Engineer, Data Scientist, Technical Test Lead)
c. Understand requirements / use case to outline technical scope and lead delivery of technical solution
d. Confirm required developers and skillsets specific to product
e. Provides leadership, direction, peer review and accountability to developers on the product (key responsibility)
f. Works closely with the Product Owner to align on delivery goals and timing
g. Assists Product Owner with prioritizing and managing team backlog
h. Collaborates with Data and Solution architects on key technical decisions
i. The architecture and design to deliver the requirements and functionality
i. Mentor other developers in development of components and related processes
Job Type: Contract
Pay: Up to $90.00 per hour
Schedule:
8 hour shift
On call
Experience:
AWS: 7 years (Preferred)
Security clearance:
Confidential (Preferred)
Work Location: Remote",$90.00 /hr (est.),1 to 50 Employees,Company - Public,,,,Unknown / Non-Applicable
"Mass General Brigham
3.8",3.8,"Somerville, MA",Sr. Data Engineer (Data Lakes),"Sr. Data Engineer (Data Lakes)
- (3244480)

About Us:
At Mass General Brigham Digital, we pride ourselves on our ability to create maximum strategic, clinical, and operational value from established and emergent technologies for our patients, care teams, researchers, and employees. Digital health will not only enhance the equity and efficiency of healthcare delivery, but it will also help make medicine more personalized and precise.
We recognize that increasing value and continually improving quality while maintaining an inclusive focus are essential to organizational excellence, and we invite you to join us on this journey. The work we do in Digital is a strategic imperative, and there is a strong and growing understanding of how together we will transform Mass General Brigham in innovative and impactful ways.
General Summary/ Overview:
At Mass General Brigham Digital, we pride ourselves on our ability to create maximum strategic, clinical, and operational value from established and emergent technologies for our patients, care teams, researchers, and employees. Digital health will not only enhance the equity and efficiency of healthcare delivery, but it will also help make medicine more personalized and precise.
We recognize that increasing value and continually improving quality while maintaining an inclusive focus are essential to organizational excellence, and we invite you to join us on this journey. The work we do in Digital is a strategic imperative, and there is a strong and growing understanding of how together we will transform Mass General Brigham in innovative and impactful ways.
Summary:
Reporting to the Engineering Manager, Data Lake, the Senior Data Engineer (Azure Data Lake) will work towards analyzing, designing, developing, and building ADF data pipelines, ELT/ETL frameworks, and Azure data lake platforms, primarily focusing on Epic (EHR) data and other healthcare data; and will thrive as a member of an experienced, high performing and highly motivated team. Role will be responsible for participating in building out our existing EDW and our new Data Lake, expanding our data ecosystem and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Requires advanced experience with data engineering and building Azure Cloud Data Lake, Azure Big Data Analytics technologies and architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures, and data sets. Expert level of experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Azure Data Bricks, Azure ML, SQL Data Warehouse. Advanced Experience with Hadoop based technologies (e.g., hdfs, Spark) and Programming experience in Python, SQL, Spark.
Principal Duties and Responsibilities:
Design, Develop, construct, test and maintain Data Lake architectures and large-scale data processing systems.
Support big data ecosystem related Tool selection and POC analysis.
Gather and process raw data at scale that meet functional / non-functional business requirements (including writing scripts, REST API calls, SQL Queries, etc.).
Develop data set processes for data modeling, mining and production.
Integrate new data management technologies ( Informatica DQ..) and software engineering tools into existing structures.
The candidate will be responsible for participating in building out our Data Lake platform, expanding and optimizing our data ecosystem and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.
The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up.
The Data Engineer will support our Software Developers, Database Architects, Data Analysts and Data Scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements on cloud based data platforms (e.g. Azure) and relational data systems (SQL Server, SSIS).
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.
Build the data infrastructure required for optimal extraction, transformation, and loading of data from traditional/legacy data sources.
Work with stakeholders including the Management team, Product owners, and Architecture teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Use/s the Mass General Brigham values to govern decisions, actions, and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability & Service Commitment, Decisiveness, Innovation & Thoughtful Risk; and how we treat each other: Diversity & Inclusion, Integrity & Respect, Learning, Continuous Improvement & Personal Growth, Teamwork & Collaboration.
Working Conditions:
This is a remote position.
Diversity Statement
As a not-for-profit organization, Mass General Brigham is committed to supporting patient care, research, teaching, and service to the community. We place great value on being a diverse, equitable and inclusive organization as we aim to reflect the diversity of the patients we serve. At Mass General Brigham, we believe in equal access to quality care, employment and advancement opportunities encompassing the full spectrum of human diversity: race, gender, sexual orientation, ability, religion, ethnicity, national origin and all the other forms of human presence and expression that make us better able to provide innovative and cutting-edge healthcare and research.

5+ Years of experience data engineering and building Azure Cloud Data Lake technologies and architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures and data sets.
5-7 Years of Experience with Hadoop based technologies (e.g. hdfs, Spark). Spark Experience desirable
5+ years of Programming experience in Python, SQL, PySpark.
Healthcare experience, most notably in Clinical data, Epic, Clarity, Caboodle, Payer data and reference data is a plus but not mandatory.
Experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Snowflake, Azure Data Bricks, Powershell.
Experience with Design and Architecture of relational SQL and NoSQL databases, including MS SQL Server, Cosmos DB.
Experience with Design and Architecture of data security and Azure security, VM, Vnet.
Experience with building processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience leading and working with cross-functional teams in a dynamic environment.
Experience building Big data pipeline with Spark and/or Data Bricks is a plus.
Leading development of Data Lake Architectures from scratch.
Experience with Azure DevOps/CI-CD, Continuous integration and deployment.
Experience with Real time analytics on Spark, Kafka, Event Hub is a plus.
Experience in petabyte scale data environments and integration of data from multiple diverse sources.
Skills/Abilities/Competencies:
Advanced hands-on SQL, Spark, Python, pySpark (2+ of these) knowledge and experience working with relational databases for data querying and retrieval.
Strong SQL skills on multiple platform (preferred MPP systems).
Data Modeling tools (e.g. Erwin, Visio).
Strong interpersonal and communication skills, both written and verbal.
Strong Scrum/Agile development experience.
Excellent organizational skills and attention to detail, manage multiple tasks and projects, meet deadlines, follow through, and manage to schedule.
Strong innovation capabilities and the ability to think creatively.
Strong collaboration and team building skills within, across and outside of an organization.
Maintain and promote a positive team environment.
Maintains stable performance under pressure, demonstrating sensitivity to diverse organizational culture.
Ability to effectively cope with change, remain flexible and adaptable within a fast-paced environment with rapidly changing requirements, and ability to negotiate situations when the big picture is not clearly defined.

EEO Statement

Mass General Brigham is an Equal Opportunity Employer. By embracing diverse skills, perspectives, and ideas, we choose to lead. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under the law. We will ensure that all individuals with a disability are provided a reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment.

Primary Location MA-Somerville-MGB Assembly Row
Work Locations MGB Assembly Row 399 Revolution Drive Somerville 02145
Job Business and Systems Analyst
Organization Mass General Brigham
Schedule Full-time
Standard Hours 40
Shift Day Job
Employee Status Regular
Recruiting Department MGB Digital
Job Posting May 12, 2023","$118,726 /yr (est.)",1001 to 5000 Employees,Nonprofit Organization,Healthcare,Health Care Services & Hospitals,1994.0,$10+ billion (USD)
"Orange County's Credit Union
3.9",3.9,"Santa Ana, CA",Data Engineer,"Great Opportunity At Orange County's Credit Union
Must reside in the state of CA, AZ, NV or TX.
Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.
Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.
Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.
Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!
More about Orange County's Credit Union:
Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.
As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.
Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!
Overview:
Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.
Essential Functions:
Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).
Technical Must Haves for this Role:
5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.","$91,788 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1938.0,$25 to $100 million (USD)
"Yes Energy, LLC
4.6",4.6,"Boulder, CO","Data Operations Engineer II (Hybrid) Boulder - CO, Chicago - IL, Dedham - MA or Houston - TX","Data Operations Engineer II

Join the Market Leader in Electric Power Trading Solutions
The electrical grid is the largest and most complicated machine ever built. Yes Energy’s industry leading electric power trading analytics software provides real time visibility into the massive amount of data that is generated by the North American electrical grid every day. Our unique and innovative view of the data informs real time trading decisions that keep utility prices low and the grid up and running. It’s both challenging work and work with a purpose.
Be a part of our successful, growing business.
We are currently working in a hybrid environment and are seeking to fill one full time Data Operations Engineer II position immediately in Boulder - CO (HQ), Chicago - IL, Dedham - MA or Houston - TX.
About the team
At Yes Energy, our Market Data Operations (MDO) team plays a crucial role in our business. We are the control room for our customers, responsible for ensuring access to reliable and timely data every minute of every day.
We take pride in our responsibility to maintain the accuracy and reliability of our data. We understand that our clients rely on us to provide them with the information they need to make informed decisions, and we take that responsibility very seriously. Like a control room operator who is constantly monitoring the grid and making adjustments to ensure stability, our team is constantly monitoring our data pipelines and making adjustments to ensure data accuracy and reliability.
Our team is passionate about what we do, and we are dedicated to helping clients navigate the complex and dynamic North American Energy Markets. We work together in a collaborative environment, and we look to continuously improve our processes to ensure that we are providing the highest quality data possible to our clients.
About you
You have a passion for working with inherently messy data
You believe that a deep understanding of the data leads to better solutions
You have a competitive attitude, taking ownership and accountability of the work you produce
You have strong problem-solving skills and a curious mindset
You have experience maintaining and designing data pipelines
Like to design, develop, analyze and troubleshoot PL/SQL code

What you will do
Maintain our real-time data pipelines and ensure so that we can provide reliable and accurate information to our clients
Support clients by answering complex data questions, providing timely and effective solutions, so that we can empower our clients to make informed decisions
Ensure highest possible quality and integrity of Yes Energy data; recommend and implement ways to improve data reliability, efficiency, and quality
Participate in weekly on-call rotations to help resolve critical data pipeline failures for our clients

Requirements
4+ years of SQL or equivalent experience
4+ years of Oracle PL/SQL or equivalent experience
Bonus points for
Experience with ETL and complex data pipelines
Energy industry experience or experience in equities/commodities trading
Experience with web scraping, including HTML parsing, HTTP protocols and network logs
Experience with Python and Bash Scripting
Familiarity with Agile development methodologies
Position Details
Full-time
Reports to Data Operation team Lead
Minimal travel may be required (up to 10 days per year)
Keywords:
Oracle, SQL, PL/SQL, REST API, Time Series Data, ETL, CLI tools.
About Yes Energy
Overview
Yes Energy delivers real-time market data and electric power trading decision solutions. Over 1,000 market participants use Yes Energy solutions daily. The business is a leader in all aspects of information content collection and management, as well as in developing and delivering data and market analytics solutions. Since its inception in 2008 Yes Energy has become a trusted and respected supplier of innovative and reliable solutions focused on the needs of power market analysts, traders and trade managers. Yes Energy has a team of amazing professionals located in Boulder, CO (HQ), Dedham, MA and Chicago, IL.
Culture
At Yes Energy we care about saying “Yes” to customers. We like to listen and learn, and develop our solutions in line with our customers’ needs. We think about customers as business partners and when we help them to be more successful … We are more successful too.
Around the office our culture is driven by some pretty fundamental values that we’re proud of:
We love innovation and solving tough challenges;
We are “high standards people” who combine passion and pride with hard work and rewards of all kinds- in an ethic that is consistent across the company;
We’re team-focused with a flat hierarchy- we work in small teams on well-defined projects that directly impact the success of the business;
We play to the strengths and experience of each person, while each of us also works along a continuum of roles adjacent to our focus area. This presents a challenge of maintaining a broad set of skills as well as an opportunity to learn and contribute in many ways;
We are constantly growing. Professional development happens every day and every year.
Compensation and Benefits
Salary Range: $90,000 - $120,000 plus bonus.
We offer strongly competitive salaries and real bonuses that are achievable and that you can impact. Our benefits package is also very competitive and it includes medical insurance, 401K Plan with matching, flexible vacation and flexible work schedules. Investment in both formal and informal professional development is encouraged and funded by Yes Energy.
At Yes Energy we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
Yes Energy provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Yes Energy complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.","$105,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2008.0,Unknown / Non-Applicable
"Millennium Physician Group
3.4",3.4,"Fort Myers, FL",IT Data Engineer,"Millennium Physician Group is seeking an experienced Data Engineer to join our Technology and Innovation Services Team in Fort Myers, FL. t
The Data Engineer role is responsible for building the foundation for a Datawarehouse using best practice ETL/ELT methodologies. This position will develop and maintain data pipelines, interfaces, and process automation. The Data Engineer will be required to investigate and understand datasets from dozens of discrete sources that may or may not always have available documentation. This position must communicate highly complex data trends to organizational leaders in a way that's easy to understand. The position requires collaborating with the existing BI/Analytics teams, Software and Database developers to create a centralized repository and platform to be used by all data consumers in the organization. The Data Engineer must demonstrate advanced knowledge of SQL, OOP development concepts, and Business Intelligence/Analytics. The position requires the ability to develop code for use in the automation of data pipelines. This position reports to the Data engineering Manager.
Essential Duties and Responsibilities include the following. Other duties may be assigned.
Build scalable data pipelines that clean, transform, and aggregate data from disparate sources
Assemble large, complex data sets that meet functional / non-functional business requirements
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Snowflake technologies
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Implements processes and systems to monitor data quality to ensure production data is always accurate and available
Writes unit/integration tests, contributes to engineering wiki, and documents work
Defines company data assets (data models)
Designs data integrations and data quality framework
Build analytics tools that utilize the data pipeline to provide actionable insights into key business performance metrics
Works closely with a team of frontend and backend engineers, developers, and analysts

Education and Qualifications:
Master's Degree preferred, Bachelor's required in Computer Science, Information Technology, Informatics, Engineering, Statistics or equivalent.
4+ Years Prior experience as a Data Engineer, Data Scientist, or Database Administrator required
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience with object-oriented/object function scripting languages: Python, Java, C#, Scala, etc.
Experience with Cloud Datawarehouse technologies (Snowflake/Redshift) and design
Ability to create and maintain optimal data pipeline architecture
Hands-on experience building and maintaining ETL/ELT processes
Experience interacting with and extracting data from RESTful APIs
Experience with commercial ETL/ELT toolsets: Matillion, Talend, Fivetran, etc.
Experience building data visualizations using BI tools
Experience with AWS Cloud services
ABOUT MILLENNIUM PHYSICIAN GROUP
Formed in 2008, Millennium Physician Group has grown into one of the largest comprehensive primary care practices with more than 400 health care providers located throughout Florida. With corporate headquarters in Fort Myers, Florida, Millennium Physician Group consists of primary care offices, Imaging Centers, Lab Services, Physical Therapy, and Wellness Programs. We also have several administrative departments supporting our medical offices, such as Quality Assurance, ACO, Business Services, Coding ACO, IT, Human Resources, and more.

If you are interested in joining an organization that emphasizes teamwork and family, Millennium Physician Group is the right choice.
Millennium's core values summarize how we treat others, patients, and fellow community members. Millennium CARES for every patient every time.

ARE YOU READY TO JOIN OUR TEAM? If you are the right candidate for this position, please click the link to apply today. We look forward to meeting you!","$94,511 /yr (est.)",1001 to 5000 Employees,Private Practice / Firm,Healthcare,Health Care Services & Hospitals,,$25 to $100 million (USD)
"Acubed
4.0",4.0,"Sunnyvale, CA",Staff ML Engineer (Perception / Data),"WAYFINDER
Wayfinder is building scalable, certifiable autonomy systems to power the next generation of commercial aircraft. Our team of experts is driving the maturation of machine learning and other core technologies for autonomous flight; we are creating a reference architecture that includes hardware, software, and a data-driven development process to allow aircraft to perceive and react to their environment. Autonomous flight is transforming the transportation industry, and our team is at the heart of this revolution.
The Opportunity
As a Staff ML Engineer, you will lead the technical execution of our data and ML pipelines with an overall focus into improving the overall end to end perception pipeline (labeling, training and deployment).
This is a hands-on role, so you will be responsible for implementing things yourself, while also acting as a tech lead for the ML and Data engineering teams.
Responsibilities
Be an architect for our overall perception pipeline (training, testing, monitoring and labeling)
Explore, prototype and validate new algorithms/solutions
Be a mentor for other team members within the ML and Data teams
Champion highest quality of engineering excellence
Be able to move from research to productize ML models (fast iteration)
Negotiate initiatives and deliverables with stakeholders
Have deep understanding of the business and operational impact for different technology tradeoffs
Capable of influencing and building consensus in technical debates
Requirements
BS, MS, or higher degree, in CS/CE/EE, or equivalent industry experience
Extensive experience with ML frameworks such as Tensorflow, Caffe, and PyTorch
Strong programming skills in Python and C++
Growing expertise with state-of-the-art perception related ML models
Excellent mathematical reasoning skills, especially with probability
8+ years of experience in computer vision and machine learning
Expertise in setting architectures that are scalable, fault-tolerant and extensible for changes.
Ability to design across multiple systems
Ability to wear several hats between coding, technical strategy, mentorship etc.
Proven record of productizing computer vision models
Strongly Preferred Qualifications
PhD in computer science or machine learning
Experience with MCAP, OpenCV
Experience with CUDA
Real-world experience applying machine learning techniques in autonomous systems such as robots, cars, and UAV
Benefits
Exceptional PPO medical, dental and vision benefits with 100% of premiums covered for employee and their family/dependents
Generous PTO of 5 weeks (6 weeks after two years) in addition to 11 national holidays and unlimited paid sick days
Professional development reimbursement or $15,750 for flight training
3 months paid parental leave from Day 1
Pay Transparency Notice: Depending on your work location and years of experience, the target annual salary for this position can range from $160,000 to $220,000 + target bonus + benefits (including medical, dental, vision, 401(k), and flight training).
Note that Acubed does not offer sponsorship of employment-based nonimmigrant visa petitions for this role.","$190,000 /yr (est.)",51 to 200 Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,,Less than $1 million (USD)
"Northwestern University
4.2",4.2,"Evanston, IL",Data Management Engineer,"The Data Management Engineer is responsible for supporting the development and implementation of secure and efficient research data management systems, environments, and workflows. In this position, you will collaborate with research data and cyberinfrastructure colleagues to help create and expand services that allow researchers to meet their research project security, compliance, analysis, and collaboration requirements. This includes working with storage systems, data transfer tools, networking, research computing resources, and cloud computing resources. As an integral member of our team, Northwestern Research Computing Services, your work will enable research across the university. In this position, you will report to the Associate Director of Research Data Services.
Note: Not all aspects of the job are covered by this job description.
Specific Responsibilities:
Aid in the design of sustainable and scalable research support services that align with faculty research goals, technology standards, and best practices.
Install, configure, test, and document applications to support data management and computing services.
Translate service requirements into functional architectural design and effective implementation.
Incorporate security best practices into the design, implementation, and monitoring of services.
Document standard and customized data workflows, including development of technical drawings, schematics, and/or reports as needed.
Automate service allocation processes.
Create proof-of-concepts and unique solutions for individual research projects and lead implementation of projects with fellow IT staff.
Build and support data workflows by identifying requirements, testing new iterations, installing and configuring applications, and adapting the process with incremental feedback
Facilitate use of cloud resources for the purposes of storing data, including providing guidance on appropriate product, setting up environment, and providing materials for users to maintain the environment
Clearly communicate project status and updates with the researchers and team in a timely manner.
Minimum Competencies: (Skills, knowledge, and abilities.)
Successful completion of a full 4-year course of study in an accredited college or university leading to a Bachelor's degree in a related field; OR appropriate combination of education and experience.
2 or more years’ experience using Windows, Mac, and Linux operating systems.
2 or more years’ experience with a programming or scripting language, such as Python or bash.
1 or more years’ experience using cloud-based storage resources, such as Amazon S3 or Google Cloud Storage.
Ability to manage and prioritize a wide variety of projects and tasks.
Demonstrated ability to write documentation and communicate effectively with team members with varied technical backgrounds.
Demonstrated interpersonal skills, with the ability to build relationships successfully across teams.
Skilled in using a variety of tools and methods for data storage and transfer, including data transfer to the cloud.
Knowledge of security protocols for data transfer, network, and storage.
Benefits:
At Northwestern, we are proud to provide meaningful, competitive, high-quality health care plans, retirement benefits, tuition discounts and more! Visit us at https://www.northwestern.edu/hr/benefits/index.html to learn more.
Work-Life and Wellness:
Northwestern offers comprehensive programs and services to help you and your family navigate life’s challenges and opportunities, and adopt and maintain healthy lifestyles.
We support flexible work arrangements where possible and programs to help you locate and pay for quality, affordable childcare and senior/adult care. Visit us at https://www.northwestern.edu/hr/benefits/work-life/index.html to learn more.
Professional Growth & Development:
Northwestern supports employee career development in all circumstances whether your workspace is on campus or at home. If you’re interested in developing your professional potential or continuing your formal education, we offer a variety of tools and resources. Visit us at https://www.northwestern.edu/hr/learning/index.html to learn more.

Northwestern strongly recommends COVID-19 vaccinations and boosters for people who can obtain them as a critical tool for minimizing severe illness. More information can be found on the COVID-19 and Campus Updates webpage.
The Northwestern campus sits on the traditional homelands of the people of the Council of Three Fires, the Ojibwe, Potawatomi, and Odawa as well as the Menominee, Miami and Ho-Chunk nations. We acknowledge and honor the original people of the land upon which Northwestern University stands, and the Native people who remain on this land today.",,5001 to 10000 Employees,College / University,Education,Colleges & Universities,1851.0,$1 to $5 billion (USD)
Cypress Consulting,,"Palo Alto, CA",Sr. Automation Data Center Design Engineer/Consultant,"Sr. Automation Data Center Engineer/Network Security Consultant
The Sr Consultant will provide expert automation support, analysis and research into complex problems and processes relating to deployed Security network equipment. The Consultant will function as the Automation Subject Matter Expert (SME) and will interact directly with the customer's personnel and will also serve as the technical expert on executive-level project teams within the customer providing technical direction, interpretation, and alternatives. The ideal candidate contributes to the development of new principles and concepts, works on unusually complex technical problems and provides solutions which are highly innovative and ingenious. This is a highly technical, hands-on role and this person will be required to develop and maintain an expertise on the products and solutions deployed within the Customer's network/Data Centers.
Technical Skills needed:
Ansible, CI/CD tools, DevOps, Git, Python, Strong Automation experience for deployment, Terraform
Job Type: Full-time
Pay: $115.00 - $125.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Professional development assistance
Referral program
Retirement plan
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Palo Alto, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Consultative/Customer-facing: 3 years (Required)
Terraform: 2 years (Required)
Data Center Design: 3 years (Preferred)
Ansible: 3 years (Required)
Python: 2 years (Required)
Work Location: In person",$120.00 /hr (est.),Unknown,Company - Public,,,,Unknown / Non-Applicable
"Synergy technologies
4.7",4.7,Remote,Data Engineer (W2),"Data Engineer
** Remote Opportunity **
JOB DESCRIPTION:
Data Engineer -
Within this role, you would be a hands-on leader in data engineering functions including schema design, data movement, data transformation, encryption, and monitoring: all the activities needed to build, sustain and govern big data pipelines.
Mandatory Skills:
SCALA
SPARK
Java, SQL
AWS
Glue
S3
Responsibilities
Own development of large-scale data platform including operational data store, real time metrics store and attribution platform, data warehouses and data marts for advertising planning, operation, reporting and optimization
Wider team collaboration and system documentation
Maintain next-gen cloud based big data infrastructure for batch and streaming data applications, and continuously improve performance, scalability and availability
Advocate the best engineering practices, including the use of design patterns, CI/CD, code review and automated integration testing.
Required Education, Experience, Skills and Training
Bachelor or above in computer science or EE
5+ years of professional programming in Scala, Java and SQL
5+ years of experience developing in Amazon Cloud technologies including S3, Glue, EC2, and Kinesis
5+ years of big data design experience with technical stacks like Spark, Flink, Druid, Clickhouse, Single Store, Snowflake, Kafka, Nifi and AWS big data technologies
Proven track record with cloud infrastructure technologies, at least two of Terraform, K8S, Spinnaker, IAM, ALB, and etc.
Experience building highly available and scalable services for public consumption
Experience with processing large amount of data at petabyte level
Strong knowledge of system design, application design and architecture
Proficiency in both written and oral English
Job Type: Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Application Question(s):
What is your Work Authorization/ Visa status?
Experience:
Data Engineering: 5 years (Preferred)
Programming in Scala, Java and SQL: 5 years (Preferred)
Big data design: 5 years (Preferred)
Cloud infrastructure: 5 years (Preferred)
Work Location: Remote
Speak with the employer
+91 9549530709",$42.50 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,,Unknown / Non-Applicable
"Delta
4.3",4.3,"Atlanta, GA","Data Engineer, Operations Analysis, and Performance","United States, Georgia, Atlanta
Operations Anlys & Performance
12-May-2023
Ref #: 19868
How you'll help us Keep Climbing (overview & key responsibilities)
Deltas brand is based on best-in-class operational performance, the foundation of which is providing safe and reliable operations for our customers travel experience. The role of Operations Analytics (OA) is to support this mission by providing strategic insights by first understanding business processes and then leveraging data and analytics to drive continuous improvement efforts.

Data has transformed the way Delta operates.The role of a data engineer is to further harness the power of data by making it accessible, available, and curated for reporting and analysis. Data engineering teams within Deltas Operations Analysis and Performance (OAP) division are typically responsible for: curating Single Source of Truth data tables to be used by analytics teams, producing and distributing automated reporting in a modular and scalable manner, leading efforts to transition to modern data and reporting tools, and acting as a center of expertise for efficient data processing and management.

Responsibilities:
Locate and extract data from a variety of sources for use in analysis, models and project development.
Clean and curate datasetsby researching new data sources and collaborating with other business units to determine the best source for the data, aggregating views into meaningful hierarchies
Be familiar with data environments and collaborate to improve automated reports and analyses in support of divisional leaders and business units
Leverage emerging technologies and proactively identify efficient and meaningful ways to communicate data and analysis in order to satisfy divisional needs.
Support process improvement and project management engagements for both individual business units and cross-divisional initiatives
Train and mentor other team members in various skillsets and subjects
Practice safety-conscious behaviors in all operational processes and procedures
Have a team first attitude with the success of our team and business partners as the top priority
Be intellectually curious, ask questions, and speak up when they have an idea
Enjoy working in a high-profile environment with fluid priorities, ambiguity and aggressive deadlines.

Benefits and Perks to Help You Keep Climbing
A career at Delta not only gives you a chance to see the world, but we also provide excellent benefits to help you keep climbing along the way!
Competitive salary, industry leading profit sharing and 401(k) with generous direct contribution and company match
Comprehensive health benefits including medical, dental, vision, short/long term disability and life benefits
A detailed wellness plan that recognizes the importance physical, emotional, financial, and social wellbeing
Domestic and International flight privileges


What you need to succeed (minimum qualifications)
3+ years of related experience
Proficiency in SQL and ETL/ELT patterns
Proficiency with Python, SAS or other similar tools and programming languages
Ability to troubleshoot a reporting database environment
Strong attention to detail and ability to work autonomously and manage multiple requests with varying timelines
Self-starter with a resilient, solution-minded approach to complex problems working individually or in a group
Demonstrates that privacy is a priority when handling personal data.
Embraces a diverse set of people, thinking and styles.
Consistently makes safety and security, of self and others, the priority.
What will give you a competitive edge (preferred qualifications)
Bachelor's degree or certificate in Computer Science, Engineering, Information Science, or other relevant quantitative field
Previous airline experience
Working knowledge of and/or experience with cloud-based solutioning (i.e. Azure, AWS, GPC, etc.)

< Go back","$102,659 /yr (est.)",10000+ Employees,Company - Public,Transportation & Logistics,"Airlines, Airports & Air Transportation",1928.0,$10+ billion (USD)
"Yes Energy, LLC
4.6",4.6,"Boulder, CO","Data Engineer (PL/SQL) Hybrid - Boulder, CO / Chicago, IL / Dedham, MA / Houston, TX","Data Engineer (PL/SQL)

Join the Market Leader in Electric Power Trading Solutions
The electrical grid is the largest and most complicated machine ever built. Yes Energy’s industry leading electric power trading analytics software provides real time visibility into the massive amount of data that is generated by the North American electrical grid every day. Our unique and innovative view of the data informs real time trading decisions that keep utility prices low and the grid up and running. It’s both challenging work and work with a purpose.
Be a part of our successful, growing business.
We are currently working in a hybrid environment and are seeking to fill one full time Data Engineer (PL/SQL) position immediately in Boulder - CO, Chicago - IL, Dedham - MA or Houston - TX.
About the team
At Yes Energy, our Market Data Operations (MDO) team plays a crucial role in our business. We are the control room for our customers, responsible for ensuring access to reliable and timely data every minute of every day.
We take pride in our responsibility to maintain the accuracy and reliability of our data. We understand that our clients rely on us to provide them with the information they need to make informed decisions, and we take that responsibility very seriously. Like a control room operator who is constantly monitoring the grid and making adjustments to ensure stability, our team is constantly monitoring our data pipelines and making adjustments to ensure data accuracy and reliability.
Our team is passionate about what we do, and we are dedicated to helping clients navigate the complex and dynamic North American Energy Markets. We work together in a collaborative environment, and we look to continuously improve our processes to ensure that we are providing the highest quality data possible to our clients.
About you
You have a passion for working with inherently messy data
You believe that a deep understanding of the data leads to better solutions
You have a competitive attitude, taking ownership and accountability of the work you produce
You have strong problem-solving skills and a curious mindset
You have experience maintaining and designing data pipelines
Like to design, develop, analyze and troubleshoot PL/SQL code

What you will do
Maintain our real-time data pipelines and ensure so that we can provide reliable and accurate information to our clients
Support clients by answering complex data questions, providing timely and effective solutions, so that we can empower our clients to make informed decisions
Ensure highest possible quality and integrity of Yes Energy data; recommend and implement ways to improve data reliability, efficiency, and quality
Participate in weekly on-call rotations to help resolve critical data pipeline failures for our clients

Requirements
4+ years of SQL or equivalent experience
4+ years of Oracle PL/SQL or equivalent experience
Bonus points for
Experience with ETL and complex data pipelines
Energy industry experience or experience in equities/commodities trading
Experience with web scraping, including HTML parsing, HTTP protocols and network logs
Experience with Python and Bash Scripting
Familiarity with Agile development methodologies
Position Details
Full time
Reports to Data Operation team Lead
Minimal travel may be required (up to 10 days per year)
Keywords:
Oracle, SQL, PL/SQL, REST API, Time Series Data, ETL, CLI tools.
About Yes Energy
Overview
Yes Energy delivers real-time market data and electric power trading decision solutions. Over 1,000 market participants use Yes Energy solutions daily. The business is a leader in all aspects of information content collection and management, as well as in developing and delivering data and market analytics solutions. Since its inception in 2008 Yes Energy has become a trusted and respected supplier of innovative and reliable solutions focused on the needs of power market analysts, traders and trade managers. Yes Energy has a team of amazing professionals located in Boulder, CO (HQ), Dedham, MA and Chicago, IL.
Culture
At Yes Energy we care about saying “Yes” to customers. We like to listen and learn, and develop our solutions in line with our customers’ needs. We think about customers as business partners and when we help them to be more successful … We are more successful too.
Around the office our culture is driven by some pretty fundamental values that we’re proud of:
We love innovation and solving tough challenges.
We are “high standards people” who combine passion and pride with hard work and rewards of all kinds- in an ethic that is consistent across the company.
We’re team-focused with a flat hierarchy- we work in small teams on well-defined projects that directly impact the success of the business.
We play to the strengths and experience of each person, while each of us also works along a continuum of roles adjacent to our focus area. This presents a challenge of maintaining a broad set of skills as well as an opportunity to learn and contribute in many ways.
We are constantly growing. Professional development happens every day and every year.
Compensation and Benefits
Salary Range: $90,000 - $120,000, plus bonus.
We offer strongly competitive salaries and real bonuses that are achievable and that you can impact. Our benefits package is also very competitive, and it includes medical insurance, 401K Plan with matching, flexible vacation and flexible work schedules. Investment in both formal and informal professional development is encouraged and funded by Yes Energy.
At Yes Energy we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
Yes Energy provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Yes Energy complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.","$105,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2008.0,Unknown / Non-Applicable
"Chewy
3.5",3.5,"Richardson, TX",Data Engineer II,"Our Opportunity:
Chewy’s Data Analytics team has an exciting opportunity for a Data Engineer III to join the pack. Leveraging your strong expertise and background in data engineering and data analysis, you will be a part of a team responsible for operational and tactical reporting generating insights to grow Customer Service operations and planning. This includes building high quality data pipelines that drives analytic solutions and creating data products for analytics and data scientist team members to improve their productivity. Our organization is a fast-paced environment with new challenges and new opportunities each day. You will be responsible for building and implementing data products and technologies which will handle the growing business needs and play a key role in redefining what it means to be a world-class customer service organization.
What You'll Do:
Design, develop, optimize, and maintain data architecture and pipelines using design and programming patterns that follow best-in-class practices and principles.
Manage, maintain, and improve our SSOT tables and data marts, which drive critical business decisions every day.
Work closely with analytics teams and business partners, serving as a trusted partner who can advise, consult, and communicate data solutions.
Mentor and coach other data practitioners on data standards and practices.
Lead the evaluation, implementation and deployment of emerging tools and process for data engineering to improve overall productivity for the organization.
Partner with leaders, vendors, and other data practitioners across Chewy to develop technical architectures for strategic enterprise projects and initiatives.
Document technical details of work and follow agile sprint methodology, using tools like Jira, Confluence etc.

What You'll Need:
Bachelor of Science or Master’s degree in Computer Science, Engineering, Information Systems, Mathematics or related field
3+ years of enterprise experience as a data engineer and/or software engineer
3+ years applying and implementing database and data modeling techniques
3+ years working with enterprise data warehouse (ex. Snowflake, Vertica) and cloud environments (ex. AWS)
3+ years of experience building data integrations and pipelines from data lake, APIs, relational databases, and third-party systems
Strong software development skills in SQL
Self-motivated with strong problem-solving and self-learning skills.
Bonus:
Strong working knowledge of Python programming
Excellent communication and collaboration skills with ability to influence and guide stakeholders
Experience building dimensional models in data warehouses
Experience with data streaming tools and technologies like Kafka, Kinesis, or similar technologies
AWS Developer Certifications
E-commerce, Retail or startup experience
Experience in BI tools such as Tableau, Plotly, Power BI, etc.
Chewy is committed to equal opportunity. We value and embrace diversity and inclusion of all Team Members. If you have a disability under the Americans with Disabilities Act or similar law, and you need an accommodation during the application process or to perform these job requirements, or if you need a religious accommodation, please contact CAAR@chewy.com.

If you have a question regarding your application, please contact HR@chewy.com.

To access Chewy's Customer Privacy Policy, please click here. To access Chewy's California CPRA Job Applicant Privacy Policy, please click here.",,10000+ Employees,Company - Public,Retail & Wholesale,Pet & Pet Supplies Stores,2011.0,$5 to $25 million (USD)
"McKinstry
4.1",4.1,"Portland, OR",Data Focused Software Engineer,"Build the future, spark innovation and align your career with purpose.
McKinstry is innovating the waste and climate harm out of the built environment and creating lasting impact. Together, we’re building a thriving planet.
Buildings are a leading contributor to the climate crisis, generating nearly 40% of total global energy-related carbon emissions. We’re making a lasting impact on our industry and within our communities by addressing the climate, affordability and equity crises through:
renewables and energy services
engineering and design
construction and facility services
To get where we’re going, we need big thinkers, problem solvers and collaborative mindsets. Does that sound like you?
The Opportunity with McKinstry
McKinstry is seeking a Data Focused Software Engineer to join our evolving Technology division in Portland, OR. In this role, you will work on key enterprise systems providing analysis, design, development, and configuration support for internal business partners and external clients. You will support the development and implementation of technical business intelligence and data warehousing solutions using a combination of on-premise and hosted technologies. To succeed in this role, you must have experience with SSIS and ETL processing, as well as a strong understanding of Microsoft data technologies. Additional responsibilities you will have include:
Develop complex data extracts, applications, and ad-hoc queries as requested by internal and external customers using the Microsoft suite of data tools (SQL, SSIS, Azure SQL, Azure Data Factory, etc.)
Design, develop, test, implement and manage ETL processes sourcing data from internal and external systems (SQL server, Excel/CSV, SOAP, API, SharePoint, SFTP, etc.).
Participate in the production operations of Business Intelligence products and solutions (Datawarehouse, Data Integration, Power BI) including resolving production issues and responding quickly to priority problems
Participate in the maintenance and publication of logical and physical data models, entity relationship diagrams, and a common data dictionary
Participate in maintenance and publication of data flow diagrams depicting source to target mappings supporting a tiered data architecture compromised of source systems, an operational data store (ODS), a data warehouse (DW) and data marts (DM)
Research, troubleshoot, and resolve data issues impacting extract delivery
Work with Business System Analysts to understand the requirements regarding solutions and ensuring data quality to match back to transaction results
Work with key business users to understand their information needs while providing intuitive data solutions
Collaborate on the definition, development, and maintenance of standards and processes
Drive improvement and automation in BI solutions and processes
What You Need to Succeed at McKinstry
BA/BS degree in Information Technology or related field, or equivalent work experience
3 years of experience using Microsoft data technologies in support of business reporting and analytics objectives such as writing T-SQL scripts
Expereince using different ETL tools
Designing and building data pipelines using SSIS is ideal
1 year of building datasets, reports and dashboards using Power BI
Experience with C# coding, performance tuning and optimizing T-SQL scripts, data modeling techniques
2 years data integration experiences in a data warehousing environment
Demonstrated ability to influence direction within a team and persuade others in researched areas and growing consensus building and facilitation.
Nice to Have
Experience with using Azure Cloud products such as Data Factory, Analysis Services, Data Lake, Data Catalog, Azure Databricks etc. preferred
Experience with Azure DevOps, Agile scrum development practice
Experience with CI/CD
PeopleFirst Benefits
When it comes to the basics, we have you covered:
Competitive pay 401(k) with employer match and profit sharing plan
Paid time off and holidays
Comprehensive medical, prescription, dental, and vision with low or zero deductible options and low out of pocket maximum.
People come first at McKinstry and we go beyond the basic benefits with:
Family formation benefits, including adoption and IVF assistance
Up to 16 weeks paid parental leave
Transgender inclusive benefits
Commuter benefits
Pet insurance
“Building Good” paid community service time
Learning and advancement opportunities via McKinstry University
McKinstry Moves onsite gyms or reimbursement for remote workers
See benefit plan documents for complete details.
If you’re driven by our vision to build a thriving planet together, McKinstry is the place to build your career.
The pay range for this position is $80,100 - $127,680 per year; however, base pay offered may vary depending on job-related knowledge, skills, and experience. A bonus may be provided as part of the compensation package, in addition to a full range of medical, financial, and/or other benefits, dependent on the position offered. Base pay information is based on market location.
The McKinstry group of companies are equal opportunity employers. We are committed to providing equal employment opportunities to all employees and qualified applicants without regard to sex, gender identity, sexual orientation, age, race, color, creed, marital status, national origin, disability, veteran status or any other basis protected by law. This policy applies to all terms and conditions of employment including, but not limited to employment, advancement, assignment, and training.
McKinstry is committed to strengthening our diversity through recruiting and retaining minority and women professionals from all backgrounds. Our commitment is consistent with our recognition that it is the outstanding people within McKinstry who have always been the source of our strength. We recognize that promoting diversity is an integral component of our continuing quest for organizational excellence.
This commitment to Equal Employment Opportunity is made equally as a social responsibility and as an economic and business necessity.
Anyone with questions or concerns regarding Equal Employment Opportunity should contact their direct supervisor or the Human Resources Department without fear of retaliation of any kind.
#LI-AK1","$103,890 /yr (est.)",1001 to 5000 Employees,Company - Private,"Construction, Repair & Maintenance Services",Construction,1960.0,$500 million to $1 billion (USD)
"AMERICAN SYSTEMS
4.0",4.0,United States,Data Analytics Engineer - Public Trust,"Wanted: An experienced Data Analytics Engineer with demonstrated expertise and experience in design, development, architecture and implementation of large-scale enterprise dataflow platforms as well as analysis of complex, legacy disparate non-conforming datasets.
As a Data Analytics Engineer, you will:
Work with the latest, cutting edge technology.
Work with a team of driven, supportive and highly skilled professionals.
Receive a robust benefits package that includes Employee Stock Ownership Plan!
Enjoy flexibility managing your work hours and personal needs with a single accrual leave plan.

A week in the life of a Data Analytics Engineer:
Provide the management, maintenance, and support of the Data Analytics’ platform hardware and software. Data Analytics shall include deployment and ongoing support for host, application, appliance and log monitoring and alerting. The Data Analytics System will be highly-available with geographically diverse operating locations.
Demonstrate expert-level knowledge/experience in Big data and Cloud technologies: Spark ML, Splunk, Elastic Search, Apache NiFi, AWS Glue, Cribl.io Logstream, and Hadoop.
Architect and deploy Person-Centric Services, including use of AI/ML technologies.
Lead efforts to modernize a portfolio of major legacy applications leveraging the cloud, DevSecOps, and industry best practices.
Architect and implement change-data-capture (CDC) replication for unique disparate datasets of records into Spark ML for deduplication and graph theory entity/identity grouping Architect a scalable, automated enterprise-wide Security Event Management framework—Splunk—to collect and analyze “events”.
Develop risk patterns through the Security Event Management framework to enable Agency ability to perform predictive analysis using billions of events per month.

AMERICAN SYSTEMS is committed to pay transparency for our applicants and employee-owners. The salary range for this position is $110,000 - $150,000. Actual compensation will be determined based on several factors permitted by law. AMERICAN SYSTEMS provides for the welfare of its employees and their dependents through a comprehensive benefits program by offering healthcare benefits, paid leave, retirement plans (including ESOP and 401k), insurance programs, and education and training assistance.
Founded in 1975, AMERICAN SYSTEMS is one of the largest employee-owned companies in the United States. We are a government services contractor focused on delivering Strategic Solutions to complex national priority programs with 100+ locations worldwide. Through our focus on quality, strong cultural beliefs and innovation we deliver excellence every day.
Company Awards:
Forbes National Best Midsize Companies 2021
Energage National Best Workplaces, National 2021
Washington Post Best Workplaces 2021 Veteran
Hiring Awards:
U.S. Department of Labor Hire Vets Medallion
BEST FOR VETS by Military Times
TOP 10 MILITARY FRIENDLY COMPANY by MilitaryFriendly.com

#LI-IG1
Job Requirements
Required: a minimum of 7 years of recent and relevant experience.
Preferred education: A Bachelor’s degree from an accredited college or university in business management, information technology management, or a related field.
Domain expertise: programming languages, data analytics, machine learning (ML/AI), DevSecOps, Cloud, Big Data.
Technical skills preferred: JavaScript, Ansible, SQL, Java, .Net, Python; Data Technologies: Oracle, Cassandra, Elastic Search, Aurora, NiFi, Cribl.io LogStream, Splunk, Hadoop, Tensorflow, Jupyter.; Software/Tools: AWS, Docker, Jira, VMware vCenter/vRealize, Kubernetes, Jenkins, Ansible, Prometheus, and Grafana; Cloud Native: AWS EC2, AWS CDK, S3, CloudWatch, CloudFront, CloudConfig, Elastic Container Services (ECS), DynamoDB, RDS, ElasticSearch, PowerBI, Tableau.
Preferred certification: AWS Solutions Architect Associate.
Previous Transportation Security Administration (TSA) personnel clearance preferred but not required.
Prior experience with and knowledge of TSA’s mission priorities, systems, and applications preferred but not required.
Strong written and oral communication skills. Ability to coordinate across large groups of people at multiple levels.
EOE Minorities/Women/Disabled/Veterans/Gender Identity/Sexual Orientation or EEO M/W/D/V/GI/SO","$130,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1975.0,$100 to $500 million (USD)
"Charles Schwab
4.1",4.1,"Lone Tree, CO",Senior Big Data Engineer,"Your Opportunity

At Schwab, the Data and Rep Technology (DaRT) organization governs the strategy and implementation of the enterprise data warehouse, Data Lake, and emerging data platforms. Our mission is to drive activation of data solutions, rep engagement technology (Sales, Marketing and Service) and client intelligence to achieve targeted business outcomes, address data risk and safeguard competitive edge. We help Marketing, Finance, Risk and executive leadership make fact-based decisions by integrating and analyzing data.

As part of the Business Data Delivery team, you will partner with our Business stakeholders and Data Engineering team to design and develop data solutions for data science, analytics and reporting. We are a team of passionate data engineers and SMEs who bring a lot of energy, focus and fresh ideas that support our mission to contribute by seeing the world “Through Clients' Eyes”. ETL Developers work with large teams, including onshore and offshore developers, using best-in-class technologies including BigQuery, Teradata, Informatica, and Hadoop. You will design, development and implement enterprise data integration solutions with opportunities to grow in responsibility, work on exciting and meaningful projects, train on new technologies and lead other Developers to set the future of the Data Warehouse.
What you are good at

Designing, Developing and implementing new data ingestion workflows by practical application of existing and new data engineering techniques
Leading large complex projects for successful delivery
Developing data ingestion workflows across wide variety of data sources and data ingestion patterns such as batch, near real-time and real time
Working with business analysts to understand business/new data requirements and use cases
Crafting and updating ETL specifications and supporting documentation
Developing solution design by working with technical directors, Data Modelers and cross-functional teams to ensure an accurate and efficient implementation of requirements and following standards Defining and executing quality assurance and test scripts
Guiding the ETL delivery team with technical expertise
Reviewing ETL delivery from 3rd party vendor teams
Advocating for agile practices to increase delivery efficiency
Ensuring consistency with published development, coding and testing standards
Applying data integration best practices for data quality and automation
Working with product vendors to identify and manage open product issues.
What you have

Demonstrated ability as an ETL lead with a track record of delivering projects with minimal defects
7+ years of hands-on experience with data integration tools such as Informatica Power Center and Talend
7+ years in Data Warehouse platforms such as Teradata and BigData/Hadoop
At least 5 years of experience in data modeling (logical and/or physical)
At least 5 years of hands-on experience working with near realtime and/or real-time data ingestion techniques
Expertise in schema design and demonstrable ability to work with complex data is required
Strong SQL experience with the ability to develop, tune and debug complex SQL applications is required
Experience with Google Cloud Platform, BigQuery and Informatica Intelligent Cloud Services (IICS) helpful. Experience with scheduling tools (eg. Control M, ESP)
Demonstrable experience in working in large environments such as RDBMS, EDW, NoSQL, BigData etc. is preferred
Prior experience collaborating with various partners, including vendors, offshore development teams and internal groups
Ability to quickly learn & become proficient with new technologies
Strong analytical, problem-solving, influencing, prioritization, decision making and conflict resolution skills
Outstanding interpersonal skills, including collaboration, communication, and negotiation
Ability to help drive processes, run projects and solve highly complex problems using innovative solutions
Ability to work independently with little instruction on day-to-day work and lead multiple projects requiring cross-team and external collaboration
Ability to coach and mentor individuals on technical matters by sharing knowledge Ability to train and handle delivery with a team of developers.","$140,000 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Investment & Asset Management,1973.0,$10+ billion (USD)
"Mars
4.3",4.3,"Newark, NJ",Data Engineer,"Job Purpose/Overview
One Demand Data & Analytics (ODDA) is a Mars Wrigley program that harnesses the power of data and insights to solve some of the critical business-wide problems we face – unlocking quality growth and operational excellence.
Through ODDA, we deliver connected insights across the entire demand ecosystem. We empower our Associates with the right data, tools and capabilities so they can take decisive action, maximizing value and making a meaningful impact on our consumers, our customers and our business.
The Portfolio & Innovation Analytics vertical within ODDA seeks to equip Mars Associates with the capabilities needed to address portfolio health and innovation from a holistic and analytics-driven viewpoint.

Key Responsibilities
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals
Solve complex data problems to deliver insights that helps business to achieve goals
Create data products for engineer, analyst, and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data and analytic professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Define and execute the Analytics Data Engineering roadmap (and work with enterprise BI to enhance the data lake and a real-time reporting environment for operations)
Lead complex process improvement and project management engagements for both individual business units and cross-divisional initiatives
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering in order to improve productivity as a team
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with machine learning engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Mentor junior members in technical proficiency and business acumen
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Interface with business unit leaders to develop and maintain internal customer relationships
Job Specifications/Qualifications
Master’s degree in computer science, application programming, software development, information systems, database administration, mathematics, engineering, or other related field
6+ years in a rapid development environment, preferably within an analytics environment
Demonstrated ability to be work with internal (Operations) and external (IT) stakeholders
Must be able to interact and collaborate at all levels within Operations Analysis & Performance, OCC, cross-divisional working groups, and outside entities
Experience with increasing code quality and implementing best practices across teams
Advanced technical skills in the following areas:
Proficiency in SQL (CTE, window functions, temporal data), SAP HANA experience is a large plus
Proficiency in a scripting language (Python preferred)
Proficiency of API Consumption
Proficiency in ETL tooling (such as Informatica)
Proven expertise in SAP ECC and SAP APO is a big plus
Excellent communication skills and ability to present concepts to non-technical audience
Must be able to interact and collaborate at all levels within Operations Analysis & Performance, OCC, cross-divisional working groups, and outside entities
Strong project management, organizational, and prioritizations skills
2 to 4 years' experience in applied data science role or equivalent; ideally in a CPG, Retail
Knowledge and experience in modelling techniques and advanced applied skills (e.g. significance testing, GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.) using tools like Spark, Scala, SAS, R, Python, Bayesia, H2O, Storm, Yarn, and Kafka
Experience querying databases (SQL, Hive)
Experience working with big data platforms such as Hadoop ecosystem (Azure), including in-memory solutions (SAP HANA and Apache Spark)
Working knowledge of data visualization tools such as Tableau, Power BI, D3, ggplot, to deliver output to the broader business community to improve decision making and productivity
Strong communication and presentation skills
What can you expect from Mars?
Work with over 130,000 like-minded and talented Associates, all guided by The Five Principles.
Join a purpose driven company, where we’re striving to build the world we want tomorrow, today.
Best-in-class learning and development support from day one, including access to our in-house Mars University.
An industry competitive salary and benefits package, including company bonus.
#LI-Hybrid","$105,850 /yr (est.)",10000+ Employees,Company - Private,Manufacturing,Food & Beverage Manufacturing,1911.0,$10+ billion (USD)
"CrowdStrike
4.2",4.2,Remote,Sr. Engineer II - FedRAMP Data Centers (Remote),"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
This senior role will be responsible for FedRAMP related needs in data centers. Segmented responsibilities may include:
FedRAMP Data Center Environments
Design Interconnectivity for connecting FedRAMP GovCloud environments (AWS Govcloud)
Site Deployments
Maintaining Servers and Infrastructure in Data Centers
Data Center Development
Site Reliability and Operation
This role will be the person responsible for FedRAMP process related to data centers.
This role will require the candidate to periodically undergo and pass additional background and fingerprint check(s) consistent with government customer requirements.
Mentoring and developing engineers, and technicians such that they can run daily operations with minimal supervision
Build and lead a diverse, data center operations team, developing both the technical capabilities and leadership qualities of individual contributors.
Support and contribute thought leadership to the development and implementation of business practices which support the growth and ongoing management of our global data center footprint
Ability to travel as needed
What You'll Need:
10+ year’s experience working in critical environments
Experience with FedRAMP certification and operations
Experience working in a cutting edge, technical, hands-on environment and leveraging technology to manage and grow environments.
Experience in personnel management, organizational leadership, people development and team growth a plus
Experience in influencing and leading a diverse group of people, partners suppliers across multiple disciplines a plus
Knowledge of data center power, cooling, network, structured cabling, server and storage infrastructure
Leadership experience making decisions with minimal direction and prioritizing across multiple competing demands
Communication and collaboration experience
#LI-Remote
#LI-LY1
#LI-DG1
This role will require the candidate to periodically undergo and pass additional background and fingerprint check(s) consistent with government customer requirements.
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The salary range for this position in the U.S. is $135,000 - $220,000 per year + bonus + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work","$177,500 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2011.0,$500 million to $1 billion (USD)
"Anywhere Real Estate
3.5",3.5,"Madison, NJ",Data Engineer,"Anywhere Real Estate Inc is seeking a remote data engineer to join the Database Service Team, DNA (Data and Analytics Division)! Reporting to Database Team Manager, the Data Engineer will be involved with all phases of IT development projects, database and data mart administration and 24x7 production support.
We expect the joining Data Engineer should have 3-5 years of significant experiences with excellent SQL server and AWS skills.
Job Responsibilities
Development, solving, and performance tuning of complicated SQL server stored procedures and SQL Server Integration Services (SSIS) packages.
Development of ETL pipelines in and out of data warehouse using combination of AWS tools/Python (or Scala) and Snowflakes stored procedures and views.
Conducting data investigations and assisting business partners with sophisticated data analysis and ad-hoc queries.
Must be available 24x7 for Production Support with a rotating on-call schedule.
Required Skills
Bachelor’s degree in Computer Science, Engineering, Information Systems, or related field or equivalent experience
Confirmed experience with SQL Server in developing, implementing, and supporting SQL server databases for web-based applications.
Must have strong Transact-SQL skill and be able to work on complicated stored procedures.
3 years working experience with AWS Cloud Services including Apache Airflow and AWS Glue, Athena, EMR, EC2, S3, Lambda, etc.
Strong Technical hands-on experience in programming languages – T-SQL, Python, Lambda, JavaScript.
In addition to Microsoft SQL Server database, Previous experience in MongoDB, DynamoDB and Snowflake databases will be a huge plus.
Good understanding of SDLC.
Good to know Agile methodology including using Jira boards and Confluence pages.
Understanding of ETL design and development.
Excellent problem solving and root cause analysis skills.
Excellent written and verbal communication skills.

#LI-JC1
#LI-Remote


Exciting News: We are excited to announce that Realogy is now Anywhere Real Estate Inc. It will take a few months for us to transition to our new brand. For more information about this change, please click here .

EEO Statement: EOE AA M/F/Vet/Disability

Compensation Range:
$85,500 - $182,200 ; At Anywhere, actual compensation within that range will be dependent upon the individual’s skills, experience, and qualifications.","$133,850 /yr (est.)",Unknown,Company - Public,Real Estate,Real Estate,,Unknown / Non-Applicable
"Lithia Motors, Inc.
3.3",3.3,Oregon,Senior Data Engineer,"Dealership:
L0105 Lithia Home Office
Senior Data Engineer
The Senior Data Engineer is responsible for developing and supporting the cutting-edge data solutions by using the Azure stake (Data Lake, Data Warehouse, Data Factory, Functions) SQL script design/dev, and stored procedures.
The Senior Data Engineer reports to a Lead Data Engineer. This role will be Remote.
Responsibilities
Design and implement data load processes from disparate data sources into Azure Data Lake and subsequent Azure SQL & SQL Data Warehouse
Migrate existing processes and data from our On Premises SQL Server and other environments to Azure Data Lake
Explore and learn the latest Azure technologies to provide new capabilities and increase efficiency
Ensure all existing data is created in the right way, and that new data is created according to appropriate standards and with proper documentation
Read, write, and configure code for end-to-end service telemetry, alerting and self-healing capabilities
Strive for continuous improvement of code quality and development practices
Work closely with the Lead Data Engineer and other Data Engineers to develop and document solutions for providing data to the enterprise
Mentor and teach more junior developers
Skills and Qualifications
3+ years of experience in working as an analytics or data engineering member working with cross functional teams
3+ years of SQL Server development or equivalent
Azure SQL DB, SQL Data Warehouse, Azure Data Factory a plus
Version control using Git or TFS
Bachelor’s Degree in computer sciences, Analytics, Systems Eng., Statistics or related field
Strong attention to detail and sense of urgency
Competencies
Does the right thing, takes action and adapts to change
Self-motivates, believes in accountability, focuses on results, makes plans and follows through
Believes in humility, shares best practices, desires to keep learning, measures performance and adapts to improve results
Thrives on a team, stays positive, lives our values
Physical Demands
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job.*
Up to 1/3 of time: standing, walking, lifting up to 25 pounds
Up to 2/3 of time: sitting, kneeling, reaching, talking, hearing
*Reasonable accommodations may be made to enable individuals to perform the essential functions.
NOTE: This is not necessarily an exhaustive list of responsibilities, skills, or working conditions associated with the job. While this list is intended to be an accurate reflection of the current job, the company reserves the right to revise the functions and duties of the job or to require that additional or different tasks be performed.
We offer best in class industry benefits:
Competitive pay
Medical, Dental and Vision Plans
Paid Holidays & PTO
Short and Long-Term Disability
Paid Life Insurance
401(k) Retirement Plan
Employee Stock Purchase Plan
Lithia Learning Center
Vehicle Purchase Discounts
Wellness Programs
High School graduate or equivalent, 18 years or older required. Acceptable driving record and a valid driver's license in your state of residence necessary for select roles. We are a drug free workplace. We are committed to equal employment opportunity (regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status). We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.",,10000+ Employees,Company - Public,Retail & Wholesale,Vehicle Dealers,1946.0,$10+ billion (USD)
"HaystackID
3.3",3.3,Remote,Automation Engineer (Data Automation Services),"Position Summary:
The Automation Engineer (AE) assist the Senior Director of Data Automation (SDDA) in developing, implementing, providing quality assurances, and supporting and all aspects of data automation processes and systems across HaystackID departments that handle, manipulate, or create client facing or client owned data. The AE assists in the development of automation planning, change management documentation, approval processes, as well as test and validation cycles. The AE ensures all automation processes have fully developed and documented standard procedures that are regularly audited, updated, and distributed to the relevant staff. The AE provides support of all automation workflows and systems.

Essential Job Duties:
Project planning and development – Developing workflows to accomplish HaystackID’s data automation needs. This includes working with the Automation team to determine the specifications and developing and implementing workflows for particular business needs. This may also include performing proof of concept on new or emerging technologies to advance automation efforts.
Automation management – Monitoring and analyzing automation workflows, operations, and computing system resources using analytic measures and models to ensure operational throughputs are meeting client needs. This also includes recommending and implementing system resource allocations to improve effectiveness and attain project milestones.
Good communication skills. – Communicating with the Automation team will be essential for both internal and external client updates and project tracking.
Back-end Development – Proficient in back-end languages including Java, Python, Rails, Ruby, .NET, and PHP. The focus of the role will be deployment of self-contained packages that will leverage APIs from cloud platforms, internal applications and other business data sources to support the aforementioned workflows and business needs.
Database Development – Successful candidates will be able to communicate with SQL, Lucene, and other databases systems.
Writing Code – Testing, reporting, bug-fixing and documenting code to a high standard.

Nice to Have:
Knowledge of the following platforms:
Nuix
Rampiva
Relativity
QuickBase
Dynamics 365
Business Central
Secure Transfer systems

Qualifications:
Must exhibit strong technical writing skills, have ability to analyze information, communicate processes, and exhibit excellent follow-up skills.
Ability to work under pressure and able to multi-task and prioritize work assignments to meet deadlines.
Have a successful tracking record of managing engagements of all sizes and durations.
Deep technical knowledge of the following:
Rampiva Automate
Nuix eDiscovery
Relativity
AccessData Forensics ToolKit
EDRM
Visual Studio
SQL Server
Python

Work Environment:
Remote

Education & Experience Preferred:
Bachelor’s degree in Computer Science, information systems, mathematics, statistics or other relevant degree.

Company Benefits:
Medical, Dental and Vision insurances
Life, Short and Long-term Disability Insurances
401(k) after 90-days
PTO and Paid Holidays",,201 to 500 Employees,Company - Private,Legal,Legal,2011.0,$100 to $500 million (USD)
"Chicago Transit Authority
3.3",3.3,"Chicago, IL",Senior Data Engineer,"Under general supervision, takes a broader-scale focus to building, developing and maintaining data architectures, including back-end infrastructures, complex data integrations and associated processes. Participates in the design, evaluation, selection, implementation and support of new data engineering techniques related to processing data in both structured and unstructured formats. Makes use of a range of industry standard programming languages and data frameworks. Recommends ways to improve the reliability, quality and usefulness of agency data as it relates to the needs of enterprise Data Scientists, Analysts, related stakeholders and departmental direction.


Qualifications
PRIMARY RESPONSIBILITIES

% time

1

45%

Develops, constructs, tests and maintains data architecture, including databases, data processing systems and related applications. Develops custom applications and infrastructure as necessary. Researches and deploys data infrastructure best practices. Builds and optimizes data pipelines to support cross-functional analytics, automation and programming needs.

2

25%

Develops and maintains best practices for marrying disparate CTA data systems. Liaises with Data Scientists, Programmers, Developers and Analysts to support integration of new data processing and warehousing technologies.

3

15%

Maintains existing data analytics infrastructure and architectures. Leads new efforts to acquire and centralize data from disparate systems. Works as directed to assist department Director in data related project oversight and execution.

4

5%

Researches best practices, recommends changes to improve efficiencies, assists in revising processes and procedures and establishes enterprise data architectures. Assesses existing business needs and anticipates future business needs in order to support the growth of department and agency infrastructure and applications.

5

5%

Hires, trains, develops, monitors and evaluates staff. Reviews and recommends personnel actions for approval.

6

5%

Performs other duties as assigned.

MANAGEMENT RESPONSIBILITIES
Reporting to this position are the following jobs:
Job Title

Senior Analyst, Data Analytics
Data Scientist
Programmer Analyst, Data Analytics
Business Intelligence Developer

CHALLENGES
Working with and managing massive amounts of data sourced from several disparate data systems and applications.
Implementing effective time and project management processes to deliver data services on time and on budget.
Balancing multiple high-priority requests simultaneously while adapting to rapidly-changing demands and meeting project deadlines.
Providing consistency in data processing efforts to help shape the utilization and actionability of information.

EDUCATION/EXPERIENCE REQUIREMENTS
Bachelor’s degree in Statistics, Computer Science, Mathematics, Operations Research, Industrial Engineering, Quantitative Analysis, Economics or a related field, plus four (4) years of experience in data engineering, development, and/or a combination of education and experience.

PHYSICAL REQUIREMENTS
Requires sitting for extended periods of time, standing, visual acumen, manual dexterity and fingering for working with computer keyboards.
Chicago Transit Authority requires all employees to be COVID-19 vaccinated. If you are offered employment, you must provide proof of full COVID-19 vaccination or proof that you are in the process of becoming fully vaccinated as part of the hiring process and as a condition of employment. Visit www.transitchicago.com/careers
Service Area Requirement: Exempt (Non-Union) employees must live within the boundaries of the CTA Statutory Service Area either at the time of employment or within 6 months of beginning employment at CTA.


KNOWLEDGE, SKILLS, AND ABILITIES
Detailed knowledge of various data programming languages and frameworks (i.e. Python, SQL, Java, JavaScript, PHP, C#/.NET, AngularJS, etc.).
Detailed experience in custom and/or industry-standard data migration, integration, and ETL tools and techniques.
Detailed knowledge of big data frameworks and tools (Hadoop, Spark, MongoDB, Cassandra, etc.)
Detailed knowledge of programming development requirements and use of source/revision control systems (i.e. Git, Mercurial etc.)
Strong knowledge of cloud-based data infrastructure (AWS, Azure, Google, Vertica)
Strong knowledge of a range of RDBMS platforms (Postgres, Oracle, SQL Server, MySQL, DB2, etc.)
Strong knowledge of microservice architectures.
Strong knowledge of continuous delivery and deployment pipelines.
Strong analytical, problem-solving, and decision-making skills.
Strong report preparation and presenting skills.
Strong oral and written communication skills.
Strong interpersonal and team skills across a variety of fields and management levels.
Strong project management skills.
Strong organization and time management skills.
Ability to effectively analyze and translate data engineering challenges to the business.
Ability to manage large amounts of data and attention to detail.
Ability to multitask competing projects and deadlines for completion.


WORKING CONDITIONS
General office environment.
Required to occasionally travel to locations throughout the CTA system and Chicago area as needed.

EQUIPMENT, TOOLS, AND MATERIALS UTILIZED
Standard office equipment.
Modern data engineering tools, platforms and processes.


Additional Details
Please note, employees and/or union members will be given priority consideration in the hiring process, per the applicable labor contracts.

Final salary will be determined in part by the qualifications of the selected candidate and may be higher or lower than target.

Applicants, if hired,must comply with CTA's residency ordinance.

CTA IS AN EQUAL OPPORTUNITY EMPLOYER

No employee or applicant for employment will be discriminated against because of race, color, creed, religion, sex, marital status, national origin, sexual orientation, ancestry, age, unfavorable military discharge, disability or any other status protected by federal, state, or local laws; except where a bona fide occupational qualification exists We are committed to providing an inclusive environment for our workforce and supporting the communities we serve. CTA will make reasonable accommodations for the known disabilities of otherwise qualified applicants for employment as well as its employees, unless undue hardship would result. If you require an accommodation in the application or hiring process, please contact arc@transitchicago.com prior to the submission of your application or upon notification of your actual test date. CTA will work with you to determine if an accommodation can be provided.

Primary Location: USA-Illinois-Chicago
Job: Data Analytics
Job Posting: May 12, 2023, 12:19:19 PM
Position Type: Full-time Permanent (FTP)","$121,471 /yr (est.)",10000+ Employees,Government,Transportation & Logistics,Taxi & Car Services,1947.0,$500 million to $1 billion (USD)
"Verily
3.8",3.8,"Waterloo, IA","Software Engineer, Data Ingestion","Waterloo, ON

Who We Are
Verily is a subsidiary of Alphabet that is using a data-driven approach to change the way people manage their health and the way healthcare is delivered. Launched from Google X in 2015, Our purpose is to bring the promise of precision health to everyone, every day. We are focused on generating and activating data from a variety of sources, including clinical, social, behavioral and the real world, to arrive at the best solutions for a person based on a comprehensive view of the evidence. Our unique expertise and capabilities in technology, data science and healthcare enable the entire healthcare ecosystem to drive better health outcomes.
DESCRIPTION
As a member of the Precision Health Platform engineering organization, you will build modular, composable, and interoperable platform components, including storage and processing of precision health data, and other product framework components across Verily products.
RESPONSIBILITIES
Work closely with the development team to design, develop, and deliver new software features across the Verily tech stack.
Come up with ideas to technical design problems, compare options, and propose solutions.
Develop using industry-standard tools including GitHub, Google Cloud, Go, Docker, and Terraform to name a few.
QUALIFICATIONS
Minimum qualifications:
BA/BS degree in Computer Science, Electrical Engineering, or equivalent practical experience in software engineering.
At least 5 years experience as a software engineer in an industry setting.
Ability to work independently and collaborate effectively.
Expertise in building software and systems on any of: GCP, AWS or Azure (GCP preferred).
Experience with one or more general purpose programming languages including but not limited to: Java, C/C++, C#, Objective-C, Go, Python, or JavaScript.
Preferred qualifications:
Demonstrated experience with Go, Java, Python, and/or SQL.
Proficiency in Apache Beam and its associated technologies for data ingestion and processing.
Solid understanding of ETL principles and best practices.
Experience with designing and implementing scalable data processing pipelines with Bioquery or other analytics-oriented databases.
Education or exposure to healthcare or life sciences, with emphasis on HL7 FHIR.
LI-TB
Why Join Us
Build What’s Vital.
At Verily, you are a part of something bigger. We are a diverse team of builders innovating at the intersection of health and technology—united by a shared spirit of curiosity, resilience and determination to make better health possible for all. This builder mindset means your fingerprints will be on the work that shapes the future of health. Fulfilling our precision health purpose starts with the health of our Veeps (what we call our employees), which is why we offer flexibility, resources, and competitive benefits to support you in your whole-person well being. We believe diversity of thought drives innovation—we unite the brightest minds, and encourage all Veeps to bring their lived experience to work with them.
If this sounds exciting to you, we would love to hear from you.
You can find out more about our company culture on our LinkedIn Company Page and Verily Careers page.","$96,116 /yr (est.)",1001 to 5000 Employees,Subsidiary or Business Segment,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2015.0,Unknown / Non-Applicable
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Aurora, CO",Data Center Operations Engineer II (Front End Day Shift),"Launch your career in Technology Operations and put your creative problem solving into action, delivering solutions that shape the future of global business. You'll work directly with clients to build strong customer relationships and problem-solve technical issues to make businesses more productive. Alongside a motivated team of fellow analysts, supervisors, and stakeholders, you'll develop innovative solutions to troubleshoot and resolve issues while accurately diagnosing problems and providing effective user support. Finally, your strong technology background will ensure that the security and standards of our commitment to excellence are met. And because professional development is a key component of our culture, you'll receive coaching, mentoring - and a host of other development opportunities - alongside your invaluable on-the-job experience.

This role requires a wide variety of strengths and capabilities, including:
Ability to identify problems and clearly communicate strategic solutions to clients
Desire to develop a working knowledge of change management, corporate IT audit processes, IT risk management, technical problem resolution, operations systems, and data sources knowledge
Strong initiative and desire to learn
Ability to effectively collaborate with team members and clients to achieve common goals
Good knowledge of Windows/MAC OS with the ability to carry out root cause analysis
Working knowledge of Microsoft Office products
Strong analytical and problem resolution skills
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Aurora,CO $31.98 - $52.16 / hour",$42.07 /hr (est.),10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799.0,$10+ billion (USD)
IntellectFaces Technology Solutions Pvt Ltd,,"Washington, DC","Network and Data Center Engineer (Ashburn,VA/ Washington, DC/ Suitland, MD)","Network Engineer and Data Center Engineer
Fulltime
Ashburn, VA / Washington ,DC / Suitland, MD
Onsite - 5 days
Ability to obtain Public trust clearance
Responsibilities:
The “Need-to-Have” Skills & Qualifications:
· Must have a College Degree
· Eight (8) years of experience in a large government organization responsible for administering, operating, maintaining, and securing a large-scale internetworking environment, including one year in a technical leadership or supervisory role
· Three (3) years of experience directly supporting a complex infrastructure covering the major aspects of network administration with expertise in areas of troubleshooting IPv4 issues across varies protocols and technologies such as OSPF, static routing, MPLS, HSRP, SSH, ACL, VLAN, VSAN, VTP, STP, Trunking, SNMP, LACP, vPC, VoIP, Port Channels, Wireshark and TCPdumps
· Three (3) years of a very strong hands on experience understanding and having the ability to implement and troubleshoot Cisco and HP routers and switches (i.e., ISR, ASR, 9500, 9300, 6513, 3850, 3750, 2960, 2920, 2910, Nexus 2k, 5k and 7k)
· Two (2) year experience configuring, administering and maintaining network IT monitoring and management software such as ForeScout CounterACT, Netscout nGenius and Solarwinds (i.e., NetFlow traffic analysis, Bandwidth monitoring, PerfStack, Network performance baselines, Traffic Analyzer, IPAM and NetPath)
· One (1) year of experience having a role in data center operations. Thorough knowledge of data center hardware specifications, lights-out-management solution practices, familiar with power requirements, racking and decommissioning of hardware
· Thorough knowledge of telecommunications, network security issues and best practices
· Exceptional customer service orientation, written and oral communications skills
Certifications:
· Technical certification (e.g., Cisco Certified Network Associate, Cisco Certified Design Associate, Cisco Certified Network Professional, Cisco Certified Design Professional, etc.)
Required Technical / Business Toolset Experience:
· Cisco management and support tools
· Solarwinds, Verizon Concord Circuit Monitoring Application, or other similar network monitoring tools
· Microsoft Visio or similar tools
The “Nice-to-Have” Skills:
· Ability to present complex network solutions to new and ongoing projects and stakeholders
· Ability to write solid, clear, detailed technical solutions, implementation plans, schedules, action items, document incidents and complete root cause analysis statements
· Self-motivated, minimally supervised with the willingness of learning and training others
· High-energy, resourceful individual, can-do attitude seeking awareness to solve problems and identify workarounds and solutions
· Exhibit work flexibility and has the capabilities to adapt to new changes or directions
· Support management and customer’s objectives, scopes, goals, and visions
Job Type: Full-time
Pay: $100,000.00 - $120,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
maintaining, and securing a large-scale internetworking: 8 years (Required)
Data center: 4 years (Required)
Work Location: In person","$110,000 /yr (est.)",,,,,,
"Analytica
3.4",3.4,Remote,Senior Data Engineer (Remote),"Analytica is seeking a Senior Data Engineer with Microsoft SQL experience to support a long term data science project for a federal government client. The ideal candidate will be comfortable being a lead data engineer working with data scientists to understand data requirements, develop SQL queries and stored procedures for feeding the database and data models.

Analytica has been recognized by Inc. Magazine as the fastest-growing private US small business. We work with U.S. government customers in health, civilian, and national security missions. As a core member you’ll work with a diverse team of professionals to solution matters, architect nuisances, and come up with alternatives. We offer competitive compensation with opportunities for bonuses, employer paid health care, training and development funds, and 401k match.

Responsibilities include (But Are Not Necessarily Limited To):
Conduct database development on Microsoft SQL Server
Develop business solution logic using SQL and T-SQL or similar query/scripting languages to execute on the reporting and data validation needs related to the data platform.
Create and maintain SQL scripts, stored procedures, and other program logic to create, update, and delete data.
Design and implement schema changes, manage indexes, and alter data objects to optimize performance.
Design efficient data models from a logical design based on business requirements and available use patterns.
Identify entity relationships, referential integrity constraints, and primary key structures. Perform analysis of tradeoffs between alternative schemas.
Extend data templates to include relevant fields, publish data standards, document and publish data taxonomy and hierarchies.
Support data science team to compile, analyze and extract data for use in advanced machine learning and modeling.
Create and maintain documentation that includes file specifications, schema, core record layouts, programs, business requirements, test plans, and other artifacts used in the administration, creation, and execution of database operations.
Use GitHub for code deployment and version control in a collaborative development environment.



Minimum Qualifications:
Bachelor’s Degree in Computer Science, IT, Computer Engineering, or related field
5+ years Structured Query Language (SQL) programming
5+ years’ experience in Microsoft SQL database programming / development / administration
Experience working in an Agile environment or Scrum teams
Expertise in designing and building enterprise-grade applications
Strong communication, leadership, and problem-solving skills
Experience developing data streams in AWS a plus
Familiarity with tax or financial related data a plus

About ANALYTICA: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD, the company is an established 8(a) small business that has been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider.

As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation.",,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2009.0,$5 to $25 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE",Lead Data Engineer,"As a Lead Data Engineer within Consumer and Community Banking, in Home Lending, you are an integral part of an agile team that works to enhance, build, and deliver data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. As a core technical contributor, you are responsible for maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job Responsibilities
Design and implement end-to-end data pipelines supporting analytical and operational needs accounting for data management practices focused on data quality, metadata management etc.
Architect, design, and implement cloud native solutions on AWS.
Define and implement event driven architecture patterns leveraging messaging / streaming solutions like Kafka, Kinesis, Flink, and Spark
Ability to decompose large initiatives / designs into manageable smaller bodies of work to demonstrate continuous progress
Collaborate with business stakeholders, product owners, architects, data domain owners to understand current landscape and develop solutions in alignment with business & technology strategy. Assist in refining /evolving data strategy highlighting clear outcomes.
Deep understanding or desire to continue to learn new database technologies, cloud computing & storage services
Understanding of the pros / cons associated with various technology choices and ability to pick the right technology based on the use case

Required qualifications, capabilities, and skills
Formal training, or certification on data engineering concepts, and 5+ years of experience. In addition, demonstrated coaching and mentoring experience
Programming experience in Java, Python, Scala etc.
Experience in using distributed frameworks like Spark, Hadoop etc.
Experience with AWS services like Lambda, EC2, EMR, Redshift, Glue, S3, IAM, RDS, Aurora, DynamoDB etc.
Knowledge of cloud networking, security, storage, and compute services
Infrastructure provisioning experience using Cloud Formation, Terraform etc.
Experience implementing solutions leveraging CI / CD etc.

Preferred qualifications, capabilities, and skills
AWS Solutions Architect / Developer or any advanced level certification preferred
Experience and proficiency across the data lifecycle
Experience with database back-up, recovery, and archiving strategy
Proficient knowledge of linear algebra, statistics, and geometrical algorithms
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans","$116,853 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799.0,$10+ billion (USD)
"ABBVIE
3.9",3.9,"Crystal Lake, IL",Senior Data Engineer,"AbbVie Information Research is seeking a Senior Data Engineer who would contribute to the architecture, design, and development of the Data & Analytics Platform supporting world-class research and development at AbbVie. As a Senior Data Engineer, you will be a core member of a high-performance team of data engineers and architects focusing on driving technology innovation and continuous improvement. This role collaborates with solution architects, product owners, program managers, business analysts, infrastructure teams, and service providers to deliver the solutions.
Responsibilities:
Demonstrate mastery across a wide variety of data engineering activities, including data warehousing, master data management, data cataloging, system integration, data streaming, data visualization, data analysis, and data ops.
Demonstrate knowledge of pharmaceutical R&D/Life Science centric datasets and utilize this knowledge to advance agile, impactful, and cost-effective solutions rapidly.
Collaborate & contribute to the architecture, design, development, and maintenance of large-scale data & analytics platforms, system integrations, data pipelines, data models & API integrations to support evolving business strategy.
Contribute and maintain the team's methodology to conform and curate data, benchmarking against industry standards. Ensure that data are optimally standardized and analysis-ready.
Prototype emerging business use cases to validate technology approaches and propose potential solutions.
Research and recommend opportunities to adopt new technologies for continuous improvement.
Ensure compliance with applicable AbbVie software development lifecycle policies and procedures.

Bachelor's degree with 7 years of IT experience
Must have experience with software development life cycle; Experience with DevOps is preferred.
Must have experience with data analysis programming languages (e.g., SQL, Python & Apache Spark, SAS & R)
Must have experience with database technologies (e.g., Oracle, Postgres, Hive, and HBase)
Must have experience with ETL/Orchestration tools (e.g., Informatica, Autosys, and Airflow, etc.)
Experience with AWS and Cloudera Public Cloud architecture is preferred.
Experience working with Pharmaceutical R&D industry-centric datasets is preferred.


AbbVie is an equal opportunity employer including disability/vets. It is AbbVie’s policy to employ qualified persons of the greatest ability without discrimination against any employee or applicant for employment because of race, color, religion, national origin, age, sex (including pregnancy), physical or mental disability, medical condition, genetic information, gender identity or expression, sexual orientation, marital status, status as a disabled veteran, recently separated veteran, Armed Forces service medal veteran or active duty wartime or campaign badge veteran or a person’s relationship or association with a protected veteran, including spouses and other family members, or any other protected group status. We will take affirmative action to employ and advance in employment qualified minorities, women, individuals with a disability, disabled veterans, recently separated veterans, Armed Forces service medal veterans or active-duty wartime or campaign badge veterans. The Affirmative Action Plan is available for viewing in the Human Resources office during regular business hours.","$119,356 /yr (est.)",10000+ Employees,Company - Public,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2013.0,$10+ billion (USD)
"GE Healthcare
4.2",4.2,Illinois,Sr Data Engineer,"Job Description Summary
Responsible for managing business critical data engineering processes and data architecture solutions in order to enable analytical and reporting solutions. Responsible for analyzing and preparing the data needed for data science based outcomes. Also responsible for managing and maintaining metadata data structures besides providing necessary support for post-deployment related activities. Accountable to deliver results in a timely manner using agile methodologies.
Job Description
Roles and Responsibilities
In this role, you will:
Design & build technical data dictionaries and support business glossaries to analyze the datasets
Perform data profiling and data analysis for source systems, manually maintained data, healthcare industry standard messages
Design & build both logical and physical data models for both Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP) solutions
Develop and maintain data mapping specifications based on the results of data analysis and functional requirements
Build a variety of data loading & data transformation methods using multiple tools and technologies.
Design & build automated Extract, Transform & Load (ETL) jobs based on data mapping specifications
Manage metadata structures needed for building reusable Extract, Transform & Load (ETL) components.
Analyze the impact of changes to downstream systems/products and recommend alternatives to minimize the impact.
Design and build data warehouses and data marts
Minimum Qualifications
Bachelor's Degree in Computer Science or “STEM” Majors (Science, Technology, Engineering and Math) with minimum 6 years of relevant experience
Exposure to industry standard data modeling tools
Exposure to Extract, Transform & Load (ETL) tools like SSIS or Azure Data Factory
Exposure to industry standard BI tools like Power BI and Tableau
Hands-on experience in writing SQL scripts for SQL Server, MySQL, PostgreSQL or HiveQL
Exposure to unstructured datasets and ability to handle Avro, Parqueet, JSON file formats
Conduct exploratory data analysis and generate visual summaries of data. Identify data quality issues proactively.

Desired Qualifications:
Knowledge of for industrial applications in healthcare settings.
A good team player with self-driven execution capabilities.
Ability to communicate ideas clearly with cross teams.
Ability to showcase teamwork skills to achieve common goals, provide resolutions and share ideas.
Demonstrate the presentation and influencing skills
Eligibility Requirements
GE HealthCare may choose to sponsor visas as business needs dictate.
GE HealthCare will only employ those who are legally authorized to work in the United States for this opening.
Work/Life Balance
Our team puts a significant value on work-life balance. Having a healthy balance between your personal and professional life is crucial to your happiness and success here. We don’t focus on how many hours you spend at work or online. Instead, we’re happy to offer a flexible schedule so you can have a more productive and well-balanced life—both in and outside of work.
Mentorship & Career Growth
We maintain diverse engineering, and leadership perspectives and backgrounds across technology and beyond. Our employees are excited to share their experiences and mentor more junior engineers. Team members are highly encouraged to set up mentorship relationships with seasoned engineers, not only in our team, but also across the broader GE Healthcare population.

Inclusive Team Culture
Here at GE HealthCare, we embrace our differences. We are committed to furthering our culture of inclusion. We have many employee-led affinity groups, innovative benefit offerings, and encourage ongoing learning experiences.
While GE HealthCare does not currently require U.S. employees to be vaccinated against COVID-19, some GE HealthCare customers have vaccination mandates that may apply to certain GE HealthCare employees.

#LI-RW1
Additional Information
GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.
GE will only employ those who are legally authorized to work in the United States for this opening. Any offer of employment is conditioned upon the successful completion of a drug screen (as applicable).
Relocation Assistance Provided: No",,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1892.0,$10+ billion (USD)
"GSK
4.1",4.1,"Collegeville, PA",Senior Data Platform Engineer,"Site Name: San Francisco, Cambridge 300 Technology Square, London The Stanley Building, Upper Providence
Posted Date: May 12 2023

At GSK, we want to supercharge our data capability to better understand our patients and accelerate our ability to discover vaccines and medicines. The Onyx Research Data Platform organization represents a major investment by GSK R&D and Digital & Tech, designed to deliver a step-change in our ability to leverage data, knowledge, and prediction to find new medicines.
We are a full-stack shop consisting of product and portfolio leadership, data engineering, infrastructure and DevOps, data / metadata / knowledge platforms, and AI/ML and analysis platforms, all geared toward:
Building a next-generation, metadata- and automation-driven data experience for GSK’s scientists, engineers, and decision-makers, increasing productivity and reducing time spent on “data mechanics”
Providing best-in-class AI/ML and data analysis environments to accelerate our predictive capabilities and attract top-tier talent.
Aggressively engineering our data at scale, as one unified asset, to unlock the value of our unique collection of data and predictions in real-time.
Automation of end-to-end data flows: Faster and reliable ingestion of high throughput data in genetics, genomics and multi-omics, to extract value of investments in new technology (instrument to analysis-ready data in <12h)
Enabling governance by design of external and internal data: with engineered practical solutions for controlled use and monitoring
Innovative disease-specific and domain-expert specific data products: to enable computational scientists and their research unit collaborators to get faster to key insights leading to faster biopharmaceutical development cycles.
Supporting e2e code traceability and data provenance: Increasing assurance of data integrity through automation, integration
Improving engineering efficiency: Extensible, reusable, scalable, updateable, maintainable, virtualized traceable data and code would be driven by data engineering innovation and better resource utilization.
We are looking for a skilled and experienced Sr. Data Platform Engineer to join our growing team. Sr. Data Platform Engineers take full ownership of delivering high-performing, high-impact data framework products, and services, from a description of a problem customer Data Engineers are trying to solve all the way through to final delivery (and ongoing monitoring and operations). They are standard bearers for software engineering and quality coding practices within the team and are expected to mentor more junior engineers; they may even coordinate the work of more junior engineers on a large project. They devise useful metrics ensuring their services are meeting customer demand, having an impact, and iterate to deliver and improve on those metrics in an agile fashion.
The Data Platform team builds and manages reusable components and architectures designed to make it both fast and easy to build robust, scalable, production-grade data products and services in the challenging biomedical data space.
A Sr. Data Platform Engineer is a highly technical individual contributor, building modern, cloud-native systems for standardizing and templatizing data engineering process, such as:
Standardized data pipeline orchestration and processing
Standardized physical storage and search / indexing systems
Metadata management (data + metadata + versioning + provenance + governance)
API semantics and ontology management
Standard API architectures
Standard streaming semantics
Standard components for publishing data to file-based, relational, and other sorts of data stores
Tooling for QA / evaluation
Etc.
A Sr. Data Platform Engineer knows the metrics desired for their tools and services and iterates to deliver and improve on those metrics in an agile fashion.
Additional responsibilities also include:
Own architecture design of data platform and integration patterns to other internal systems
Mentor junior team members for better engineering standard and process
Partner with Infra and DevOps team where modifications to underlying tools (e.g. infrastructure as code, Cloud Ops, DevOps, logging / alerting) are needed to serve new use-cases, and to ensure operations are planned
Write fantastic code along with the proper unit, functional, and integration tests for code and services to ensure quality. Mentor more junior engineers in these skills
Stay up to date with developments in the open-source community around data engineering, data science, and similar tooling.
Spot opportunities to test out new tooling for internal use cases, as well as opportunities to contribute back to the community.
Why you?
Basic Qualifications:
We are looking for professionals with these required skills to achieve our goals:
Master's in computer science with a focus in Data Engineering, DataOps, DevOps, MLOps, Software Engineering, etc., plus 5 years job experience, (or PhD or bachelor’s degree in computer science plus 3-8 years job experience)
Experience with common distributed data tools in a production setting (Spark, Kafka, Hive, Presto, etc.)
Experience with specialized data architecture (e.g., data lake, lake house, data fabric, data mesh, optimizing physical layout for access patterns)
Experience with public cloud providers like AWS, Azure and GCP
Experience with search / indexing systems (e.g., Elasticsearch)
Preferred Qualifications:
If you have the following characteristics, it would be a plus:
Experience building and designing a DevOps first way of working.
Demonstrated excellence writing production Python, Java, Scala, Go, and/or C#/C++
Practical experience with agile software development and DevOsps-forward ways of working
Demonstrated experience building reusable components on top of the CNCF ecosystem including platforms like Kubernetes (or similar ecosystem)
Metrics-first mindset
#LI-GSK
#GSKOnyx
#GSKDSDE2022
#GSKDEN2022

Why GSK?
Our values and expectations are at the heart of everything we do and form an important part of our culture. These include Patient focus, Transparency, Respect, Integrity along with Courage, Accountability, Development, and Teamwork. As GSK focuses on our values and expectations and a culture of innovation, performance, and trust, the successful candidate will demonstrate the following capabilities:
Operating at pace and agile decision making – using evidence and applying judgement to balance pace, rigor, and risk.
Committed to delivering high-quality results, overcoming challenges, focusing on what matters, execution.
Continuously looking for opportunities to learn, build skills and share learning.
Sustaining energy and wellbeing
Building strong relationships and collaboration, honest and open conversations.
Budgeting and cost consciousness
GSK offers a competitive compensation package inclusive of the following: Competitive base salary, annual bonus based on company performance, access to healthcare and wellbeing programs, retirement savings program, paid time off, and employee recognition programs which reward exceptional achievements. The salary range for this role is: $145,877 to $197,363
GSK is a global biopharma company with a special purpose – to unite science, technology and talent to get ahead of disease together – so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns – as an organisation where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to positively impact the health of 2.5 billion people by the end of 2030.
Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it’s also about making GSK a place where people can thrive. We want GSK to be a workplace where everyone can feel a sense of belonging and thrive as set out in our Equal and Inclusive Treatment of Employees policy. We’re committed to being more proactive at all levels so that our workforce reflects the communities we work and hire in, and our GSK leadership reflects our GSK workforce.
If you require an accommodation or other assistance to apply for a job at GSK, please contact the GSK Service Centre at 1-877-694-7547 (US Toll Free) or +1 801 567 5155 (outside US).
GSK is an Equal Opportunity Employer and, in the US, we adhere to Affirmative Action principles. This ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, national origin, religion, sex, pregnancy, marital status, sexual orientation, gender identity/expression, age, disability, genetic information, military service, covered/protected veteran status or any other federal, state or local protected class.
Important notice to Employment businesses/ Agencies
GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.
Please note that if you are a US Licensed Healthcare Professional or Healthcare Professional as defined by the laws of the state issuing your license, GSK may be required to capture and report expenses GSK incurs, on your behalf, in the event you are afforded an interview for employment. This capture of applicable transfers of value is necessary to ensure GSK’s compliance to all federal and state US Transparency requirements. For more information, please visit GSK’s Transparency Reporting For the Record site.","$171,620 /yr (est.)",10000+ Employees,Company - Public,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,1830.0,$10+ billion (USD)
"MAQ Software
3.3",3.3,"Redmond, WA",Software Data Operations Engineer,"About MAQ Software
As 2021 Microsoft Power BI Partner of the Year, we enable leading companies to accelerate their business intelligence and analytics initiatives. Our solutions enable our clients to improve their operations, reduce costs, increase sales, and build stronger customer relationships.
Our clients consistently recognize us for providing architecture and governance frameworks, implementing best practices to optimize reports, and building team capability through training programs. Our innovative tools and 33 certified visuals expand Power BI capabilities to save time for decision makers.
As a premier supplier to Microsoft for two decades, our clients benefit from our extensive insights on the platform and engineering practices. As a Microsoft Partner with 10 Gold competencies, our clients improve their implementations with our breadth and depth of expertise.
With globally integrated teams in Redmond, Washington, and Mumbai, Hyderabad, and NOIDA India, we deliver solutions with increased velocity and tech intensity.
Inc. magazine has recognized us for sustained growth by listing us on the Inc. 5000 list ten times – a rare honor.

Engineering culture
We foster a strong engineering culture with a can-do attitude. All our key managers come from excellent educational backgrounds and have significant experience growing a company and mentoring software engineers. Due to our smaller size, we adopt the latest technologies and computing trends ahead of the larger industry players. As a part of the company’s globally distributed engineering team, our engineers gain exposure to the latest software engineering practices and fast development cycles.
Our developers routinely work on challenging technical problems that utilize the latest technologies for fast-paced software delivery.

Examples of some of our projects:
We built a supervised machine learning model that forecasts the impact of retail sales on our client’s overall revenue. We collected data from existing customer relationship management (CRM) and sales systems. We created a forecasting model in Azure Databricks using existing and custom linear regression to process the collected data. To reduce forecast runtime and achieve near real-time analysis, we modified the existing R libraries to SparkR. The improved insight helped our client proactively focus on retailers with the highest sales impact.
We built a check-in app for one of our client’s most attended event. A multinational technology company organizes an annual multi-event internal expo attended by thousands of their employees. The manual process of tracking attendance, sending acknowledgments, and receiving feedback was time consuming. To automate the process, we built a check-in app that uses mobile devices’ camera to capture the identification badge of each participant. The captured images are stored in an Azure Blob. An Azure Logic App reads the image content utilizing Optical Character Recognition (OCR) API to update attendance records. After the event, notifications are sent to attendees via Microsoft Teams to complete a feedback survey using a Microsoft Power Automate Bot. The Feedback App reports the survey responses to determine the Customer Satisfaction (CSAT) score of the event.
For another client with high volume data, we developed and implemented a hybrid data processing solution using Azure Stream Analytics and Azure Databricks to reduce data refresh time from 3 hours to less than 30 minutes. We sourced data from the Azure Event Hub, where refreshes originate. Refreshes are captured through stream analytics and the updated data is pushed to Azure Data Lake Storage (ADLS). The data is processed in ADLS, then pushed to Power BI for reporting.
To read about some of our recent projects, visit https://maqsoftware.com/case-studies

Responsibilities:
Analyze existing systems (30%)
Collect requirement specifications to analyze business processes and determine the exact nature of user’s system requirements, map process flow, discuss with module leaders and core team members to decide on the architecture.
Analyze existing system structures to provide solutions to improve computer systems to use cloud-based systems and services.
Analyze user requirements to match data available to large computer database source systems to implement solutions at reasonable performance and cost.
Design the processing steps and propose new systems based the user’s requirements. Interact with systems analysts/programmers to develop data migration tools, create processes for the new computer system and attend to ad-hoc issues related to day-to-day activities. Work with software developers in the implementation and testing phase.

Develop specifications and workflow (25%)
Prepare software specifications, flow charts, and process diagrams for software programmers to follow. Develop and maintain systems documentation such as design specifications, user manuals, technical manuals, descriptions of application operations, and methodology documentation.
Analyze feasibility using commercially available software systems (e.g., Microsoft Azure versus Amazon Web Services) and reporting systems (e.g., Power BI versus Tableau).

Analyze and verify implementation (25%)
Interact with systems analysts/programmers to develop data migration tools, create processes for the new computer system and attend to ad-hoc issues related to day-to-day activities.
Work with other software developers in the implementation and testing phase.
Setup test environment and compare data from multiple sources to verify reports for end users.

Review implementation status and reporting (10%)
Participate in technical collaboration meetings and periodical reviews of implementation status.
Report weekly task plan to the project management team for implementation of custom software.

Training and certifications (10%)
Participate in technical trainings and complete relevant industry courses and certifications.

Qualifications:
Undergraduate or graduate degree in Computer Science, Information Systems, Electrical Engineering, Applied Computational Math Sciences or related Engineering discipline.

Benefits & Salary:
Annual pay range $80,000 - $120,000.
Paid time off.
Comprehensive medical, dental and vision insurance with employee premiums paid in full.
401(k) retirement plan with 3% company match and immediate vesting.","$100,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,2000.0,$25 to $100 million (USD)
"Chubb INA Holdings Inc.
3.7",3.7,North Carolina,Principal Engineer Data Pipelines/Analytics,"Job Description (Principal Engineer Data Pipelines/Analytics):

We are looking for an experienced data engineer to join our team. You will use various methods to transform raw data into useful data systems. For example, you’ll create algorithms and conduct statistical analysis. Overall, you’ll strive for efficiency by aligning data systems with business goals. To succeed in this data engineering position, you should have strong analytical skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods. If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you.

Job Duties and Responsibilities:
Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Interpret trends and patterns
Conduct complex data analysis and report on results
Prepare data for prescriptive and predictive modeling
Build algorithms and prototypes
Combine raw information from different sources
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition
Develop analytical tools and programs
Collaborate with data scientists and architects on several projects

Qualifications

Requirements and Skills:
Previous experience as a data engineer or in a similar role
Technical expertise with data models, data mining, and segmentation techniques
Hands-on programming languages (e.g. Python and Java)
Hands-on experience with SQL/No SQL databases. Must have worked with Big Data.
Great numerical and analytical skills
Degree in Computer Science, IT, or similar field; a master’s is a plus
Knowledge of Azure cloud ecosystem

In Jersey City, NJ the pay range for the role is $128,500 to $215,000. The specific offer will depend on an applicant’s skills and other factors. This role may also be eligible to participate in a discretionary annual incentive program. Chubb offers a comprehensive benefits package, more details on which can be found at https://careers.chubb.com/global/en/north-america. This range is specific to Jersey City, NJ and may not be applicable to other locations.
EEO Statement
At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion,and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin,ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law.Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliationagainst any individual who reports discrimination or harassment.","$171,750 /yr (est.)",10000+ Employees,Company - Public,Insurance,Insurance Carriers,1792.0,$10+ billion (USD)
"BAE Systems
3.9",3.9,"Reston, VA",Data Analyst and Visualization Engineer,"Job Description
What’s it like realizing your potential at an innovative company that takes on some of the world’s most important challenges? Rewarding.

As a member of our Defense and Space Intelligence Technology team, you will join a diverse group of driven professionals who design the products and systems that support enhanced military capabilities, protect national security, and keep critical information and infrastructure secure. With us, you will be able to make an impact while you hone your skills and grow in your career.

BAE Systems is looking for a Data Analyst and Visualization Engineer to add their skills to our team supporting enterprise engineering and operations efforts for DIA IT infrastructure and communications services. This is an exciting and dynamic job opportunity where you will support a dynamic operational customer and help advance the United States military's analytic and operational capabilities.

In this job, the ideal candidate will:
Perform data collection, normalization, and visualization activities associated with Network and UC performance, cost, location, system warranty, and ticket data.
Performs data queries using monitoring systems and custom queries in a variety of programming languages (C#, SQL, Java, Python, and/or R).
Oversees data ingesting into enterprise data mining solutions, and generates visualization and dashboards suited to the purpose of the data query.
Develop data visualization resources and strategy to strategically increase enterprise view of data within customer’s specific context.
Create visual displays for collection system usage statistics from algorithm output, etc. (e.g., dashboards and automated system reports)
Collaborate with domain experts to ensure data interpretability.
Review and analyze collected data to determine the performance and health of a system or network; assists in developing, implementing, monitoring, and analyzing key performance indicators.
Identify new patterns and areas requiring attention or improvement.
Map and perform data transformation (database development, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation)
Perform data analysis interpretation and data management duties.

Required Education, Experience, & Skills
Bachelor's Degree in Statistics or Data Science and 7+ years of data analysis experience
A Statistics or Data Science Master's Degree or PhD can be applied in lieu of a relevant Bachelors
Demonstrated experience utilizing Splunk ITSI, and Tableau data mining solutions.
Familiarity with existing and emerging data mining technology
8570 Information Assurance Technical (IAT) Level II certification (Security+ or other) Training and Certifications
Splunk IT Service Intelligence Certified Admin credential

Preferred Education, Experience, & Skills
Comfortable working with a variety of programming languages (C#, SQL, Java, Python, and/or R).

About BAE Systems Intelligence & Security
BAE Systems, Inc. is the U.S. subsidiary of BAE Systems plc, an international defense, aerospace and security company which delivers a full range of products and services for air, land and naval forces, as well as advanced electronics, security, information technology solutions and customer support services. Improving the future and protecting lives is an ambitious mission, but it’s what we do at BAE Systems. Working here means using your passion and ingenuity where it counts – defending national security with breakthrough technology, superior products, and intelligence solutions. As you develop the latest technology and defend national security, you will continually hone your skills on a team—making a big impact on a global scale. At BAE Systems, you’ll find a rewarding career that truly makes a difference. Intelligence & Security (I&S), based in McLean, Virginia, designs and delivers advanced defense, intelligence, and security solutions that support the important missions of our customers. Our pride and dedication shows in everything we do—from intelligence analysis, cyber operations and IT expertise to systems development, systems integration, and operations and maintenance services. Knowing that our work enables the U.S. military and government to recognize, manage and defeat threats inspires us to push ourselves and our technologies to new levels. At BAE Systems, we celebrate the array of skills, experiences, and perspectives our employees bring to the table. For us, differences are a source of strength. We’re laser-focused on high performance, and we work hard every day to nurture an inclusive culture where all employees can innovate and thrive. Here, you will not only build your career, but you will also enjoy work-life balance, uncover new experiences, and collaborate with passionate colleagues.",,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1999.0,$10+ billion (USD)
"Midmark Corporation
4.6",4.6,"Traverse City, MI",RTLS Data Analytics Engineer,"JOB SUMMARY:
Work closely with an Agile team to build, deploy, and maintain a transformative cloud based data infrastructure that supports an analytics solution. This solution will generate insights from our customersâ€™ data, enabling them to advance the data-driven decision-making capabilities of our Real Time Locating System (RTLS). This role requires understanding of cloud computing services and infrastructure, architecture, data engineering, data analysis, data visualization, and a basic understanding of data science techniques and workflows. Adopts best practices to ensure security, privacy, and compliance for all data assets. Oversees CI/CD processes within the team to ensure sustainable and efficient deployment of code. Develops solutions supporting the movement of data and information assets. Experience with the Azure data platform required. Experience with Tableau or other industry leading BI tool is preferred.
ESSENTIAL/PRIMARY DUTIES:
Assess existing analytics infrastructure and business processes; advise on and contribute to the design of best-in-class, modern solutions
Design and develop the analytical layer, building cloud data warehouses, data lakes, ETL/ELT pipelines, and orchestration tools
Write code in SQL and Python, and use software engineering best-practices such as Git and CI/CD; experience with Spark preferred
Communicate clearly, effectively, and proactively with local and remote users, teammates and stakeholders
Install and maintain cloud-based applications, systems, or associated infrastructures in the data and analytics space and support the deployment of analytics solutions across multiple environments
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Review security assessment recommendations and remediate issues to ensure that appropriate information security standards are met
SECONDARY DUTIES:
Manage and monitor SLAs for uptime and performance requirements in the Microsoft Azure environment, troubleshooting and resolving errors when they occur
Document and adhere to all critical processes
Stay informed of emerging cloud technologies and evaluates the value to the organization's operations
EDUCATION and/or EXPERIENCE:
Bachelor's degree (B. A.) from four-year college or university in computer science or related engineering field; or equivalent combination of education and experience.
5+ years working with relational databases and query languages
5+ years building data pipelines in production and the ability to work across structured, semi-structured and unstructured data
5+ years data modeling (e.g., star schema, entity-relationship)
3+ years writing clean, maintainable and robust code in Python, Scala, Java or similar coding languages is desirable
Experience working with healthcare data encompassing PHI/PII
Experience in writing queries across large data sets
Expertise in Agile software engineering concepts and best practices and/or DevOps is desirable
Experience working with big data technologies (Spark, Hadoop) is desirable
COMPETENCY/SKILL REQUIREMENTS:
Exceptional knowledge of data analytics, such as dimensional modeling, ETL\ELT, reporting tools, data governance, data warehousing, structured and unstructured data
Must have proven success with developing solutions on cloud computing services and infrastructure in the data and analytics space, with focus on Microsoft Azure data platform and DevOps
Must have experience managing stakeholders and collaborating with customers
Use of data visualization through BI tool sets, Tableau preferred
Ability to define and complete work self-directed, under limited supervision
Ability to work remotely to resolve issues
Ability to document and follow procedures
Knowledge and understanding of BI Reporting Tools and Troubleshooting
Knowledge and understanding of cloud scaling and security practices
SUPERVISORY RESPONSIBILITIES:
Mentoring of teammates in similar responsibility","$90,326 /yr (est.)",1001 to 5000 Employees,Company - Private,Manufacturing,Health Care Products Manufacturing,1915.0,$500 million to $1 billion (USD)
"Kia America, Inc.
3.8",3.8,"Irvine, CA",Field Data Analysis Engineer,"At Kia, we’re creating award-winning products and redefining what value means in the automotive industry. It takes a special group of individuals to do what we do, and we do it together. Our culture is fast-paced, collaborative, and innovative. Our people thrive on thinking differently and challenging the status quo. We are creating something special here, a culture of learning and opportunity, where you can help Kia achieve big things and most importantly, feel passionate and connected to your work every day.
Kia provides team members with competitive benefits including premium paid medical, dental and vision coverage for you and your dependents, 401(k) plan matching of 100% up to 6% of the salary deferral, and time off starting at 14 days per year. Kia also offers company lease and purchase programs, company-wide holiday shutdown, paid volunteer hours, and premium lifestyle amenities at our corporate campus in Irvine, California.
Status
Exempt
Summary
The purpose of this position is to support the Field Data Analysis Manager and Data Evaluation Team (DET) by identifying at an early stage in a model’s life cycle, any emerging Potential Safety Issues (PSIs) through data searches and sophisticated analysis of Kia America (KUS) data from all primary data sources, including warranty, Techline, Consumer Affairs, customer pay, NHTSA Vehicle Owner Questionnaires (VOQs), as well as other data sources, as applicable. This position will also support safety and noncompliance recall decision-making and requests by NHTSA for Defect Petitions, Preliminary Evaluations, investigations, and pre-investigative requests for information. This position will also support KUS Legal Department for defense of class action suits and in-house and outside counsel requests.
Major Responsibilities
1st Priority - 40%
Review data sources as assigned for new PSIs. Work from analytic dashboards and generate reports showing emerging field issues using primary field data sources, as well as other data sources as appropriate. Investigate PSIs across data sources to determine current complaint counts, rates, and any trends from the data, from emerging field issues stage through safety evaluation list stage and to field action decision stage, if applicable. Identify PSIs and potential noncompliance issues and inform Field Data Analysis Manager of them at the earliest point in time.
2nd Priority - 25%
Coordinate with Forensic Engineering and Investigation team to hand over and collaborate on analyses and updates. Provide analysis explanations to management, coordinators, other departments, and Kia HQ staff.
3rd Priority - 20%
Provide data analysis support for NHTSA inquiries, investigations, monthly NHTSA meeting preparation, and KUS Legal in-house and outside counsel.
4th Priority - 15%
Develop sophisticated processes for evaluating data and identifying PSIs by utilizing new technologies and analytic tools. Assist in the continuous improvement and development of Safety Data Analytics Infrastructure (SDAI). Share new data analysis techniques with team.
Education/Certification
BS degree in Engineering or Automotive Technology or equivalent work experience required.
Overall Experience
3-7 years of experience in technical positions such as Safety Analysis, Product Quality, or in a product engineering environment.
Directly Related Experience
Automotive, aerospace, or similar technical work experience required
Working knowledge of statistical analysis concepts and software.
Other Requirements:
Schedule(s) may vary due to needs of the business including but not limited to working outside of normal business hours, travel, weekends and/or holidays.
Perform other duties as assigned.
Skills
Basic knowledge of SAS programing language and Structured Query Language (SQL).
Excellent analytical skills and attention to detail.
Excellent teamwork skills.
Excellent written and oral communication skills.
Intermediate knowledge of automotive systems and components.
Knowledge of PC software such as Microsoft Excel and PowerPoint.
Knowledge of SAS Visual Analytics.
Knowledge of statistics.
Competencies
CHALLENGE - Solving Complex Problems
COLLABORATION - Building and Supporting Teams
CUSTOMER - Serving Customers
GLOBALITY - Showing Community and Social Responsibility
PEOPLE - Interacting with People at Different Levels
Pay Range
$69,547.00 - $90,401.00
Pay will be based on several variables that are unique to each candidate, including but not limited to, job-related skills, experience, relevant education or training, etc.

Equal Employment Opportunities
KUS provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, ancestry, national origin, sex, including pregnancy and childbirth and related medical conditions, gender, gender identity, gender expression, age, legally protected physical disability or mental disability, legally protected medical condition, marital status, sexual orientation, family care or medical leave status, protected veteran or military status, genetic information or any other characteristic protected by applicable law. KUS complies with applicable law governing non-discrimination in employment in every location in which KUS has offices. The KUS EEO policy applies to all areas of employment, including recruitment, hiring, training, promotion, compensation, benefits, discipline, termination and all other privileges, terms and conditions of employment.

Disclaimer: The above information on this job description has been designed to indicate the general nature and level of work performed by employees within this classification and for this position. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities, and qualifications required of employees assigned to this job.","$79,974 /yr (est.)",501 to 1000 Employees,Subsidiary or Business Segment,Manufacturing,Transportation Equipment Manufacturing,1994.0,$10+ billion (USD)
"AEEC, LLC.
2.8",2.8,"Salt Lake City, UT",Data and Machine Learning Engineer,"AEEC is currently seeking a Data and Machine Learning Engineer to support our customer in a remote work location. The Data and Machine Learning Engineer has demonstrated expertise with IT Platform implementation, Machine Learning model implementation, experience with analytic solutions that drive cost vs. risk analysis. Statistics, statistical modeling, and statistical process control. Developing, documenting and communicating comprehensive business and data understanding to all stakeholders; Solid understanding of relational data structures and business intelligence reporting concepts.
Responsibilities
Develop machine learning models and deploy them as part of a pipeline.
Build data sets that provide answers to key business decisions like evaluating risk and reducing costs.
Use analytics data to create visualizations.
Research, design, develop, and enhance analytics solutions using a range of reporting, data mining, and analytics techniques, methods, and tools.
Perform analysis of data for Extraction, Transformation, and Load (ETL) strategies, pattern recognition, and application of analytical tools.
Contribute to approaches to data quality, metadata, and/or business rules as they apply to data mining and analytics projects.
Create and maintain scripts to manipulate data and automate complex workflows.
Provide guidance on business and technical issues affecting projects, such as data access, data quality, storage capacity, and analytic tools and software.
Independently, or as part of a team, perform comprehensive and thorough data collection, utilizing a variety of data sources to analyze trends, problems, or other related issues of major importance in support of audits and investigations.
Technical Skills
Azure and AWS cloud platform analytics and ML service offerings.
Data visualization software experience using power bi, Tableu or equivalent.
3+ years in a Data Analyst Role
2+ years Data Mining, Modeling and Reporting
Strong attention to detail
Ability to collaborate effectively and work as part of a team
Data query; import/read/export files
Table joins, lookups, conditional processing, loops, arrays, macros
Automation scripting for routine tasks
Languages:
SAS / Enterprise Guide
SQL/Proc SQL
Python
R
Korn Shell/Perl
HTML/CSS/JavaScript

Statistical knowledge:
Regression modeling
Confidence interval
Correlation
Hypothesis testing
Sample size calculation
Education

Bachelor’s Degree in Mathematics, Computer Engineering or Computer Science or equivalent experience in related areas.
Citizenship
US citizenship is not required for this role, but must be located in the United States. All applicants will be subject to a background investigation.
Assessment
Candidates may be required to complete a skills assessment to assess their skills.

Physical Demands: While performing duties of the job, incumbent is occasionally required to stand, walk, sit, use hands and fingers, handle or feel objects, tools, or controls, reach with hands and arms, talk and hear. Employee must occasionally lift and/or move up to 25 pounds. Specific vision abilities required by job include close vision, distance vision, color vision, peripheral vision, depth perception and the ability to adjust and focus.
Work Environment: The noise level in the work environment is usually moderate.

About AEEC
AEEC is an award winning CMMI Level 3 and ISO accredited professional services organization with a proven track record of providing technology and engineering solutions to the commercial and federal market since 1995. AEEC provides leading edge of innovative technology solutions to solve customer’s complex business problems. We build long lasting business relationships based upon integrity, resourcefulness, ingenuity, and fully delivering commitments. AEEC possesses the fundamental IT and engineering skills needed to objectively evaluate problems and develop technically sound, cost effective solutions. AEEC has been featured in a Harvard Business School Case Study on the topic of innovation.

AEEC Values: Our customers and their missions come first. We want to add value to every engagement. We will demonstrate integrity and responsiveness in all of our business practices. Teamwork and respect for everyone.

AEEC Vision: To be the best partner to our customers and a great place to work and grow.

AEEC Mission: Our mission is our customers’ success. We strive to responsively solve our customers’ needs. Our current skills are IT and Engineering services.

Benefits: AEEC offers competitive wages with benefits (Medical Insurance, Life Insurance, Short term/Long term disability dismemberment, dental and vision insurance coverage, Flex Spending, 401K, and a 529 college savings plan). Medical, vision, and dental benefits start the first of the month following start of full time employment.

AEEC is a Federal Contractor and agrees to comply with all provisions set forth in Equal Employment Opportunity, Executive Order 11246, as amended, Section 503 of the Rehabilitation Act of 1973, as amended, 38 U.S.C. 4212 of the Vietnam Veterans’ Readjustment Assistance Act of 1974, as amended, 29 CFR Part 471, Appendix A to Subpart A (Executive Order 13496), and Executive Order 11246. If you are an individual with a disability and would like to request a reasonable accommodation as part of the employment selection process, please contact Kim Hartley at 703-766-4300.

AEEC is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.

AEEC invites any applicant and/or employee to review the Company’s written Affirmative Action Plan. This plan is available for inspection upon request by contacting Kim Hartley at 703-766-4300.","$103,231 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,1995.0,$25 to $100 million (USD)
"Capgemini
3.8",3.8,"Malvern, PA",data engineer,"Duration: 8+ months

Job Description:

Collaborate with product owners, UX designers, data analysts, scrum masters, and developers to develop a world-class user experience.
Recommend and effectively explain improvements in functionality, and UX elements that enable users to achieve their goals.
Be an expert in building working relationships across divisions.
Research, advocate and implement industry standard coding methodologies to improve workflows through the selection of evolving technologies and tools that your team will use to build expert-level user experiences.

Qualifications
Excellent analytical skills are essential
Successful candidates will have exhibited the ability to begin working independently and have demonstrated ability to apply theory and concept
Must have ability to communicate effectively with team members and others in the work group, as well as with customers
Must be comfortable working in MS Office, and industry standard statistics and data visualization Packages Power BI/Tableau
Experience working with SQL or statistical languages R or python
An accredited 4-year degree, preferably in accounting, finance, statistics, economics, mathematics, or a similar field with quantitative coursework and 1-2 years of experience OR Masters degree in an associated field.

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.","$92,252 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,1967.0,$10+ billion (USD)
"R1 RCM, Inc.
3.4",3.4,Remote,Data Engineer III,"The Software Engineer III serves as a Data Engineer focused on building next generation of healthcare products and services. As part of a collaborative, motivated Agile-based team, the Software Engineer III is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
Responsibilities
Create and maintain optimal data pipeline architecture to assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders to optimize the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies.
Work with stakeholders including the Product, Data and Engineering teams to assist with data-related technical issues and support their data infrastructure needs.
Adhere to and ensure engineering best practices using modern SDLC that enables CI/CD and favors automation, auditability, automated testing, infrastructure, and policy as code.
Ability to collaborate effectively with other architects and engineers delivering end to end, high-performing, highly reliable, scalable, and operable solutions.
Required Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical field; or equivalent experience.
3+ years of experience delivering data engineering applications and services.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets .
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience in any one of the big data technologies (Hadoop, MapReduce , Spark, Data Lake etc. )
Experience in any one of the stream-processing systems ( Kafka, Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages : Python, Java, Scala, etc.
Understanding of modern software development including version control, unit testing, and continuous integration and deployment.
Strong communication skills, with proven ability to present complex ideas and to document concisely.
Preferred Qualifications
Working knowledge of Cloud providers and services such as Amazon AWS or Microsoft Azure.
Experience with relational SQL and NoSQL databases, including MongoDB and ElasticSearch.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12.
Knowledge of the healthcare revenue cycle, EMRs, practice management systems.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2003.0,Unknown / Non-Applicable
"Riverside Research
4.2",4.2,"Wright Patterson AFB, OH",Data Engineer (TS/SCI clearance),"Riverside Research is an independent National Security Nonprofit dedicated to research and development in the national interest. We provide high-end technical services, research and development, and prototype solutions to some of the country's most challenging technical problems.

Job Number: 1246
Riverside Research is seeking full-time Data Engineer to support Director, Air Force Chief Data Office (SAF/CO) sponsored activities across the Air Force Enterprise to ensure the visibility, accessibility, understanding, sharing, and trustworthiness of data across air, space, and cyberspace domains. Candidates will provide subject matter expertise in and perform on multidisciplinary teams that support data preparation and architecture, development of agile algorithmic solutions, evaluate and/or execute data governance and data maturity models; and conduct data analytics using state of the art mathematical and machine learning/artificial intelligence techniques and other data analytic lines of research/effort. Positions will be at various CONUS Air Force installations and the National Capital Region.
All Riverside Research opportunities require U.S. Citizenship
Job Duties
Provide expertise on all data concepts for the broader advanced analytics group, and inspire the adoption of advanced analytics, data engineering and data science across the organization.
This will include Installing continuous pipelines of large pools of filtered information so that data analyst/scientists can pull relevant data sets for their analyses.
Required Qualifications:
Top Secret clearance with SCI adjudication
Bachelor's degree in the requisite relevant field. A Master's degree in a relevant field may be substituted for 3 years of general experience.
7 years or more of experience in the data engineering field, at least three of which must have been in a data analytics environment preferably in DoD or the intelligence community.
Familiarity with the manipulation of unstructured data in a data analytics environment, and the use of open-source tools, cloud computing, machine learning and data visualization.
Familiar with specialized languages relevant to the technologies employed such as Apache, Hadoop, etc.

Riverside Research strives to be one of America's premier providers of independent, trusted technical and scientific expertise. We continue to add experienced and technically astute staff who are highly motivated to help our DoD and Intelligence Community (IC) customers deliver world class programs. As a not-for-profit, technology-oriented defense company, we believe service to customers and support of our staff is our mission. Our goal is to serve as a destination company by providing an industry-leading, positive, and rewarding employee experience for all who join us. We aspire to be a valued partner to our customers and to earn their trust through our unwavering commitment to achieve timely, innovative, cost-effective and mission-focused solutions.
All positions at Riverside Research are subject to background investigations. Employment is contingent upon successful completion of a background investigation including criminal history and identity check.
Riverside Research does not mandate COVID vaccination as a condition of employment. However, proof of vaccination or negative test may be required to enter certain government facilities and sites. Vaccination requirements will depend on the status of the federal contractor mandate and customer site-specific requirements. To protect the health and safety of its employees, their families, and to comply with customer requirements, the company requires all employees to disclose vaccination status (upon hire).
Our EEO Policy
Riverside Research is an equal opportunity employer. We recruit, employ, train, compensate and promote without regard to race, religion, sex, color, national origin, age, gender identity, sexual orientation, marital status, disability/veteran, status as a protected veteran, or any other basis protected by applicable federal, state and local law.
If you need assistance at any time in our application or interview process, please contact Recruiting at email Recruiting@RiversideResearch.org. A member of the Recruiting team will be available to assist.
This contractor and subcontractor shall abide by the requirements of 41 CFR 60-741.5(a). This regulation prohibits discrimination against qualified individuals on the basis of disability and requires affirmative action by covered prime contractors and subcontractors to employ and advance in employment qualified individuals with disabilities.
This contractor and subcontractor shall abide by the requirements of 41 CFR 60-300.5(a). This regulation prohibits discrimination against qualified protected veterans and requires affirmative action by covered contractors and subcontractors to employ and advance in employment qualified protected veterans.
For more information on ""EEO is the Law,"" please visit:
http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf
https://www.dol.gov/sites/dolgov/files/ofccp/regs/compliance/posters/pdf/eeopost.pdf","$64,624 /yr (est.)",501 to 1000 Employees,Nonprofit Organization,Government & Public Administration,National Agencies,1967.0,$25 to $100 million (USD)
"Deloitte
4.1",4.1,"Lake Mary, FL",Senior Cloud Data Migration Engineer,"Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with our US Delivery Center - we are breaking the mold of a typical Delivery Center.

Our US Delivery Centers have been growing since 2014 with significant, continued growth on the horizon. Interested? Read more about our opportunity below ...

Work you'll do/Responsibilities
Work with the team to evaluate business needs and priorities, liaise with key business partners and address team needs related to data systems and management.
Translate business requirements into technical specifications; establish and define details, definitions, and requirements of applications, components and enhancements.
Participate in project planning; identifying milestones, deliverables and resource requirements; tracks activities and task execution.
Generate design, development, test plans, detailed functional specifications documents, user interface design, and process flow charts for execution of programming.
Develop data pipelines / APIs using Python, SQL, potentially Spark and AWS, Azure or GCP Methods.
Use an analytical, data-driven approach to drive a deep understanding of fast changing business.
Build large-scale batch and real-time data pipelines with data processing frameworks in AWS, Azure or GCP cloud platform.
Moving data from on-prem to cloud and cloud data conversions.

The Team

Artificial Intelligence & Data Engineering:

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Artificial Intelligence & Data Engineering team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Artificial Intelligence & Data Engineering will work with our clients to:

Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.

Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.

Qualifications

Required
6+ years of experience in data engineering with an emphasis on data analytics and reporting.
6+ years of experience with at least one of the following cloud platforms: Microsoft Azure, Amazon Web Services (AWS), Google Cloud Platform (GCP), others.
6+ years of experience in SQL, data transformations, statistical analysis, and troubleshooting across more than one Database Platform (Cassandra, MySQL, Snowflake, PostgreSQL, Redshift, Azure SQL Data Warehouse, Databricks, etc.).
6+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines.
6+ years of experience with one or more of the follow scripting languages: Python, SQL, Kafka and/or other.
6+ years of experience designing and building solutions utilizing various Cloud services such as EC2, S3, EMR, Kinesis, RDS, Redshift/Spectrum, Lambda, Glue, Athena, API gateway, etc.
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or equivalent experience
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future.
Travel up to 10% annually, on average, based on the work you do and the clients and industries/sectors you serve
Co-location expectation 10-30%
Must live within a commutable distance or relocate near a center location:

1.Lake Mary, FL
2.Mechanicsburg, PA
3.Gilbert, AZ

Preferred
AWS, Azure and/or Google Cloud Platform Certification.
Master's degree or higher.
Expertise in one or more programming languages, preferably Scala, PySpark and/or Python.
Experience working with either a Map Reduce or an MPP system on any size/scale.
Experience working with agile development methodologies such as Sprint and Scrum.

LI-FA1",,10000+ Employees,Company - Private,Financial Services,Accounting & Tax,1850.0,$10+ billion (USD)
"LER TechForce
4.3",4.3,"Auburn Hills, MI",Battery Systems Data Analyst Engineer,"The Battery Systems Data Analyst Engineer will be responsible for the development of battery data analytics and predictive system engineering tools and application of these tools for design and development of xEV Battery Systems. The candidate will take a leading role in analyzing, mining, processing, and tracking the increasing amount of battery test data generated from battery, vehicle and fleet testing during the course of product development, testing, and validation. The candidate will be involved in multiple vehicle programs and deal with a broad range of data, i.e. cell level, pack-level, vehicle-level, and fleet-level. The ideal candidate is expected to be highly analytical, have experience in data analysis/processing, and have some background in electrified powertrain vehicles.
The responsibilities include but not limited to:
Develop, document the engineering requirements for automotive traction battery systems. Lead other engineers and teams to balance requirements.
Develop battery system usable energy walk system model by taking account of all system factors in usable energy walk. Create initial models, and develop the improvement plans and targets to optimize the system usable energy.
Develop and apply battery data analytics and predictive system engineering tools (1D, simullink/matlab) for prediction and verification of battery system performance and thermal.
Develop battery usage profiles and mission profiles for battery life modeling/ prediction/simulations and component designs for durability and other applications.
Analyze battery and vehicle usage data from Fleet Test Vehicles to compare and validate battery life and performance models.
Support development of appropriate battery usage history statistics and their online recording
Analyze critical battery performance during the entire product lifecycle including: power and energy performance, battery life, safety performance.
Help develop tests as needed for verifying battery system performance and execute product design improvements.
Lead the data analysis activities and interpretation related to battery field performance to guide requirements development and product improvements
Provide technical expertise in investigation of system issues and support root cause analysis and mitigation and corrective action plans.
Requirements:
Bachelor's degree in Electrical, Chemistry, Applied Physics, Aerospace, Industrial, Systems, or Computer Engineering, or related Engineering field from an ABET accredited, or ABET equivalent university.
3 years in product engineering design and development in automotive industry with 2 years of experience in mathematical modeling and data analysis/processing/mining
Good understanding of Li-ion cell electrochemistry, cell/battery life models and battery life behavior.
Excellent Matlab scripting skills or other languages e.g. Octave, Python
Knowledge of design of experiments, parameter fitting, optimization, and data regression methods
General knowledge of xEV Battery/Client systems, xEV Vehicle/powertrain operation
Demonstrate excellent level of analytical ability, communication and interpersonal skills required to build relationships and coach team members, and work with customers to solve problems and resolve issues.
Preferred Requirements:
Master's degree in Engineering, as accredited by ABET or ABET equivalent; other graduate level technical degrees in the specific area of relevant specialty expertise may be considered.
Minimum of 5 years in product engineering design and development in automotive industry with 3 years working experience in xEV technology applications and areas such as development of battery storage systems, high voltage architecture development, and/or electric powertrain components such as electric traction motors & power electronics.
Understanding of thermal system performance modeling as it pertains to energy storage components
Thorough knowledge of electrification technology functionality and features on the system level.
High Voltage training and advance battery design/development experience is desirable.
Experience in performance modeling with Matlab/Simulink, COMSOL, GT-Suite and ANSYS.
Experience using a disciplined system development process based on ASPICE or CMMI model.
Experience with DOORs, Canalyzer and Canape
Knowledge of battery test methods and procedures
Strong proactive and reactive engineering problem solving skills (including DFSS, Kepner Tregoe, 8D, 5Why, etc.)
Job Type: Full-time
Pay: $84,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Schedule:
Day shift
Monday to Friday
Ability to commute/relocate:
Auburn Hills, MI: Reliably commute or planning to relocate before starting work (Preferred)
Work Location: In person","$84,000 /yr (est.)",201 to 500 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,2001.0,$5 to $25 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE","Software Engineer III, Cloud Data Engineer","As a Software Engineer III, Cloud Data Engineer, within Corporate Enterprise Technology, in Finance, Risk, Data, & Controls, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Executes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems
Creates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems
Produces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets both on-prem and on AWS cloud in service of continuous improvement of software applications and systems
Proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture
Contributes to software engineering communities of practice and events that explore new and emerging technologies
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Formal training or certification on software engineering concepts and 3+ years applied experience
3+ years of working on Big Data Platforms and building frameworks for data pipelines
2+ years of building cloud solutions - AWS preferred.
1+ year of working on cloud data lake solution. Experience working with Terraform, Glue DB, Collibra, Athena, Snowflake, Redshift, EMR would be big plus.
Proficient in coding in one or more languages - Java, Scala and Python are mostly used.
Experience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages
Solid understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security

Preferred qualifications, capabilities, and skills
Advanced knowledge of Apache Spark, Scala, Java, Python and Spring
Understanding of integration technologies such as Apache Kafka
Working knowledge of API-Apigee Edge, Swagger
Containers-Docker advanced development, Kubernetes
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans","$104,623 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799.0,$10+ billion (USD)
"Chick-fil-A, Inc.
3.9",3.9,"Atlanta, GA","Sr. Lead Software Engineer, Data Ecosystem","Overview:
Chick-fil-A has successfully implemented a modern cloud-native, self-service data ecosystem comprised of AWS S3, Glue, Redshift and Databricks. In this role you will drive the design and implementation of the software components and features required to evolve it into the next-generation architecture meeting the needs of data engineers throughout Chick-fil-A.

You will be responsible for architecting, designing, and leading the implementation of features for metadata management as well as advanced data management components related to our enterprise data lake, data warehouses, Spark platform, and other relational and non-relational data stores. Integration between components to deliver the best possible developer experience is key.

Your daily work will require partnering with fellow engineers, the product owner, data and enterprise architects, stakeholders, vendor teams, and other parties following an agile methodology, while being part of a diverse team that values high performance and excellence as much as work-life balance.

This role is based in the Atlanta, GA area. Relocation available for the selected candidate.

Our Flexible Future model offers a healthy mix of working in person and virtually, strengthening key elements of the Chick-fil-A culture by fostering collaboration and community.
Responsibilities:
Lead, mentor and assess multiple partner engineering teams with minimal supervision
Identify opportunities to improve the developer experience then design and architect revisions to reduce friction in the user experience.
Partner with data scientists and data engineers to fully understand emerging needs and unmet needs. Collaborate and promote value-based adoption.
Review the work of multiple partner led pods – ensuring conformance to standards, adoption of patterns, sound designs and good development practices.
Exercise skills in cloud infrastructure and deployment as well as areas like application security, data analytics, machine learning, and site reliability engineering (SRE)
Define patterns and processes for data transformation, movement, and manipulation using (among others) Hadoop/Spark, SQL, Airflow, Databricks, Amazon Aurora, DynamoDB, Athena, Redshift, ML libraries/tools
Identify & propose emerging technologies, methodologies and/or approaches related to data and analytics
Be a key participant of the team’s Agile process
Address engineering assignments by autonomously deciding which ones to delegate and which ones to execute hands-on
Note - Working in a DevOps model, this opportunity includes both building and running solutions that could require off hours support. This support is shared amongst the team members to cover weekends and weeknights. The goal is to design for failure and, using cloud-native infrastructure patterns, automate responses to issues so they can be worked during normal hours.
Minimum Qualifications:
5+ years or more related work experience
Master’s degree in Computer Science, Analytics Engineering or related technical field or the equivalent combination of education, training and experience from which comparable skills have been acquired
Broad and deep programming experience in Python, JavaScript, Java, Scala, or other comparable languages
Experience with SQL, data modeling, and the Hadoop ecosystem
Experience with source-control systems like Git or Subversion, and CI/CD tools like GitHub Actions or Jenkins
Experience implementing application security, software design patterns, and the SDLC
Good interpersonal and team collaboration skills
Preferred Qualifications:
Experience architecting software solutions on Amazon Web Services (AWS) or other major CSP
Experience working with an Agile development methodology featuring sprints, point-estimation, and daily standups
Proficiency in Spark programming or equivalent big data technology
Experience with Unix/Linux and container technologies such as Docker
Minimum Years of Experience: 5 Travel Requirements: 5% Required Level of Education: Master's Degree Major/Concentration: Computer Science, Analytics Engineering, or related technical field","$116,088 /yr (est.)",5001 to 10000 Employees,Company - Private,Restaurants & Food Service,Restaurants & Cafes,1946.0,$5 to $10 billion (USD)
"Peraton
3.6",3.6,"Huntsville, AL",Sr Data Engineer - ETL,"Responsibilities:
Technology is constantly changing and our adversaries are digitally exceeding law enforcement’s ability to keep pace. Those charged with protecting the United States are not always able to access the evidence needed to prosecute crime and prevent terrorism. The Government has trusted in Peraton to provide the technical ability, tools, and resources to bring criminals to justice. In response to this challenge, Peraton is seeking a Senior Data Engineer with ETL expertise to provide proven, industry leading capabilities to our customer.

Experience with data exploration techniques and development of quantitative and qualitative data analysis process, design robust ETL pipelines.
Manage team of data engineers and should have experience working with Data Scientists and Data architects. Databricks or equivalent platform using Apache Spark with Scala, Python, Java.
Work in a team environment to design, develop, and support a software system which is undergoing a modernization.
Participate in developing new functionality and migrating the application into the cloud and introducing new technologies into the tech stack.
Participate in Agile Scrum SDLC activities.
Support developing Agile SDLC phase documentation.
Perform unit and integration testing of software/systems prior to release to the users for user acceptance testing.
Qualifications:
Required Qualifications
BS degree and twelve (12) years of experience with programming and software development including analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
Five (5) years of experience architecting software solutions based on customer requirement.
Led a technical team for at least five (5) years.
Three (3) years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
A current Top Secret security clearance with SCI eligibility and the ability to obtain a polygraph.

Desired Qualifications
Knowledge of Data Dictionary
Knowledge of Normalizations
Experience with AWS cloud services including Glue, Kinesis and Container services.
Knowledge of data acquisition and ingestion of structured and unstructured data sources ensuring quality and data integrity
Experience with open source technologies like Docker, ElasticSearch, and NoSQL Databases,
Experience with an Agile environment and have developed User Stories.
Peraton Overview:
Peraton drives missions of consequence spanning the globe and extending to the farthest reaches of the galaxy. As the world’s leading mission capability integrator and transformative enterprise IT provider, we deliver trusted and highly differentiated national security solutions and technologies that keep people safe and secure. Peraton serves as a valued partner to essential government agencies across the intelligence, space, cyber, defense, civilian, health, and state and local markets. Every day, our employees do the can’t be done, solving the most daunting challenges facing our customers.
Target Salary Range: $112,000 - $179,000. This represents the typical salary range for this position based on experience and other factors. EEO Tagline (Text Only): An Equal Opportunity Employer including Disability/Veteran.","$145,500 /yr (est.)",10000+ Employees,Company - Private,Information Technology,Information Technology Support Services,2017.0,$5 to $10 billion (USD)
"CVS Health
3.1",3.1,Pennsylvania,Lead Data Engineer,"Designs and develops complex and large-scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs

Writes complex ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing

Develop frameworks, standards & reference material for architecture and associated products

Designs data marts and data models to support Data Science and other internal customers. Behaves as mentor to junior team members to provide technical advice

Applies knowledge of Aetna systems and products to consult and advise on additional efforts across multiple domains spanning broader enterprise

Collaborates with data science team to transform data and integrate algorithms and models into highly available, production systems

Uses in-depth knowledge on Hadoop architecture, HDFS commands and experience designing & optimizing queries to build scalable, modular, and efficient data pipelines

Uses advanced programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems

Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards

Experiments with available tools and advice on new tools in order to determine optimal solution given the requirements dictated by the model/use case

Pay Range
The typical pay range for this role is:
Minimum: 115,000
Maximum: 230,000

This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.

In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company's 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (PTO) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.

For more detailed information on available benefits, please visit
jobs.CVSHealth.com/benefits

Required Qualifications
5+ years of progressively complex related experience

5+ years experience with bash shell scripts, UNIX utilities & UNIX Commands

5+ years experience building and implementing data transformation and processing solutions

Advanced knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar

Advanced knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment

Preferred Qualifications
Has in-depth knowledge of large-scale search applications and building high volume data pipelines

Ability to leverage multiple tools and programming languages to analyze and manipulate large data sets from disparate data sources

Ability to understand and build complex systems and solve challenging analytical problems

Education
Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.

Master’s degree or PhD preferred

Business Overview
Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.","$172,500 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1963.0,$10+ billion (USD)
"GE Renewable Energy
3.8",3.8,"Charleroi, PA",Customer Order and Data Engineer,"Job Description Summary
Execute the design, analysis, or evaluation of assigned projects using sound engineering principles and adhering to business standards, practices, procedures, and product / program requirements. This work would include mechanical and/or electrical engineering for all products
Job Description
Required Qualifications
Developing in-depth knowledge of low voltage schematics as they pertain to high voltage circuit breakers.
In-depth understanding of key business drivers; uses this understanding to accomplish own work. In-depth understanding of how work of own team integrates with other teams and contributes to the area.
Uses some level of judgment and has ability to propose different solutions outside of set parameters but with guidance. Uses prior experience and on-the-job training to solve straightforward tasks. Has access to technical skills and analytic thinking required to solve problems. May use multiple internal sources outside of own team to arrive at decisions
Utilize business systems to receive a customer specification, translate the specification to schematics and bills of material, and provide this information to production in a timely manner free from errors.
Maintain a digital tool that involves database maintenance as well as programming logic in order to generate a Bill of Materials given user input
Participate in Phase-In Phase-Out reviews and manage product phasing
Investigate and evaluate current state of the product configurator, understand cross-functional needs of product within the factory, define opportunities for improvement, and create a project plan. The goal of the project is to refine, improve, simplify, and evolve the product configurator for Dead Tank Circuit Breakers to meet factory growth demands in products, options, and function of the tool
Develop/incorporate routing in the product configurator for SAP, BOM, and MES systems as required.
Writes rules, guidelines, and operating procedures necessary for product configurator use.
Train, coach, and develop additional users for product configurator in the future.
Performs other duties as assigned
Position Requirements:
Bachelor's degree in Mechanical or Electrical Engineering (or Associate's degree in Electrical or mechanical with a minimum of 6 years of relevant experience)
Minimum of 4 years of relevant experience related to data analysis, bills of materials, manufacturing, project management, and/or product design
Desired Characteristics
Strong SAP experience
Good understanding of product configurations, R&D Engineering, Planning, Sales, Contract engineering, and Manufacturing.
High voltage and Dead tank circuit breaker knowledge
Minimum of 1 year with ProEngineer design and AutoCAD
Strong Microsoft Excel experience
Experience in project management, planning, activity follow-up.
Experience in MS Office software including MS Excel and MS PowerPoint.
Experience working cooperatively and effectively in a team environment.
GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.
GE will only employ those who are legally authorized to work in the United States for this opening. Any offer of employment is conditioned upon the successful completion of a drug screen (as applicable).
Relocation Assistance Provided: Yes","$62,861 /yr (est.)",10000+ Employees,Subsidiary or Business Segment,"Energy, Mining & Utilities",Energy & Utilities,2015.0,$10+ billion (USD)
Power3 Solutions and Partnering Companies,,"Annapolis Junction, MD",SOFTWARE ENGINEER Data storage w/ TS/SCI Poly Clearance,"SOFTWARE ENGINEER
Location: Annapolis Junction, MD
Clearance: TS/SCI with polygraph
TELEWORK- up to 2 days/ week

Company Overview
Metrea Algorithmics (MAX) is electronic warfare and cyberspace solutions and engineering services company support the Department of Defense. With deep asset, mission, industry and finance expertise, and a truly global reach, Metrea focuses on de-risking our customer's aerospace requirements across a number of different mission verticals.
Description
The selected candidate will augment a development team supporting a capability that provides a mechanism to
develop, implement, and maintain workflow management. The team develops services and implements new workflow
processes using free and open source workflow tools. The role requires a self-motivated individual that is comfortable
working in a team environment. The position will require working in a fast-paced team and a willingness to take on
challenges. Experience working in an agile environment will benefit the candidate. The tech stack includes React, Node,
Java, and Spring. The position is required to utilize software development and software design methodologies
appropriate to the development environment and consistent with CMS’ DevOps objectives. Telework up to two days a
week is available in this position.

Key Technologies:
Java
Postgres
React
Node
Spring
Camunda

Desired Skills
Experience working in an Agile Development environment
Experience managing the full software development lifecycle (SDLC) to include requirements definition, design, development, test, deployment, and sustainment
Proficient in Linux


Qualifications:
Three (3) years of experience in software development/engineering
Bachelor’s degree from an accredited college or university in Computer Science or related discipline. Four (4) years of additional software development experience may be substituted for a bachelor’s degree.
Experience with Java, Spring, React, Node
Willingness to learn new technologies
Experience developing and updating technical documentation


Benefits:
Comprehensive benefits program
401k with 6% match
Room to grow within organization

Metrea provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.",,,,,,,
"SAP
4.4",4.4,"Herndon, VA",SAP NS2 Data Scientist/Engineer TS/SCI + poly-MD-20701,"We help the world run better
Our company culture is focused on helping our employees enable innovation by building breakthroughs together. How? We focus every day on building the foundation for tomorrow and creating a workplace that embraces differences, values flexibility, and is aligned to our purpose-driven and future-focused work. We offer a highly collaborative, caring team environment with a strong focus on learning and development, recognition for your individual contributions, and a variety of benefit options for you to choose from. Apply now!

SAP NS2 Data Scientist/Engineer TS/SCI + poly-MD-20701
Hiring will be contingent on contract award
NS2 COMPANY DESCRIPTION
SAP is the global market leader for business software and related services. SAP National Security Services Inc.® (SAP NS2®) is an independent U.S. subsidiary of SAP. At SAP NS2, we leverage best-in-breed technologies engineered by SAP to protect the lives, assets and information of Americans. We offer SAP solutions with specialized levels of security and support to meet the requirements of U.S. national security and critical infrastructure customers.
Must be a U.S. Person; NS2 does not offer Visa sponsorships for this role.
All internals must have manager’s approval to transfer.
Responsibilities
Collaborate with subject matters experts (SMEs) to understand source data
Incorporate SME input into feature vectors suitable for analytic development and testing
Translate customer inputs to be developed into enterprise facing software visualizations
Oversee the development of individual analytic efforts and guide team in analytic development process
Partner with software engineers and cloud developers to develop production analytics
Desired Qualifications:
Experience with SQL, Spark, ETL, Tableau, PowerBI, or other data engineering tools
Experience with enterprise relational databases and other data sources
Understanding of enterprise business data
Ability to quickly learn technical concepts and communicate with multiple functional groups Envision and design data visualizations that exceed business requirements
Strong interpersonal, written, and verbal communication skills
Ability to work independently
Education Requirements:
Bachelor degree in Computer Science, Information Systems or related field.
Professional certification(s) desired.
Clearance Requirements:
Active TS/SCI + Polygraph
#LI-onsite

We build breakthroughs together
SAP innovations help more than 400,000 customers worldwide work together more efficiently and use business insight more effectively. Originally known for leadership in enterprise resource planning (ERP) software, SAP has evolved to become a market leader in end-to-end business application software and related services for database, analytics, intelligent technologies, and experience management. As a cloud company with 200 million users and more than 100,000 employees worldwide, we are purpose-driven and future-focused, with a highly collaborative team ethic and commitment to personal development. Whether connecting global industries, people, or platforms, we help ensure every challenge gets the solution it deserves. At SAP, we build breakthroughs, together.
We win with inclusion
SAP’s culture of inclusion, focus on health and well-being, and flexible working models help ensure that everyone – regardless of background – feels included and can run at their best. At SAP, we believe we are made stronger by the unique capabilities and qualities that each person brings to our company, and we invest in our employees to inspire confidence and help everyone realize their full potential. We ultimately believe in unleashing all talent and creating a better and more equitable world.
SAP is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to the values of Equal Employment Opportunity and provide accessibility accommodations to applicants with physical and/or mental disabilities. If you are interested in applying for employment with SAP and are in need of accommodation or special assistance to navigate our website or to complete your application, please send an e-mail with your request to Recruiting Operations Team: Careers@sap.com.
For SAP employees: Only permanent roles are eligible for the SAP Employee Referral Program, according to the eligibility rules set in the SAP Referral Policy. Specific conditions may apply for roles in Vocational Training.
EOE AA M/F/Vet/Disability:
Qualified applicants will receive consideration for employment without regard to their age, race, religion, national origin, ethnicity, age, gender (including pregnancy, childbirth, et al), sexual orientation, gender identity or expression, protected veteran status, or disability.
SAP believes the value of pay transparency contributes towards an honest and supportive culture and is a significant step toward demonstrating SAP’s commitment to pay equity. SAP provides the annualized compensation range inclusive of base salary and variable incentive target for the career level applicable to the posted role. The targeted combined range for this position is 102,100-221,700 USD. The actual amount to be offered to the successful candidate will be within that range, dependent upon the key aspects of each case which may include education, skills, experience, scope of the role, location, etc. as determined through the selection process. Any SAP variable incentive includes a targeted dollar amount and any actual payout amount is dependent on company and personal performance.
Requisition ID: 363265 | Work Area: Consulting and Professional Services | Expected Travel: 0 - 10% | Career Status: Professional | Employment Type: Regular Full Time | Additional Locations: #LI-Onsite","$108,163 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,1972.0,$10+ billion (USD)
"Chime
3.6",3.6,"San Francisco, CA",Staff Security Engineer - Data Security,"About the role
We are looking for a Security Software Engineer with experience or background working with the Data Engineering and Data Science teams. In this role, you will be part of a hands-on security team focused on securing our data pipelines, cloud infrastructure, data warehouses, and tools utilized by Chime's machine learning and data engineering teams. As a security engineer embedded into an engineering team, you excel in clear communication to articulate problems and present reasonable solutions that reduce our overall security risk. The ideal candidate for this role has the insight to find, passion to drive, and skill to implement optimal security solutions.
We're a small dedicated team that's always thinking of innovative ways to tackle hard security problems. We take on ambitious projects that have a huge impact on our members and build a strong security culture of our company. The team encourages talking about what problems we are solving for, the methods and accomplishments in public blogs and at conferences. If these are things that resonate with the way you work, we'd love to hear from you.
The base salary offered for this role and level of experience will begin at $188,500 and up to $267,100. Full-time employees are also eligible for a bonus, competitive equity package, and benefits. The actual base salary offered may be higher, depending on your location, skills, qualifications, and experience.
In this role, you can expect to
Recommend and implement security solutions and systems for cloud data warehouses, data analytics tools, and machine learning infrastructure
Define data encryption and masking standards and develop procedures and tools to ensure the desired data protection requirements are implemented
Design and build solutions that empower machine learning and data engineering teams to own their data security posture with confidence
Enhance the security posture of our machine learning data infrastructure in Snowflake, AWS, and Looker by introducing preventive controls and detection alerts
Be the security point of contact for any data security questions or concerns and be heavily involved in developing data security and handling policies
To thrive in this role, you have
Proven experience of building and deploying security features and projects
Experience securing data warehouses, and relational databases, particularly AWS RDS and Snowflake
Experience with data analytic tools (Looker experience preferred)
Experience with data ingestion and ML tools in AWS (Glue, Airflow, and Sagemaker)
Proficiency in designing security solutions for data processing pipelines
Terraform hands-on experience
Practical coding knowledge (Python or Golang is preferred)
A self-starter attitude who knows how to gather requirements and hit milestones
A little about us
We created Chime because we believe everyone deserves financial peace of mind. By eliminating unnecessary fees and helping people grow their savings automatically, we've empowered millions of Americans to take control of their finances.
Chime is the largest and fastest-growing U.S. player in the challenger-banking space. Through our banking partners, we offer access to bank accounts with fee-free overdraft, provide members the chance to receive early access to their paychecks, help them improve their credit, and more!
We've built one of the most experienced leadership teams in Fintech and were recently valued at over $25.5B. We've raised over $1.7B in funding from leading investors including Sequoia Capital Global Equities, SoftBank Vision Fund 2, General Atlantic, Tiger Global, Dragoneer, DST, Coatue, Iconiq, Menlo Ventures and others.
What we offer
Competitive salary based on experience
✨ 401k match plus the usual medical, dental, vision, life, and disability benefits
Generous vacation policy and company-wide Take Care of Yourself Days
Virtual events to connect with your fellow Chimers- think cooking classes, music festivals, mixology classes, paint nights, etc., and delicious snack boxes, too!
A challenging and fulfilling opportunity to join one of the most experienced teams in FinTech and help create a completely new kind of banking service
We know that great work comes from great, and inclusive teams. At Chime, we specifically look for individuals of varying strengths, skills, backgrounds, and ideas. We believe this gives us a competitive advantage to better serve our members and helps us all grow as Chimers and individuals.
We hire candidates of any race, color, ancestry, religion, sex, national origin, sexual orientation, gender identity, age, marital or family status, disability, Veteran status, and any other status. Chime is proud to be an Equal Opportunity Employer and will consider qualified applicants with criminal histories in a manner consistent with the San Francisco Fair Chance Ordinance. If you have a disability or special need that requires accommodation, please let us know. To learn more about how Chime collects and uses your personal information during the application process, please see the Chime Applicant Privacy Notice.
#LI-AY1","$128,583 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Banking & Lending,2012.0,Unknown / Non-Applicable
"School Specialty, LLC
3.0",3.0,"Lombard, IL","Oracle Data Engineer (Mansfield OH, Greenville WI or Lombard IL)","People Passion Purpose
Everything School Specialty offers is designed for one purpose – to help students succeed. We believe every student can flourish in an environment where they feel safe and inspired to explore and grow.

We’re determined to positively impact the future, one child at a time. We need to talk if you share our passion:

Transforming more than classrooms.®

Benefits

School Specialty offers Medical, Dental, & Vision plans (Effective Day 1), Wellness programs, Health Savings Accounts, Flexible Spending Accounts, 401 (k), PTO, Promise Hours dedicated to volunteering, Education Reimbursement, Paid Holidays, Fall & Winter Flexible Hours, Employee Discounts and much more!

Data Engineer
(Hybrid role- in Greenville WI, Lombard IL, or Mansfield OH)

Candidate will be responsible for designing, building, and maintaining scalable and efficient data systems supporting our organization's data-driven decision-making. You will work closely with our data scientists and business analysts to develop ETL processes and pipelines that transform raw data into valuable insights. The Data Engineer will serve as a backup to the DBA role in the event of their absence or unavailability.

The base salary range for this role is $77-100K Annually

Summary of Primary Responsibilities
Design, build, and maintain scalable and efficient data systems and pipelines
Develop and maintain ETL processes that transform raw data into valuable insights, including data cleansing, data mapping, and data transformation
Work closely with data scientists and business analysts to understand their data requirements and develop solutions to meet their needs
Design and implement data storage solutions that are secure, reliable, and accessible
Develop data quality checks and monitoring to ensure data accuracy and completeness
Develop and implement data processing and validation procedures
Develop and maintain documentation on data pipelines, data dictionaries, and data lineage
Perform data profiling, data mapping, and data modeling to support data analysis and reporting
Collaborate with cross-functional teams to integrate data from different sources
Continuously optimize and improve data systems and pipelines for performance, scalability, and reliability
Creating and executing backups, performing database tuning and optimization, monitoring database activity and usage, and providing support to end-users
Stay up-to-date with emerging trends and technologies in data engineering
Minimum Experience Requirements
Proven experience as a Data Engineer or similar role
Strong understanding of data modeling, database design, and data architecture principles
Experience building ETL processes and pipelines, including data cleansing, data mapping, and data transformation
Proficiency in SQL and experience working with Oracle and MS-SQL database technologies
Experience in database administration and be able to troubleshoot issues related to database connectivity, security, and performance
Ability to work independently and collaboratively in a fast-paced environment
Excellent problem-solving and communication skills
Willingness to learn and adapt to new environments and technologies
Self-starter and confident
Preferred Knowledge and Skills
Experience with big data technologies such as Hadoop, Spark, or NoSQL databases
Experience with distributed data processing frameworks like Apache Hadoop, Apache Spark, or Apache Flink
Knowledge of data warehousing concepts and tools such as Redshift, Snowflake, or BigQuery
Familiarity with data visualization and reporting tools such as Tableau or Power BI
Experience working with data streaming and real-time data processing frameworks like Apache Kafka or Apache Storm
Disclaimers
The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. All personnel may be required to perform duties outside of their normal responsibilities from time to time, as needed.
School Specialty, LLC. is a Drug Free Workplace. All applicants are subject to a drug screen and background check as a condition of employment.
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.
If you need a reasonable accommodation for any part of the employment process, please contact us by email at Opportunities@SchoolSpecialty.com and let us know the nature of your request and your contact information.
#LI-Hybrid","$88,500 /yr (est.)",1001 to 5000 Employees,Company - Private,Education,Education & Training Services,1959.0,$500 million to $1 billion (USD)
"Woongjin, Inc
3.3",3.3,"Ridgefield Park, Bergen, NJ",Data ETL Lead (Software Engineer) - bilingual (Korean/English),"Company Description

Our Mission
WOONGJIN INC. is a rapidly growing team who provides a range of unique, exceptional, and enhanced services to our clients. We have a strong moral code that includes the service of goodness without expectations of reward. We are motivated by the sense of responsibility and servant leadership.

Job Description

Drive design and development of Data Provider Framework
Conducting a thorough analysis of operational, technical, systemic requirements and any required modification to maintain the engineering tools infrastructure
Works on different database management systems efficiently
Extracting company data and transferring it into the new data hub environment
Ensure that the data is scalable and architecture
Work with clients to deliver scalable and maintainable data storage and data flow
Gather essentials and process them for business needs
Has a good command over data hub concepts to load the data effectively
Design, develop, and automate the data processing
Develops data flow models for ETL applications
Testing the storage system after transferring the data
Works on different database management systems efficiently
Prepare documentation for the further reference
Collaborate with other IT teams to modify, and enhance existing IT assets.
Troubleshoot and resolve issues like database performance, database capacity, replication, and other data-related issues
Testing the new storage system once all the data has been transferred.
Troubleshooting any issues that may arise.
Providing maintenance support.
Works on different database management systems efficiently
Understanding Structured Query Language and information management best practices
Designing, implementing, and sustaining networked and multi-sited application instances
Working closely with the customer and other engineers to ensure system requirements are met
Providing timely and accurate information and status updates to leadership, project sponsors, end users, and management

Qualifications

9 to 6 ET. support required
Bachelor’s degree in Business or related discipline with an information technology focus
9+ years of experience as a ETL development with Java Script ,.Net, Spring, C#, python
5+ Application development using JAVA, python
Database modeling with Oracle, MS SQL preferred
Artificial Intelligence development experience using python preferred
Web development framework experience preferred
Working with the company Global Systems such as Next ERP/GSCM preferred
Demonstrated experience and understanding of system development life cycle, dynamic of application development and information technology practices and methods
Full lifecycle ETL implementation with analysis, architecture, design, development, data extraction, staging and storage, transformation, presentation, testing, cutover/go-live planning

Additional Information

Describe your perks and culture","$93,494 /yr (est.)",1 to 50 Employees,Company - Public,,,,Less than $1 million (USD)
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902.0,Unknown / Non-Applicable
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018.0,Unknown / Non-Applicable
OpenSecrets.org,,Remote,Snowflake Data Engineer,"About us
Our goal is to is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy..
Our work environment includes:
Modern office setting
Work-from-home days
Wellness programs
Data Engineer
OpenSecrets is seeking to add a new member to our data team. The Data Engineer will be responsible for helping build and maintain our data systems.
Responsibilities include:
Implementing methods to improve data reliability and quality
Analyzing raw data and implementing that data into our system using data pipelines
Troubleshooting data errors and pipeline issues
REQUIRED QUALIFICATIONS:
Experience working with and analyzing large datasets, equivalent to 2-3 years in a similar position
Proven ability to build complex data pipelines using Snowpipe and SSIS
The ability to work with diverse populations
Experience with MSSQL Server and Snowflake
Advanced knowledge of ANSI SQL
PREFERRED QUALIFICATIONS:
Excellent written and verbal communication skills.
The ability to manage multiple priorities and projects both independently and as a member of a team
Snowflake SnowPro Data Engineer certification
Experience with Apache Spark
Experience with machine learning matching models
Knowledge of and interest in American government and money in politics.
SALARY AND BENEFITS:
We are a fully remote organization and this person can be based anywhere in the U.S.
OpenSecrets offers an excellent benefits package, including full medical, dental, 403(b) match, generous vacation and sick leave, and all federal holidays paid.
TO APPLY:
Please visit this link: https://airtable.com/shrGe2bugeapKVr8a
People of diverse backgrounds are strongly encouraged to apply. OpenSecrets is an equal opportunity employer committed to creating a diverse and inclusive organizational culture. We do not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under the law.
ABOUT US: OpenSecrets is the nation's premier research group tracking money in U.S. politics and its effect on elections and public policy. Our mission is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy.
Job Type: Full-time
Pay: $83,874.82 - $100,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Experience:
SQL: 3 years (Required)
Snowflake: 1 year (Preferred)
ETL: 2 years (Required)
Work Location: Remote","$91,937 /yr (est.)",,,,,,
"Trellis
4.4",4.4,Remote,Data Engineer,"Overview
As a Data Engineer for Trellis, you will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure to support the deployment of machine learning models for educational applications. You will be working with large volumes of text data, and will be responsible for ensuring data quality, performing data cleaning, and implementing data transformation processes.
Responsibilities
Build and maintain data pipelines for processing large volumes of text data
Perform data cleaning and preprocessing to ensure data quality and consistency
Implement data transformation processes to convert raw data into formats suitable for machine learning models
Collaborate with data scientists and other stakeholders to understand project requirements and provide data engineering support as needed
Develop and maintain data infrastructure to support machine learning workflows
Monitor and troubleshoot data pipelines to ensure high availability and reliability
Qualifications
Bachelor's or Master's degree in Computer Science, Data Science, or a related field
Strong programming skills in Python and experience with data processing frameworks such as Spark and Hadoop
Experience with text data processing and natural language processing (NLP) techniques
Familiarity with machine learning workflows and frameworks such as TensorFlow and PyTorch
Experience with cloud-based data storage and processing technologies such as AWS, Google Cloud, or Azure
Strong problem-solving skills and attention to detail
Excellent communication and collaboration skills
Benefits
SF office, but remote-friendly. Come into the office 60% of the time — you’ll want to! It’ll be built as a library in a way that’s anti-fatigue. We will also have offices in NYC and Montreal.
Health insurance with 100% premium covered
Generous PTO / sick leave
401(k) plan with employer match
Free lunch and snacks
Annual company retreat in Montreal
Bring your dog to work",,51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2012.0,$5 to $25 million (USD)
"Purpose Financial
4.2",4.2,Remote,Data Engineer,"Address : 322 Rhett St, Greenville, South Carolina, United States - 29601
Purpose Financial, Inc. is an innovative consumer financial services company that offers a diverse suite of credit products, promoting financial inclusion and meeting consumers wherever they are. Through its brands, the company is committed to helping customers achieve their version of financial stability in the moment and in the future. Since 1997, Purpose Financial has been a pioneer in the consumer credit and financial services market offering money solutions in over 1,000 storefronts locations and online lending. Providing services in over 20 states, Purpose Financial employs over 2,700 team members.
At Purpose Financial, our people are our greatest asset. We are inclusive and encourage different points of view. We all belong here. We respect our colleagues. We value our customers, and we support our communities.
What do we do? Our sole purpose is to help each customer achieve their version of financial stability in the moment and in the future.
We have a history! Consumers know that when they choose Purpose, they’re backed by a trusted, reliable financial services company with 25 years in the business, 2,700+ employees, 1,000+ store locations, and operating in 24 states.
Benefits and Perks. We believe our employee benefits should make life better. Purpose Financial offers a comprehensive benefits package to everyone on our team.
Health & Welfare Benefits. When you take care of yourself, you’re a better you – which is better for everyone!
Health Savings & Flexible Spending Accounts. Pre-tax money for your family’s out-of-pocket expenses.
401k with Matching Contributions. Think bigger! You can build a better future.
Wellness & Employee Assistance Programs. Take care of yourself in every way that matters.
Generous Vacation Policy & Paid Time Off. Time off to take care of yourself, your family – and your bucket list!
Volunteer Days. We want you to care. Always. Take 2 days off each year to volunteer and make a difference in your community.
Tuition Reimbursement. Pursue your goals without fretting over all the costs.
Employee Discounts. Save on home and auto insurance, cell phone service, entertainment, shopping, pet insurance and even travel.
Performance Incentives & Awards. We want you to feel valued for all your contributions.
Hybrid Workforce Support. We believe in getting it done – in whatever way works best for you and your team.
Want to meet our executive team? Visit our home page at www.havepurpose.com.
Position Summary
The job function is responsible for building a scalable data infrastructure. The role will be part of the effort to enable Purpose Financial to democratize data by developing self-serving and curated data models for our single source of truth. The job function includes understanding of business requirements, defining data requirements, identifying enterprise data sources, planning, designing, and building data acquisition from sources, defining data model, designing data ingestion pipelines, data normalization, data transformation, and consumption using platforms such as Snowflake and AWS.
Job Responsibility
Take ownership of building/optimizing data pipelines to increase efficiency and performance
Develop and maintain data ingestion and integration processes that focus on data quality and monitoring
Use AWS and Snowflake tools such as Glue, Lambda, Snowpark, and Snowpipe to build data ingestion pipelines from various sources
Incorporate standards and best practices into data pipeline solutions
Participate in troubleshooting and problem-solving design discussions
Provide logical and physical data design, and database modeling
Solve complex data issues around data integration, unusable data elements, unstructured data sets, and other data processing incidents
Continuously design the future state of our data & reporting process by collaborating with IT and business partners.
Partner with enterprise teams, data scientist, architects to define development standards and solutions.
Manage cloud infrastructure as code (IaC), using Terraform.
Work closely with other members of the data team to better design and build metrics that enhance our analytical capabilities
Manage workflows and data build processes that help the BI team build key business metrics and design dashboard visualizations
Assess the effectiveness and accuracy of new data sources, data points, and data gathering techniques
Troubleshoot problems with data (missing, corrupted, invalid) and build data recovery strategy
Apply best practices for AWS & Snowflake architecture, ELT and data models
Be a leader for our data journey, act as SME for data and database related topics, and assist with development of data lakes and warehouses
Education Required
Bachelor’s degree (or foreign equivalent degree) in any Computer, Engineering, or other technical field.
Experience Required
5+ years experience in data warehousing
3+ years of experience in data architecture and design, with a good understanding of various data systems and structures (i.e. SQL, NoSQL, Key-Value, Streams)
3+ years of experience with building data pipelines and using data orchestration tools and implementing end to end Monitoring & Alerting solutions
3+ years of experience working with AWS or public cloud provider.
Strong understanding of SQL, data structures, data integrity, and schema design best practices
Programming experience required, preferably Python.
A strong advocate of data governance and data quality
Experience with database internals, database design, SQL and database programming
Experience in financial services (Banking and consumer lending preferred)
Experience of authoring CI/CD pipelines, automation elements related to infrastructure composition, deployment orchestration, and monitoring.
Familiarity and experience with Agile/SCRUM based development methodology.
Knowledge Required
Excellent written and verbal communications skills as would be needed to communicate in person, by phone, and through email; adaptability and flexibility to changing environment; and comfortable working in a dynamic, high volume, fast-paced environment. Ability to read, write, evaluate, and apply information. Ability to interact professionally and exhibit appropriate social skills. Ability to understand and ensure compliance with policies, procedures, and laws governing our industry/business and products. Ability to develop and maintain business relationships.
Physical Requirements
Sitting for long periods of time; standing occasionally; walking; bending; squatting; kneeling; pushing/pulling; reaching; twisting; frequent lifting of less than 10 lbs., occasional lifting of up to 20 lbs.; driving and having access during the workday to an insured and reliable transportation; typing; data entry; grasping; transferring items between hands and/or to another person or receptacle; use of office equipment to include computers; ability to travel to, be physically present at, and complete the physical requirements of the position at any assigned location.
Competencies
Integrity
Customer Advocacy
Interpersonal Skills
Results/Accountability
Inclusiveness
Technical Proficiency/Leadership
Big Data Trends
Data Management
Adaptability
Travel - None
Attire - Business Casual
Other - Must be eligible to work in the USA and able to pass a background check.
Job Type: Full-time
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Day shift
Work Location: Remote",,Unknown,Company - Private,Financial Services,Financial Transaction Processing,,Unknown / Non-Applicable
"Integration Developer Network LLC
4.9",4.9,Remote,Data Engineer with PL/SQL Background,"Requirements:
Recent PL/SQL development work experience with Oracle 19.
Must have working experience with tools like Airflow, Git.
Must have python coding experience and REST API development experience.
Plus would be experience with microservices, docker, Kubernetes.
Plus would be experience with accounting and financial data
Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.
Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL.
Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations. Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts.
Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
2+ years’ experience in the design, implementation and support of full life-cycle data engineering projects.
Bachelor's degree in Computer Science, Engineering or other equivalent degree, with 5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree with 2+ years data engineering preferred.
High school diploma or GED is required.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
9 years
Schedule:
8 hour shift
Experience:
Python: 8 years (Required)
PL/SQL: 5 years (Required)
ETL: 5 years (Required)
Work Location: Remote",$62.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2011.0,$1 to $5 million (USD)
Proits Hub LLC,,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,,,,Unknown / Non-Applicable
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015.0,Unknown / Non-Applicable
"iManage
4.5",4.5,Remote,Senior Data Engineer (Azure),"We offer a flexible working policy that supports the health and well-being of our iManage employees. As an organization, we value collaborating and learning from our peers in person, while providing the necessary flexibility for our employees to have a meaningful work-life balance. Please reach out to learn more.
Being a Senior Data Engineer at iManage Means…
You are excited about data and believe in the democratization of data to support data driven decision-making. You will partner with our Information Technology team to implement, support, and extend our Enterprise Data Lake hosted on Azure and built using Azure Synapse. You will gather requirements from iManage business units and craft solutions which provide access to critical business data. You will develop data models and data pipelines for our Enterprise Data Lake, and provide integration with BI platforms and tools such as Totango and Power BI. You are passionate about lakehouse architecture and have experience using Delta Lake and bronze, silver, and gold data lake design.
Here is what one of our leaders, Cloud Services Director (Jacqueline Toepfer), has to say about the role: “As a Senior Data Engineer on our team, you will get the opportunity to showcase your expertise and make a real difference across the organization. You will be part of a truly collaborative team that is passionate about delivering quality solutions. You will be the in-house expert in the data models of multiple, disparate enterprise SaaS systems and utilize your wealth of knowledge to provide recommendations and solutions for consolidation, transformation, and integration of the disparate data sources.”
iM Responsible For…
Modeling, managing, and reporting of data stored in Azure Data Lake.
Gathering data requirements from various business units and translating these requirements into data models.
Using Python, PySpark, and system specific APIs to extract, transform, store and analyze data from a variety of systems.
Data modeling, defining data pipelines, and integrations necessary to present data in BI platforms such as Totango, or BI tools like Power BI.
Identifying and modeling all current disparate data sources and the data flows between these data sources.
Analyzing current repositories and proposing changes to data repositories and data flows to better support company objectives for the measurement of user experience and customer success.
Understanding the business needs of data integration and governance from disparate systems to drive the enhancement of the enterprise data lake.
Applying best practices to ensure the security and privacy of the data repositories.
Ensuring data repositories meet company standards for storage of PII.
Developing proficiency with the iManage product APIs for all iManage Cloud services.
iM Qualified Because I Have...
A Bachelor’s degree or higher in Computer Science or equivalent field.
3-5 years of experience working with data in a business setting.
Proficiency in data extraction, manipulation, and subsequent reporting with Spark and Python.
Experience designing data pipelines with a cloud-native mindset using Azure or AWS.
Knowledge and experience with architecting a data lake with Azure Synapse or adjacent technologies like Databricks.
Experience ingesting data from SaaS solutions and other services via API or other related technologies.
A passion to be a thought leader and work collaboratively within a team.
Commitment to understanding data requirements and delivering scalable, robust solutions that meet those requirements.
A creative mindset with a desire to explore new technologies and create innovative solutions.
Bonus Points If I Have…
Familiarity with Delta Lake.
A background with relational databases and data warehouse design using star schemas.
Experience with cloud-based data models for business solutions like Salesforce, Zendesk, and NetSuite.
Don't meet every qualification listed above? Studies show that women and people of color are less likely to apply to jobs unless they meet all qualifications. At iManage, we are committed to building a diverse and inclusive environment and encourage everyone to show up as their full authentic selves. We welcome those that come with a growth mindset and a hunger for learning; so, if you are excited about this role but your past experience doesn't align perfectly with every qualification, we encourage you to apply anyways!
iM Getting To…
Join a supportive, experienced team with an inclusive, encouraging, and vibrant culture.
Have flexible work hours that allow me to balance my ‘me time’ with my work commitments.
Collaborate in a modern open plan workspace, with a gaming area, free snacks, drinks and regular social events.
Focus on impactful work, solving complex, real challenges utilizing the latest technologies and protocols.
Own my career path with our internal development framework. Ask us more about this!
Learn new skills and earn certifications with access to unlimited courses in LinkedIn Learning.
Join an innovative, industry leading SaaS company that is continuing to grow & scale!
iManage Is Supporting Me By...
Creating an inclusive environment where I can help shape the culture not just by fitting in, but by adding to it.
Providing a market competitive salary that is applied through a consistent process, equitable for all our employees, and regularly reviewed based on industry data.
Rewarding me with an annual performance-based bonus.
Offering comprehensive Health/Vision/Dental/Life Insurance, and a 401k Retirement Savings Plan with a company match up to 4%.
Giving access to HealthJoy, a healthcare concierge service, to help me maximize my health benefits.
Granting enhanced leave for expecting parents; 20 weeks 100% paid for primary leave, and 10 weeks 100% paid for secondary leave.
Providing me with a flexible time off policy to take the time off that I need. Be it for vacation, volunteering, celebrating holidays, spending time with family, or simply taking time to recharge and reset.
Caring for my mental health and well-being with multiple company wellness days and free access to the Healthy Minds app for mindfulness, meditation and more.
About iManage…
iManage is dedicated to Making Knowledge WorkTM. Over one million professionals across 65+ countries rely on our intelligent, cloud-enabled, secure knowledge work platform to uncover and activate the knowledge that exists inside their business content and communications.
We are continuously innovating to solve the most complex professional challenges and enable better business outcomes; Our work is not always easy but it is ambitious and rewarding.
So we’re looking for people who love a challenge. People who are happiest when they’re solving problems and collaborating with the industry’s best and brightest. That’s the iManage way. It’s how we do things that might appear impossible. How we develop our employees’ strengths and unlock their potential. How we find meaning in everything we do.
Whoever you are, whatever you do, however you work. Make it mean something at iManage.
iManage provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Learn more at: www.imanage.com
Please see our privacy statement for more information on how we handle your personal data: https://imanage.com/privacy-policy/
#LI-LM1
#LI-Remote
V436F7WSwa",,501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015.0,$100 to $500 million (USD)
"Braintrust
4.6",4.6,"San Francisco, CA",Data Engineer,"ABOUT US:
Braintrust is a user-owned talent network that connects you with great jobs with no fees or membership costs—so you keep 100% of what you earn.

JOB TYPE: Freelance, Contract Position (no agencies/C2C - see notes below)
LOCATION: Remote - United States only (TimeZone: EST | Partial overlap)
HOURLY RANGE: Our client is looking to pay $100 – $105/hr
ESTIMATED DURATION: 40h/week - Long term

ABOUT THE HIRING PROCESS:
When you join Braintrust, you will be invited to a screening process for Braintrust to learn more about your previous work experiences. Once completed, you will have access to the employer for this role and other top companies that seek high-quality talent. Apply to this job to kick off the process.
THE OPPORTUNITY
Requirements
Summary:
Advanced analytics SQL - minimum 2 years, preferable 5
Python - minimum 1 year, preferable 2
Ability to work in cloud platform
Qualities that will help you thrive in this role:
You understand that being an effective engineer is about communicating with people as much as it is about writing code.
You are willing to work with and improve code you did not originally write, primarily in SQL and Python.
You are generous with your time and experience and can mentor and learn from other engineers.
You are comfortable with best practices for traditional data warehousing.
You love SQL and writing efficient and optimized ETL pipelines.
You are familiar with building and monitoring cloud services and infrastructure.
What you’ll be working on
What’s the role?
As a member of our client's Data Applications, Data Warehouse team, you’ll help us improve the stability, performance, and usability of their BigQuery data warehouse while advising their stakeholders on best practices and optimizations. Your work will enable other developers, data scientists, and analysts to write the high-performing pipelines that power data science, machine learning, and product development.
In addition to BigQuery SQL, our client's toolset includes Looker, Java, Python, and Spark, as well as Airflow, Terraform, and Kubernetes, and GCP services like Dataproc and Dataflow.
About The Team
They build highly-performant systems and data warehouses that are maintainable and cost effective.
They develop robust, highly available, well-monitored data infrastructure.
They stay in close communication with the internal customers and make strategic improvements to ensure those that depend on us have a great experience using data
What does the day-to-day look like?
You should have experience building data warehouses, data marts, and aggregate tables - supporting them at scale, and collaborating with other teams that depend on them.
Experience building applications and managing infrastructure using one of the major cloud providers is preferred but not required. (Our client uses Google Cloud).
Our client values curiosity, passion, responsibility, and generosity of spirit.
Apply Now!
Braintrust Job ID: 6590


C2C Candidates: This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.

Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.

This is a remote position.",$102.50 /hr (est.),51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2018.0,$5 to $25 million (USD)
"Dobbs Defense Solutions, LLC",,"Fort Meade, MD",Data/NiFi Engineer,"Our Mission
At Dobbs Defense, we deliver mission-centric IT, Cyber, and data analytics solutions for our government and commercial clients through the convergence of automation, innovation, training, and education. Delivering high-quality IT, cybersecurity, and data analytics solutions through proven and innovative methods is our vision.

Job Description
Dobbs Defense Solutions is seeking a Senior Data/NiFi Engineer to design, develop, implement, test, secure, and support data transport and brokering for multiple classified networks and between networks.

Duties
Design, develop, implement, test, secure, and support data transport and brokering.
Data flow engineering for one of DISA’s premier boundary defense systems.
Perform system administration of Niagara Files (NiFi) on Linux VMs.
Support backup/failover/load balancing of data brokering to improve availability.
Monitor availability & amp, usage.
Respond to outages, take corrective action, and issue outage reports.
Familiarity with DISA STIGs and SCAP.
Familiarity with Java programing.

Qualifications
Required education and experience:
5+ years of Data Engineering experience.
Significant experience with Niagara Files (NiFi) and MiNiFi or similar data engineering tools.
Excellent written and verbal communication skills.
IAM or IAT Level II Certification (e.g., GSEC, SEC+, SCNP, SSCP, etc.).
Active Secret clearance.

Required Clearance
Secret

Working Environment
Onsite

Our Equal Employment Opportunity Policy
Dobbs Defense Solutions is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or medical conditions related to pregnancy, childbirth or breastfeeding), sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), gender expression and transgender status, national origin, ancestry, age, disability, military or veteran status, marital or domestic partner status, genetic information, citizenship, low-income status or any other status or characteristic protected by applicable law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits, and all other privileges, terms, and conditions of employment.
4Cmw9yg2oT","$83,799 /yr (est.)",1 to 50 Employees,Company - Public,,,,Unknown / Non-Applicable
"PSEG
3.9",3.9,"Edison, NJ",AWS Data Engineer,"Requisition: 73755
PSEG Company: PSEG Services Corp.
Salary Range: $ 78,600 - $ 149,400
Incentive: PIP 10%
Work Location Category: Remote Local
PSEG operates under a Flexible Work Model where flexible work is offered when job requirements allow. In support of this model, roles have been categorized into one of four work location categories: onsite roles, hybrid roles that are a blend of onsite and remote work, remote local roles that are primarily home-based but require some level of purpose-driven in-person interaction and living within a commutable distance, and remote non-local roles that can be effectively performed remotely with the ability to work in approved states.
PSEG offers a unique experience to our more than 12,000 employees – we provide the resources and opportunities for career development that come with being a Fortune 500 company, as well as the attention, camaraderie and care for one another you might typically associate with a small business. Our focus on combatting climate change through clean energy technology, our new net zero climate vision for 2030 and enhanced commitment to diversity, equity and inclusion; and supporting the communities we serve make this a particularly exciting time to join PSEG.
Job Summary
The Technology Engineer is a direct report to the Senior Technology Engineer with understanding of business goals, business processes, technology expertise in one or more domains, technology solution design and implementation.
The Technology Engineer is involved in the full technology life cycle and responsible for designing, coding, configuring, testing, implementing and supporting application software. Technology Engineers work closely with Analysts and Product Managers to understand the business requirements that drive the analysis and physical design of technical solutions. Technology Engineers may be assigned to either implementation or support functions. Implementation involves creating new technology based solutions whereas support involves upgrades, maintenance or issue resolution to existing technology solutions.

Job Responsibilities
Primary Responsibilities:
Design, configuration, development, documentation and testing of technology solutions to meet business or technology requirements. Evaluation of existing technology solutions to determine fit for purpose for the new business or technology requirements. Recommendation of technology alternatives. Collaboration with other individuals to ensure proper integration of the new technology solution with the existing technology solutions.
Analysis of end user’s needs, business and technology requirements. Translation of these requirements into technology solution capabilities and design. Alternatively, using the user needs, business and technology knowledge to peer review designs, implementation plans, software code, configuration settings or other artifacts to ensure technology solution being created by others meets the user needs, business and technology requirements.
Provides support toward resolution of escalated support tickets. May perform the needed analysis to identify root cause of reported incidents, identify the short and long term remediation for the identified incidents
Specific responsibilities include:
Analyzes end-user needs and designs, configures, develops and tests technology solutions to satisfy demand. Performs build versus buy analysis for business demand.
Partners with business analysts to translate business needs to technology solution requirements.
Evaluates existing technology solutions and platforms and provides recommendations for improving technology performance by conducting gap analysis, identifying feasible alternative solutions, and assisting in the scope of needed modifications.
Collaborates with all stakeholders such as enterprise architects, software development, operations, cybersecurity and infrastructure to integrate applications and hardware.
Ensures that the design and technology solution implementation meets security and QA standards. Suggests fixes to issues by doing a thorough analysis of root cause and impact of any defect(s).
Provides support toward resolution of escalated support tickets. Applies operation break fixes and performs other proactive maintenance activities until permanent solutions can be implemented. Supports and participates in the solution deployment process for new functionality/ subsystems/modules, upgrades, updates and fixes to the production environment.
Makes solutions production-ready by following the standard change management processes, completing required forms, following procedures, completing version control documents, etc.

Job Specific Qualifications
Bachelor’s degree in Computer Science or a related field 4 years of professional technology solution engineering
Demonstrated technology solution ownership and adoption, projects or other work experiences.
Demonstrated experience in analysis of end-user needs to configure, develop and test low to medium complexity technology solutions to satisfy business demand.
Demonstrated track record of implementing technology solutions using structured methodologies such as agile (SCRUM, Kanban etc.) and waterfall.

Required Competencies:
AWS Data Cloud development experience
Experience developing and sustaining data load jobs using Extract, Transform, Load (ETL/ELT) methodologies and tools such as, Python, SQL, Shell Scripts and AWS Services.
Developing and sustaining a data ingestion pipeline to AWS leveraging services such as Athena, Glue, Lambda, S3, Relational Database Service (RDS) and Redshift.
Integration of AWS Data Lake with reporting tools such as PowerBI.
Experience using databases such as Oracle, MS SQL Server, and developing software code in one or more programming languages (Java, Python, etc.)
Experience with production support of mission critical technology solutions.
Experience documenting technical solutions and system process flow techniques.
Ability to work independently, multi-task effectively, and be flexible to accommodate change in priorities as necessary.
Strong analytical ability, communication skills, excellent problem solving skills, and ability to learn new technical concepts.
Ability to foster working relationships with Client departments, IT Management and Software Service Providers.
Desired Qualifications:
Graduate degree or MBA
AWS Developer/Architect certification
Minimum Years of Experience
4 years of experience
Education
Bachelors in Engineering or Computer Science
Bachelor in Information Technology
Certifications
None Noted
Disclaimer
Certain positions at the Company may require you to have access to Part 810-Controlled Information. Under the law, the Company is limited in who it can share this information with and in certain circumstances it is necessary to obtain specific authorization before the Company can share this information. Accordingly, if the position does require access to this information, you must complete a 10 CFR Part 810 Export Control Compliance Nationality Request Form, a copy of which will be provided to you by Talent Acquisition if an offer is made. If there is a need for specific authorization, due to the time it takes to obtain authorization from the government, we will likely not be able to further proceed with an offer
Candidates must foster an inclusive work environment and respect all aspects of diversity. Successful candidates must demonstrate and value differences in others' strengths, perspectives, approaches, and personal choices.
As an employee of PSE&G or PSEG LI, you should be aware that during storm restoration efforts, you may be required to perform functions outside of your routine duties and on a schedule that may be different from normal operations.
Certain positions at the Company may require you to have access to 10 CFR Part 810 controlled information. If the position does require access to this information, the Talent Acquisition representative will provide further details upon making an offer.
PSEG is an equal opportunity employer, dedicated to a policy of non-discrimination in employment, including the hiring process, based on any legally protected characteristic. Legally protected characteristics include race, color, religion, national origin, sex, age, marital status, sexual orientation, disability or veteran status or any other characteristic protected by federal, state, or local law in locations where PSEG employs individuals.
Business needs may cause PSEG to cancel or delay filling position at any time during the selection process.
This site (http://www.pseg.com) is strictly for candidates who are not currently PSEG employees. PSEG employees must apply for jobs internally through emPower which can be accessed through sharepoint.pseg.com by clicking on the emPower icon, then selecting careers.

PEOPLE WITH DISABILITIES:
PSEG is committed to providing reasonable accommodations to individuals with disabilities. If you have a disability and need assistance applying for a position, please call 973-430-3845 or email accommodations@pseg.com. If you need to request a reasonable accommodation to perform the essential functions of the job, email accommodations@pseg.com. Any information provided regarding a disability will be kept strictly confidential and will not be shared with anyone involved in making a hiring decision.

ADDITIONAL EEO/AA INFORMATION (Click link below)
Know your Rights: Workplace Discrimination is Illegal
Pay Transparency Nondiscrimination Provision","$114,000 /yr (est.)",10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1903.0,$5 to $10 billion (USD)
"LatentView Analytics
4.0",4.0,"Dallas, TX",Data Engineer,"Role : Data Engineer
Experience : 6 - 8+ Years
Location : Dallas,Tx (Onsite)
Position: FullTime Only
Skills :Python,SQL Server ,Scala, Hadoop, HPCC, Storm, Cloudera, Cassandra,Excel, R,Docker,Kubernetes,Snowflake,Azure,Kafka,Redshift,Hadoop,AWS.
Job Type: Full-time
Pay: $80,000.00 - $1,100,000.00 per year
Compensation package:
Yearly pay
Experience level:
6 years
Schedule:
8 hour shift
Experience:
data engineer: 6 years (Preferred)
Work Location: In person
Speak with the employer
+91 9876543210","$109,012 /yr (est.)",1001 to 5000 Employees,Company - Public,Management & Consulting,Business Consulting,2006.0,$25 to $100 million (USD)
"Octo
4.2",4.2,"Chantilly, VA",Data Engineer,"You…
As a Data Engineer, you will be joining the team that is deploying and delivering a cloud-based, multi-domain Common Data Fabric (CDF), which provides data sharing services to the entire DoD Intelligence Community (IC). The CDF connects all IC data providers and consumers. It uses fully automated policy-based access controls to create a machine-to-machine data brokerage service, which is enabling the transition away from legacy point-to-point solutions across the IC enterprise.
Us…
We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client’s missions.
Program Mission…
The CDF program is an evolution for the way DoD programs, services, and combat support agencies access data by providing data consumers (e.g., systems, app developers, etc.) with a “one-stop shop” for obtaining ISR data. The CDF significantly increases the DI2E’s ability to meet the ISR needs of joint and combined task force commanders by providing enterprise data at scale. The CDF serves as the scalable, modular, open architecture that enables interoperability for the collection, processing, exploitation, dissemination, and archiving of all forms and formats of intelligence data. Through the CDF, programs can easily share data and access new sources using their existing architecture. The CDF is a network and end-user agnostic capability that enables enterprise intelligence data sharing from sensor tasking to product dissemination.
Responsibilities...
Primary responsibility is to work with data providers within the IC and DoD Enterprise to identify and ingest data sets into the CDF data broker. In this role you will:
Develop, optimize, and maintain data ingest flows using Apache Nifi and Python.
Develop within the components in the cloud platform, such as Apache Kafka, NiFi, and HBase.
Communicate with data owners to set up and ensure CDF streaming and batching components are working (including configuration parameters).
Document SOP related to streaming configuration, batch configuration or API management depending on role requirement.
Document details of each data ingest activity to ensure they can be understood by the rest of the team
What we’d like to see…
A minimum of 3 years of experience with programming and software development including analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems
DoD 8570 IAT Level II Certification (e.g. Security+) or the ability to obtain the certification within 90 days
Demonstrable CentOS command line knowledge
Working knowledge of web services environments, languages, and formats such as RESTful APIs, SOAP, FTP/SFTP, HTML, JavaScript, XML, and JSON
Understanding of foundational ETL concepts
Experience implementing data ignorations with in the IC DoD Enterprise.
Desired Skills:
Experience or expertise using, managing, and/or testing API Gateway tools and Rest APIs (desired)
2+ Experience in Python Development
Experience or expertise configuring an LDAP client to connect to IPA (desired)
Advanced organizational skills with the ability to handle multiple assignments
Strong written and oral communication skills
Years of Experience: Junior Level (0-4 years),Mid Level (5-8 years), Senior Level (9+)
Education: Bachelor's degree in systems engineering, computer engineering, or a related technical field (preferred)
Location: Chantilly, VA
Clearance: Active TS/SCI w/ ability to obtain CI Poly (preferred)","$99,041 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2006.0,$100 to $500 million (USD)
"TIAA
4.0",4.0,"Iselin, NJ",Senior Cloud Data Engineer,"The Senior Data Platform Engineer, Cloud role designs datastore systems that are appropriate for applications, customer needs and consistent with the overall design of the organization's information systems architecture. Under limited supervision, this job is responsible for the solution engineering and design, provisioning, delivery, service management, continuous automations of the organization's datastore systems.

Key Responsibilities and Duties
Design, develop and deliver cloud datastore solutions and develop automation pipelines to migrate data sets from On-prem to Cloud platforms. Practice Infrastructure as code to develop automation routines and integration flows to manage state of the datastore platform systems

Provision secures from start datastores and enable them with required security controls including encryption, masking, certificate/keys rotation etc.

Collaborates with developers, analysts, various system administrators to identify business requirements in designing efficient datastore solutions and interfaces.

Identifies and documents all system constraints, implications, and consequences of various proposed system changes.

Reviews technical documentation to guide system users and to assist with the ongoing operation, maintenance, and development of the system. Evaluates the efficiency and effectiveness of application operations and troubleshooting problems.

Provide expert level IT technical lead services, including the direction, evaluation, selection, configuration, implementation, and integration of new and existing technologies and tools in a cloud platform.

Responsible for development of cloud integrations and data migrations to support operations of Cloud infrastructure, provisioning, monitoring with (IaaS) and (PaaS) models

Deploy, automate, maintain, and manage AWS cloud-based production system, to ensure the availability, performance, scalability, and security of productions systems.

Manage the governance framework for DB-Services specific to private, hybrid and public cloud platform adhering to standards and integration with existing tools.

Ability to anticipate technology changes within a rapidly evolving environment.
Educational Requirements
Bachelor's Degree Preferred
Work Experience
3+ Years Required; 5+ Years Preferred
Physical Requirements
Physical Requirements: Sedentary Work

Career Level
7IC

Required Skills:
3 or more years of experience in SQL, ETL and ELT Tools.
Experience working with Data Virtualization Platforms like Starburst, Presto, Denodo, Dremio.
Preferred Skills:
Experience with AWS or GCP, PySpark, CI/CD Pipelines using ElectricFlow.
Base Pay Range: $88,600/yr. - $147,600/yr.
Actual base salary may vary based upon, but not limited to, relevant experience, time in role, base salary of internal peers, prior performance, business sector, and geographic location. In addition to base salary, the competitive compensation package may include, depending on the role, participation in an incentive program linked to performance (for example, annual discretionary incentive programs, non-annual sales incentive plans, or other non-annual incentive plans).
_____________________________________________________________________________________________________
Company Overview
TIAA is the leading provider of financial services in the academic, research, medical, cultural and government fields. We offer a wide range of financial solutions, including investing, banking, advice and education, and retirement services.
Benefits and Total Rewards
The organization is committed to making financial well-being possible for its clients, and is equally committed to the well-being of our associates. That’s why we offer a comprehensive Total Rewards package designed to make a positive difference in the lives of our associates and their loved ones. Our benefits include a superior retirement program and highly competitive health, wellness and work life offerings that can help you achieve and maintain your best possible physical, emotional and financial well-being. To learn more about your benefits, please review our
Benefits Summary
.
Equal Opportunity
We are an Equal Opportunity/Affirmative Action Employer. We consider all qualified applicants for employment regardless of age, race, color, national origin, sex, religion, veteran status, disability, sexual orientation, gender identity, or any other protected status.
Read more about the Equal Opportunity Law
here
.
Accessibility Support
TIAA offers support for those who need assistance with our online application process to provide an equal employment opportunity to all job seekers, including individuals with disabilities.
If you are a U.S. applicant and desire a reasonable accommodation to complete a job application please use one of the below options to contact our accessibility support team:
Phone: (800) 842-2755
Email:
accessibility.support@tiaa.org
Privacy Notices
For Applicants of TIAA, Nuveen and Affiliates residing in US (other than California), click
here
.
For Applicants of TIAA, Nuveen and Affiliates residing in California, please click
here
.
For Applicants of Nuveen residing in Europe and APAC, please click
here
.
For Applicants of Greenwood residing in Brazil (English), click
here
.
For Applicants of Greenwood residing in Brazil (Portuguese), click
here
.
For Applicants of Westchester residing in Brazil (English), click
here
.
For Applicants of Westchester residing in Brazil (Portuguese), click
here
.","$118,100 /yr (est.)",10000+ Employees,Company - Private,Financial Services,Investment & Asset Management,1918.0,$100 to $500 million (USD)
"Delta
4.3",4.3,"Atlanta, GA",Senior Data Engineer Modeler,"United States, Georgia, Atlanta
Information Technology
11-May-2023
Ref #: 20782
LinkedIn Tag: #LI-JM2
How you'll help us Keep Climbing (overview & key responsibilities)
Location: Atlanta GA - NOT Remote

Delta IT is on a journey of transformation. We are changing the way we do business from top to bottom. As leaders with vision within Delta, we strive to build important and innovative solutions and are looking for team members to help us realize our vision.

Delta employees are problem solvers, doers, innovators.

We are proactive.

We are collaborative.

We deliver impact to our customers.

Join us on our transformation journey in becoming a best-in-class IT organization at the world's best airline!

The Senior Data Engineer, Modeler to join our Enterprise Data team. This position is responsible for building, modifying, modernizing the conceptual, logical, and physical data models used in Enterprise Data databases. The position also requires establishing and maintaining effective partnerships with the Business and IT stakeholders.

We are looking for someone with strong analytical and organizational skills to transform data into insights, distill requirements, and develop processes. The candidate should have excellent communication skills, business maturity, and feel comfortable in a fast-paced environment.

Responsibilities
Understand the data needs of the company for ingestion, migration, storage, and access
Work with business teams to gather requirements for the database design and model
Collaborate with the Enterprise Data team and business teams to define requirements, then design and build database models
Design and build conceptual, logical, and physical data models in accordance with companys data standards
Apply relational and dimensional models for raw ingestion and curated/semantic layers
Create Physical Data Structures (DDLs) and corresponding metadata
Creating and maintaining data reference architecture architectures and integration patterns
Updating knowledge by tracking and understanding emerging large data and modeling practices and standards
Benefits and Perks to Help You Keep Climbing
Our culture is rooted in a shared dedication to living our values Care, Integrity, Resilience, Servant Leadership, and Teamwork every day, in everything we do. At Delta, our people are our success. At the heart of what we offer is our focus on Sharing Success with Delta employees. Exploring a career at Delta gives you a chance to see the world while earning great compensation and benefits to help you keep climbing along the way:
Competitive salary, industry-leading prot sharing program, and performance incentives.
401(k) with generous company contributions up to 9%
Paid time off including vacation, holidays, paid personal time, maternity and parental leave.
Comprehensive health Benefits including medical, dental, vision, short/long term disability and life Benefits.
Family care assistance through fertility support, surrogacy and adoption assistance, lactation support, subsidized back-up care, and programs that help with loved ones in all stages.
Holistic Wellbeing programs to support physical, emotional, social, and financial health, including access to an employee assistance program offering support for you and anyone in your household, free financial coaching, and extensive resources supporting mental health.
Domestic and International space-available flight privileges for employees and eligible family members
Career development programs to achieve your long-term career goals.
World-wide partnerships to engage in community service and innovative goals created to focus on sustainability and reducing our carbon footprint
Business Resource Groups created to connect employees with common interests to promote inclusion, provide perspective and help implement strategies
Recognition rewards and awards through the platform Unstoppable Together
Access to over 500 discounts, specialty savings and voluntary benefits through Deltaperks such as car and hotel rentals and auto, home, and pet insurance, legal services, and childcare
What you need to succeed (minimum qualifications)
7 or more years of experience in Information Technology or related technical capacity
Expert in concepts and principles of data modeling
Knowledge of entity relationship, dimensional modeling, big data, enterprise data, and physical data models
Knowledge of relational databases and data architecture computer systems, including SQL Familiarity
Ability to design, build, and develop a new product, technology, or service from feasibility through to production
Familiarity with data modeling software such as SAP PowerDesigner, Microsoft Visio, E/R Studio or Erwin Data Modeler
Must have hands-on experience with cloud platforms; AWS preferred
Knowledge of big data platforms such as Teradata, Oracle DB, DB2, AWS Aurora, AWS Athena, etc.
Experience using Python and/or PowerShell scripting for data processing
Strong attention to detail
Excellent communicationwith both technical and business stakeholders
Ability to work in a fast-paced environment
Ability to work both independently and as part of a team
Understanding of the business and the ability to assess and address risk without negatively impacting the business
Consistently prioritizes safety and security of self, others, and personal data.
Embraces diverse people, thinking, and styles.
Possesses a high school diploma, GED, or high school equivalency.
Is at least 18 years of age and has authorization to work in the United States.
What will give you a competitive edge (preferred qualifications)
Bachelors or masters degree in Information Technology, Computer Science, Mathematics, Engineering, Information Systems, or equivalent
In-depth understanding of ETL and data ingestion processes used in large scale data warehouses
Knowledge of data architecture principles for on-prem and cloud solutions
Flexibility to adapt and plan for changing business objectives
Solid Understanding of Agile/Scrum development methodologies
Experience translating business outcome requirements into data model requirements
Ability to adapt communication style for technical and business audiences
< Go back","$124,024 /yr (est.)",10000+ Employees,Company - Public,Transportation & Logistics,"Airlines, Airports & Air Transportation",1928.0,$10+ billion (USD)
"Great Dane
2.9",2.9,"Chicago, IL",Data Engineer,"Data Engineer - (230005R)
Description
With thousands of employees worldwide, teamwork and collaboration are valued here.

We look for employees who are driven, determined and ready to accelerate their future. By joining our team, you will earn competitive pay, benefits, insurance, 401k, pension and more while working in an environment with the highest safety standards in the industry.
The Position:
The Data Engineer is responsible for expanding and optimizing our current data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is experienced in both data pipeline creation and data transformation. The Data Engineer will support our software developers, system architects, data analysts and Business Analysts on all data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s current data architecture to support our next generation of products and data initiatives.
Key Responsibilities:
Create and maintain optimal data pipeline architecture.
Assemble complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep data separated and secure across all platforms used.
Work with data and business experts to strive for greater functionality in our data systems.
Other duties as assigned.
Qualifications
Requirements:
Education: Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Experience: 5+ years of experience in Data Engineer role
Skills: Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing data pipelines architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from disconnected datasets.
Project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with relational SQL databases, including MS SQL server, Oracle, DB2, and Maria.
Experience with Data Cloud platforms like SnowFlake and or Data Bricks.
Experience with data pipeline and workflow management tools: SQDR, Airflow, Fivetran, Airbyte, etc.
Experience with object-oriented/object function scripting languages: Python, Java
Travel: 20% at most
Physical Demands/Work Environment:
The physical demands and work environment characteristics described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Physical demands: While performing duties of job, employee is occasionally required to stand; walk; sit; use hands to finger, handle or feel objects; reach with hands and arms; talk and hear. Specific vision abilities required by the job include close and distance vision.
Work environment: The noise level in the work environment is usually minimal to moderate.
Must be willing to work occasional nights and/or weekends as business commitments dictate.
Great Dane is an Equal Opportunity Employer
Primary Location
: US-IL-Chicago
Work Locations
: Chicago N. LaSalle St. 222 N. LaSalle St. Suite 920 Chicago 60601
Job
: Information Systems
Schedule
: Full-time
Shift
: 1st Shift","$104,652 /yr (est.)",5001 to 10000 Employees,Company - Private,Manufacturing,Transportation Equipment Manufacturing,1900.0,$500 million to $1 billion (USD)
"Texas Capital Bank
3.0",3.0,"Richardson, TX",Senior Data Engineer,"Overview:
A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses on development and delivery of analytical solutions using various tools including AWS Glue, Lambda, Snowflake and AWS RDS. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.
Responsibilities:
Partner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (onshore and offshore)
Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storage
Advanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plans
Skilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumption
Expert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging, and error handling
Support continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review board
Develop and ensure adherence to published system architectural decisions and development standards
Lead and foster junior data engineers in their careers to produce higher quality solutions at a faster velocity through optimization training and code review
Multi-task across several ongoing projects and daily duties of varying priorities as required
Interact with global technical teams to communicate business requirements and collaboratively build data solutions
Qualifications:
Education
Bachelor’s degree in computer science or MIS related area required or equivalent experience (industry experience substitutable)
Job experience
4-8 years of total experience in data engineering/Cloud development activity.
1+ years of experience in Banking and financial domain

Technical Requirement
Must be extremely proficient in Data Warehouse ETL Design/Architecture, dimensional/relational data modelling.
Experience in atleast one ETL development project, writing/analyzing complex stored procedures.
Should have entry level/intermediate experience in Python/PySpark – working knowledge on spark/pandas dataframe, spark multi-threading, exception handling, familiarity with different boto3 libraries, data transformation and ingestion methods, ability to write UDF.
Snowflake – Familiarity with stages and external tables, commands in snowflake like copy, unload data to/from S3, working knowledge of variant data type, flattening nested structure thru SQL, familiarity with marketplace integrations, role-based masking, pipes, data cloning, logs, user and role management is nice to have.
Familiarity with Coalesce is an added advantage for this job
Collibra integration experience for Data Quality and Governance in ETL pipeline development is nice to have.
AWS – Should have hands-on experience with S3, Glue (jobs, triggers, workflow, catalog, connectors, crawlers), CloudWatch, RDS and secrets manager.
AWS - VPC, IAM, Lambda, SNS, SQS, MWAA and Athena is nice to have.
Should have hands-on experience with version controlling tools like github, working knowledge on configuring, setting up CI/CD pipelines using yaml, pip files.
Streaming Services – Familiarity with Confluent Kafka or spark streaming or Kinesis (or equivalent) is nice to have.
Data Vault 2.0 (hubs satellite links) experience will be a
Highly proficient in Publisher, PowerPoint, SharePoint, Visio, Confluence and Azure DevOps
Working knowledge of best practices in value-driven development (requirements management, prototyping, hypothesis-driven development, usability testing)
Good communicator with problem solving mindset and focus on process improvement
Strong time management skills and a proven track record of meeting various deadlines
Strong executive presentation skills with expertise in PowerPoint and presentation best practices.
Consistently demonstrates clear and concise written and verbal communication skills
Good interpersonal skills, ability to interact with Senior Management
Highly self-motivated with a strong sense of initiative
Excellent multitasking skills and task management strategies
Ability to work well in a team environment, meet deadlines, demonstrate good time management, and multi-task in a fast-paced project environment.","$106,721 /yr (est.)",1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,1998.0,Unknown / Non-Applicable
"Mastery Logistics Systems, Inc
3.9",3.9,"Omaha, NE",Senior Data Engineer (Kafka),"About the Role
In the world of transportation, data is constantly moving, and Kafka is the roadway that keeps that traffic running smoothly to its destination. As a technical expert, you must be comfortable working across teams on multiple, high impact projects. You will be a valued part of a team that is constantly maturing Kafka use and event-driven architecture. Members of this team are responsible for the overall use and implementation of Kafka components including the Confluent platform, observability, governance, best practices, and solution development. An understanding of Kafka principles and enterprise integration patterns is required.
In order to be successful:
You are a self-directed person who can identify priorities.
You are a detail-oriented person who takes pride in keeping data correct and always having a backup plan.
You are a problem-solver who might write a script or find a tool to get things done when there isn't an established solution.
You want to learn and grow in the event-driven world.
You love Kafka! When you hear terms like ""event-driven"" or ""real-time streaming"" you're ready ready to dive in!
Responsibilities
Lead a team of Kafka engineers in an operational capacity
Develop and implement solutions using Kafka.
Administer and improve use of Kafka across the organization including Kafka Connect, ksqlDB, Streams, and custom implementations.
Work with multiple teams to ensure best use of Kafka and data-safe event streaming.
Understand and apply event-driven architecture patterns and Kafka best practices. Enable development teams to do the same.
Assist developers in choosing correct patterns, event modeling, and ensuring data integrity.
Continuous learning to be a Confluent/Kafka subject matter expert.
Work with Kafka and Confluent API's (e.g. metadata, metrics, admin) to provide pro-active insights and automation.
Work with SRE's to ensure Kafka-related metrics are exported to New Relic.
Perform regular reviews of performance data to ensure efficiency and resiliency.
Contribute regularly to event-driven patterns, best practices, and guidance.
Review feature release and change logs for Kafka, Confluent, and other related components to ensure best use of these systems across the organization.
Work with lead to ensure all teams are aware of technology changes and impact.
Acquire a deep understanding of source and sink connector technical details for a variety of platforms including PostgreSQL, MS SQL Server, Snowflake, and others as required.
Requirements
Be able to describe the primary components of Kafka and their function (brokers, zookeeper, topics).
At least two years of experience supporting applications in a production environment.
You will be expected to read and navigate code in multiple languages. Multi-language fluency and writing is not required.
Experience in a microservice architecture
Experience with event driven architecture
Proficiency in at least one programming language and one scripting language.
Proficiency with Docker containers.
Ability to participate in and contribute to code management in Github including actively collaborating in peer-reviews, feature branches, and resolving conflicts and commits.
Excellent written and verbal communication skills.
Strong sense of responsibility with a bias towards action.
Comfortable self-directing and prioritizing your own work.
Microservices experience is a plus.
Distributed tracing experience a plus.
An understanding of any cloud (Azure preferred) infrastructure and components is a plus, but is not required.
Create reference solutions.","$96,493 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"University of Arizona
4.2",4.2,"Tucson, AZ",KMAP Data Engineer,"Posting Number
req16040

Department
Research Innovation & Impact

Department Website Link
https://kmap.arizona.edu/

Location
Main Campus

Address
Tucson, AZ USA

Position Highlights
The Institutional Knowledge Map (KMap) program at the University of Arizona is in search of a KMap Data Engineer (Applications Developer Programmer Analyst I). KMap is a data science system that helps organizations to understand their knowledge landscape. It uses advanced technologies to analyze research information like research papers, patents, grants, biography, CV, and web presence to make an artificial map of the organization's knowledge. This visual map helps to find experts and collaborators. This position is responsible for collecting, cleaning, and aggregating data for research activities of the university. This position will take care data collections and data processing pipelines of the KMap project.
The ideal candidate must possess proficiency in Python and MongoDB for efficient handling, processing, and analysis of research information.
Outstanding UA benefits include health, dental, and vision insurance plans; life insurance and disability programs; paid vacation, sick leave, and holidays; UA/ASU/NAU tuition reduction for the employee and qualified family members; state and optional retirement plans; access to UA recreation and cultural activities; and more!
The University of Arizona has been recognized for our innovative work-life programs.


Duties & Responsibilities
Duties & responsibilities:
Develop and maintain the data collection and processing pipelines.
Collaborate with data partners and other related groups
Technical documentation of the project
Collecting, cleaning, and transforming new data from internal and external sources
Write and execute automatic test cases to test data and application
Additional duties may be assigned
Knowledge, skills & abilities:

Knowledge of data processing techniques and tools
Knowledge of using API, and different data collection methodologies
Strong analytic skills related to working with unstructured and RDMBS datasets
Proficiency in Python and MongoDB for efficient handling, processing, and analysis of research information

Minimum Qualifications
Bachelor's degree in Information Technology or equivalent advanced learning attained through experience required and a minimum of 1 year of relevant work experience required.

Preferred Qualifications
Experience in data science tasks with Python is a plus
Experience working with complex data in a NoSQL-based environment
Experience with Python packages such as pandas, NumPy
Experience working with Linux server environments including shell scripting
Experience with aggregation tasks in MongoDB
Experience working with Neo4j graph database
Experience with graph data analysis and visualize
Experience with Python programming and MongoDB database
Experience with using source-controlling systems

FLSA
Exempt

Full Time/Part Time
Full Time

Number of Hours Worked per Week
40

Job FTE
1.0

Work Calendar
Fiscal

Job Category
Information Technology

Benefits Eligible
Yes - Full Benefits

Rate of Pay
DOE

Compensation Type
salary at 1.0 full-time equivalency (FTE)

Grade
8

Career Stream and Level
PC1

Job Family
Applications Development

Job Function
Information Technology

Type of criminal background check required:
Name-based criminal background check (non-security sensitive)

Number of Vacancies
1

Target Hire Date

Expected End Date

Contact Information for Candidates
Iqbal Hossain
hossain@arizona.edu

Open Date
5/11/2023

Open Until Filled
Yes

Documents Needed to Apply
Resume and Cover Letter

Special Instructions to Applicant

Diversity Statement
At the University of Arizona, we value our inclusive climate because we know that diversity in experiences and perspectives is vital to advancing innovation, critical thinking, solving complex problems, and creating an inclusive academic community. As a Hispanic-serving institution, we translate these values into action by seeking individuals who have experience and expertise working with diverse students, colleagues, and constituencies. Because we seek a workforce with a wide range of perspectives and experiences, we provide equal employment opportunities to applicants and employees without regard to race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, gender identity, or genetic information. As an Employer of National Service, we also welcome alumni of AmeriCorps, Peace Corps, and other national service programs and others who will help us advance our Inclusive Excellence initiative aimed at creating a university that values student, staff and faculty engagement in addressing issues of diversity and inclusiveness.","$79,143 /yr (est.)",10000+ Employees,College / University,Education,Colleges & Universities,1885.0,$1 to $5 billion (USD)
"Vertex, Inc.
3.9",3.9,Remote,Sr. Data Engineer- Cloud (Remote),"Job Description:
This position is responsible for performing analysis, design, implementation, testing, maintenance, and support tasks for data-intensive software applications programming. Improves system quality by identifying issues and common patterns and developing standard operating procedures. Enhances applications by identifying opportunities for improvement, making recommendations, and designing and implementing systems. ESSENTIAL JOB FUNCTIONS AND RESPONSIBILITIES: · Prepare technical design specifications based on functional requirements and analysis documents. · Implement, test, maintain and support software, based on technical design specifications. · Improve system quality by identifying issues and common patterns, and developing standard operating procedures · Enhance applications by identifying opportunities for improvement, making recommendations, and designing and implementing systems · Maintain and improve existing codebases and peer review code changes · Liaise with colleagues to implement technical designs · Investigating and using new technologies where relevant · Provide written knowledge transfer material · Review functional requirements, analysis, and design documents and provide feedback. · Assist customer support with technical problems and questions. · Assist and mentor other development staff. · Perform special assignments. · Participate in architecture and code reviews. · Lead or participate in other projects or duties. · Occasional travel required. (Up to 5%) · Participate in other projects or duties. SUPERVISORY RESPONSIBILITIES: · N/A KNOWLEDGE, SKILLS, AND ABILITIES: * * * * * * * · Ability to network with key contacts outside own area of expertise. * * · Must possess strong interpersonal, organizational, presentation and facilitation skills. · Must be results oriented and customer focused. · Must possess good organizational skills. * * EDUCATION AND TRAINING: · Bachelor’s degree in computer science, Information Systems, or related field; or equivalent combination of education/experience. Master’s degree is a plus. · 7 years or more of extensive experience developing mission critical and low latency solutions. · At least 4 years of experience with developing and debugging distributed systems and data pipelines in the cloud. AWS is a must. · Extensive experience with SQL cloud databases like Snowflake (a must-have experience), and MS SQLServer. Experience with NoSQL databases like AWS DynamoDB and Azure Cosmos is a plus. · Good understanding of data modeling, ETL, data curation, and big data performance tuning. · Experience with Business Intelligence tools is a plus. · Experience working with AWS and/or Azure DevOps and extensive debugging experience. · Ability to code in one or more languages like Python, Java, Scala. · An understanding of unit testing, test driven development, functional testing, and performance · Knowledge of at least one shell scripting language. Other Qualifications The Winning Way behaviors that all Vertex employees need to meet the expectations of each other, our customers, and our partners. • Communicate with Clarity - Be clear, concise, and actionable. Be relentlessly constructive. Seek and provide meaningful feedback. • Act with Urgency - Adopt an agile mentality - frequent iterations, improved speed, resilience. 80/20 rule - better is the enemy of done. Don’t spend hours when minutes are enough. • Work with Purpose - Exhibit a ""We Can"" mindset. Results outweigh effort. Everyone understands how their role contributes. Set aside personal objectives for team results. • Drive to Decision - Cut the swirl with defined deadlines and decision points. Be clear on individual accountability and decision authority. Guided by a commitment to and accountability for customer outcomes. • Own the Outcome - Defined milestones, commitments and intended results. Assess your work in context, if you’re unsure, ask. Demonstrate unwavering support for decisions. COMMENTS: The above statements are intended to describe the general nature and level of work being performed by individuals in this position. Other functions may be assigned, and management retains the right to add or change the duties at any time.",,1001 to 5000 Employees,Company - Public,Information Technology,Software Development,1978.0,$100 to $500 million (USD)
"Delaware North
3.6",3.6,"Buffalo, NY",Data Engineer,"The Opportunity
Delaware North Global Headquarters is hiring a Data Engineer to join our Information Technology team in Buffalo, New York. As a Data Engineer, you will work with programming languages, frameworks, databases, front-end tools, back-end tools, and applications connected via APIs to collect raw data and transform the data into canonical models. The Data Engineering team is tasked with harmonizing and enhancing the data to provide trusted datasets to our consumers. The work our data manager team does forms the foundation of company initiatives to help automate business processes and gather insights, to help the company make more informed decisions.
Minimum - Anticipated Maximum Salary: $70700 - $93700 / year
The advertised pay range represents what we believe at the time of this job posting, that we would be willing to pay for this position. Only in special circumstances, where a candidate has education, training, or experience that far exceeds the requirements for the position, would we consider paying higher than the stated range. Information on our comprehensive benefits package can be found at https://careers.delawarenorth.com/whatweoffer.

At Delaware North, we care about our team member’s personal and professional journeys. These are just some of the benefits we offer:
Health, dental, and vision insurance
401(k) with company match
Performance bonuses
Paid vacation days and holidays
Paid parental bonding leave
Tuition and/or professional certification reimbursement
Generous friends-and-family discounts at many of our hotels and resorts
Responsibilities
Utilize technology stacks such as Apigee, Python, Django, Apache Airflow, MongoDB, PostgreSQL, and Amazon Web Services such as Lambda, EBS, S3, SQS, ECS, RDS, EC2 and Redshift to build datasets.
Design and implement project-based solutions.
Implement data platform improvements and new features.
Assist the support team with the resolution of data platform bug fixes.
Interface with clients, vendors, and internal users of the data platform on understanding the data.
Author documentation for standard operating procedures and knowledge base articles.
Develop integration tests to validate solutions.
Qualifications
Bachelor’s degree or equivalent from an accredited college or university in Computer Science, Information Systems, or similar STEM field preferred.
Minimum of 2 years of experience developing data pipeline ETL processes.
Extensive experience following End to End Agile Development Lifecycle and writing in SQL and Query.
Data persistence methods such as NoSQL and RDBMS, data structures and formats such as JSON, XML, and parquet.
Cloud computing experience as it relates to event-based serverless architecture, AWS preferred.
Extensive experience with handling large data sets, high performance computing, building high performance solutions and data integration projects.
Technical specification and use case documentation, such as UML, Domain and Entity-Relationship Modeling, Business Process Notation.
Must be legally authorized to work in the US without sponsorship.
Who We Are
At Delaware North, you’ll love where you work, who you work with, and how your day unfolds. Whether it’s in sporting venues, casinos, airports, national parks, iconic hotels, or premier restaurants, there’s no telling where your career can ultimately take you. We empower you to do great work in a company with 100 years of success, stability and growth. If you have drive and enjoy the thrill of making things happen - share our vision and grow with us.
Delaware North Companies, Incorporated and its subsidiaries consider applicants for all positions without regard to race, color, religion, creed, gender, national origin, age, disability, marital or veteran status, sexual orientation, or any other legally protected status. Delaware North is an equal opportunity employer.","$82,200 /yr (est.)",10000+ Employees,Company - Private,Restaurants & Food Service,Catering & Food Service Contractors,1915.0,$1 to $5 billion (USD)
"Equifax
3.6",3.6,"Alpharetta, GA",Data Reliability Engineer - P3,"Equifax is where you can power your possible. If you want to achieve your true potential, chart new paths, develop new skills, collaborate with bright minds, and make a meaningful impact, we want to hear from you.
The Data Reliability Engineer position is a high-performing role in support of data reliability and quality functions for various domains within USIS’s broader data library.
This individual will work within a team of data reliability engineers and data stewards, who serve as experts for the data sources, to ensure the appropriate quality and internal and external support are in accordance with our service level commitments. Candidates should be courageous, tenacious, creative, optimistic and curious, with an appetite for growth in business-minded leadership.
This role is located in our Alpharetta, Georgia office. We have a hybrid work model which requires being in the office 3 days/week.
What you’ll do
Domain Expertise: Understand the lifecycle of the Equifax data sources across all stages from ingestion to production
Drive Data Reliability and Quality across our Data Pipeline
Quality Monitoring and Incident Management: Own threshold-based reports that efficiently and reliably identify areas for further investigation and improvement. Act on the findings of these reports in a timely manner, prioritizing effort based on business impact.
Continuous Improvement: Work with an evergreen roadmap of initiatives, powered by work in Quality Monitoring and Customer Engagement, helping to drive these monitors and incident management process to an automated approach for each data domain/index.
Be an ambassador for our data assets and encourage data awareness which can assist with new opportunities and points of synergy.
What experience you need
5+ years of relevant experience with industry-leading lenders and/or data companies, with a working knowledge of data structure, data handling, the credit lifecycle and data management
5+ years of analysis experience in credit risk, telecommunications, financial services, marketing, fraud, or insurance
3+ years of experience with data handling/quality tools and Big Data ecosystems i.e. SQL, Google Cloud Platform, BigQuery
3+ years of experience with data, business insights and data & data processing audit analysis which identify and drive ongoing excellence in data quality practices
What could set you apart
A strong sense of governance protocols, the need to respect highly sensitive data and proactive attention to regulatory, security and compliance driven policies
Strong aptitude and proven ability to develop data-driven solutions to meet business objectives
Demonstrates leadership with cross-functional efforts; develops effective working relationships with peers, managers, and senior management within and across organizational lines
The ability to manage multiple high-impact projects at the same time
Creativity and drive, coupled with a desire to grow in thought leadership, initiative and strategic thinking
Experience with Collibra, or similar metadata management tool
Experience with Spotfire, or similar data visualization tool
Experience with SQL, Google BigQuery, or equivalent database queries and views
We offer comprehensive compensation and healthcare packages, 401k matching, paid time off, and organizational growth potential through our online learning platform with guided career tracks.
If this sounds like somewhere you want to work, don’t delay, apply today - we’re looking for you!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.","$85,081 /yr (est.)",10000+ Employees,Company - Public,Management & Consulting,Research & Development,1899.0,$1 to $5 billion (USD)
"Veolia
3.8",3.8,"Paramus, NJ",Senior Data Engineer,"Company Description Veolia Group aims to be the benchmark company for ecological transformation. With nearly 220,000 employees worldwide, the Group designs and provides game-changing solutions that are both useful and practical for water, waste and energy management. Through its three complementary business activities, Veolia helps to develop access to resources, preserve available resources and replenish them. In 2021, the Veolia group provided 79 million inhabitants with drinking water and 61 million with sanitation, produced nearly 48 million megawatt hours and recovered 48 million tonnes of waste. Veolia Environnement (Paris Euronext: VIE) achieved consolidated revenue of 28,508 billion euros in 2021. www.veolia.com
Job Description
Develop and operate data management tools, monitoring data flows, data quality, data cleansing and data processing
Create and document logical data integration strategies for data flows between disparate systems and the enterprise data warehouse/data lakes
Collaborate with different stakeholders (engineers, data stewards) to collect required data from internal and external systems
Work in an Agile environment that focuses on collaboration and teamwor
Improve and extend existing data infrastructure services
Monitor production job schedule and correct job failures in a timely manner

Qualifications
MS degree in Computer Science or computer related field from an accredited institution.
5+ years hands proven experience as a Data Engineer or similar role.
5+ years of strong experience building, running and maintaining datalake(s) and warehouse(s) in a cloud environment.
More than 4 years of experience developing with Python.
4+ years performing with production environments in a DevOps culture managing code composed of multi-developer teams, following industry best practices.
4+ years SQL development experience.
Experience with data modeling
4+ years bash scripting experience.
Strong experience with Git, CI/CD (preferably GitLab) and Docker.
Experience deploying and running services in Cloud Big Data platforms such as BigQuery and Snowflake.
Strong experience with GCP services.
Experience designing and building data pipelines using tools like Apache Beam, CDAP (Data Fusion) or other ETLs.
Knowledge with CDC design patterns and their challenges.
Experience with DAG workflows orchestration such as Apache Airflow.
Experience with NoSQL databases is a plus (i.e Firestore, MongoDB).
Experience designing and developing APIs is a plus (i.e using FastAPI, Flask).

(Nice to have) Google Cloud Data Engineer
Abilities:
Being able to work in a large company with different stakeholders.
Embrace mentorship through design sessions, code reviews, and community building.
Take ownership and support solutions you develop.
Value collaboration with other members of the team.
Have a product mindset.
Good communication.

Additional Information

A subsidiary of Veolia group, Veolia North America (VNA) offers a full spectrum of water, waste and energy management services, including water and wastewater treatment, commercial and hazardous waste collection and disposal, energy consulting and resource recovery. VNA helps commercial, industrial, healthcare, higher education and municipality customers throughout North America. Headquartered in Boston, Mass., Veolia North America has more than 10,000 employees working at more than 350 locations across the continent. www.veolianorthamerica.com As an inclusive company, Veolia is committed to diversity and gives equal consideration to all applications, without discrimination. We are an Equal Opportunity Employer! All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. As an inclusive company, Veolia is committed to diversity and gives equal consideration to all applications, without discrimination. We are an Equal Opportunity Employer! All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.
Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. The Company reserves the right to modify this information at any time, subject to applicable law.","$119,169 /yr (est.)",10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1853.0,$1 to $5 billion (USD)
"Crisis Prevention Institute
3.0",3.0,"Milwaukee, WI",Senior Data Engineer,"Our Story:
Crisis Prevention Institute Inc. is the worldwide leader in evidence-based de-escalation and crisis prevention training, and dementia care services. Since 1980, we’ve helped train more than 15 million people within service-oriented industries including education, healthcare, behavioral health, long-term care, human services, security, corrections, corporate, and retail.

At CPI, we are dedicated to changing behaviors and reducing conflict for the Care, Welfare, Safety, and SecuritySM of everyone. We believe in the power of empathy, compassion, and meaningful connections. We believe personal safety and security are the antidotes to fear and anxiety. It’s a philosophy that is central to everything we do, and traces back to our beginning. It is what defines and differentiates us, and informs our core beliefs.
As a member of the team, you can expect to:
Make a difference through your work – You’ll be proud to tell your family and friends about what you do.
Gain significant career experience only obtained within a fast-growing organization – Entry-level roles through executive leadership.
Feel fulfilled and have fun – We work hard but make the time to build meaningful relationships and celebrate the wins.

The Role:
The Senior Data Engineer will focus on quality engineering best practices to meet and exceed internal and external client expectations. In this position, you will analyze, design, develop, test and document solutions supporting data integration, performance tuning, and data modeling to drive organization growth objectives. The Senior Data Engineer will define the standards for data architecture, platform architecture, and data quality and governance. This role is responsible for ensuring that the function is aligned with the overall CPI organization and continuously works to meet critical service levels in access, delivery and security.
What You Get To Do Everyday:
Co-architect CPI’s next gen cloud data analytics platform.
Increase operating efficiency and adapt to new requirements.
Monitor and maintain the health of solutions generated.
Support and enhance our data-ops practices.
Provide task breakdowns, identify dependencies, and provide effort estimates.
Model data warehouse entities in Erwin.
Build data transformation pipelines with Data Build Tools (DBT).
Evaluate the latest technology trends and develop proof-of-concept prototypes that align with CPI opportunities.
Develop positive relationships with clients, stakeholders, and internal teams.
Understand business goals, drivers, context, and processes to suggest technology solutions that improve the organization.
Work collaboratively on creative solutions with engineers, product managers, and analysts in an agile like environment.
Perform, design, and code reviews.
Perform other position-related duties as assigned.
You Need to Have:
Bachelor’s degree in computer engineering, computer science, data science, or related field
Two years or more experience designing and implementing data warehouses in Snowflake
Eight years or more experience working with data modeling, architecture and engineering
Experience with all core software development activities, including requirements gathering, design, construction, and testing
Experience performing data transformation using DBT
Experience working with DQ products such as Monte Carlo, BigEye, or Great Expectations
Experience with Azure DevOps (Repos, Pipelines, Boards, Wiki, Test Plans)
Experience with formal software development methodologies including Software Development Life Cycle (SDLC), Agile or SCRUM
Experience building high-performance and highly reliable data pipelines
Experience Knowledge of data warehouse design patterns (star schema, data vault)
Experience building dashboards with business integrations tools
Knowledge of DataOps
with cloud-based compute, storage, integration and security patterns
Knowledge and understanding of RESTful APIs
Knowledge of current data engineering trends, best practices, and standards
Knowledge of SQL and Python
Ability to work in a collaborative environment
Ability to facilitate evaluation of technologies and achieve consensus on technical standards and solutions among a diverse group of information technology professionals
Ability to work in an organization driven by continuous improvement or with equivalent focus on process improvement
Ability to manage multiple, competing priorities and attain the best possible outcomes for the organization
Excellent verbal and written communication and effective listening skills
We'd Love to See:
Experience in delivering an end-to-end data analytics platform using modern data stack components
Experience with AI/ML
SnowPro Advanced Certification
DBT Analytical Engineer Certification
What We Offer:
Competitive salary
Comprehensive benefits package
401k
PTO
Health & Wellness Days
Paid Volunteer Time Off
Continuing education and training
Hybrid work schedule
???????Crisis Prevention Institute is an Equal Opportunity Employer that does not discriminate against any applicant or employee on the basis of age, race, color, ethnicity, national origin, citizenship, religion, creed, sex, sexual orientation, gender, gender identity, or expression (including against any individual that is transitioning, has transitioned, or is perceived to be transitioning), marital status or civil partnership/union status, physical or mental disability, medical condition, pregnancy, childbirth, genetic information, military and veteran status, or any other basis prohibited by applicable federal, state, or local law. The Company will consider for employment qualified applicants with criminal histories in a manner consistent with local and federal requirements. Our management team is dedicated to this policy with respect to recruitment, hiring, placement, promotion, transfer, training, compensation, benefits, employee activities, and general treatment during employment.","$95,913 /yr (est.)",201 to 500 Employees,Company - Private,Education,Education & Training Services,1980.0,$25 to $100 million (USD)
"Skillable
4.7",4.7,Remote,Senior Data Engineer,"Job Type
Full-time
Description


Skillable is a 100% remote and virtual tech company that’s modernizing the world of training. Come share your professional magic with highly talented, drive and fun colleagues who believe in the power of “skilling.” Experience what a true team focused on doing the right thing feels like!

Our people and talent are what make us great and fun! We work together to create amazing solutions and experiences for our customers and their clients. We utilize our employees’ personal strengths to help our company grow and ensure our team is living their best, authentic life. We don’t just share our appreciation for our team members once a year with a branded mug—it’s shared on a daily basis. Our remote work environment blends the demands of work and life without the added pressure of commuting or feeling guilty about leaving early to visit the dentist.

Come work with us and learn what teamwork and integrity blended with an emphasis on well-being and balance can do for your career!

The Senior Data Engineer is a highly skilled data professional responsible for executing and guiding on the strategic development and maintenance of our data resources. Responsible for a broad range technical and detailed initiatives including being a thought partner to leadership in the architecture and creation of new data products to working with security and infrastructure resources to ensure the health of databases and data pipelines. This role will work closely with cross-functional teams, including data science, software engineering and product management to build, improve and maintain our data infrastructure, and contribute to the overall success of the company. Partner with leadership to give insights to the team of data engineers and database administrators, ensuring project organization around databases and focusing on continuous stability, security, and performance of databases to contribute to the overall success of the company.
Requirements
Provide technical data expertise and guidance to support core business data systems and data-driven processes.
Serve as a key contributor in the design, development, and maintenance of data pipelines for real-time and batch processing with the support and partnership of other technical team members.
Design, develop, execute on the implementation and management of databases.
Build and optimize data models for efficient querying and analysis.
Monitor and troubleshoot data pipeline and database issues.
Design and implement database structures and ensure their stability, reliability, and performance.
Troubleshoot database performance and optimize as necessary.
Assist in planning database backups and maintenance, disaster recovery, and replication.
Continuously improve data infrastructure and processes.
Provide technical knowledge and experience in serving as a lead in the process of creating, delivering, and scaling “Data as a Service” projects or products and data-related features.
Collaborate with data scientists, engineers, and analysts to understand data needs and requirements.
Parter with outside teams, stakeholders and executives to understand business needs and requirements.
Build and develop key relationships across the enterprise to help build internal enthusiasm and momentum for project initiatives.
Regularly assist leadership and key Company stakeholders in the development and implementation of data governance policies to ensure data quality.
Stay current with new database technologies and assist in upgrading systems as necessary.
Regularly review current and new data technologies and industry trends and implement as appropriate.
Assist in guiding, influencing and developing a company culture of data-driven decision making.
Assist leadership in mentoring a growing team of data professionals.
Represent Skillable at industry events and conferences as required.
Support and promote the company values through positive interactions with both internal and external partners and customers on a regular basis.
Other strategic business initiatives or cross-functional project involvement as required.
Qualifications
Bachelor’s degree in related field (product management, marketing, business, finance, product development, project management, etc.) or equivalent work experience.
10+ years of relevant professional experience working intimately in data engineering or database administration.
Experience as a functional leader on key projects or teams of technical talent or providing guidance on cross-functional work groups or project teams successfully preferred, but not required.
Experience mentoring and coaching others within an assigned function or cross-functionally.
Proven track record of strong problem-solving and business analysis skills using data creatively.
Naturally inquisitive with a desire to solve problems and dig into detailed analysis.
Experience working cross-functionally and promoting collaborative partnerships to drive results.
Proven ability to communicate effectively to various audiences/levels including leadership through various mediums.
Ability to take complex data and problems and deconstruct it into a concise, impactful message(s).
Ability to present and convey material both formally and informally to all levels of the organization.
Thorough understanding (or willingness to learn expeditiously) of business operations and processes.
Provide a high-level of confidence, integrity, enthusiasm and professional presence.
Experience with real-time data systems, data warehousing, ETL technologies, data modeling, and data governance.
Experience with cloud computing platforms such as AWS, GCP, or Azure.
Strong knowledge of database management systems, especially SQL Server.
Experience with cloud-based databases, especially Azure SQL.
Experience managing high-availability and -uptime databases.
Strong programming skills in SQL.
Demonstrated ability to prioritize and manage workload and meet project deadlines.
Interest and ability in mentoring other team members as applicable.
Strong MS Office, web conferencing and internal communication software experience.
Detail oriented and organized.
What’s in it for You? Rewards and Perks

We believe in providing a suite of benefits that ensure our employees know we appreciate them as people first. Skillable wants to be a company that promotes physical, emotional and all around well-being through our benefit offerings! Subject to eligibility requirements, the Company offers comprehensive benefits including:
Fully remote with a monthly stipend to pay for office services and supplies
Medical (2 plan options), dental (2 plan options), vision, health savings account with generous employer contributions, healthcare spending accounts, dependent care spending accounts, EAP, group paid life insurance, group paid STD and LTD and voluntary life/AD&D insurance, accident and critical illness options.
401(k) with Company match, tuition reimbursement, healthy lifestyle reimbursements.
Open PTO, Paid holidays, bereavement leave, parental leave, caregiver leave and paid FMLA leave.
Friends and Family Friday to end our standard workweek at 2pm local time; Full company closure during the 4th of July holiday week.
Access to pet insurance; Access for employees and dependents to Skillable learning opportunities through our product and more!
Working Conditions

The job conditions for this position are in a remote home office setting, requiring a space that supports privacy and focus to attend to regular and frequent video and voice calls. Employees in this position use PC and phone on an on-going basis throughout the day. Travel is anticipated up to 10% of the time for critical business meetings and industry events.",,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2004.0,Unknown / Non-Applicable
"(ISC)2
3.3",3.3,Remote,Data Engineer,"Overview:
(ISC)² is an international nonprofit membership association focused on inspiring a safe and secure cyber world. (ISC)² offers a portfolio of credentials that are part of a holistic, pragmatic approach to security. Our association of candidates, associates, and members, nearly 330,000 strong, is made up of certified cyber, information, software, and infrastructure security professionals who are making a difference and helping to advance the industry. Our vision is supported by our commitment to educate and reach the general public through our charitable foundation – The Center for Cyber Safety and Education™. For more information on (ISC)², visit www.isc2.org, follow us on Twitter, or connect with us on Facebook and LinkedIn.

We are committed to an inclusive and equitable environment that values the unique perspectives and experiences of our entire workforce. We strive for a true sense of belonging for all our employees and to foster authenticity, trust, empowerment and connectedness that leads to everyone’s success. For more information, visit www.isc2.org/dei.
Position Summary:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities:
Implement Azure Data Services and tools to ingest , egress and transform data from multiple sources and create and maintain optimal data pipeline architecture.
Responsible for creating ETL pipeline with Azure Ecosysem like Azure Synapse ,Azure Data Factory.
Implement and support ETL related jobs to curate , transform and aggregate data to create models for end user consumption.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Azure from Sales Force ,Pearson Vue etc.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Maintain ownership of a given pipeline or domain and raise flags to leadership when appropriate regarding architectural concerns.
Demonstrates commitment to valuing diversity and contributing to an inclusive working and learning environment.
Miscellaneous duties as assigned.
Qualifications:
Bachelor’s degree in computer science or other equivalent degree
7+ years of experience in a Data Engineer role.
Experience with Cloud Data warehouses such as Azure , AWS and Google BigQuery
Experience with big data tools: Hadoop, Spark and Kafka.
Experience with Azure Dedicated and Azure Synapse
Experience with Analytics one or more of the following reporting tools; Tableau, PowerBI, Looker, Domo and Microstrategy
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++,etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Skills/Competencies:
Ability to demonstrate and support the 5 Company Core Values: Integrity, Excellence, Unity, Accountability, Agility
Ability to build an inclusive culture that encourages, supports and celebrates diversity; serve as a role model to promote DEI best practices.
Strong analytic skills related to working with unstructured datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Physical & Mental:
Up to 5% travel may be required.
Work normal business hours and extended hours when necessary.
Remain in a stationary position, often standing or sitting, for prolonged periods
Regular use of office equipment in a remote environment such as a computer/laptop and monitor computer screens
Dexterity of hands and fingers to operate a computer keyboard, mouse, and other computer components
#LI-remote
Equal Employment Opportunity Statement:
All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic as protected by applicable law.
Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.",,201 to 500 Employees,Nonprofit Organization,Management & Consulting,Membership Organizations,1989.0,$25 to $100 million (USD)
"Procore Technologies
4.4",4.4,Oregon,Staff Data Engineer,"Job Description

We’re looking for a Staff Data Engineer to join Procore’s Data Intelligence group. In this role, you’ll be a part of the data platform engineering team focused on building datamarts on the Procore Data Platform. The products and services that you build will support the data driven needs of many existing and forthcoming products of Procore.
As a Staff Data Engineer, you’ll partner with data engineers, platform engineers and product leaders to create and support Procore Risk Advisors Underwriting product portfolio. Use your analytical, data modeling, pipeline development skills to create and enhance the core data driven decision making at Procore. Procore Risk Advisors is a burgeoning and high value organization at Procore that is bringing technology driven experiences for quick and secure experiences in the Insurance industry. Backed by the wealth of data and industry leadership of Procore, PRA is breaking new ground in how the construction industry perceives Insurance.
This position reports into Senior Manager and will be based in remotely. We’re looking for someone to join us immediately.
What you’ll do:
Build the design and development of big data predictive analytics using object-oriented analysis, design and programming skills, and design patterns
Implement ETL workflows for data matching, data cleansing, data integration, and management
Maintain existing data pipelines and develop new data pipeline using big data technologies
Responsible for leading the effort of continuously improving reliability, scalability, and stability of the enterprise data platform
Contribute to and lead the continuous improvement of the software development framework and processes by collaborating with Quality Assurance engineers
Reproduce, troubleshoot and determine the root cause of production issues
Deliver observable, reliable, and secure software, embracing the “you build it, you run it” mentality, and focus on automation and GitOps
Participate in daily standups, team meetings, sprint planning, and demo/retrospectives while working cross-functionality with other teams to drive the innovation of our products
What we’re looking for:
BS degree in Computer Science, a similar technical field of study, or equivalent practical experience; MS or Ph.D. degree in Computer Science or a related field is preferred
8+ years of experience in a Data Engineering position
Strong expertise with 2+ years of experience building enterprise techniques for large scale distributed system design and data processing including:
Building streaming data pipelines using Kafka, Spark, or Flink
Building and maintain data warehouses in support of BI tools
Building data pipeline framework for data workflow to process large data sets and Real-Time & Batch Data Pipeline development
Experience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metric providers ranging from advertising, web analytics, and consumer devices
Desire to be actively hands-on with code using Java and Python (70-90%) along with willingness and passion for mentoring junior engineers and performing code reviews
Possess strong knowledge or familiarity with Apache Beam or AWS managed services for data (Glue, Athena, Data Pipeline, Flink, Spark) and Snowflake
Develop data catalogs and data cleanliness to ensure clarity and correctness of key business metrics

Additional Information

Base Pay Range $169,280-$232,760. Eligible for Bonus Incentive Compensation. Eligible for Equity Compensation. Procore is committed to offering competitive, fair, and commensurate compensation, and has provided an estimated pay range for this role. Actual compensation will be based on a candidate’s job-related skills, experience, education or training, and location.
Perks & Benefits
At Procore, we invest in our employees and provide a full range of benefits and perks to help you grow and thrive. From generous paid time off and healthcare coverage to career enrichment and development programs, learn more details about what we offer and how we empower you to be your best.
About Us
Procore Technologies is building the software that builds the world. We provide cloud-based construction management software that helps clients more efficiently build skyscrapers, hospitals, retail centers, airports, housing complexes, and more. At Procore, we have worked hard to create and maintain a culture where you can own your work and are encouraged and given resources to try new ideas. Check us out on Glassdoor to see what others are saying about working at Procore.
We are an equal-opportunity employer and welcome builders of all backgrounds. We thrive in a diverse, dynamic, and inclusive environment. We do not tolerate discrimination against employees on the basis of age, color, disability, gender, gender identity or expression, marital status, national origin, political affiliation, race, religion, sexual orientation, veteran status, or any other classification protected by law.
If you'd like to stay in touch and be the first to hear about new roles at Procore, join our Talent Community.","$201,020 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2002.0,Unknown / Non-Applicable
"University of California San Francisco
4.0",4.0,California,Software and Data Engineer,"The software and data engineer role includes the design, build, configuration, and support of research projects within UCSF’s APeX Enabled Research (AER) team. Most projects will be in partnership with other UCSF technical teams and involve highly customized research solutions. Communication skills and inventive technical solutioning are crucial.

The AER team provides a large array of services to the UCSF Research community, including project consultation, grant support, budget estimations, and project implementation and support. Project examples include:
Development of EHR-based interventions via clinical trials embedded within healthcare delivery systems to generate scientific evidence while delivering healthcare.
Enabling UCSF researchers with algorithms, digital tools, and/or clinical interventions with strong evidence of feasibility and acceptability.
Develop technical approaches and budgets in order to implement these tools within the electronic medical record.
Supporting the development of scalable, low-cost infrastructure to enable ongoing research.

Specifically, the software and data engineer will develop, implement, and maintain infrastructure and applications that support research informatics priorities and research projects priorities The largest piece of infrastructure for which they will be responsible will be the Health Informatics Platform for Advanced Computing (HIPAC), cloud infrastructure which supports deployment and maintenance of artificial intelligence in the health system. The engineer will work closely with data scientists and health informatics experts in the ongoing design and optimization of this infrastructure, in addition to other complex software applications.

Competitive applicants for this position are software engineers who have experience writing and maintaining production-ready and scalable applications. Candidates are ideally proficient in Python and SQL, have working experience with common DevOps and CI/CD tools such as docker, and have experience developing cloud-based applications.
To see the salary range for this position (we recommend that you make a note of the job code and use that to look up): TCS Non-Academic Titles Search (ucop.edu)
Please note: The compensation ranges listed online for roles not covered by a bargaining unit agreement are very wide, however a job offer will typically fall in the range of 80% - 120% of the established mid-point. An offer will take into consideration the experience of the final candidate AND the current salary level of individuals working at UCSF in a similar role.
For roles covered by a bargaining unit agreement, there will be specific rules about where a new hire would be placed on the range.
To learn more about the benefits of working at UCSF, including total compensation, please visit: https://ucnet.universityofcalifornia.edu/compensation-and-benefits/index.html
Department Description
The University of California, San Francisco (UCSF) Department of Information Technology Academic Research Systems (ARS) group is chartered to provide data services and infrastructure that support the UCSF Research Community’s computing and analytic requirements through centralized informatics services in the areas of Data, Tools, Secure Compute Environments, and Consulting Services.
Required Qualifications
Bachelor's degree in Computer Science, Computer Engineering, or related area and/or equivalent experience/training.
Demonstrated advanced knowledge of full software development lifecycle
Advanced experience with Python; ability to write clean, efficient, and production-level Python code
Advanced experience with SQL (e.g., SQLServer, PostgreSQL)
Experience working with DevOps and CI/CD pipeline toolsets such as Docker, Jenkins, GitHub, etc.
Demonstrated experience in developing complex, automated testing
Advanced experience with cloud-based architecture in platforms such as AWS, GCP, Azure, etc.
Demonstrated effective communication and interpersonal skills
Demonstrated ability to communicate technical information to technical and non-technical personnel at various levels in the organization
Self-motivated and works independently and as part of a team. Able to learn effectively and meet deadlines
Demonstrated broad problem-solving skills
Demonstrated ability to interface with management on a regular basis
Strong interest in working with healthcare data and understanding the challenges that face complex healthcare delivery systems
Ability to work in a highly matrixed organization, reporting to multiple teams
Preferred Qualifications
Master’s degree or Ph.D. in Computer Science, Computer Engineering, or related area and/or equivalent experience/training.
Cloud development certifications such as AWS Developer – Associate
Epic Clarity or Clinical Data Model
Demonstrated experience with data modeling, data warehousing, and building ETL pipelines
Familiar with data analysis and machine learning tools such as Jupyter, Pandas, scikit-learn, Numpy/Scipy, TensorFlow, etc.
Familiar with data visualization tools (e.g., Tableau).
Experience with the Epic Clarity Data structures and data
About UCSF
The University of California, San Francisco (UCSF) is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It is the only campus in the 10-campus UC system dedicated exclusively to the health sciences. We bring together the world’s leading experts in nearly every area of health. We are home to five Nobel laureates who have advanced the understanding of cancer, neurodegenerative diseases, aging and stem cells.
Pride Values
UCSF is a diverse community made of people with many skills and talents. We seek candidates whose work experience or community service has prepared them to contribute to our commitment to professionalism, respect, integrity, diversity and excellence – also known as our PRIDE values.

In addition to our PRIDE values, UCSF is committed to equity – both in how we deliver care as well as our workforce. We are committed to building a broadly diverse community, nurturing a culture that is welcoming and supportive, and engaging diverse ideas for the provision of culturally competent education, discovery, and patient care. Additional information about UCSF is available at diversity.ucsf.edu

Join us to find a rewarding career contributing to improving healthcare worldwide.
Equal Employment Opportunity
The University of California San Francisco is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Organization
Campus
Job Code and Payroll Title
000652 APPLICATIONS PROGR 4
Job Category
Clinical Systems / IT Professionals
Bargaining Unit
99 - Policy-Covered (No Bargaining Unit)
Employee Class
Career
Percentage
100%
Location
Mission Center Building (SF)
Shift
Days
Shift Length
8 Hours
Additional Shift Details
Mon - Fri 8:00 to 5:00",,10000+ Employees,College / University,Education,Colleges & Universities,1864.0,$25 to $100 million (USD)
"ArchWell Health
4.0",4.0,Remote,IT - Data Engineer,"ArchWell Health is a new, innovative healthcare provider devoted to improving the lives of our senior members. We deliver best-in-class care at comfortable, accessible neighborhood clinics where seniors can feel at home and become part of a vibrant, wellness-focused community. Our members experience greater continuity of care, as well as the comfort of knowing they will be treated with respect by people who genuinely care about them, their families, and their communities.

ArchWell Health requires all new hires to provide proof that they are fully vaccinated from COVID-19, or represent that they will be fully vaccinated within 30 days of their start date.

Duties/Responsibilities:
Build data integrations from internal and external sources to centralize data into a Data Warehouse environment.
Monitor data integration operations, data quality, troubleshoot, and resolve problems.
Profile data sources and map to target table formats.
Develop and monitor data quality processes and address problems.
Develop, unit test and system test integration components.
Create support documentation describing the functionality of the integrations.
Participating in technical design & requirements gathering meetings.
Participate in planning and implementing data integration and data migration activities.
Perform QA tests to ensure data integrity and quality.
Research data issues between source systems and the data warehouse.
Required Skills/Experience:
Bachelor’s degree required; Master's degree (in data science, computer science or MIS, mathematics, engineering, or related field) preferred.
5+ years of prior experience in Data Management / ETL / ELT / Data Warehousing
Experience in writing Data Quality routines for cleansing of data and capturing confidence score
Experience with master data management
Strong knowledge of Structured Query Language (SQL) and Transact-SQL (T-SQL)
Experience using scripting languages such as JavaScript or Python
Experience Healthcare data models, datasets, and source systems (e.g. EHR, claims, labs, etc.)
Experience with healthcare reference data (ICD, CPT etc.)
Experience with agile delivery methodologies
Data Modeling experience preferred.
Strong organizational, administrative, and analytical skills required.
Experience managing and working in cloud environments such as Amazon Web Services or Azure
Knowledge of HIPAA; ability to implement systems and processes in accordance with regulations
Excellent interpersonal communication skills, both written and verbal
ArchWell Health is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to their race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other protected classification.",,201 to 500 Employees,Company - Public,,,,Unknown / Non-Applicable
"McDonald's Corporation
3.5",3.5,"Chicago, IL","Data Engineer, Global Data, CRM","Company Description

McDonald’s evolving Accelerating the Arches growth strategy puts our customers and people first, and leverages our competitive advantages to strengthen our brand. We are recognized on lists like Fortune’s Most Admired Companies and Fast Company’s Most Innovative Companies.
Doubling Down on the 4Ds (Delivery, Digital, Drive Thru, and Development)
Our growth pillars emphasize the important role technology plays as the leading, global omni-channel restaurant brand. Technology enables the organization through digital technology, and improving the customer, crew and employee experience each and every day.
Global Technology forging the way
Leading the digitization of our business is the Technology organization made up of intrapreneurs who build industry defining tech using the latest innovations and platforms, like AI and edge computing to deliver on the next set of cutting-edge opportunities for the business. At McDonald’s you get to solve technology innovation challenges at an incredible scale, and work across global teams who are always hungry for a challenge. This provides access to exciting career paths for technologists. It’s bonus points when you get to see your family and friends use the tech you build at their favorite McD restaurant.

Job Description

We are seeking a highly motivated individual for the Data Engineering Manager for the Marketer Customer Data Platform team on Global Data and Architecture. This role will manage, develop, and lead the execution and implementation cloud-based data delivery systems for the MCD Customer Data Platform. This role will specialize in leading and managing the engineering teams in both onshore and offshore locations, as well as working with the relevant product management teams on overall product direction and implementation of the product vision.
What you will do:
Lead and manage the day-to-day operations for the CRM Data Engineering team with both, onshore and offshore, resources to with a focus on CDP/ CRM customer data technologies
Independently drive efforts in marketing technology and automation, collaborating with key stakeholders and business units. Must be able to find marketing solutions alongside building a long-term strategic vision of a marketing ecosystem across a highly technical, complex data stack
Participate in activities of the data engineering team on the solution, design, architect, implementation, support, and monitoring of CRM & CDP related data products
Develop and implement monitoring and remediation structure and applications for critical data flows
Continuously develop and deploy solutions to streamline and enhance the processes, logic, and tactics around customer cohort identification, and targeting via corporate CRM, marketing, and advertising systems. Additionally, develop the overall best practice framework for use by
Gather business and functional requirements and translate into robust, scalable, and user-friendly solutions

Qualifications
Bachelor’s degree in Information Technology or related technical fields such as computer science, marketing tech, mathematics, engineering, etc.
Minimum 2-5 years of relevant work experience in data analytics, engineering, computer science, mathematics or similar logic based fields
Minimum 2 years hands on experience with complex marketing automation tech stacks, leveraging CDP tools such as mParticle, Segment, and Tealium
Strong analytics skills, root cause analysis experiences on internal and external data and processes to answer business questions and improve application performance
Comfortable partnering and working with a wide range of stakeholders at various levels of management and leadership
Ability to focus on details as well as big picture
Competence and comfort focusing on specific detail engineering questions, as well as “big picture” questions
Ability to leverage critical thinking, problem-solving, and analytical skills to improve user experience, adoption, and outcomes
Curious, task-oriented self-starter with a strong bias for action; Proven track record of “getting things done”
Self-motivated with ability to set priorities and mentor others in a performance driven environment.
Passion for technology and innovation & uncompromising attention to detail & passion for problem solving
Desired Skills

Extensive experience working with Agile methodologies and expertise in production deployment processes and continuous deployment best practices.
1-2 years of hands-on experience with Braze and/or similar market automation delivery platforms
1-2 years of hands on experience with marketing automation and technology experience, understanding key components of marketing technology stacks, including CDP tools such as mParticle, Tealium, or Segment
Experience with clickstream tracking architecture implementations, and knowledge of Adobe Analytics and/or Google Analytics
Demonstrated ability to problem solve in complex technology solutioning
Experience developing Confluence pages and working with Jira
Demonstrable knowledge of multiple programming languages leveraged in martech/adtech stacks – javascript, ruby (liquid logic), html/css, python etc.

Additional Information

McDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.com
McDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Nothing in this job posting or description should be construed as an offer or guarantee of employment.","$104,499 /yr (est.)",10000+ Employees,Company - Public,Restaurants & Food Service,Restaurants & Cafes,1955.0,$10+ billion (USD)
"aventiv
3.5",3.5,"Carrollton, TX",Data Center Implementation Engineer,"Welcome to Aventiv! Please watch this brief video to find out if this is the place you want to be!
https://vimeo.com/391578629/5ba31cc5e9

Job Purpose:
Responsible for Engineering and installation of all hardware systems into the enterprise Data Center environments. Spares management, logistics management, vendor management, configuration management and hardware asset tracking will also be key deliverables of this assignment.
THIS IS A HYBRID POSITION WITH APPROXIMATELY 10% TRAVEL REQUIRED.

Essential Duties
Prepare all change controls related to hardware Engineering and installations
Work with various departments to compile requirements and update all Data Center Engineering.
Perform all Installations for IT hardware
Provide Spares management tracking of the Spares hubs kept to maintain component failures
Provide details to create hardware project budgets.
Perform or manage vendors as required to perform periodic preventative maintenance.
Provide all logistics support for all material deliveries to and between IT Data Center environments.
Perform other duties as assigned

Knowledge, Skills, and Abilities
Highly motivated
Good organizational skills
Great interpersonal and communication skills
Excellent analytical and reporting skills
Solid IT Data Center background
Proficient at using MS Visio, Excel, and Word
Knowledgeable and trouble ticketing (Heat) and Environmental monitoring Systems (Orion)

Minimum Qualifications
HS Diploma or GED
Minimum 5 years’ experience in Data Center Engineering and implementation
Experience with AC and DC electrical systems, Data Center Rack, flooring and cable management systems, Camera, environmental and infrastructure monitoring systems used within Data Centers, and Safety practices for Data Center implementations
10% travel required

Preferred Qualifications
Bachelor’s degree in Electrical Engineering, Mechanical, or Business Administration or equivalent field of study
Network Engineering experience a plus
System administration (Microsoft or Linux) experience a plus
Project Management experience or certification is plus

Physical Demands
Standing, sitting, walking, speaking, listening, bending, reaching, pushing, pulling, lifting, grasping and manipulating tools, typing, using peripheral computer tools.
May be required to lift up to 50 pounds.

Salary and Benefits:
At Aventiv, our salary and benefits are designed to fit you as a whole person. We offer a salary range based on experience and qualifications to ensure your unique contributions are met with our most competitive offer.
$83,500-$95,200 /year
Health Insurance
401(k)
Disability
Life Insurance
Paid Time Off
Voluntary Benefits

Aventiv Privacy Policy:
www.aventiv.com/privacy

Equal Employment Policy:
Aventiv is proud to be an equal opportunity employer. All decisions regarding recruiting, hiring, promotion, assignment, training, termination and other terms and conditions of employment will be made without regard to race, color, national origin, biological sex, sexual orientation, gender identity, gender expression, gender presentation, religion, age, pregnancy, disability, work-related injury, veteran status, genetic information, marital status, or any other factor that the law protects from employment discrimination. We do not discriminate based on genetic information in accordance with the Genetic Information Nondiscrimination Act.","$89,350 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004.0,$500 million to $1 billion (USD)
"Certec Consulting
5.0",5.0,"Durham, NC",Oracle PL/SQL Data Engineer 1189,"Title: Oracle PL/SQL Data Engineer 1189
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B, Hire Friendly ) ONLY NO C2C
50-55/Hour W2



Location: hybrid 5 days onsite/month. Locations are Boston MA, Durham NC, Merrimack NH or Westlake TX
Duration: 9+ months, open ended

What does your team do?
This position is for equity team and will soon need another req for fixed income. Could place this candidate on either team. They take care of all things data within Asset Management, data transformation, data quality, building API.
Primarily responsible for the development of large scale data efforts tied to the cloud such as moving data to new cloud based solutions and building data lakes etc. To accomplish this the resource uses AWS, Python, Snowflake, and other data driven technologies
What are the top must have skills?
6-10 yrs
Oracle pl/sql sql is primary skill, AWS,
Go to our Website Job listing here: Job Listing 1189
Please download and complete this Matrix prior to submission.
PM 1189
Then
PLEASE USE EASY ""APPLY BUTTON"" (not just apply button) TO SUBMIT RESUME AND SKILLS MATRIX
Job Listing 1189
The send an email to the listers email address with just candidate name and job number. NO need to attached resume or anything else.
Thanks,
Jay Kernes
Certec Consulting, inc
Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$85,804 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,,Unknown / Non-Applicable
"BOEING
3.9",3.9,"Everett, WA",Product Data Mgmt Engineer,"At Boeing, we innovate and collaborate to make the world a better place. From the seabed to outer space, you can contribute to work that matters with a company where diversity, equity and inclusion are shared values. We’re committed to fostering an environment for every teammate that’s welcoming, respectful and inclusive, with great opportunity for professional growth. Find your future with us.
This is a position within the Airplane Level Engineering Integration (ALEI) function, specifically the Visualization Center of Excellence team (ViCE). This role is critical in assisting with design reviews for all BCA Programs to support first pass quality and verify the product is integrated, interference free, is producible, maintainable, serviceable and safe.
This position must meet Export Control compliance requirements, therefore a “US Person” as defined by 22 C.F.R. § 120.15 is required. “US Person” includes US Citizen, lawful permanent resident, refugee, or asylee.
This position is an onsite position. The selected candidate will be required to perform work onsite in Everett, WA.
Position Responsibilities:
Collaborates with teams in the development, analysis, management and compliance verification of process and product baselines of complex products. Defines, plans, coordinates and conducts product and subsystem level technical design reviews and audits for new and derivative products.
Analyzes complex product trades and/or changes and develops technically complete change proposals.
Contributes to the development and implementation of Configuration and Data Management standards, processes and tools.
Defines and allocates Configuration and Data Management requirements for product hardware, software and engineering design data systems throughout the product lifecycle.
Coordinates the integration of product elements and analyzes & resolves issues with engineering product structure.
Develops, integrates and implements engineering technical program plans including impacts, risks and incorporation of lessons learned spanning multiple engineering functions.
Works under general direction.
Additional job description:
Support Enterprise Level PI process and tools development to support the Enterprise Model Based Engineering (MBE) and Digital Transformation Plans
Develop and Maintain the PI closure criteria for the Enterprise Standard Gated Process (ESGP Gate 2 through Gate 8)
Develop and Maintain the PI closure criteria for the Non-Recurring Product Development (NRPD) Process
Support PI Project Management plans across multiple Product Development programs
Create & Coordinate Tier (1) and (2) PI Milestones with all affected program stakeholders for an integrated program plan and schedule – summary presentations and artifacts to support all PI milestones and for Gate 2, 3, 4, 7, 8 closure
Create Program Level PI Plans and Lead Physical Integration across functions to support design First Pass Quality and to verify the product is Integrated, Interference Free, Producible, Maintainable, Serviceable, and Safe
Product Design
Establish & Maintain Accurate Digital Integration Environments to support configuration management
Manage Geometry Completeness / Maturity State Progression and Closure
Verify Geometry Model attributes for accurate integration (PIN, Instance Type, other)
Verify physical interface development and closure
Manage Special Purpose Models (space reservations, moving parts, threat models, stayout zones, other)
Manage Interferences / System Separations
Visualize Engineering Bill of Material (EBOM)
Production System
Verify engineering design supports the production system objectives for parts, plans, tools
Visualize and simulate of the Condition of Assembly (COA) / Condition of Support (COS)
Visualize Production System Special Purpose Models (manufacturing stay-out zones, tooling paths, other)
Visualize Tooling, Equipment, and associated key interfaces to the product
Support PE with visualization and development of Installation Plans
Support PE with visualization of the Manufacturing Bill of Material (MBOM)
Support, Services, and Safety
Support Airplane / Product Safety Engineering with the System Separation Requirements Team (SSRT)
Detailed 3D Design Integration including Kinematic Simulations
Maintainability (accessible, removable, replaceable per service requirements)
Safety - Human Model check before layout closure for EHS, HF / ERGO (reach, access, lifting, hazards, confined space, fall hazards, trip hazards, other)
Support (ground ops, maintenance, simulations, and training)
Create and manage the Program Physical Interface / Bracket Management Plan
Create and manage the Program Interference Management Plan
Provide visualization and integration support for the Systems Separation Requirements Team (SSRT) and associated team level Systems Separation Analysis
Tools / Systems Used:
Integration Visualization Tool (IVT) for massive model visualization to support geometry integration and analysis
IC.IDO / Virtual Reality (3D Immersive) Tool for integration, simulation, mixed reality, and analysis (interference, separation, production/build, maintenance, human factors /ergonomics, kinematics / physics)
Virtual Reality (VR) to support 3D immersive integration and simulation
CATIA V5 / ENOVIA Tools for authoritative source geometry design and configuration management
(3DX) Tool for Collaborative integration and analysis to support Enterprise Digital Transformation
Basic Qualifications (Required Skills/Experience):
Systems thinking
Excellent oral and written skills
Strong background in model-based definition
Adaptive thinking, strong team player
Preferred Qualifications (Desired Skills/Experience)
Background in IVT and CATIA
Typical Education/Experience:
Education/experience typically acquired through advanced technical education from an accredited course of study in engineering, computer science, mathematics, physics or chemistry (e.g. Bachelor) and typically 5 or more years' related work experience or an equivalent combination of technical education and experience (e.g. PhD, Master+3 years' related work experience). In the USA, ABET accreditation is the preferred, although not required, accreditation standard.
Relocation: This position offers relocation based on candidate’s eligibility.
Shift:
This position is 1st shift.
Union:
This is a union represented position.
Drug Free Workplace:
Boeing is a Drug Free Workplace where post offer applicants and employees are subject to testing for marijuana, cocaine, opioids, amphetamines, PCP, and alcohol when criteria is met as outlined in our policies.
At Boeing, we strive to deliver a Total Rewards package that will attract, engage and retain the top talent. Elements of the Total Rewards package include competitive base pay and variable compensation opportunities.
The Boeing Company also provides eligible employees with an opportunity to enroll in a variety of benefit programs, generally including health insurance, flexible spending accounts, health savings accounts, retirement savings plans, life and disability insurance programs, and a number of programs that provide for both paid and unpaid time away from work.
The specific programs and options available to any given employee may vary depending on eligibility factors such as geographic location, date of hire, and the applicability of collective bargaining agreements.
Please note that the information shown below is for general guidance only. Pay is based upon candidate experience and qualifications, as well as market and business considerations.
Summary Pay Range Level 3: $96,050-$129,950

Export Control Requirements: U.S. Government Export Control Status: This position must meet export control compliance requirements. To meet export control compliance requirements, a “U.S. Person” as defined by 22 C.F.R. §120.15 is required. “U.S. Person” includes U.S. Citizen, lawful permanent resident, refugee, or asylee.

Export Control Details: US based job, US Person required

Equal Opportunity Employer:
Boeing is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, physical or mental disability, genetic factors, military/veteran status or other characteristics protected by law.","$113,000 /yr (est.)",10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1916.0,$10+ billion (USD)
"TransPecos Banks
4.2",4.2,Remote,Data Integrations Engineer-Mulesoft,"Job Description
Job Title: Mulesoft/API Engineer
Summary: We are seeking a talented API Engineer with deep Mulesoft experience to join our team. In this role, you will be responsible for designing, developing, testing, and deploying integration solutions that connect various systems and applications across the organization. You will work with cross-functional teams to identify integration requirements, design and implement solutions, and ensure that our systems are performing at their best. The ideal candidate will have experience in Mulesoft and a deep understanding of data integration concepts and technologies, as well as strong problem-solving skills and the ability to work collaboratively with cross-functional teams.
Wage Type: Salaried
Organizational Structure:
Reports to: Head of Engineering/Chief Architect
Provides guidance to other technical team members
Essential Duties & Responsibilities:
To perform this job successfully, an individual must be able to perform each of the essential duties satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Design and develop integrations between various systems and applications, using integration technologies such as REST APIs, SOAP, JSON, XML, and ETL tools
Collaborate with cross-functional teams to understand integration requirements, design and implement solutions that meet business needs.
Ensure high-quality, reliable, and scalable integrations by performing unit testing, integration testing, and troubleshooting issues.
Create and maintain technical documentation related to integrations, including specifications, design documents, and test plans.
Work with third-party vendors and partners to integrate their systems with our own
Stay current with industry trends and best practices in integration technologies and tools
Design, develop and implement Mulesoft-based integration solutions for our enterprise systems.
Qualifications:
Required Knowledge/Skills:
5+ years of experience in designing and developing integrations between various systems and applications
Strong experience with integration technologies such as REST APIs, SOAP, JSON, XML, and ETL tools
Strong programming skills in Java, Python, or other programming languages
Strong problem-solving skills and ability to troubleshoot issues quickly and efficiently
Excellent communication and collaboration skills with the ability to work effectively in a team environment
Experience in designing and developing integration solutions using Mulesoft.
Strong understanding of SOA architecture and principles.
Experience with cloud technologies, such as AWS or Azure (Azure preferred).
Strong analytical and problem-solving skills.
Competent in API and fundamental network security
Mulesoft Developer Certifications are a plus.
Talent:
Strong project management and organizational skills.
Must be a self-starter and self-motivated individual that is goal oriented, organized, and analytical and can bring a positive impact to our organization.
Energetic, resourceful, and appropriate work intensity to get the work done
Strong people acumen and relationship skills; Naturally pre-disposed to quickly establish positive personal and professional relationships.
Desired Experiences:
Familiarity with Microsoft tools like Power BI and Dataverse
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience working in a start-up or rapidly growing organization.
Education:
Degree in Information Technology, Computer Science or Management Information Systems
MuleSoft Developer Certifications are a plus.
Non-degree certifications also considered.
Other:
Ability to interpret a variety of instructions furnished in written, oral, diagram or schedule form.
Must be able to lift to 20 pounds.
Job Type: Full-time
Benefits:

401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance

Schedule:

Monday to Friday
Weekend availability
Application Question(s):

How many years as a developer have you worked in a Financial Institution?
Will you now or in the future require H1B sponsorship?

Experience:

MuleSoft: 5 years (Preferred)
Azure: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
APIs: 5 years (Preferred)
SOA architecture: 1 year (Preferred)

License/Certification:

MuleSoft Developer Certification (Preferred)
Work Location: Remote",,Unknown,Company - Private,Financial Services,Banking & Lending,,Unknown / Non-Applicable
Splunk OLD (Read Only),,"San Jose, CA",Software Engineer - Data Platform - Remote,"A little about us. Splunk is here to build a safer and more resilient digital world. We’re proud to say that we’re the key to enterprise resilience for more than 11,000 enterprise organizations that use our Unified Security and Observability Platform to keep their systems secure and reliable. We’re also especially proud of our award-winning culture and our regular appearance on those “Best Places to Work” lists.
If you end up joining us, we’ll only ask you one thing: bring your whole, authentic self, what we call your million data points. So bring your work experience, problem-solving skills and talent, of course, but also bring your joy, your passion and all the things that make you, you. Because when you feel free to be you, it makes it a lot easier for us to be us.
Role:
Are you interested in being part of building the next-generation and highly-scaled industry-leading data platform for machine data? The Splunk Data Platform Search Execution team is looking for a Software Engineer to join our backend distributed systems engineering team.
You will be working on the core of Splunk's Search technology and be part of the team to solve the most challenging and exciting problems in the Search backend area to delight our customers with high-performance, reliable, and efficient solutions.
We give our engineers an environment in which they can contribute from day one while also providing learning and growth opportunities. You'll learn how our entire stack works, from data ingestion and storage to searching, reporting, and building dashboards, all in distributed environments. The work you’ll do will directly impact the experience of our customers.
Responsibilities:
Design, develop, and maintain features for Splunk search infrastructure.
Build robust, fault-tolerant distributed systems in a multi-threaded/multi-process environment.
Analyze and improve the scalability of data collection, routing, storage, and retrieval.
Define and perform various search language layer optimizations/transformations.
Requirements:
5 years of related experience with a technical Bachelor’s degree; or equivalent practical experience; or 3 years and a technical Master’s degree; or equivalent practical experience.
Master knowledge of developing and debugging any object-oriented language like C++.
You have knowledge of backend systems, storage, filesystem, memory, and multithreading.
You have familiarity with any query language and processing like SQL, SPL, etc.
You have a proven foundation in operating systems, data structures, algorithms, and software design.
You have knowledge of modern distributed system design and implementation in the Unix/Linux environment.
Passion for solving hard problems and exploring new technologies.
You have knowledge and experience with AWS services, like EC2, S3, etc.
What We Offer You
A constant stream of new things for you to learn. We're always expanding into new areas, bringing in open source projects and contributing back, and exploring new technologies.
A set of exceptionally talented and dedicated peers, all the way from engineering and QA to product management and customer support.
A stable, collaborative and encouraging work environment.
We don't expect people to work 12-hour days. We want you to have a successful time outside of work too. Want to work from home sometimes? No problem. We trust our Colleagues to be responsible with their time and dedication, and believe that balance helps cultivate an outstanding environment.
We value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which the candidate is applying.
For job positions in San Francisco, CA, and other locations where required, we will consider employment qualified applicants with arrest and conviction records.

Note: Splunk provides flexibility and choice in the working arrangement for most roles, including remote and/or in-office roles. We have a market-based pay structure which varies by location. Please note that the base pay range is a guideline and for candidates who receive an offer, the base pay will vary based on factors such as work location as set out below, as well as the knowledge, skills and experience of the candidate. In addition to base pay, this role is eligible for incentive compensation and may be eligible for equity or long-term cash awards.
Benefits are an important part of Splunk's Total Rewards package. This role is eligible for a competitive benefits package which includes medical, dental, vision, a 401(k) plan and match, paid time off, an ESPP and much more! Learn more about our comprehensive benefits and wellbeing offering at https://splunkbenefits.com.
Base Pay Range
SF Bay Area, Seattle Metro, and New York City Metro Area
Base Pay Range: $136,000.00 - 187,000.00 per year
California (excludes SF Bay Area), Washington (excludes Seattle Metro), Washington DC Metro, and Massachusetts
Base Pay Range: $124,000.00 - 170,500.00 per year
All other cities and states excluding California, Washington, Massachusetts, New York City Metro Area and Washington DC Metro Area.
Base Pay Range: $116,000.00 - 159,500.00 per year","$137,750 /yr (est.)",,,,,,
"Republic National Distributing Company
3.8",3.8,"Atlanta, GA",Data Engineer - Senior,"Overview: The Senior Data Engineer is responsible for managing and organizing RNDC's enterprise data. They will translate requirements and designs into functional data pipelines while ensuring the continued quality and completeness of information. Senior Data Engineers will combine raw information from different sources to create consistent and machine-readable datasets that are easy to analyze and support company initiatives. They will support other Data Engineers and Data Analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They will also implement methods to improve data reliability and quality, improve data visibility and reduce effort through automation. Responsibilities:
Contribute on a team of data engineers through design, demand delivery, code reviews, release management, implementation, presentations, and meetings.
Mentor fellow data engineers and contribute to ongoing process improvements for the team
Evaluate business needs and objectives and align architecture/designs with business requirements
Build the data pipelines required for the optimal extraction, transformation, integration and loading of raw data from a wide variety of data sources
Assemble large, complex data sets and model our data in a way that meets functional / non-functional business requirements
Create data tools for analytics team members that assist them in generating innovative industry insights that provide our business a competitive advantage
Implement data tagging mechanisms and metadata management so data is accurately classified and visible to the organization
Build processes to help identify and improve data quality, consistency and effectiveness
Ensure our data is managed in a way that it conforms to all information privacy and protection policies
Use agile software development processes to iteratively make improvements to our data management systems
Identify opportunities for automation
Be an advocate for best practices and continued learning Qualifications: Bachelor's/Tech School degree in Computer Science, Information Systems, Engineering or equivalent and/or commensurate years of real-world experience in software engineering. 4+ years of relevant experience in data management3+ years in data engineering with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELTExperience with performance analysis and optimizationExperience in data acquisition, transformation and storage design using design principles, patterns and best practicesData engineering certification is a plus Informatica, Kafka, CDC, SQL, Irwin, Python, AWS (S3, Athena, Glue, Kinesis, Redshift), Spark, Scala, AI/MLWe are an Equal Opportunity employer.","$97,233 /yr (est.)",5001 to 10000 Employees,Company - Private,Retail & Wholesale,Wholesale,1898.0,$5 to $10 billion (USD)
"LTIMindtree
3.8",3.8,"Ontario, CA",Senior Data Engineer,"SQL DB /Technical Skills
o Experience with SQL Server or an equivalent database product – Must have.
o Experience in troubleshooting and problem solving role – Must have.
o Experience with Relational Database management systems and concepts
o High Availability: Experience/Knowledge of Windows Server Cluster Services in On-Premise environments
o Experience in SQL Server Performance
o Experience in Database and Server Administration
o Should able to deploy and Troubleshoot SQL Server Failover Cluster Instances etc.
o Experience/Knowledge of Windows Server Cluster Services in Azure cloud
o Having Azure domain experiences is a big asset.
o Experience in a customer facing or customer support role
Time of operation: This track will be in 24X7 shift model. Therefore, candidates should be flexible to cover during holidays and weekends to accommodate MS roster

Disclaimer :L&T Infotech has an accommodation process in place and provides accommodations for applicants with disabilities. If you require a specific accommodation because of a disability or a medical need during our recruitment processes, please let us know so that arrangements can be made for the appropriate accommodations to be in place.

Job Segment: Database, System Administrator, SQL, Engineer, Technology, Customer Service, Engineering","$108,343 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Information Technology Support Services,1997.0,Unknown / Non-Applicable
"OpenSea
4.6",4.6,Remote,"Senior Data Engineer, Infrastructure","OpenSea is the first and largest marketplace for non-fungible tokens, or NFTs. Applications for NFTs include collectibles, gaming items, domain names, digital art, and many other items backed by a blockchain. OpenSea is an open, inclusive web3 platform, where individuals can come to explore NFTs and connect with each other to purchase and sell NFTs. At OpenSea, we're excited about building a platform that supports a brand new economy based on true digital ownership and are proud to be recognized as Y Combinator's #3 ranked top private company.

When hiring candidates, we look for signals that a candidate will thrive in our culture, where we default to trust, embrace feedback, grow rapidly, and love our work. We also know how critical it is to celebrate and support our differences. Employing a team rich in diverse thoughts, experiences and opinions enables our employees, our product and our community to flourish. We are dedicated to equal employment opportunities regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. To help facilitate this, we support remote, hybrid or onsite work at either New York City, San Francisco or the Silicon Valley for the majority of our opportunities.

Our engineering team at OpenSea is in search of a strong and curious Data Engineer to take charge of our analytics and machine learning pipelines. As a member of our data engineering team, you will collaborate with other engineers, data analysts, data scientists, and product managers, contributing significantly to the growth of one of the most rapidly expanding NFT marketplaces in the Web3 ecosystem.
Responsibilities
Design, build, and maintain data pipelines from end-to-end, ensuring data accuracy, availability, and quality for the Analytics and Data Science teams
Collaborate closely with Data Scientists to understand data requirements, develop data models, and optimize data pipelines for advanced analytics and machine learning use cases
Develop and maintain scalable, efficient, and reliable ETL processes, using best practices for data ingestion, storage, and processing
Work with stakeholders to identify and prioritize analytics requirements, and build out necessary analytics tools and dashboards
Proactively monitor data pipelines, troubleshoot, and resolve data-related issues
Contribute to the continuous improvement of data engineering practices, including documentation, code reviews, and knowledge sharing
Desired Experience
5+ years of experience in data engineeringExperience with big data technologies such as Snowflake, Hadoop, Spark, Airflow, or Flink
Strong knowledge of AWS services, particularly those related to data storage, processing, and analytics (e.g., S3, Redshift, Glue, EMR, Kinesis, Lambda, and Athena)
Expert in SQL and proficiency in at least one programming language (Python, Go, Java)
Familiarity with data warehousing concepts and schema design principles (e.g., Star Schema, Snowflake Schema)
Strong problem-solving skills, a data-driven mindset, and a passion for working with large, complex datasets
Excellent communication and collaboration skills, with the ability to work effectively across teams and stakeholders

If you don't think you meet all of the criteria below but still are interested in the job, please apply. Nobody checks every box, and we're looking for someone excited to join the team.

The base salary for this full-time position, which spans across multiple internal levels depending on qualifications, ranges between $160,000 to $305,000 plus benefits & equity.

#LI-Remote",,201 to 500 Employees,Company - Private,Information Technology,Internet & Web Services,2017.0,Unknown / Non-Applicable
"Farm Credit Financial Partners
3.4",3.4,"Springfield, MA",Data Engineer III,"For over 25 years, Farm Credit Financial Partners, Inc. (FPI) has provided technology products and services to the Farm Credit System. We care deeply about the agricultural credit associations (ACAs) we serve through our mission of delivering trusted technology solutions to help American agriculture thrive. As a customer-owned service organization, we support six ACAs from Maine to California with over 62,000 customer-members and over $40 billion in loan volume . Everyone here contributes to the success of our customers, and to the vibrant culture that makes FPI a great place to work. Throughout the year, you will find us having fun and jamming out to FPI’s band, coming together to support local charities, and celebrating our wins together.
We offer a robust benefits package that includes competitive earnings, hybrid and remote work options, tuition reimbursement, generous 401(k) matching, and development opportunities through company-sponsored trainings and certifications.
Come grow with us: financialpartners.com .
Farm Credit Financial Partners, Inc. is an Equal Opportunity Employer, and all qualified applicants will receive consideration for employment without regard to age, race, color, national origin, sex or gender, religion, pregnancy, marital status, status as a veteran, sexual orientation, gender identity, disability, or any other characteristic protected by law. EEO / AA / Minorities / Female / Disabilities / Veterans

JOB SUMMARY: The Data Engineer III is responsible for transforming data that can be easily analyzed. The position will be responsible for expanding and optimizing our data and data pipeline for our Association partners. The Data Engineer III primarily works with project teams on developing new data platforms to support strategic initiatives in alignment with business and/or enterprise strategies.
ESSENTIAL FUNCTIONS:
Work with architects, modelers and other data engineers to implement design and assemble large, complex data sets that meet end user business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Participate in technical design and code reviews to ensure quality and best practices are maintained.
Perform as technical lead on smaller projects and be a collaborative member on larger projects.
Contribute to the effective data governance of the organization’s business data. This includes data quality, data management, data policies, business process management, and risk management surrounding the handling of organizational data.
ADDITIONAL FUNCTIONS:
Coach and mentor junior engineers
Communicate effectively with stakeholders regarding project status and delivery timeframe
Foster innovative team culture and process improvement during development phase
OTHER DUTIES: This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that
are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without
notice.

QUALIFICATIONS:
Bachelor’s degree in Computer Science or MIS, and 5+ years’ experience in data modeling and cloud technologies. Strong working experience with the Azure technology stack including:
Databricks
Data Factory
SQL
Python
Power BI
Experience with Agile and Waterfall methodologies
Strong organization, analytical, and communication skills
Proficiency in the following technologies: R, Microsoft Office Suite
Strong customer service focus (data consumers as customers)
Familiar with software development best practices
WORK ENVIRONMENT: Typical noise levels for an open, cubicle-styled environment.
PHYSICAL DEMANDS: This position requires periods of standing, walking, and the use of computer equipment. Additional physical demands include, but may not be limited to, talking or hearing, push/pull, stooping, kneeling, reaching w/hands and arms, and lifting at least 10 pounds.
Working off-hours nights and weekends may be required on occasion for mission-critical needs.
WORK AUTHORIZATION: Authorization to work in the United States is required.
REASONABLE ACCOMMODATION : Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.","$119,377 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1995.0,$25 to $100 million (USD)
"Banner Health
3.3",3.3,"Dallas, TX",Cybersecurity Data Protection Engineer II,"Primary City/State:
Phoenix, Arizona
Department Name:
IT Data Protection-Corp
Work Shift:
Day
Job Category:
Information Technology
Primary Location Salary Range:
$40.11 - $66.85 / hour, based on education & experience
In accordance with State Pay Transparency Rules.
Innovation and highly trained staff. The Information Technology professionals at Banner Health are utilizing cutting edge technology to change health care for the better. If you’re ready to change lives, we want to hear from you.
In this role you will get to work with industry-leading tools and technologies to protect Banner’s sensitive data. You will be responsible for engineering CASB, DLP (and possibly other solutions), training operational staff on response procedures, and working with business partners such as HR, Legal, and Privacy to deliver solutions that secure and enable Banner business. We are looking for a cybersecurity engineer that will proactively implement solutions to complex technical problems, develop and maintain meaningful metrics, collaborate with other technical and business partners, and drive efficiency. The typical schedule for this role will be determined upon hire. Successful candidate can work remote with occasional on site visits OR can work from Banner Corporate (Phoenix Plaza - off Thomas and Central).
Banner Health IT was awarded Inside Pro and Computerworld's 100 Best Places to work in IT for 2020, 2021, 2022, and 2023!
Within Banner Health Corporate, you will have the opportunity to apply your unique experience and expertise in support of a nationally-recognized healthcare leader. We offer stimulating and rewarding careers in a wide array of disciplines. Whether your background is in Human Resources, Finance, Information Technology, Legal, Managed Care Programs or Public Relations, you'll find many options for contributing to our award-winning patient care.
POSITION SUMMARY
This position designs, develops, configures, implements, tunes, maintains solutions, resolve technical and business issues related to cybersecurity threat & vulnerability management, identity management, security operations center, forensics, and data protection. Cybersecurity Engineers work with Cybersecurity Architects to execute strategic cyber initiatives, evaluate security components of the network, applications and end-user devices, and provides guidance to ensure new systems meet regulatory and technical standards. Cybersecurity Engineers participate in root-cause analysis efforts to determine improvement opportunities when failures occur. Manage Cyber systems, ensures they are tuned, on the current release and manages appropriate change management across the IT organization and the business.

CORE FUNCTIONS
1. Leads in the design and implementation of cybersecurity solutions.

2. Leads in providing technical expertise and support for cybersecurity solutions, including operational aspects of the software, hardware, network/firewall.

3. Leads in the design, implementation, and compliance of secure configurations for applications and infrastructure components.

4. Leads in technical assessments of systems and applications to ensure compliance with policy, standards and regulations.

5. Leads in the ongoing evaluation and development of security policies and procedures. Leads the revision of policies and procedures, as needed.

6. Serves as technical lead of cybersecurity projects, including the development of project scope requirements, cybersecurity product implementation, tuning, operational support model creation.
7. Under general direction, this position is responsible for cybersecurity across multiple departments system-wide and requires interaction at all levels of staff and management. Work closely on cross functional IT Teams.

Performs all functions according to established policies, procedures, regulatory and accreditation requirements, as well as applicable professional standards. Provides all customers of Banner Health with an excellent service experience by consistently demonstrating our core and leader behaviors each and every day.

MINIMUM QUALIFICATIONS

Must possess strong knowledge of business, information security and/or computer science as normally obtained through the completion of a bachelor's degree in Computer Science, Information Security, Information Systems, or related field.

Four to six years of experience of enterprise-scale information security engineering, preferably in healthcare. Must also possess one to three years’ experience in a healthcare environment or an equivalent combination of relevant education, technical, business and healthcare experience. Experience, IT operations, automation of cybersecurity processes, coding and scripting languages, ability to document cybersecurity processes as well as use case development. Experience with the assessing cyber products, including vendor selection, define requirements, contractual documentation development. Experienced in planning, designing and implementing cybersecurity solutions. Experienced in operating, maintaining and implementing, upgrading and lifecycle of cybersecurity solutions. Proficient understanding of regulatory and compliance mandates, including but not limited to HIPAA, HITECH, PCI, Sarbanes-Oxley. Advanced knowledge of Security Engineering Principles, including risk management, resilience, vulnerability management, Information Security, NIST, MITRE ATT@CK, etc. Expertise in Cyber products supporting Data Loss Prevention, EDR, AntiVirus, Perimeter services, Threat systems, cyber platform analytics, SIEM, CASB, CLOUD Security, ETC. Requires independent judgment, critical decision making, excellent analytical skills, with excellent verbal and written communications. Ability to think quickly under difficult or complex conditions and clearly communicate to appropriate staff; ability to balance project workloads with customer support and on-call demands. Must demonstrate knowledge of information technology and information security principles and practices. Requires communication and presentation skills to engage technical and non-technical audiences. Requires ability to communicate and interact across facilities and at various levels. Incumbent will have skills to mentor less experienced team members. As is typical in this industry, variable shifts and hours and responding to after-hours notifications may be required.

PREFERRED QUALIFICATIONS

Certification in two or more of the following areas: Systems Security Certified Practitioner (SSCP), HealthCare Information Security & Privacy Practitioner, (HCISPP), CompTIA Security+, Certified Information Systems Security Professional (CISSP) – Engineering (ISSEP), Certified Ethical Hacker (CEH), SANS GIAC, or Certified Information Systems Auditor (CISA). Three plus years as a System Administrator, Security Operations or in IT Operations. Or three plus years in risk management or GRC experience in the healthcare/medical environment. Must also possess three plus years’ experience in a healthcare environment or an equivalent combination of relevant education, technical, business and healthcare experience.

Additional related education and/or experience preferred.
EOE/Female/Minority/Disability/Veterans
Our organization supports a drug-free work environment.
Privacy Policy",,10000+ Employees,Nonprofit Organization,Healthcare,Health Care Services & Hospitals,1999.0,$5 to $10 billion (USD)
Luttechub,,"Columbus, OH",Lead Data Engineer / Data Architect,"Lead Data Engineer / Data Architect
Only candidate with years of experience: +7
Those authorized to work in the U.S. are encouraged to apply.
Remote
Job description
Responsibilities
Stitch and normalize sparse and noisy data across various data sources.
Undertake the preprocessing of structured and unstructured data
Design, develop, test, implement, and support technical solutions in full-stack development tools and technologies.
Work with a team of developers with deep experience in machine learning, distributed microservices, and full-stack systems
Utilize programming languages like Python, Java, and Open Source RDBMS and cloud-based data warehousing services such as Aurora or Big Query
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with engineering and product development teams
Deliver on timeline commitments where necessary
Build trust and strong relationships at all levels
Desirable Qualities:
Strike a balance between critical thinking and actual hands-on analyses.
Have a keen interest in learning data science while bringing engineering rigor to the team.
Research mindset- the ability to structure a project from idea to experimentation to prototype to implementation.
Be a driven and focused self-starter, great communicator, with exceptional follow-through. You aggressively tackle work and love the responsibility of being individually empowered.
Be resolute to overcome challenges that will inevitably arise.
Qualifications & Experience
Required
Must have designed, developed, and supported a complex software solution.
Proficient in SQL.
Proficient in dimensional/multidimensional data modeling.
Experience with graph, transactional, and operational data modeling is a plus.
Familiarity with all stages of the product development cycle. Experience maintaining engineering best practices, including defect tracking, design reviews, and appropriate testing.
Hold strong organizational and problem-solving skills.
Take a pragmatic, product-oriented approach.
Possess the ability to work cross-functionally with minimal supervision.
Preferred
B.S. in Computer Science, Electrical Engineering, Mathematics, Statistics, Physics, or similar quantitative fields/work experience.
Experience with cloud environments
7+ years of experience in application or data warehousing development
7+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)
4+ years of experience working on streaming data applications
5+ years of experience with Agile engineering practices
5+ years of experience developing Java-based software solutions, scripting and OOP languages
5+ years of experience with UNIX/Linux, including basic commands and shell scripting
4+ years of experience with GCP
4+ years of experience with Ansible / Terraform
3+ years in the healthcare industry and knowledge of their business practices.
Job Type: Contract
Pay: $65.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Columbus, OH 43081: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 7 years (Required)
data warehousing development: 7 years (Required)
scripting language (Python, Perl, JavaScript, Shell): 7 years (Required)
Work Location: Hybrid remote in Columbus, OH 43081",$65.00 /hr (est.),201 to 500 Employees,Company - Public,,,,Unknown / Non-Applicable
"Palo Alto Networks
4.2",4.2,"Santa Clara, CA",Principal Engineer Software (Cloud / Data Lake),"Company Description

Our Mission
At Palo Alto Networks® everything starts and ends with our mission:
Being the cybersecurity partner of choice, protecting our digital way of life.
We have the vision of a world where each day is safer and more secure than the one before. These aren’t easy goals to accomplish – but we’re not here for easy. We’re here for better. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.
We’re changing the nature of work. Palo Alto Networks is evolving to meet the needs of our employees now and in the future through FLEXWORK, our approach to how we work. From benefits to learning, location to leadership, we’ve rethought and recreated every aspect of the employee experience at Palo Alto Networks. And because it FLEXes around each individual employee based on their individual choices, employees are empowered to push boundaries and help us all evolve, together.

Job Description

Your Career
Cortex Data Lake (CDL) enables AI-based innovations for cybersecurity with the industry’s only approach to normalizing and stitching together an enterprise’s data. Cortex Data Lake can:
Radically simplify customer security operations by collecting, integrating, and normalizing an enterprise’s security data.
Effortlessly run advanced AI and machine learning with cloud-scale data and compute.
Constantly learns from new data sources to evolve customer defenses.
CDL is built to benefit from public cloud scale and locations ready for elastic scale, eliminating the need for local compute and storage. CDL has strict privacy and security controls in place to prevent unauthorized access to sensitive or identifiable information. Its infrastructure is secured with industry-standard best practices for security and confidentiality, including rigorous technical and organizational security controls.

Principal Engineers are
Technologists who can architect solutions to complex problems
Experts who deliver on critical business needs in their domains
Role models and mentors who exemplify the best of Palo Alto Networks culture
Leaders who can communicate cogently with hands-on engineers as well as executives
Hands-on engineers that can code and build great products
Your Impact
Driving the technical roadmap with hands-on participation in developing next generation data processing systems optimized for AI-powered use cases
Conceptualize, Architect and develop highly efficient multi-cloud data pipelines to process & store multi-tenant data at Petabyte scale to enable AI/ML Applications
Keep the product updated with best-of-the-times technology in a fast-evolving environment
Work with SRE and Technical Support teams to investigate and resolve critical customer defects
Recruit and Mentor new team members

Qualifications

Your Experience
8+ years of hands-on experience in building large enterprise applications with 3+ years experience in technical leadership roles building large scale data pipelines.
Must have extensive hands-on programming skills in Java
Experience in Python programming, expertise in Linux and shell scripting skills are a significant plus
Experience with one or more cloud platforms such as AWS, GCP , their services (DataFlow, BQ, Athena etc) and Kubernetes
Strong knowledge of databases SQL, NoSQL, Time series, GraphDB etc
Good leadership and communication skills
High energy with the ability to work in a fast-paced environment

Additional Information

The Team
Our engineering team is at the core of our products and connected directly to the mission of preventing cyberattacks. We are constantly innovating — challenging the way we, and the industry, think about cybersecurity. Our engineers don’t shy away from building products to solve problems no one has pursued before.

We define the industry instead of waiting for directions. We need individuals who feel comfortable in ambiguity, excited by the prospect of a challenge, and empowered by the unknown risks facing our everyday lives that are only enabled by a secure digital environment.
Our Commitment
We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.
We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.
Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.
All your information will be kept confidential according to EEO guidelines.
The compensation offered for this position will depend on qualifications, experience, and work location. For candidates who receive an offer at the posted level, the starting base salary (for non-sales roles) or base salary + commission target (for sales/commissioned roles) is expected to be between $140,100yr to $226,600/yr. The offered compensation may also include restricted stock units and a bonus. A description of our employee benefits may be found here .
#LI-TD1","$183,350 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2005.0,$1 to $5 billion (USD)
"Hexagon Asset Lifecycle Intelligence
3.9",3.9,"Houston, TX","Software Support Engineer, Reference Data (Remote - USA)","Responsibilities:
Are you a problem solver, someone who enjoys troubleshooting and resolving complex issues? Hexagon’s Asset Lifecycle Intelligence division (Hexagon) is seeking a full-time Software Support Engineer, Reference Data to join our global Reference Data support team. The role offers flexibility, with limited travel required, and the option to work from either home or any one of Hexagon’s US office locations.
Works as part of a global team to support customers by troubleshooting and resolving technically complex support issues related to Hexagon’s Asset Lifecycle Intelligence division’s Solutions and interfaces.
Regularly interacts with solution development and product ownership teams to investigate problems, discuss, and document design issues, and provide customer feedback.
Possesses strong interpersonal skills and effectively communicates with internal and external stakeholders.
Exercise a sizeable degree of self-direction and decision making focused on delivering a solution to customer’s workflow implementation issues.
Accurately documents support activities and outcomes; Maintains knowledge base.
Develops strong proficiencies in Customer Relationship Management systems.
Maintains adherence to customer service level agreements.
Develops technical documents, instructions, and training for internal and external purposes.
Keeps updated on current Hexagon’s Asset Lifecycle Intelligence division’s solution portfolio through training and sprint reviews.
May conduct customer training or work on service activities when required.
#LI-REMOTE #LI-RM
Qualifications:
As a minimum possess strong competency of Intergraph Smart Reference Data with knowledge of additional Hexagon Solutions an advantage.
Communicate via the telephone, email, online video, and in person, employing excellent oral and written communication skills.
Experience participating in consulting functions and services.
Customer Service experience and ability.
Degree qualification in an Engineering or related discipline or equivalent industry experience.
Fluency in the basic technologies underlying the IT industry.
US Citizenship is required","$61,366 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Computer Hardware Development,1992.0,$500 million to $1 billion (USD)
"Intone Networks
4.5",4.5,Remote,Lead Data engineer,"MUST HAVE: - 10+ years of experience in: Technical solutioning and system architecture design Evaluation and Recommend of ETL tools Implementation of recommended ETL tools Strong in SQL and Python, with 3+ years hands-on coding experience with both Experience building automated big data pipelines Experience performing data analysis and data exploration Experience working in an agile delivery environment Strong critical thinking, communication, and problem solving skills Experience with big data frameworks (i.e. Hadoop and Spark) Experience with cloud-based platforms (i.e. Azure, GPC, AWS) Experience working in multi-developer environment, using version control (i.e. Git) Experience with real-time and streaming technology (i.e. Azure Event Hubs, Azure Functions Kafka, Spark Streaming) Experience with deployment/scaling of apps on containerized environment (i.e. Kubernetes, AKS) Experience partnering cross-functionally with other technical teams (i.e. data ingestion, data science, operational systems) to align priorities and achieve deliverable outcomes Experience with setting coding standards, performing code reviews, and mentoring junior developers Experience overseeing project delivery by mentoring junior technical developers PREFERRED TO HAVE: Previous healthcare experience and domain knowledge Exposure/understanding DevOps best practice and CICD (i.e. Jenkins) Exposure/understanding of containerization (i.e. Kubernetes, Docker) Experience with Snowflake and hands-on query tuning/optimization. Experience with orchestrating pipelines using tools (i.e. Airflow, Azure Data Factory) Experience with REST API/Microservice development using Python Full legal Name: Contact Number: Email: Current Location: Work Authorization: Years of Total Work Experience: DOB Linkedin: Pay rate Education Details and Years of Graduation:",,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,,$5 to $25 million (USD)
"JM Family Enterprises
3.4",3.4,"Deerfield Beach, FL",Senior Data Engineer,"Sr. Data Engineer will work in the Actuary, Data & Analytics Department of Jim Moran and Associates Inc., a division of JM Family Enterprises, and will play a critical role in development and management of organizational data assets for data science, advanced analytics and self-service reporting to deliver business insights and drive actionable results.
You will interface with business stewards, data end-users and the data storage/architecture team, leveraging a common data platform, to fulfill data-related requests that meet business needs.
Key Responsibilities:
Acquire data from primary or secondary data sources, prepare, integrate, and maintain data structures in support of advanced analytics, self-service reporting, and strategic data initiatives
Work closely with management, data product team, and data end-users to prioritize business and information needs
Validate data for completeness and accuracy based on guidelines set by project stakeholders or downstream data and analytics systems
Perform necessary data profiling, data cleansing, aggregations, and normalization to deliver data products that are available for end-user consumption
Work with data providers to improve data management workflow
Develop and execute standard or custom queries and/or reports to retrieve data
Conduct root cause study of business problems; suggest areas for process improvement
Execute and promote data management and data governance strategies and standards
Qualifications:
Bachelor’s degree in computer science, Management Information Systems, or related field
5+ years of experience in data warehousing, data lifecycle management, database designing, data modelling and computer programming
Excellent understanding of Structured query language (SQL), Coding experience with a general-purpose, dynamic programming language (e.g., Python, PERL)
Strong learning orientation and curiosity
Comfortable learning new systems/software applications
Strong analytical thinking skills and problem-solving skills
Solid written and oral communication skills
Detail oriented
Good time management skills and multitasking ability
Knowledge in the following is preferred:
MS Azure: Azure Data Lake Store, Synapse
Data Factory/SSIS or related tool
Python or similar programming languages
Business visualization tools (e.g., Tableau, PowerBI)
Statistical software (e.g., SAS, SPSS)
#LI-AM1
#LI-Onsite
JM FAMILY IS PROUD TO BE AN EQUAL OPPORTUNITY EMPLOYER
JM Family Enterprises, Inc. is an Equal Employment Opportunity employer. We are committed to recruiting, hiring, retaining, and promoting qualified associates without regard to age, race, religion, color, gender, sex (including pregnancy, childbirth and related medical conditions), sexual orientation, gender identity, gender expression, mental or physical disability, national origin, marital status, citizenship, military status, genetic information, veteran status, or any other characteristic protected by federal, state, provincial, or local law.

DISABILITY ACCOMMODATIONS
If you have a disability and require a reasonable accommodation to complete the job application process, please contact JM Family’s Talent Acquisition department at
talentacquisition@jmfamily.com
for assistance. If you have an accommodation request for one of our recruiting events, please notify us at least 72 hours prior so that we may provide assistance.","$116,093 /yr (est.)",1001 to 5000 Employees,Company - Private,Retail & Wholesale,Vehicle Dealers,1968.0,$10+ billion (USD)
"Akamai
4.5",4.5,United States,Senior Software Engineer - Data Platform - Remote,"Do you love Big Data Development and are a team player?
Would you like to work with cutting-edge technology?
Join our world-class Platform team
Our CTG (Cloud Technology group) develops and operates multiple services deployed on the cloud. These services collect, process, and store data about every transaction that is executed on Akamai edge. The Data is being consumed for the purposes of business analytics and decisions support, customer analytics, Akamai's cost structure management & Akamai's network management.
Make a global impact
You will work with talented team members that develop a Data Warehouse, which provides support for ingesting massive data volumes and executing Big Data analytical queries on that data. Technology stack that we use include Spark and Spark Structured Streaming, Kubernetes on Cloud (Azure - AKS), Spring, Prometheus, Grafana, In-house implementations and more.
As a Senior Software Engineer, you will be responsible for:
Designing, developing and rolling out Big Data systems and services
Collaborating with other teams to deliver to market mission critical data systems
Developing new and enhancing existing capabilities within data warehousing services
Following SW development methodology best practices
Handling feature development from idea inception through design and testing to operational deployment
Working on projects that focus on system scalability, performance and security
Do what you love
To be successful in this role you will:
Have 5 years of relevant experience and a Bachelor's Degree in Computer Science or related field, or equivalent experience as a Senior Backend Engineer
Have experience developing Java or Scala software
Demonstrate solid communication skills and be able to present to various stakeholders
Be a self-starter, proactively taking the initiative to solving problems and motivation to improve results
Work in a way that works for you
FlexBase, Akamai's Global Flexible Working Program, is based on the principles that are helping us create the best workplace in the world. When our colleagues said that flexible working was important to them, we listened. We also know flexible working is important to many of the incredible people considering joining Akamai. FlexBase, gives 95% of employees the choice to work from their home, their office, or both (in the country advertised). This permanent workplace flexibility program is consistent and fair globally, to help us find incredible talent, virtually anywhere. We are happy to discuss working options for this role and encourage you to speak with your recruiter in more detail when you apply.

Learn what makes Akamai a great place to work
Connect with us on social and see what life at Akamai is like!

We power and protect life online, by solving the toughest challenges, together.
At Akamai, we're curious, innovative, collaborative and tenacious. We celebrate diversity of thought and we hold an unwavering belief that we can make a meaningful difference. Our teams use their global perspectives to put customers at the forefront of everything they do, so if you are people-centric, you'll thrive here.
Working for you
At Akamai, we will provide you with opportunities to grow, flourish, and achieve great things. Our benefit options are designed to meet your individual needs for today and in the future. We provide benefits surrounding all aspects of your life:
Your health
Your finances
Your family
Your time at work
Your time pursuing other endeavors
Our benefit plan options are designed to meet your individual needs and budget, both today and in the future.
About us
Akamai powers and protects life online. Leading companies worldwide choose Akamai to build, deliver, and secure their digital experiences helping billions of people live, work, and play every day. With the world's most distributed compute platform from cloud to edge we make it easy for customers to develop and run applications, while we keep experiences closer to users and threats farther away.
Join us
Are you seeking an opportunity to make a real difference in a company with a global reach and exciting services and clients? Come join us and grow with a team of people who will energize and inspire you!

Akamai Technologies is an Affirmative Action, Equal Opportunity Employer that values the strength that diversity brings to the workplace. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of gender, gender identity, sexual orientation, race/ethnicity, protected veteran status, disability, or other protected group status.

#LI-Remote
Compensation
Akamai is committed to fair and equitable compensation practices. The base salary for this position ranges from $113,430 - $170,043/year; a candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location. The compensation package may also include incentive compensation opportunities in the form of annual bonus or incentives, equity awards and an Employee Stock Purchase Plan (ESPP). Akamai provides industry-leading benefits including healthcare, 401K savings plan, company holidays, vacation (in the form of PTO), sick time, family friendly benefits including parental leave and an employee assistance program including a focus on mental and financial wellness; Eligibility requirements apply.","$141,737 /yr (est.)",5001 to 10000 Employees,Company - Public,Information Technology,Information Technology Support Services,1998.0,$1 to $5 billion (USD)
"Phasorsoft LLC
4.0",4.0,"Seattle, WA",1348 - Cloud Data Engineer - Onsite - W2,"NOTE
NOT FOR C2C
We are looking for the candidates who can work on w2 ( Pay Roll )
Job description
Responsibilities:
Design and implement ETL data pipelines using Azure pipelines or other cloud technologies.
Extract data from diverse sources and transform it using Pyspark and Python languages within Azure Data Bricks.
Develop and optimize advanced SQL queries for data retrieval and manipulation.
Collaborate with cross-functional teams to understand data requirements and perform data mapping.
Utilize Snowflake for data storage and retrieval.
Follow Scrum/Agile methodologies to ensure timely delivery of projects.
Apply Kafka experience to enable real-time data streaming and processing.
Write Pyspark code to enhance data processing capabilities.
Use Powershell for automation and orchestration tasks.
Requirements:
Strong exposure to cloud platforms such as GCP, AWS, or Azure.
Proven experience in ETL development and cloud-based data pipelines.
Proficiency in using Azure Data Bricks and Pyspark for data transformations.
Advanced SQL skills for complex data querying and manipulation.
Familiarity with Snowflake for data warehousing.
Knowledge of data mapping and data modeling concepts.
Experience working in Agile/Scrum development environments.
Previous exposure to Kafka for real-time data streaming is preferred.
Proficiency in Pyspark coding.
Familiarity with Powershell scripting for automation.
Job Type: Contract
Pay: $76,479.45 - $158,309.78 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: In person","$117,395 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"SpartanNash
4.0",4.0,"Avondale Estates, GA",Senior Data Engineer,"At SpartanNash, we deliver the ingredients for a better life through customer-focused innovation. We do this for our supply chain customers and U.S. military commissaries, retail store guests and, most importantly, our Associates. In fact, we see a day when each will say, “I can’t live without them.”

Our SpartanNash family of Associates is 17,500 strong, ranging from bakery managers to order selectors; from IT developers to vice presidents of finance; from HR Business Partners to export specialists. Each of them plays an integral role in SpartanNash’s People First culture, Operational Excellence and Insights that Drive Solutions. Ready to contribute to the success of our food solutions company? Apply now!

Location: 850 76th Street S.W. - Byron Center, Michigan 49315

Job Description:
Position Summary:
This role is responsible to create, integrate, implement, and support large, highly complex systems and/or suites of applications. Provide expert technical leadership and guidance in projects. Oversee and review the programming/development work of others.

Here’s what you’ll do:
Technical Expert / Reference Resource acting independently under limited direction provides expert level technical design and development services and guidance for one or more systems or specialized applications
Provides technical leadership on complex solutions Scope and Function Definition develops, defines and documents project/system/function scope and objectives
Designs or enhances systems, applications, functions, and procedures to solve complex problems and meet business objectives
Prepares detailed functional specification Solution Development has full technical knowledge of all phases of analysis, integration, development, quality assurance, and implementation procedures
May be responsible for multiple phases of a project
May be responsible for the instruction, oversight, and review work of other associates
Application/System Troubleshooting interfaces with appropriate corporate area to provide detailed application/system information to assist with problem determination and resolution
Business Liaison/Mentor Acts as the primary reference resource interacting with business users to provide detailed application/system information
Mentors and provides guidance and training to other development associates
Maintain an efficient infrastructure by recognizing and reducing latencies and gaining efficiencies where recognized or needed
Participate in the on-call support rotation

Here’s what you’ll need:
Bachelor's Degree (Required) Computer Science, Computer Science Information Technology, or related field or equivalent combination of education and/or experience
Minimum 5+ years of work experience in Business Intelligence, Relational Databases, and ETL processes with modern ETL/ELT tools (Talend/Matillion/DataStage/Informatica) preferably in a cloud environment
Experience with Cloud Databases (Snowflake preferred) and Big data technologies
Qlik Replicate/Qlik Compose experience preferred
Advanced SQL skills, able to write and optimize complex queries in multiple database platforms (DB2, Oracle, Informix, SQL Server, MySQL, Netezza, Snowflake)
Experience with Unix scripting
Strong written and verbal communication skills, and effective interpersonal skills to work with internal and external customers and vendors
Good analytical/research and problem-solving skills
As part of our People First culture, SpartanNash is proud to offer a robust and competitive Total Rewards benefits package .

SpartanNash is an Equal Opportunity Employer, including disability and veteran, that celebrates diversity and believes employing a diverse workforce is key to our success. We are committed to providing equal employment opportunities to all individuals.

We are not able to sponsor work visas for this position.","$114,024 /yr (est.)",10000+ Employees,Company - Public,Retail & Wholesale,Grocery Stores,,$5 to $10 billion (USD)
"Healogics, Inc
3.0",3.0,"Jacksonville, FL",Data Solutions Engineer - Visualization,"Job Summary:

Healogics is the largest provider of advanced wound care services in the United States, treating more than 300,000 chronic wound patients annually across over 600 sites. With an aging society, obesity and diabetes on the rise, and an uptick in surgical procedures, the number of patients with non-healing wounds that would benefit from expert care is dramatically increasing. As a result, the company is working to provide our differentiated, quality outcomes to as many patients that would benefit through our out-patient clinic partnerships.

At a Data Solutions Visualization Engineer, you will provide technical leadership in the area of big data systems development including data ingestion, data curation, data storage, high-throughput data processing, analytics, user access, and security. You will partner with the Data Solutions team and business stakeholders to understand their goals, how they think about their area of the business, and how we can support them through analytics to Find, Treat and Heal patients in the community. You’re an active listener who asks good questions and puts the company first, you move seamlessly between code and analytical concepts, and you appreciate beautiful & simple technical solutions.

All Healogics employees must perform their job responsibilities according to all Healogics policies, Hospital policies, as well as to accrediting organizations, federal and state regulation, and to the Centers for Medicare and Medicaid Services (CMS) guidelines, as applicable.
Essential Functions/Responsibilities:

Architect and engineer tools, visualizations, and dashboards for the business to have everything they need at their fingertips to answer their difficult questions.
Be a bridge between the business and technical teams, enabling insight that can empower better decision-making.
Produce dashboards / visualizations for the business to explore.
Deliver ad hoc analysis using Data Solutions tools (ETL and Reporting Services, SQL) to answer timely questions.
Work with non-technical people to analyze their information needs and create effective solutions for them.
Acquire data from source systems and integrate them with the data warehouse to support any of the above.
Execute QA testing and resolve any defects.
Can speak knowledgably about the business of Healogics and offer solutions to complex problems.
Evaluate new visualization technologies and methods to constantly improve the offerings of Data Solutions.
Work interactively with all levels of company employees, including executives, and guide and advise them towards the answers for business questions.
Performs other duties as required.

Required Education, Experience and Credentials:

Bachelor of Science or Bachelor of Arts degree in Information Technology (or a related field) required; or equivalent combination of education and experience.
Minimum of three (3) years of SQL Server experience, real world experience in dimensional data warehouse development and design.
Experience building and maintaining reporting systems using tools such as Microsoft SSRS and Tibco Spotfire, required.
Experience with cloud services such as AWS and Azure, desired

Required Knowledge, Skills, and Abilities:

Excellent oral and written communication skills.
Strong knowledge of Windows operating system, virtual systems, and cloud systems.
Strong knowledge of Microsoft SQL Server.
Implementation experience with industry standard Business Intelligence/Data Visualization tools
Excellent problem-solving skills.
Interpersonal skills.
Analytical thinking skills.
Conceptual thinking skills.
Project scheduling skills.

Please contact Ashley at Ashley.Helfrich@healogics.com for more information!

Salary for this position generally ranges between $71,800-$101,400. This range is an estimate, based on potential employee qualifications: education, experience, geography as well as operational needs and other considerations permitted by law.","$92,219 /yr (est.)",1001 to 5000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,1995.0,Unknown / Non-Applicable
"hims & hers
4.2",4.2,United States,Senior Data Engineer,"Hims & Hers Health, Inc. (better known as Hims & Hers) is a multi-specialty telehealth platform building a virtual front door to the healthcare system. We connect consumers to licensed healthcare professionals, enabling people to access high-quality medical care—from wherever is most convenient—for numerous conditions related to sexual health, hair care, mental health, skincare, primary care, and more.
With products and services available across all 50 states and Washington, D.C., Hims & Hers is on a mission to help the world feel great through the power of better health. We believe how you feel in your body and mind transforms how you show up in life. That's why we're building a future where nothing stands in the way of harnessing this power. We normalize health & wellness challenges—and innovate on their solutions—to make feeling happy and healthy easy to achieve. No two people are the same, so we provide access to personalized care designed for results. At our core, our mission is deeply personal—because we too are customers.
In January 2021, the company was listed on the NYSE and is traded under the ticker symbol ""HIMS"". To learn more about our brand and offerings, you can visit forhims.com and forhers.com.
About the Role:
As a Senior Data Engineer, you will work with analytics engineers, product managers, engineers, security, DevOps, designers and others to build a data platform that backs the self-service analytics, machine learning models, and products serving 900,000+ Hims & Hers users.
You Will:
Architect and develop data pipelines to optimize for performance, quality, and scalability
Collaborate with analytics engineering data analysts, and business partners to build tools and data marts that enable self-service analytics
Build, maintain & operate scalable, performant, and containerized infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Design, develop and own robust, scalable data processing and data integration pipelines using Python, dbt, Kafka, Airflow, Spark, and REST API endpoints to ingest data from a variety of external data sources to data lake
Develop testing frameworks and monitoring to improve data quality, observability, and data quality
Partner with the analytics engineers to ensure the performance and reliability of our data sources
Orchestrate sophisticated data flow patterns across a variety of disparate tooling
Partner with machine learning engineers to deploy predictive models
Partner with the security team to build frameworks and implement data compliance and security policies
Partner with DevOps to build IaC and CI/CD pipelines
Support code versioning and code deployments for data pipelines
You Have:
5+ years of professional experience designing, creating and maintaining scalable data pipelines using Python, API calls, and scripting languages
Demonstrated experience writing clean, efficient & well-documented Python code and are willing to become effective in other languages as needed
Demonstrated experience writing complex, highly-optimized SQL queries across large data sets
Experience with cloud technologies such as AWS or Google Cloud Platform
Experience with IaC technologies like Terraform
Experience with data warehouses like BigQuery, Databricks, Snowflake, and Postgres
Experience with event streaming technologies like Kafka / Confluent
Experience with modern data stack (Airflow, Databricks, dbt, Fivetran, Tableau / Looker)
Experience with containers and container orchestration tools such as Docker or Kubernetes
Project management skills and a demonstrated ability to work autonomously
Understanding of SDLC and Agile frameworks
Nice to Have:
Experience with Machine Learning & MLOps
Experience building data models using dbt
Experience with Javascript
Experience with CI/CD (Jenkins, GitHub Actions, Circle CI)
Experience designing and developing systems with desired SLAs and data quality metrics
Experience with microservice architecture
Our Benefits (there are more but here are some highlights):
Employee Stock Purchase Program
An inclusive culture where we are always seeking improvement and cherish your input
Great compensation package with equity compensation
Unlimited PTO (10 holidays off), Mental Health days (1 day off per quarter)
Generous Parental Leave
High-coverage medical, dental & vision
Mental health & wellness benefits
Offsite team retreats
Access to Amazon HIMS Store to order any additional equipment to ensure you have the gear you need
Employee discounts on hims & hers & Apostrophe online products, and Apple Store
$75 monthly connectivity stipend (phone/internet)
401k Match
We are focused on building a diverse and inclusive workforce. If you're excited about this role, but do not meet 100% of the qualifications listed above, we encourage you to apply.
Hims is an Equal Opportunity Employer and considers applicants for employment without regard to race, color, religion, sex, orientation, national origin, age, disability, genetics or any other basis forbidden under federal, state, or local law. Hims considers all qualified applicants in accordance with the San Francisco Fair Chance Ordinance.

#LI-Remote
Outlined below is a reasonable estimate of H&H’s compensation range for this role.

H&H also offers a comprehensive Total Rewards package that includes equity grants of restricted stock (RSU’s) so that H&H employees own a piece of our company.

The actual amount will take into account a range of factors that are considered in making compensation decisions including but not limited to, skill sets, experience and training, licensure and certifications, and location.

Consult with your Recruiter during any potential screening to determine a more targeted range based on the job-related factors. We don’t ever want the pay range to act as a deterrent from you applying!
An estimate of the current salary range is
$125,000—$175,000 USD",,201 to 500 Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2017.0,Unknown / Non-Applicable
"Commonwealth Care Alliance
3.5",3.5,"Boston, MA",Lead Data Engineer,"Why This Role is Important to Us:


The Data Engineer role at Commonwealth Care Alliance (CCA) is a heavy technical and architect role that designs, contributes to, and maintains data pipelines and systems that support clinical point of care and business decisions. This role is highly collaborative with other data-focused departments at CCA including Analytics, Actuarial, and Finance. This role is a “forward-deployed” engineer that is expected to engage with and continually build a greater understanding of the clinical and operational context.
The Data Engineer must understand existing data, infrastructure, and transactional systems to design the appropriate usage of existing data assets or the development of new data assets, including large structured and unstructured data. The Data Engineer must automate robust workflows to efficiently perform extract, load, transform, modeling, and computational tasks in continuous integration and continuous delivery pattern. The Data Engineer will also support and contribute to rapid prototyping and deployment of analytics, model training, and model deployment. This role will report to the Director of Data Strategy, Mgmt. and Sol. at CCA.



What You'll Be Doing:


Engage with stakeholders on pre-defined projects
You will be responsible to understand the business requirements and architect robust data platform on cloud technologies
Build scalable and robust data model/infrastructure
Help managing enterprise data platform
Dealing with data governance, such as documentation, data integrity/quality and data security
Build data pipelines (ETL, validation, automation, monitoring and logging...) that enable analysts and other stakeholders across the organization for data-focused product and data-driven business decisions
Work closely with product managers and engineers to design, implement, test and continually improve scalable applications and services running on Azure
Make changes on the existing data system to optimize and improve accuracy of the data process
Give instruction or/and help stakeholder on the best practice to pull and use data
Create scalable and actionable solutions, in the form of analytics, reports and dashboards for stakeholders to solve business and technical problems through ad-hoc requests
Write andrevise technical documents and blogs, including design, development and application
Complete data engineering projects under supervision and guidance
You will be responsible to understand the business requirements and architect robust data platform on cloud technologies
You will be responsible for creating reusable and scalable data pipelines
You will be responsible for development and deployment of new data platforms
You will be responsible for deploying AI algorithms into the data platform to run predictive analytics at scale
Help the Data Engineering team produce high-quality code that allows us to put solutions into production
Collaborate with IT Architect and IT Security and Privacy teams to architect and deploy data pipeline solutions that are secure and performant
Day to day management and reliability of data pipelines deployed securely on the public cloud
Executes quality excellence through standards, best practices, and continuous improvements



What We're Looking For:


Education Required:
Master's degree + 5 year of equivalent work experience (STEM major or related field preferred)
OR bachelor's degree + 8 years of equivalent work experience (STEM major or related field preferred)
Experience Required:
Experience in design and develop code, scripts, and data pipelines that leverage structured and unstructured data
Strong foundation in data engineering principles and an architectural best practice
Experience in architecting solution on public cloud (preferably Azure)
Experience in instituting data architecture best practices (i.e. dimensional modeling, ETL pipeline, large scale distributed ETL pipelines)
Extensive experience of database query languages (i.e. SQL or equivalent), database design, optimizing queries, internals knowledge of query planning
Experience in authoring or reviewing system design documents for enterprise solutions
Knowledge, Skills & Abilities Required:
Technical Skills:
SQL, Oracle and Python
ETL tool (Informatica, Talend)
BI tool (Looker, Tableau, PowerBI ...)
Git-based version control systems
Strong knowledge of data modeling and mapping
Good understanding of different dimensional modeling techniques such as Star vs SnowFlake Schemas
Soft skills:
Be organized and flexible
Take initiative to own projects
Strong analytical skills
Strong verbal and written communicational skills
Be able to work with interdisciplinary teams
Be able to explain technical concepts/results to non-technical audiences
Passion for creating work that is well-documented and reproducible
Ability to work with short iteration times in agile mode as well as the ability to carry out projects in a self-directed manner
Healthcare: passion for working at a healthcare organization
Preferred Skills and Experience:
Experience with R
Experience in healthcare or health insurance organization
Experience with clinical claims and health record data
Experience building data pipelines in the cloud under the constraints of HIPAA
Publication or presentation of innovation in data science, machine learning or related area
Experience with Azure cloud and data services (cosmos, synapse, datafactory, databricks etc.)
Understanding of infrastructure (including hosting, container based deployments and storage architectures) would be advantageous","$133,540 /yr (est.)",1001 to 5000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2003.0,$1 to $5 billion (USD)
"Nova Ltd.
4.5",4.5,"Fremont, CA",Big Data/Machine Learning Software Engineer,"The Data Analytics team at Nova is looking for a Big Data/Machine Learning Software Engineer to work with Data Scientists and other Software Engineers to gather requirements and implement solutions. For you, it is exciting to see your code interacting with algorithms and data to produce results via complex interactions in Big Data systems. In this role, you will work with Data Scientists to adapt new and existing Machine Learning algorithms to Big Data microservices and build functionality to access those via web-based user interfaces.
Nova Ltd. is a leading innovator and key provider of metrology solutions for advanced process control used in semiconductor manufacturing. Our products are used in-line by leading chip foundries as well as original equipment manufacturers. Nova’s technology serves critical sectors of patterning, thin film deposition, CMP and diffusion in leading logic and memory fabs worldwide. market.

About us:
Nova provides insights into process control in the world’s most technologically advanced industry. We employ physics, mathematics, algorithms, software and hardware expertise to redefine the limits of what is possible in semiconductor manufacturing.
We invite you to join our dreamers and winners and brilliant high- aimers who see impossible as the starting point to exciting challenges, and work together in multidisciplinary global teams to find answers.
We dive deep to extract unique insights and provide our customers and partners with crucial decision-making data. Each and every one of us helps redefine what people can achieve through technology.
Why Nova:
Fortune magazine chose Nova as one of the fastest growing companies in the world in 2019 and 2020
Great Place to Work-Certified™ 2022 & 2023
Opportunity to collaborate with the best in this field, our 1000+ employees love coming to work every day in our 12 offices across the globe and share their passion for technology and innovation

Requirements:
Self-starter and quick learner of new technologies and processes – we are using some technologies not easily found in current skill portfolios, so this is the most important requirement! If you are under-skilled in some of the other requirements below, evidence of your ability to learn quickly will be seriously considered. Example of technology needed to learn quickly is customization/extension of ElasticSearch-Logstash-Kibana (ELK) instances
5+ years of software development experience in Agile environment
5+ years of coding and development experience using Python 2/3, preferably using OO approach
1-2 years of Big Data deployment, monitoring and troubleshooting of microservices, and interactions with the following Big Data concepts:
Extract/Transform/Load workflows
Kafka or other message queues
Hive/Hadoop/Impala and Big Data data storage including NOSQL solutions
Big Data configuration and monitoring using a number of technologies
Extensive experience working in a multi-threaded environment
Experience working with SQL databases and/or Big Data datastores (HDFS, etc)
Exposure to web-based application development – optimally Dash (with Python), React or Angular to facilitate workflows and graphic visualization
Extra Spice:
Strong communication and problem-solving skills – possess the ability to translate business requirements into application code
Ability to take ownership of the complete software development cycle from requirements gathering to design to implementation
Team player who will work in a collaborative environment with users and the engineering team
Passionate about well-designed software that is modifiable, efficient, reliable and meets coding standards
Experience customizing/extending ElasticSearch-Logstash-Kibana (ELK) instances

Role Responsibilities:

What will you do as a Big Data Machine Learning Software Engineer?
Implement Machine Learning solutions that live on Big Data systems, with input and guidance from Data Scientists and the algorithms they develop to effect Machine Learning
Work as part of a Scrum team to analyze requirements, scope, estimate, implement, and test changes to meet these requirements in a Big Data system
Debug existing source code, analyze logs and fix bugs as needed
Work independently and collaboratively as needed
Take ownership of assigned tasks and finish in a timely manner
Continuously learn and improve skills
Apply significant attention to detail to ensure all tasks are carried out to the highest standard
What will make you succeed in the role?
Extensive experience in Python software development
Experience configuring, deploying, and monitoring micro services in a Big Data system
Experience with User Experience (UX) and implementation of python approaches to meet UX needs
Working knowledge of GIT
Working knowledge of JIRA
Test driven development
Database application development and data modeling techniques
Experience with Scrum/Agile","$138,647 /yr (est.)",501 to 1000 Employees,Company - Public,Manufacturing,Machinery Manufacturing,1993.0,$100 to $500 million (USD)
"ECI - Sacramento
4.4",4.4,"Sacramento, CA",Lead Data Quality Engineer (C),"Senior Lead Data Quality Engineer with Snowflake experience:
12-24 months.
Public sector experience is preferred.
Must have these specific experiences in data management, data integration, data qualify and Lead experience.
Must have Snowflake platform experience.
Project will start end of June.

Mandatory Qualifications:
Must have a bachelor's degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field. Please attach degree(s).
Must have 10 or more years of experience working in Information Technology.
Must have at least seven (7) years or more of work experience in data management disciplines including data integration, modeling, optimization, and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.
Must have at least five (5) years of experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative.
Must have at least three (3) years of experience in data integration, data warehouse, big-data-related initiatives, development, and implementation.
Must have at least three (3) years of experience in architecture patterns and data integration design principles.
Must have at least three (3) years of experience with database technologies (e.g., SQL, NoSQL, SQL Server).
Must have at least three (3) years of experience with the Snowflake Data Warehouse Platform.
Must have at least three (3) years of experience with extract, transform, and load (ETL) and other business intelligence tools (e.g., SSIS, Azure Data Factory, Power BI, Tableau).

Here are the desired which is best to get as many as possible:
Desired Qualifications:
Possess an advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (post-graduation diploma or related) or a related quantitative field.
Possess Azure Cloud and Snowflake certifications.
Three (3) years working experience in the Azure Cloud platform using Synapse, Azure BLOB Storage/ADLS, Azure Data Factory, and Purview.
One (1) year experience with Azure DevOps and CI/CD pipelines.



About ECI - Sacramento:

Estrada Consulting, Inc. (ECI) delivers technology-enabled services and solutions to clients all over the USA and British Columbia. We provide system integration, custom application development, data warehouse and business intelligence, project management, custom reporting solutions and consulting services to mid-size and large enterprises in all major industries. The Company headquarters is in Sacramento, California, and was established in year 2000. Visit http://www.estradaci.com/ to learn about our projects, managed services, awards and certifications delivering value for a range of businesses and government agencies.",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2000.0,Unknown / Non-Applicable
"Fuge Technologies Inc
4.7",4.7,"Jersey City, NJ",Big Data Engineer,"Big Data Engineering Technical Lead
Mandatory Skills
Big Data – Spark, Kafka, Databricks, Hive, Nifi
AWS, Java, Scala
Client Interview Needed for Selection (yes / No)
Yes
Detailed JD (Pl share the Detailed Description, 1 liner JD will not work)
10+ yrs experience as Data Engineer
Extensive experience in spark on data processing
Kafka design experience like monitoring & parallelism.
Comfortable with Python/Java/Scala coding experience
Crisp / clear cut communication
Flexible with learning new technology
Flink experience is a plus
AWS experience is nice to have
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$72.50 /hr (est.),1 to 50 Employees,Contract,,,,Unknown / Non-Applicable
"RelMap Consulting
4.8",4.8,"Addison, TX",Sr. Data Engineer (Hybrid),"The data engineer entails:
Advanced SQL Server Development
Ability to create and modify views
Ability to create and modify custom functions
Ability to write complex queries, including joins, temp table processing, and common table expressions (CTE’s)
Ability to create and modify stored procedures using TSQL
Ability to analyze and optimize query performance using indices
Ability to use Red Gate’s SQL Compare to perform a schema (AKA “do a manual roll”) between one server an another, and between revision control and a database
Ability to stand-up new database server VM’s in Linux and Windows using VMWare
Ability to backup and restore a database in a Linux and Windows environment
Ability to manage and script automated database jobs
Ability to use PowerShell / Bash to interact with databases from the command line
Addition skills that would be desired but not required
· Experience with Redis
· Experience with MongoDB / CosmosDB
· Experience with Azure and Azure SQL
· C#, JavaScript (NodeJS / ES6), or Python experience
Job Type: Contract
Pay: $70.00 - $80.00 per hour
Compensation package:
Hourly pay
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Addison, TX 75001: Reliably commute or planning to relocate before starting work (Required)
Work Location: Hybrid remote in Addison, TX 75001",$75.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2010.0,$1 to $5 million (USD)
Atika Technologies,,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location","$125,000 /yr (est.)",,,,,,
"spar information systems
3.5",3.5,Remote,Azure Data Engineer,"Role: Sr Azure Data Engineer
Location: Remote
Duration: 3 Months Contract to hire Full Time (W2 Only)
Must have 11+ IT Experience
Required Skills:
3+ years of experience in data software development, programming languages and developing with big data technologies
2+ years of experience designing and building on existing and new data applications
2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework
1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express.
Thanks & Regards,
Arvind Kumar Bind
Cell: 732 716 7403 (Text)
Direct Number:- 469-750-0607
Email : Arvind.B@sparinfosys.com
Job Types: Full-time, Contract, Permanent
Pay: $120,555.79 - $150,000.00 per year
Benefits:
401(k)
Health insurance
Paid time off
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Big data: 3 years (Required)
SQL: 1 year (Required)
Data lake: 3 years (Preferred)
Work Location: Remote","$135,278 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012.0,$25 to $100 million (USD)
"Appsintegration INC
4.5",4.5,"California, KY",Data Engineer,"Role:- Data Engineer
Position:- Onsite (W2 position)
Experience:- 10+ Years
Responsibilities
Working closely with business teams to understand their requirements and develop reporting or analytic solutions.
Producing and automating the delivery of key metrics and KPIs to the business.
In some cases, this will focus on making data available and others will be developing full reports for end users.
Managing stakeholder demand and a backlog of reporting requirements and needs.
Actively working with users to understand data issues, tracing back data lineage, and helping the business put appropriate data cleansing and quality processes in place.
Managing secure access to data sets for the business.
Requirements
Bachelor's degree in Computer Science or equivalent.
Proficient in creating Data Factory pipelines for on-cloud ETL processing.
Experience implementing data pipelines using the latest technologies.
Experience developing using SQL, relational databases, and data warehousing technologies.
Extensive ETL experience.
Ability to scale systems in Cloud as per requirement.
Experience in handling structured and unstructured datasets.
Good to have experience in data modeling and Advanced SQL techniques.
Must-Have Skills: Azure Data Factory (ADF), SQL, Cloud-based ETL.
Good to Have Skills: Azure File Storage Services, Databricks, Azure Synapse.
you can reach me out on LinkedIn
here is my link :- linkedin.com/in/shravanthi-thakur-705a81231
Job Type: Contract
Salary: $42.00 - $45.00 per hour
Ability to commute/relocate:
California, KY 41007: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$43.50 /hr (est.),1001 to 5000 Employees,Company - Public,,,,Unknown / Non-Applicable
"DSMH LLC
5.0",5.0,Remote,"Data Engineer (AWS, Python)","Title: Data Engineer
Duration: 12+ Months
Technical Skills
(Required)
Four years or more of experience in data engineering and/or software
development.
Three years or more of experience with development, operations, or
architecture in AWS using Python.
Experience with development and delivery of microservices using serverless
AWS services (S3, RDS, Aurora, DynamoDB, Lambda, SNS, SQS, Kinesis,
IAM)
Strong competency in testing, including unit testing to coverage standards and
integration testing.
(Desired)
Software development experience with object-oriented development and design
patterns
Experience with serverless design in AWS
AWS technical certifications (Developer Associate, Solution Architect Associate)
Familiarity with machine learning and data design to support machine learning
Experience with productionizing machine learning workloads
Experience with ElasticSearch/ELK, Kibana, Grafana, and/or Prometheus
Experience with OpenTelemetry
Soft Skills
(Required)
Verbal and written communication skills, problem solving skills, customer
service and interpersonal skills.
Basic ability to work independently and manage one’s time.
(Desired)
Ability to work collaboratively in a complex, rapidly changing, and culturally
diverse environment.
Ability to clearly communicate complex technical ideas.
Comfortable working in a dynamic environment where digital is still evolving as
a core offering.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
AWS: 5 years (Required)
Python: 5 years (Required)
Work Location: Remote",$52.50 /hr (est.),1 to 50 Employees,Company - Private,,,,$1 to $5 million (USD)
"Apolis
3.9",3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",$65.00 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996.0,$25 to $100 million (USD)
Fracsys Inc,,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person","$96,337 /yr (est.)",,,,,,
"Fulcrum Analytics
4.3",4.3,Remote,Data Engineer,"At Fulcrum, we provide cutting-edge data science consulting and software solutions to help companies solve challenging business problems. Through the utilization of our innovative products and dedicated team, clients are able to operate more efficiently, manage risk, and make informed business decisions every day. Fulcrum Analytics has stood at the forefront of data, analytics, and innovative software for over 25 years, offering sophisticated solutions, groundbreaking technologies, and winning strategies that help companies achieve their targeted results.
Role Description:
We are seeking a highly-skilled data engineer with experience in data analytics and data science. The ideal candidate will be responsible for building and maintaining data pipelines, designing and implementing databases and data models, and developing data-driven solutions to drive business growth for our clients.
Responsibilities:
Design, build, and maintain efficient, scalable, and reliable data pipelines using SQL, or programming languages such as Python or Java.
Develop and maintain databases and data models to meet clients’ analytical needs.
Identify data quality issues and develop processes to ensure data accuracy, completeness, and consistency.
Work with cross-functional teams to define and implement data integration strategies for new and existing systems.
Develop and maintain automated testing and deployment of analytics code and data pipelines.
Collaborate with data scientists to develop and implement machine learning models, statistical models, and other advanced analytics techniques.
Communicate analytical results and recommendations to technical and non-technical stakeholders through presentations and visualizations.
Stay up to date with emerging technologies and industry best practices, and recommend tools and processes that can improve data engineering and analytics processes.
Qualifications:
Bachelor's or master's degree in computer science, statistics, data science, or a related field.
2+ years of experience in data engineering, data analytics, and data science.
Strong programming skills in Python, with experience in building data pipelines using frameworks such as Airflow, Glue, Lambda, Athena, and BigQuery as well as data platforms such as Hadoop, Snowflake, and GCP.
Experience working with SOAP or RESTful web services.
Familiarity with version control, in particular Git.
Experience in designing, building, and optimizing relational and NoSQL databases.
Familiarity with data visualization tools, such as Tableau or Power BI.
Ability to understand and transform complex data into actionable insights.
Strong analytical and problem-solving skills, with a focus on delivering practical solutions to business challenges.
Ability to learn and solve problems independently.
Excellent communication and collaboration skills, with the ability to explain complex concepts to both technical and non-technical audiences.
If you are passionate about data analytics and want to work in a dynamic environment with talented colleagues, we encourage you to apply for this role. We offer competitive salary and benefits packages, as well as opportunities for professional growth and development.


Position Type: Full-time salaried
Reports to: Senior Data Engineer
Base Compensation: $110,000 - $140,000
Benefits: Medical, Dental, Vision, 401k with Match, Bonus, Unlimited PTO
Location: Remote within the United States of America; Eastern time zone preferred","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,1993.0,$5 to $25 million (USD)
"Take Command Health
4.9",4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014.0,Unknown / Non-Applicable
"Intersec
4.5",4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",$72.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004.0,Unknown / Non-Applicable
"Cureatr
4.1",4.1,Remote,Data Engineer,"About Us
We are a thriving, medication management solutions company, headquartered in Manhattan, but largely virtually distributed, committed to tackling the $528B suboptimal medication management problem. Patient medication information is often incomplete for healthcare providers and systems – it’s our mission to fill in the gaps. We provide real-time, universal access to accurate medication data for over 265 million patients and high-quality interventions by board certified telepharmacists. Through our solutions, healthcare providers can reduce preventable hospitalizations, avoid readmissions, and ultimately, improve patient outcomes. We’re already used by dozens of hospital systems, with tens of thousands of clinicians and supporting staff using us to care for millions of patients.
Here at Cureatr, we are driven by a core set of values and strive to incorporate them into everything we do. We care deeply about our users, patients, and employees, and we aim to live and operate compassionately. We take pride in creating a diverse, inclusive, and equitable company culture, where our open and honest approach to work fosters innovation and encourages employees to achieve professional and personal growth.

The Role
We’re looking for a Data Engineer to join our growing Engineering team. Use the power of healthcare data to find insights that will demonstrate the value Cureatr is delivering to our customers, help improve patient outcomes, and expand the scope of Cureatr’s mission. As a Data Engineer at Cureatr, you will be responsible for designing, implementing and maintaining our data infrastructure, pipelines, automation, and ETL processes. You will work closely with our Data Science and Analytics teams to ensure that our data is clean, accurate, and readily available for analysis. You will also be responsible for ensuring that our data meets industry standards for security, privacy, and compliance.
This is a remote, work from home position, but candidates must be located within the U.S. Ideal candidates must be able to travel to our New York City office, or TBD company-wide offsite locations, up to 4x per year.

You’ll be part of a team working to solve problems like:
Increasing the number of patients who receive the opportunity to do a medication reconciliation, which can help keep them out of the hospital.
Streamlining and automating processes so that our pharmacists can serve as many patients as possible.
Predicting patient events likely to occur so that our clinical team can adequately prevent them before it’s too late.
Helping our pharmacists make the most of data and insights that will help them be as effective as possible in serving patients.
Evaluating the effectiveness of our clinical services impact at the population level.

What You’ll Do:
Design, implement, and maintain our data infrastructure, pipelines, automation, and ETL processes to support our data needs.
Build and maintain scalable data models and databases.
Collaborate with Data Science and Analytics teams to understand their data needs and provide data solutions accordingly.
Ensure data quality and integrity by implementing data validation and cleansing processes.
Ensure data security and compliance with healthcare regulations.
Monitor and optimize data performance and scalability.
Design and implement data governance policies and procedures.
Evaluate and implement new data technologies and tools to improve our data capabilities.

Your Skills:
3+ years of experience as a Data Engineer or similar role.
Strong programming skills in languages such as Python and SQL.
Experience using git to track changes.
Experience with data modeling and database design.
Experience with the “modern data stack” preferred
Experience with ETL and data pipeline development.
Knowledge of healthcare data and related standards (e.g., HL7, FHIR) is a plus.
Familiarity with AWS.
Strong problem-solving and analytical skills.
Excellent communication and collaboration skills.

Technology Stack:
Python
RedShift
dbt
Dagster
MongoDB
Looker
AWS

Educational Requirement: Bachelor’s Degree or relevant work experience

Salary: $110,000 - $140,000 plus full benefits

Why work with us?
Make a measurable impact on patients’ lives
Be part of a company dedicated to living its core values:
Empathy
Honesty
Integrity
Impact
Fun
Equity in Healthcare
Work in the company without micromanagement
Share your vision and make an impact on the product
Opportunities for career growth
Remote work
Competitive salary
Premium benefits (Health, Dental, Vision, 401k, unlimited PTO, more)

Equal Employment Opportunity Policy
Cureatr is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, pregnancy, race, religion or religious creed, color, gender, gender identity, gender expression, national origin, ancestry, physical or mental disability, medical condition, genetic information, marital status, registered domestic partner status, age, sexual orientation, military or veteran status, protected veteran status, or any other basis protected by federal, state, local law, ordinance, or regulation and will not be discriminated against on these bases
Disclaimer: we are aware of a scam targeting applicants using a cureatrcareers.com email address. Please block and report this sender, as they are not affiliated with Cureatr. If you have any questions, please contact us at jobs@cureatr.com. Thank you!
uBajh3OfeT","$125,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2012.0,Less than $1 million (USD)
"SECURE RPO
4.3",4.3,Manhattan,Senior data engineer,"Must have skills:
8+ years of experience building high performance scalable enterpris`e analytics or data centric solutions
8+ or 5+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines
At least 5 years of experience implementing complex ETL pipelines preferably in connection with Glue and/or Spark
Exceptional coding and design skills in Python or Java/Scala
Hands-on experience with AWS (i.e. Glue, Aurora Postgres, Lambda, EMR, EKS, Redshift, etc.)
Experience with visualization tools like QuickSight, PowerBI, Looker or Tableau
Experience with Talend (ETL) is a big plus
Roles and Responsibility:
Drive a high impact and high visibility project that enables data availability, encompasses data analytics, machine learning, and petabyte scale datasets, and provides reliable and timely access to thousands of data sources
Design, architect and support systems for collecting, storing, and analyzing data at scale
Recommend improvements and modifications on new and existing data and ETL pipelines. Create optimal data pipeline architecture and systems using Apache Airflow
Create data analytics for d ata scientists to innovate, build and optimize our ecosystem
Assemble large, complex data sets that meet functional and non-functional business requirements
Be responsible for ingesting data into our data lake and providing frameworks and services for operating on that data including the use of Spark
Analyze, debug and correct issues with data pipelines
Operate on or build solution required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, AWS and Spark technologies
Job Type: Full-time
Salary: $56.80 - $80.16 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 7 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: On the road",$68.48 /hr (est.),1 to 50 Employees,Unknown,,,,Unknown / Non-Applicable
Atalan Tech,,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote","$119,526 /yr (est.)",,,,,,
Radiant System,,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",,,,,,,
"Integrated Technology Strategies, Inc.
4.0",4.0,"New York, NY",Data Engineer,"Integrated Technology Strategies is a provider of information technology consulting. Digital transformation, improving business performance, developing strategies and enhancing value is at the core of what we are.

Integrated Technology Strategies is looking to hire a Data Engineer to be a part of our Consulting team.

We are looking for someone passionate about data is focused on developing the platform for critical data products, including real time business metrics and analytics capabilities. The role requires supporting and collaborating with groups including Data Analytics/BI/Product, as well as our core backend API team. The individual will not be afraid to think out of the box and will play a key role in technical decision making. We are highly focused on giving ownership and responsibility to autonomous teams, using the right tools for the job, and building flexible architectures.

Responsibility:
Continue to evolve the internal Reporting and Analytics platform on top of Snowflake on AWS infrastructure.
Experience in Architect, design and implementing scalable ETL and data processing systems to handle the big data ecosystem including data collection, processing, ETL and Data warehouse.
Build soft real time capabilities and insight into product metrics to help product managers and BI/Analytics understand and optimize product features and guide product decisions.
Participate and contribute to the capabilities and engineering priorities across the organization.
Contribute to the codebase and participate in code review.
Build analytics tools that utilize the data pipeline to
provide actionable insights into operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product,
Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Reporting to the Senior Director, Software Engineering, you’ll be responsible for overseeing engineering product quality and delivery and setting and overseeing technical standards for teams who are working on everything from customer-facing applications.

Skill Requirements:
Solid understanding of real time data processing with Kafka, Spark and Flink and batch data processing frameworks on EMR and Snowflake.
Passion for building world-class data platforms that support a global customer base
Solid engineering background and understanding of programming languages such as Python, Java or equivalent
5+ years of progressive experience in data infrastructure development, with a track record of successful high-quality deliveries
Experience of working in an agile environment and embracing engineering best practices
Ability to apply both technical competence and interpersonal skills to achieve business outcomes
High emotional intelligence, sound temperament, and professional attitude
Strong understanding of SQL, experience with key databases such as Snowflake, MS-SQL and Postgres
Knowledge of the internals of how database systems work to design models for varied use cases.
Experience with CI and CD in an AWS environment with Terraforms
Experience with key Data technologies, such as Sqoop. Kafka will be a plus
Proven experience in building secure data platforms
Bachelor’s degree in Computer Science or equivalent","$110,128 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007.0,$5 to $25 million (USD)
"Egrove Systems Corporation
4.4",4.4,"Dallas, TX",Data Engineer Flink,"Java, Bigdata, Azure & Flink
Flink Sr. Developer, who has implemented and dealt with failure scenarios of processing data through Flink.
A senior who can help his Flink development team, guiding and helping them implement custom solutions through Flink.
Seasoned Java developer who knows about all aspects of SDLC.
Worked on integrations of other technologies with Flink, eg: Kafka, MongoDB, etc
Azure experience in the areas of Messaging, Data processing, preferably on Flink or on Databricks.
Job Type: Contract
Salary: From $51.83 per hour
Compensation package:
Monthly bonus
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
Work Location: On the road
Speak with the employer
+91 732)-860-7733",$51.83 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,,Unknown / Non-Applicable
DocsInk LLC,,"Wrightsville Beach, NC",Data Engineer,"DocsInk is on a mission to provide a collaborative and exciting alternative to the traditional healthcare model. Through the complete virtualization of healthcare organizations, we are equipping providers with the ability to deliver better patient care with speed, efficiency, and accuracy. With the widespread focus on providing value-based care, DocsInk is making it possible to provide superior care while also maximizing reimbursements. In today’s environment, shifting the way we approach healthcare is essential and DocsInk is on the cutting edge of where the industry is headed. We are excited to be growing our team and are looking for driven individuals with a passion for making a difference in the healthcare industry.

As our Data Engineer, you will be in the driver’s seat as our team creates elegant and forward-thinking software solutions for our healthcare customers. You will work with fellow developers, designers, training & support, quality assurance, and management in a fast-paced environment to take our products to the next level and exceed expectations. We are looking for someone who is incredibly talented to do what most cannot and yet humble enough to easily work with peers, accept guidance from more experienced staff, and mentor more junior staff. If designing and developing complex integration efforts is what gets you out of bed each morning, then we want you to help us reshape healthcare.

People are our most important asset and the number one reason we all love working at DocsInk. As a team, we value accountability, transparency and collaboration. We have agile teams, with clear, outcomes-focused goals. Delivering innovative solutions and superior service are at the forefront of what we do, and the collective ideas of the team allow us to continue to excel in a fun and energetic environment.

Professional Requirements and Responsibilities:

Role / Responsibilities:

Implement robust ETL solutions that integrate healthcare data feeds using tools such as Apache NiFi, NextGen Connect (JavaScript), PostgreSQL, MySQL, and Linux shell scripting
Configuring and managing deployments to AWS infrastructure consisting of both traditional servers and serverless components. CI/CD tools include Bitbucket and AWS Code Deploy
Building direct interfaces with hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing interfaces that consume formats such as XML, JSON, and delimited files
Utilizing APIs and web services to permit bi-directional flows of data to and from our partner organizations
Monitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAs
Writing PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologies
Maintaining data standards (including adherence to HIPAA), documenting work, and championing process improvements
Work as a project team member to jointly deliver high-quality business solutions consistent with project objectives and constraints

Knowledge and Skills Required:

4+ years experience with HL7 2.x
2+ years experience with Apache Nifi
2+ years experience with NextGen Connect
Advanced knowledge of PostgreSQL, MySQL, AWS Code Deploy, HL7 2.x, C-CDA, FHIR, and HIPAA data compliance

Qualifications and Other Attributes Required:

Degree in Computer Science or equivalent field, or equivalent work experience
Highly self-motivated team player who takes initiative
Strong sense of personal accountability; contributes beyond job role and responsibilities
Excellent problem-solving skills; meticulous & methodical","$85,000 /yr (est.)",,,,,,
"N9 it solutions
4.6",4.6,Remote,Data Engineer,"Job title: Data Engineer
Visa's: CPT, OPT-EAD, H1B transfer
Employment: W2 position ( should be ok with Marketing)
Location: Hybrid Or Remote
(Authorized to work anywhere in the USA and for only those who are staying in the USA)
It's a long term project
Job description
Data Engineer (primary technologies: Azure Data Factory, Synapse, Synapse Pipelines, ADLS Gen 2, understanding of DataWarehouse concepts, ELT, Azure DevOps, Azure resource groups)
Develop architectural strategies for data modeling, design and implementation to meet stated requirements for metadata management, operational data stores and Extract Transform Load environments
Work with business leaders and teams to collect and translate information requirements into data to develop data-centric solutions
Apply industry-accepted data architecture principles and standards for modeling, stored procedures, replication, regulations, and security, among others, to meet technical and business goals.
Work to streamline data flows and models; improve consistency, quality, accessibility, and security; unify data architecture; remove unnecessary costs; and, optimize database activity across company needs.
Analyze and understand Data sources & APIs
Design and Develop methods to connect & collect data from different data sources
Design and Develop methods to filter/cleanse the data
Work closely with Data Scientists to ensure the source data is aggregated and cleansed
Work with Cloud and Data architects to define robust architecture in cloud setup pipelines and workflows
Benefits:
H1B and GC filling
Free training and Placement
E-verified
On-job technical support
Guesthouse facilities are also available
Skill Enhancement
Opportunity to work with Fortune 500 Companies
Job Type: Full-time
Salary: $40.26 - $56.00 per hour
Experience level:
6 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91",$48.13 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2016.0,$5 to $25 million (USD)
Staff Bees Solutions,,"Dallas, TX",Data Engineer,"We are looking for OPT/CPT individuals, Helping Them to Train, Providing knowledge, and Placing Them in Fortune companies with the help of our Direct Clients in Big Data, Machine Learning, And Data Engineering Suitable Positions. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics..
Qualifications for Data Engineer
An individual who has valid visa, Opt/Cpt are Applicable
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong project management and organizational skills.
Job Types: Full-time, Part-time, Contract
Salary: $70,000.00 - $80,000.00 per year
Benefits:
Employee assistance program
Health insurance
Professional development assistance
Relocation assistance
Compensation package:
Yearly pay
Experience level:
1 year
No experience needed
Under 1 year
Schedule:
10 hour shift
4 hour shift
8 hour shift
Monday to Friday
Ability to commute/relocate:
Dallas, TX 75243: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$75,000 /yr (est.)",,,,,,
"Quadrant Resource
4.1",4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004.0,Unknown / Non-Applicable
"Gridiron IT
4.5",4.5,Remote,Data Engineer,"GridironIT is seeking a Data Engineer.
Responsibilities:
We are looking for seasoned Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications:
US Citizen Only
Ability to hold a position of public trust with the US government.
8+ years industry experience coding commercial software and a passion for solving complex problems.
8+ years direct experience in Data Engineering with experience in tools such as:Big data tools: Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra
Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
AWS cloud services: EC2, EMR, RDS, Redshift
Data streaming systems: Storm, Spark-Streaming, etc.
Search tools: Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Amazon S3, Athena, Redshift Spectrum, AWS Glue, AWS Glue Catalog, AWS Functions, and Amazon EC2 with SQL Server Developer
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Experience with Informix and Data Stage
Job Type: Full-time
Pay: $140,000.00 - $150,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Hourly pay
Yearly pay
Experience level:
10 years
11+ years
7 years
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Due to the nature of the role, US Citizenship is required. Do you possess US Citizenship?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 7 years (Required)
SQL: 7 years (Required)
AWS: 4 years (Required)
Big data: 5 years (Preferred)
NoSQL: 5 years (Preferred)
Work Location: Remote","$145,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017.0,Unknown / Non-Applicable
"Kastech Software Solutions Group
4.0",4.0,"Dallas, TX",Data Engineer,"In KastechSSG we are are seeking for a Cloud DB Platform Engineer with Data Enablement Services experience ($50/hour), to work in a Hybrid Schedule (3 days on-site, 2 days home office) for one of our partner companies!
Hybrid Schedule info: onsite 3 days per week from list of locations: Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ - Must be local to one of these locations.
** Contract would be $50 per hour on W2 (18 months). American Citizens or GC Holders Only**
3-5 years experience in:
Terraform, Azure/GCP, DevOps Engineering, ETL, Client/AI, Data lake, Data architecture.
GCP Key Services Experience (ie Big Query, Composer, Vertex AI, DataProc, Data Catalog, Notebooks).
Azure Key Service Experience (ie Synapse, Purview, Data Lake, Logic Apps, Cognitive Services).
Job Type: Full-time
Salary: $50.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you American? If not, please specify your visa type
Would you be willing to work under W2?
Experience:
Terraform: 3 years (Preferred)
Azure Data Lake: 3 years (Preferred)
GCP Key Service Exp: 3 years (Preferred)
DevOps: 3 years (Preferred)
Work Location: In person",$50.00 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2008.0,$25 to $100 million (USD)
"Virtualan Software LLC
4.5",4.5,"Atlanta, GA","Sr. Data Engineer (Python,Spark & Databricks)","Role : Senior Data Engineer (Python, Spark & Databricks)
Location : Hybrid (Atlanta, GA)
Pay: 60-70$ on C2C / 1099
RESPONSIBILITIES:
Create or modify the conceptual, logical and physical data models.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources like Hadoop, Spark, AWS Lambda, etc.
Lead and/or mentor a small team of data engineers.
Design, develop, test, deploy, maintain and improve data integration pipeline.
Develop pipeline objects using Apache Spark , PySpark and/or Python.
Design and develop data pipeline architectures using Spark and related AWS Services.
Communicate effectively with client leadership and business stakeholders.
Participate in proposal and/or SOW development.
REQUIREMENTS:
5+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
2+ years of experience migrating/developing data solutions in the AWS cloud is required.
1+ years of experience building/implementing data pipelines using DataBricks is required.
Expert level knowledge of using SQL to write complex, highly-optimized queries across large volumes of data.
Hands-on object-oriented programming experience using Python is required.
Professional work experience building real-time data streams using Spark.
Knowledge or experience in architectural best practices in building data lakes.
Bachelor or Master degree in Computer Science, Engineering, Information Systems or relevant degree.
Hybrid work schedule, and must live in the Atlanta, GA metro area. Must be open to up to 25% national travel to client locations when on engagements outside of the Atlanta, GA area.
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Apply only if you are interested in Hybrid Role in Atlanta, GA. Interested ?
Experience:
Data Engineer: 4 years (Required)
Databricks: 1 year (Required)
Willingness to travel:
25% (Required)
Work Location: On the road",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2019.0,Less than $1 million (USD)
"GOBankingRates
3.3",3.3,"North, SC",Staff Data Engineer,"GOBankingRates™ is unique in the digital marketing and media industry - we combine marketing, digital, content and fintech. Our performance based approach increases brand awareness and generates targeted audience engagement on our internal web properties and partner sites.
Learn More About What We Do

What's interesting about this role?
GOBankingRates has big growth plans ahead and is looking for a strong Staff Data Engineer experienced in Data warehousing and Python to join our Data Engineering team. The GOBankingRates Data Engineering team is responsible for designing and developing the Data lake, enterprise database, data warehouse, reporting solutions, and pipelines for data processing. If you are a critical thinker with a solid track record of developing data solutions and solving complex problems with SQL and Python, we want you to join our team! You will play a vital role in designing and developing our next generation data pipelines and data platform. Join our team and prototype new data product ideas and concepts!
How will you make an impact?
Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by external users and internal teams.
Optimize by building tools to evaluate and automatically monitor data quality and develop automated scheduling, testing, and distribution of feeds.
Work with data engineers, data scientists, and product managers to design, rapid prototype, and productize new data product ideas and capabilities.
Design and build cloud-based data lakes and data warehouses.
Conquer complex problems by finding new ways to solve them with simple, efficient approaches focusing on our platforms' reliability, scalability, quality, and cost.
Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions.
What will you bring to us?
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Experience with dimensional data modeling and schema design in a database or data warehouse
Expertise with scripting languages such as Python and writing efficient and optimized SQL.
Working experience in building data warehouses and data lakes.
Experience working directly with data analytics to bridge business requirements with data engineering.
Experience with AWS infrastructure
Ability to operate in an agile, entrepreneurial start-up environment and prioritize
Excellent communication and teamwork, and a passion for learning
Curiosity and passion for data, visualization, and solving problems
Willingness to question the validity, accuracy of data, and assumptions
Preferred Qualifications:
Experience building data warehouse, data lake, and data pipeline using Snowflake/Redshift and other AWS Technologies.
Experience with large-scale distributed systems with large datasets.
Experience with event streams and stream processing (e.g., Kafka, Spark, Kinesis)
Hands-on experience with event streaming with modern event streaming tools like Pulsar, Kafka, and Kinesis. Understanding when streaming vs. batch processing is appropriate, and tradeoffs in a given context
Knowledge of advertising platforms.

Benefits
Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity and passion.
Awesome medical, dental and vision plans with heavy employer contribution
Paid maternity leave and paternity leave programs
Paid vacation, sick days and holidays
Company funding for outside classes and conferences to help you improve your skills
Contribution to student loan debt payments after the first year of employment
401(k) - employees can start contributing immediately. After the first year, GOBankingRates matches your contribution up to 4% of your salary
A note about our response to COVID -19 and our new norm: The world has changed and we know it's important to adapt and to do our part to take care of our teams in this global pandemic. Our number one priority is to have our team feel safe, balanced and connected. We're committed to providing our teams with the best resources and tools to navigate this new virtual world that we're living in. We've also reinvented the ways in which we recognize, celebrate, and engage with each other to keep our culture strong!
Here's a peek into our world at GOBankingRates -
Our teams are working remotely 100% for the foreseeable future and have flex time. We're in the digital media space so we're mobile and flexible!
Option to work from an office (if you need to get away!)
Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)
To keep our community engaged and connected, virtual team building events are held weekly and monthly.
For wellness and balance, weekly virtual fitness classes such as yoga are available.
To care for the local communities that we're a part of across the U.S our team members host socially distanced philanthropic events every quarter.
And most importantly, we've committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.
We are an equal-opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.","$134,519 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,,$25 to $100 million (USD)
"MoneyDolly
5.0",5.0,"Salt Lake City, UT",General Data Engineer,"Position Title: General Data Engineer
Location: Remote (US Based only)
Commitment: Full-Time

Job Brief

About Us
MoneyDolly is a fast growing Fintech Saas tech company, headquartered in Sandy, Utah. We are leading innovation in supporter relationship management, wherein teams meet all their fundraising goals. Teams can create their own page for your supporters to visit, offer products and incentives for their contributions, then simply invite their team to spread the word and watch the money roll in. MoneyDolly has already helped thousands of organizations, groups and teams nationwide raise over $20 million. We are still private, and our best work is still ahead of us. This is a massive industry with antiquated methods and no clear market leader. This is the spot for a qualified team player looking to build something new, make a real impact, and actually change the world.

Job Description:
We are looking for a highly motivated and talented Generalist Data Engineer to join our development team. The ideal candidate will be someone who loves to wear multiple hats and thrives in an environment of high autonomy to accomplish our business goals. You will be responsible for collaborating with our business team, creating flexible data models, and maintaining data pipelines to ensure the smooth flow of data in our organization.
Responsibilities:
Interact with our business team to gather requirements and understand data needs
Design and create flexible data models that allow for easy report generation and ad-hoc analysis
Write and maintain DBT transformations to generate flexible data models from our production database
Develop and maintain required data pipelines in Python to collect and process data from various sources
Develop and maintain Tableau and Hex.Tech dashboards to analyze the collected data
Ensure data quality, integrity, and security in all data processes
Collaborate with other team members to implement data solutions and integrate them into the existing infrastructure
Continuously monitor and optimize data models and pipelines to meet changing business requirements
Requirements:
Bachelor's degree in Computer Science, Engineering, or a related field
Strong experience in data modeling, ETL processes, and data pipeline development
Proficiency in Python and SQL
Familiarity with DBT transformations and best practices
Experience working with relational databases and big data technologies
Excellent problem-solving skills and attention to detail
Strong communication and collaboration skills
Ability to work independently and adapt to a fast-paced, dynamic environment
Nice to have:
Experience with data visualization tools (e.g., Tableau, Power BI)
Experience with AWS and GCP (RDS Postgres and BigQuery are our current stack)
Knowledge of the fundraising industry or a strong interest in learning more about it
What We Offer:
A competitive salary ($100k-$140k range) and benefits package
A supportive and collaborative work environment
Opportunities for professional growth and development
The chance to make a significant impact in a growing startup
If you are passionate about data engineering and excited about the prospect of revolutionizing the fundraising industry, we'd love to hear from you. Apply now to join the MoneyDolly team!","$120,000 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"Tipico - North America
4.0",4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus","$120,000 /yr (est.)",201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004.0,Unknown / Non-Applicable
National Group Corporation,,"Dallas, TX",Data Engineer,"Who are we?
ReAlign Insurance Holdings is a leading primary underwriter of program administrator managed specialty insurance programs in the U.S. With the extensive knowledge and experience our team has in building and managing carriers and programs we are able to reach decisions on program opportunities quicker than our competition.
Why choose ReAlign?
At ReAlign, we believe that we are only as strong as our people. We strive to create an inclusive and welcoming culture where employees feel comfortable and empowered to be themselves. We support a diverse and inclusive work environment, with a focus on a healthy work-life balance. We understand that comprehensive benefits are important which is why we offer a benefit package that includes: medical, dental, vision, company paid short and long-term disability, life insurance, health spending account, 401(k) match every pay period, PTO, paid holidays, floating holidays, tuition reimbursement, and referral bonuses.
Job Summary
The Data Engineer position will involve data exchange process analysis, design, development, unit testing, and support. This role would work with other data team members on database ETL processes and reporting needs in both the on-premise database environment as well as the Azure data environment. This position requires strong technical and analytical skills to review business processes and data, recommend solutions, conduct functional and code analysis, and lead and perform database development tasks. This position will also involve production support and require experience within a DevOps environment.
Essential Functions
· Design, development, and support data ETL processing for both external and internal customers
· Support the internal and Azure cloud-based data warehouse platforms
· Work with DBA/System Admin to develop and maintain automated data processing
· Design and analyze queries and stored procedures
· Write and support SSRS and PowerBI reports
· Work in a data development life cycle environment
Work with stakeholders and data analysts to understand business requirements
Work with end users and the testing team to perform unit, functional, and integration testing and validation prior to production implementation
Troubleshoot and resolve operational problems with a focus on delivery
· Communicate solutions and architectural design to IT leadership
· Work in both Kanban and Scrum Agile environments
· Continually improve processes and solutions, seeking ways to use new tools, features, and capabilities
Job Requirements
3-10 years of experience as a Database Developer
Highly proficient in writing advanced SQL queries, stored procedures, SSRS reports, and ETL processes
Strong knowledge of relational and transactional database architecture
Strong technical and analytical skills
Experience with the Microsoft Azure and PowerBI platforms and Agile development methodology a plus
Bachelor’s Degree in computer science, database development, or related field preferred
Detail oriented with the objective of delivering a solution to a problem in a timely and efficient manner
Self-driven, willing to take initiative when an opportunity presents itself
Strong collaboration and communication skills and the ability to work in a team environment a must
Ability and willingness to work in a partial on-prem and partial remote office environment
Experience in the Property and Casualty Insurance industry a plus
Proficient with standard desktop software used in a business environment (MS Word, Excel, Visio, etc.)
Must be able to work in a hybrid in-office and remote work environment
Must be authorized to work in the United States
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you currently local to the Dallas area and able to commute to the office 2 to 3 times per week? (Deal breaker)
Experience:
writing advanced SQL queries and ETL processes: 3 years (Required)
cloud database platform (i.e. Azure, AWS): 1 year (Preferred)
Azure Data Factory: 1 year (Preferred)
Work Location: In person","$107,500 /yr (est.)",,,,,,
"Stonehenge Technology Labs
5.0",5.0,"Bentonville, AR",Data Engineer (ETL),"At Stonehenge Technology Labs, we help leading CPG teams win in the omni-commerce space by bringing together data that powers insights and drives autonomous actions. The faster STOPWATCH™ Members identify opportunities and command gaps across their total online and in-store assortment, the faster they win! Learn more here: https://stopwatch.tech
The team at Stonehenge Technology Labs operates off three distinct principles: 1) Empower Unique Talent; 2) Operate as a World-Class Team; 3) Solve Complex Problems with Speed & Simplicity. Stonehenge recruits, employs, trains, compensates, and promotes regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Visa sponsorship is not available for this role.
Purpose:
As the Data Engineer at STOPWATCH™, you will be a data engineering powerhouse for the organization, tasked with pulling in data from a variety of sources, storing them in their raw format for long term accessibility, and then ingesting and transforming the data for easy consumption by teams downstream.
You will:
· Develop, monitor, maintain, optimize, and orchestrate end-to-end data pipelines while leveraging best practices.
· Develop data solutions to provide relevant, timely, and insightful information supporting both upstream and downstream teams.
· Leverage APIs to their full potential and for the betterment of our platform offerings.
· You may be tasked with deploying cloud resources as needed within the processes and procedures established.
· Leverage our stack, including but not limited to Databricks and SQL, in the most optimal way, making recommendations and executing on them in close partnership with the Lead Data Engineer and other functions.
· Take on projects hands-on with a high level of professionalism and varying levels autonomy.
· Be able to independently make decisions around tradeoffs between different methods for building pipelines and data extraction, with a core emphasis on data safety, cost optimization, and data accuracy.
· Partner closely with security engineer, cloud architect, product teams, and Power BI engineers to achieve business goals.
We expect you to operate with speed, simplicity, and store your work on the group system with a focus on process mapping and documentation enabling rapid scale and interconnected efficiencies between work groups.
As part of the application process, all candidates are required to complete a skills test.
SKILLSET/EXPERIENCES:
· Expert level understanding data engineering, specifically in end-to-end pipeline solutions is a must
· 2+ Years of data engineering experience is a must
· 2+ Years of cloud experience, preferably in an Azure environment is a must
· 2+ Years of SQL experience is a must
· 1+ Years of Python or Scala experience is a must
· 1+ Years of owning data pipeline orchestration is a must
· 1+ Years of DevOps experience or comparable is a must
· 1+ Years of Databricks experience is a must
· 1+ Years of experience with Azure Data Factory or equivalent is a plus
· Experience working with Typescript/Node.js is a plus
· Extreme attention to detail including self-monitoring QA
· Penchant for documentation, reproducibility, and standardization especially in terms of standard data model matrices
· Familiarity with retail/retailer/supply chain data is a plus; experience working for a major retailer or CPG brand even bigger plus
· Scrappy mindset; resourceful and relentless in finding answers
This role (and every role at Stonehenge Technology Labs) is expected to read, notate and arrive ready to discuss THE GOAL by Eliyahu Goldratt on Day 1.
Here’s what we offer in addition to competitive base pay:
Stock incentives based on individual and team performance
5 weeks paid vacation, 11 paid holidays
Flexible (albeit intense) work schedule
100% Company Paid Health Care for Employee and Spouse/Partner
Available Dental and Vision Plans
Available 401K Program
Contribution matching to qualified 501c3 organization (team member’s choice),
Kid/pet/partner focused culture
Paid maternity/paternity leave and financial assistance in support of adoptive & fostering activities
Executive coaching (certain roles)
Job Type: Full-time
Pay: $110,000.00 - $135,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Health savings account
Paid time off
Parental leave
Retirement plan
Vision insurance
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you now or in the future require visa sponsorship to continue working in the United States?
Experience:
Databricks: 1 year (Required)
Python: 1 year (Required)
Data Engineering: 2 years (Required)
Cloud infrastructure: 2 years (Required)
SQL: 2 years (Required)
Work Location: Hybrid remote in Bentonville, AR 72712","$122,500 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2019.0,Unknown / Non-Applicable
,,,,,,,,,,,
"DataArt
4.6",4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997.0,$100 to $500 million (USD)
"GTA (Global Technology Associates)
4.6",4.6,"Plano, TX",Data Logging Engineer,"Looking for a Data Logging Engineer
What you will be doing as a Data Logging Engineer
· Perform system level log captures/analysis of several Network elements in the 4G/5G Network architecture to identify root causes of product and key performance issues.
· Document and maintain the troubleshooting steps including updating technical documentation for primary areas of responsibility
· Identify automation opportunities & implement scripts for log analysis, for discussions with external customers/internal customers
· Strong on customer support as well as data analysis of logs, etc.
· Must be able to drive for issue resolution with other Engineering team
· Initial analysis of Network KPI degradation to quantify and qualify the issue
· Be able to proactively perform Ad-hoc/periodic parameter audit, issue signature audit and SW/FW audit
· Ensure the latest ticket status ticket status is provided to the customers
· Software and patch upgrades on network element to support feature testing and issue resolution verification
· Provide 24X7 support as needed for commercial Network outages and network performance issues. Also provide technical reviews and provide support to customers as needed
What you will bring to the table as a Data Logging Engineer:
· Bachelor’s Degree in Telecommunication or Network Engineering with at least 2 years of experience in Wireless Telecom environment.
· Good knowledge of UNIX/LINUX operating systems
· Good Scripting skills using but not limited to Python, Shell or Perl
· Strong knowledge of MS office tools like Excel, power point and Word.
· Good analytical skills to investigate and evaluate trending data for issue resolution
· Good knowledge of 4G and 5G wireless network architecture concepts, RAN protocols and eNB interfaces S1-MME, S1-U is a plus
· Good knowledge of trouble ticket and knowledge management systems
· Willing to work during night to accommodate debugging involving offshore R&D teams
What you didn’t know about us:
· Competitive salary
· Health, Dental and Vision Benefits
· Short/Long Term Disability and Critical Care/Illness Protection
· Life Insurance and Retirement Plans
· Employee Assistance Program
· With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: From $50.00 per hour
Schedule:
8 hour shift
Work Location: In person",$50.00 /hr (est.),Unknown,Company - Private,,,,Unknown / Non-Applicable
"Staffbee Solutions Pvt Ltd
5.0",5.0,"Dallas, TX",Data Engineer,"Dear OPT/CPT candidates,
We currently hiring OPT/CPT Individuals , we also specialized in training ( data engineering course , How to handle the interviews etc...) and placement for Data Engineering positions, With many benefits like
Free accomodation untill you get free accomodation
100% placement
Modification of resumes according to the market standards
Best Package
H1b sponsership
Travel allowances
+1469 902 8976 (wapp/call)
Or share resume at vijay.komma@staffbees.com
Job Type: Full-time
Salary: $60,000.00 - $70,000.00 per year
Application Question(s):
This requirment is for only OPT/CPT visa status candidates , Do you aware?
Are you sure you have read the JD?
Work Location: One location","$65,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2018.0,Unknown / Non-Applicable
Vibrant Planet,,California,Data Engineer,"About Vibrant Planet
We are a team of leaders in science, forestry, policy, and tech, building a cloud-based, data-driven platform to increase the pace and scale of forest restoration and reduce catastrophic wildfire, tree mortality, forest degradation, and deforestation. Our current software modernizes land management planning and monitoring with AI-driven data development, user friendly scenario building and decision support, and forest resilience trends and treatment outcome detection. The system quantifies potential and actual treatment benefits, helping to grow markets for ecosystem services (carbon, water, biodiversity, sustainable forest-derived products).
Our initial platform (launched in September 2021) is focused on temperate, “fire adapted” landscapes in California and the Western US. Our mission is global; our roadmap expands into other geographies and forest types accordingly.
Driven by our sense of urgency to protect vital forests and the services they provide, we use our expertise to build sophisticated, AI/ML-driven, user friendly products to democratize the use of data and accelerate the process of stabilizing and restoring our forests.
Vibrant Planet is backed by climate and ecosystem resilience solutions leaders, including Grantham Foundation, Earthshot, Elemental Excelerator, Ecosystem Integrity Fund, Chris Cox (CPO at FB), Neil Hunt (ex CPO of Netflix), Cisco, Valia Ventures, and Halogen Ventures.
For further information please visit: VibrantPlanet.net
Equal Opportunity Employer: Vibrant Planet is committed to diversity. We encourage applicants from all cultures, races, colors, religions, sexes, national or regional origins, ages, disability status, sexual orientation, gender identity, military, or other status protected by law to apply.
We are most interested in finding the best candidate for the job, and that candidate may come from a less traditional background, but have capacity to grow into and thrive in the position after some mentoring. We do not require that you have experience with every job description task. We will consider any equivalent combination of knowledge, skills, education, and experience to meet minimum qualifications. We encourage each candidate to think broadly about their unique background and skill set and how it may relate to the role. This is important to us. We aren’t just saying this, we mean it.
For further information please visit: https://vibrantplanet.net
About the Role
We are looking for a versatile, hands-on data focused engineer to help us scale and build our geospatial pipelines and tooling. You will be part of a small team of engineers and scientists, tackling some of the most important climate change related issues facing the world today. This is a remote role (and company) so being comfortable and effective working in a distributed team is crucial.
Key Responsibilities:
Design, develop, scale, and maintain data pipelines that ingest, transform, and store large volumes of geospatial data from multiple sources.
Optimize data processing and storage performance and cost efficiency by leveraging cloud-based technologies and services.
Collaborate with engineers, scientists and other stakeholders to share knowledge and build expertise.
Lead and participate in development life cycle activities like design, coding, testing, and production release.
Contribute to our evolving engineering culture, standards, tooling, and processes.
Mentor and support other engineers and deeply review code.
Technical Qualifications
Strong software engineering skill set.
2+ years experience in data engineering or a related field.
Experience with distributed data processing frameworks and tools (e.g., Airflow, Hadoop, Spark).
Proficiency with Python or an equivalent language. Ability to write clean, high-quality code and tests to keep our system fast, reliable, and monitorable.
Bonus Qualifications - Absolutely not required, but nice to have
Airflow
Pandas / Geopandas
Jupyter notebooks
Geospatial processing
Lidar data
Satellite imagery
Experience with ArcGIS/QGIS
Other Expectations:
Strong communication skills; discussing complex technical concepts to engineers and non-engineers is no problem to you.
Collaborative and supportive team player, with a desire to enrich our engineering culture.
Eagerness to learn, think creatively, and share knowledge with others.
Ability to write understandable, testable code with an eye towards maintainability.
Proactive and empathetic mindset - you love to roll up your sleeves to fix problems
Location - We are a remote first company with the majority of the team in the US west coast time zone. We also have a small and growing presence in New Zealand. Location can be flexible, but we are primarily targeting those two time zones and anything in between.",,,,,,,
Siri info solutions inc,,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",$48.50 /hr (est.),,,,,,
"American Power & Gas
2.5",2.5,"Largo, FL",Data Engineer,"Because of expansive growth American Power & Gas is seeking a Data Engineer to add to our technical team. This is a fulltime permanent on-site role.
We have been offering Green Energy solutions to both residential and small commercial customers for over 20 years and have won the award for fastest growing company in the Tampa Bay Business Journal as well as being featured in Forbes and the Huffington Post.
**
**
Key Responsibilities
Support operational executives in solving business problems by designing, developing, troubleshooting, and implementing data driven solutions to complex technical objectives.
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Deploy sophisticated analytics programs, machine learning and statistical models to predict business outcomes and continually optimize performance through data science.
Gather and summarize technical requirements associated with strategic business initiatives.
Document, maintain, support and enhance company technology platforms and analytics applications.
Look for opportunities to streamline and automate various reporting processes to help support internal teams.
Assist with the research and development of internal business case studies to support onboarding new data tools.
Extract, manipulate and cleanse raw data from various data sources – call center, web, and CRM systems.
Work independently as well as collaboratively with other team members and key stakeholders as required to troubleshoot and resolve data issues.
Provide comprehensive technical and consultative services to support development and maintenance of internal and third-party platforms.
Create functional test cases/criteria to verify all functionality adheres to specifications and create end user manuals.
Routinely represent the Analytics department in cross-functional status & data strategy meetings.
Assist business analysts in provision of regular performance trends reporting, forecasts, and insights for marketing and sales team leaders as well as senior executive management to maintain a tight finger on the pulse of emerging performance trends and opportunities.
Partner with internal cross functional teams to identify business needs and analytics opportunities, developing tools and techniques to analyze and provide performance-improving recommendations.
Partner with the Operations team to optimize data workflows from sourcing to storage to reporting to deliverables to maximize for value-added and time-efficiency.
Develop, enhance, and manage various analytical solutions in support of business objectives.
Determine what data is needed and how to consume and store this data to support reporting needs and ad hoc performance-improving analysis for internal stakeholders.
Ensure deliverables are adapted properly to stakeholder audience; adapting terminology and visuals as needed to “speak the stakeholder’s language”, thus communicating with maximum effectiveness.
Troubleshoot and QA data, reporting, and tracking anomalies as needed, with proactive communication to stakeholders.
Relentlessly challenge the status quo. Always be critical of how we can be more effective or more efficient as an individual, as a team, and as a business.
Provide ongoing and proactive client service to your internal customers as required to continue elevating the performance of the business.
Regularly work with and analyze data across marketing channels and the customer journey through website analytics, call center activity, and CRM systems.
Requirements
University degree or college graduate in Engineering, Computer Science, Mathematics, Statistics, related technical/programming discipline, or the equivalent hands on experience in Data Engineering or Software Development.
Proficiency with coding in SQL is required. Ability to also write in Python, R, Java, or similar programming languages is preferred.
Strong technical prowess, including an understanding of algorithms, systems architecture and end user experience.
Experience with modern source, build, and deploy tools such as Git, Grub, Maven, Yeoman, etc. is a plus.
Ability to think unconventionally to derive innovative and creative solutions.
Competency in accurately estimating development timelines.
Experience with data warehouse design, relational databases, SQL/NoSQL data modeling, RESTful API standards and large scale data processing solutions.
Demonstrated skill in database development with solid understanding of schema design, stored procedure development, query optimization and ETL processes.
Excellent troubleshooting ability. Must be able to resolve issues tied to capturing and processing data in a timely manner.
Excellent English written and verbal communication skills, especially explaining technical concepts to non-technical business leaders.
Exceptional critical thinking and problem-solving skills; able to distill overall objectives into the actionable steps required to achieve those objectives.
Capable of effectively managing projects, priorities, timelines, and working relationships.
Must possess the intellectual curiosity to succeed in a dynamic, entrepreneurial, fast paced, sales driven organizational culture.
Excited by the opportunity to disrupt the status quo and uncover the eureka moment insights that will take the business to the next level – proactively searching for problems to solve, knowing there is so much to learn.
We offer Health, Dental, Optical and Life Insurance, PTO (paid time off) and the opportunity for promotions and room to advance.
For immediate consideration please send a resume to Carl Schumacher the Manager of Recruiting CarlS@goapg.com
Job Type: Full-time","$92,733 /yr (est.)",201 to 500 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,2010.0,$100 to $500 million (USD)
Kanini,,"North Druid Hills, GA",Senior Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for Data Engineer who has a deep experience in Data Engineering, Agile planning, Snowflake, SQL, MySQL, PostgreSQL, Git, AWS Data Tools, Python, Jira
Job Description
The Senior Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Senior Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies.
Responsibilities
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understands and enforces appropriate data master management techniques.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Work with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Understands the challenges that the analytics organization faces in their day-to-day work and partner with them to design viable data solutions.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Installs, maintains, monitors, and supports business intelligence, distributed computation, and big data analytics tools.
Provides ongoing support, monitoring, and maintenance of deployed products.
Actively works with less experienced data engineers providing technical guidance and oversight.
Actively participates in the engineering community, staying up to date on new data technologies and best practices and shares insights with others in the organization.
Qualifications:
Bachelor’s degree in computer science or related field and 4 years work experience
Working experience with batch and real-time data processing frameworks
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience in design, development, and implementation of highly scalable, high-volume software systems and components, client-facing web applications, and major Internet-oriented applications and systems
Working experience working with relational databases such as SQL, MySQL, Postgres/PostgreSQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Working experience with application lifecycle methodologies (e.g. waterfall, agile, iterative)
Experience with ETL processes and tools
Experience working with Git.
Desirable Qualifications
Scrum Developer Certification or equivalent
Working experience with Sisense platform
Working experience with SQL Server Integration Services
Working experience with AWS data tools (Database Migration Service)
Working experience with Python
Experience working with Jira, Rally or similar tools
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Application Question(s):
Current Location?
Work Location: Remote",$62.50 /hr (est.),,,,,,
KNN IT,,"Hillsboro, OH",Data Center Engineer,"KNN IT is looking for a Data Center Engineer for a role in Hillsboro, OR.
Applicant must have relevant experience
Must have an active VISA
Must have the basic tools like laptop etc
Candidates from near vicinity are encouraged to apply.
Job Types: Full-time, Contract
Salary: Up to $4,000.00 per month
Ability to commute/relocate:
Hillsboro, OH 45133: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data center: 1 year (Preferred)
Work Location: In person","$4,000 /mo (est.)",Unknown,Company - Public,,,,Unknown / Non-Applicable
"Hiscox
4.1",4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000","$155,000 /yr (est.)",1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901.0,$1 to $5 billion (USD)
"Gold Coast Health Plan
3.8",3.8,"Camarillo, CA",Sr. ETL DEV/Data Engineer,"Data Engineers will be responsible for transformation and modernization of enterprise data solutions on Cloud Platforms integrating Azure services and 3rd party data technologies. Data Engineer will work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions.
ESSENTIAL FUNCTIONS
Reasonable Accommodations Statement
To accomplish this job successfully, an individual must be able to perform, with or without reasonable accommodation, each essential function satisfactorily. Reasonable accommodations may be made to help enable qualified individuals with disabilities to perform the essential functions.
Essential Functions Statement
As a Data Engineer, you will be responsible for assisting our clients envision, design, and deploy data engineering workloads as part of our solutions. As part of a small, dynamic team, you will have the opportunity to contribute to multiple phases of the solution life cycle including designing and implementing models and processes for large-scale datasets used for descriptive, diagnostic, predictive, and prescriptive purposes
Work with the team to evaluate business needs and priorities, liaise with key business partners and address team needs related to data systems and management.
Translate business requirements into technical specifications; establish and define details, definitions, and requirements of applications, components and enhancements.
Generate design, development, test plans, detailed functional specifications documents, user interface design, and process flow charts for execution of programming.
Develop data pipelines / APIs using Python, SQL, potentially Spark and AWS, Azure or GCP Methods.
Build large-scale batch and real-time data pipelines with data processing frameworks in Azure cloud platform.
Assist in the migration from on-prem SQL Server data analytics platform to MS Azure cloud platform.
Work as part of a team to build upon ingestion framework to intake new data sources.
Analyze, design, code and test multiple components of application code across one or more clients.
Perform maintenance, enhancements and/or development work

Qualifications
BA/BS in computer science, mathematics, information management, business, or equivalent experience
6+ years of experience in SQL
4+ years of experience in Cloud Platforms: Azure or AWS or GCP
4+ years of experience in Python and Pyspark
4+ years of experience in Synapse highly preferred
Experience using SQL, dB Visualizer, AWS, Azure, Cloud technologies
Experience with Power BI or similar data visualization tools
knowledge of HL7 v2, HL7 CDA and FHIR interface mapping highly preferred
Exposure to non-relational databases and tools, such as Cassandra, JSON, JAVA, Python, and Spark
In-depth knowledge of healthcare interoperability and patient data aggregation
Ability to effectively communicate, at times in a non-technical language, with customers at all levels of the organization.","$127,500 /yr (est.)",Unknown,Self-employed,Insurance,Insurance Agencies & Brokerages,,Unknown / Non-Applicable
"Barracuda Networks Inc.
3.6",3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote","$98,161 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004.0,$100 to $500 million (USD)
"Encore Technologies
4.4",4.4,"Atlanta, GA",Senior Data Engineer,"Encore Technologies is seeking a Senior Data Engineer to work for a client in Atlanta, GA (zip code 30339). This is a Direct Hire role that will be worked in a hybrid schedule, with 2-3 days a week onsite.
Summary:
The Senior Data Engineer is responsible for managing and organizing enterprise data. They will translate requirements and designs into functional data pipelines while ensuring the continued quality and completeness of the information. Senior Data Engineers will combine raw information from different sources to create consistent and machine-readable datasets that are easy to analyze and support company initiatives. They will support other Data Engineers and Data Analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They will also implement methods to improve data reliability and quality, improve data visibility and reduce effort through automation.
Essential Duties:
Contribute to a team of data engineers through design, demand delivery, code reviews, release management, implementation, presentations, and meetings.
Mentor fellow data engineers and contribute to ongoing process improvements for the team
Evaluate business needs and objectives and align architecture/designs with business requirements
Build the data pipelines required for the optimal extraction, transformation, integration, and loading of raw data from a wide variety of data sources
Assemble large, complex data sets and model our data in a way that meets functional / non-functional business requirements
Create data tools for analytics team members that assist them in generating innovative industry insights that provide our business a competitive advantage
Implement data tagging mechanisms and metadata management so data is accurately classified and visible to the organization
Build processes to help identify and improve data quality, consistency, and effectiveness
Ensure our data is managed in a way that conforms to all information privacy and protection policies
Use agile software development processes to iteratively make improvements to our data management systems
Identify opportunities for automation
Be an advocate for best practices and continued learning
Requirements:
Bachelor's/Tech School degree in Computer Science, Information Systems, Engineering or equivalent and/or commensurate years of real-world experience in software engineering.
4+ years of relevant experience in data management
3+ years in data engineering with detailed knowledge of data warehouse technical architectures, infrastructure components, and ETL/ ELT.
Experience with performance analysis and optimization.
Experience in data acquisition, transformation, and storage design using design principles, patterns, and best practices.
Data engineering certification is a plus.
Experience with Informatica, Kafka, CDC, SQL, Irwin, Python, AWS (S3, Athena, Glue, Kinesis, Redshift), Spark, Scala, AI/ML, Modern data platforms, Snowflake, dbt, Fivetran, and Airflow.
Encore Technologies is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills, and experiences within our workforce.
Job Type: Full-time
Pay: $120,000.00 - $160,000.00 per year
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Atlanta, GA 30339: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
ETL/ELT: 3 years (Required)
data management: 4 years (Required)
Work Location: Hybrid remote in Atlanta, GA 30339","$140,000 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2014.0,$100 to $500 million (USD)
Cox powered by Atrium,,"Atlanta, GA",AWS Data Engineer- Hybrid,"Minimum Qualifications:
Bachelor’s degree or equivalent work experience
A minimum of 3+ years’ experience in Microsoft Windows/ SQL Server Technologies, .Net development, AWS Administration.
Experience working on 24x7 environments oriented towards a zero downtime target.
Working knowledge or previous administration of SQL 2016-SQL 2022 and Windows Server 2012+ preferred.
Ability to work with minimal direction, in a team environment.
Performance tuning for AWS/DataLake systems.
Some Experience with SQL in virtual, physical and cloud-based environments.
Experience with Athena and data modeling for cloud technologies.
Proven ability to quickly learn and implement new technologies.
Experience with Administration, Security/Identity Management and Terraforms in AWS.
Preferred Qualifications:
Experience with SentryOne, a plus.
Ability to code Powershell commands and maintain code in GitHub, a plus.
Some Experience with Metabase and Collibra, a plus.
Experience with ETL in AWS, a plus.
Pay Range:
$60-$68/hr
Requisition Disclaimer:
This job posting is for a temporary role as an employee of Atrium on assignment at Cox. The individual selected for this role will be offered the role as an employee of Atrium; compensation, medical benefits, fringe benefits and other terms and conditions of employment shall be presented by Atrium upon offer. The pay rate range provided is a reasonable estimate of the anticipated compensation range for this job at the time of posting. The actual pay rate will be based on a number of factors, including skills, competencies, experience, location and/or being pursued and other job-related factors permitted by law. In addition, this role will be eligible for overtime pay, in accordance with federal and state requirements

By applying for this position you agree to the Atrium Terms and Conditions. Agreeing to these terms, includes permission to use the email address and mobile phone number you provide during the application process or throughout the duration of your prospective or actual employees to notify you of job openings, profiles, articles, news, and other employment-related information, as well as to notify you of special promotions or additional products and services offered by us or our affiliates and partners (collectively, “Atrium Alerts”). Atrium Alerts may be sent by email, phone or text message. Your personal information will be safely stored in our database. Atrium does not sell your personal information to third parties. Text message and data rates may apply. To OPT OUT of text messaging or to modify your communication preferences for Atrium Alerts at any time, please contact us at privacyadministrator@atriumstaff.com.

If you do not agree with the Atrium Terms and Conditions, you can still complete your application for this position by emailing your resume to our team at coxrecruit@atriumworks.com. Please include the job title in the subject of your email.

As a woman-owned firm, Atrium values diversity. We are an equal opportunity employer and will consider all applications without regard to race, sex, age, color, religion, national origin, veteran status, disability, genetic information or any other characteristic protected by law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Posting: #zip",$64.00 /hr (est.),,,,,,
The Data Sherpas,,Remote,Data Engineer - LATAM,"Who We Are:
Working at The Data Sherpas is like being part of a dynamic and collaborative team of talented individuals passionate about helping clients navigate the ever-changing information technology landscape. At The Data Sherpas, you can work with cutting-edge tools and technologies, tackle challenging data problems, and continuously develop your skills in a supportive environment. As a Data Sherpa, you'll be empowered to lead projects, take ownership of your work, and make meaningful contributions to our client's success.
What We Are Looking For:
We're seeking an ambitious and driven Data Engineer to join our team. As a Data Engineer, you can work on exciting, challenging projects supporting our client's needs. You will be part of a dynamic team of experts dedicated to improving business performance and driving data-driven results.
What You'll Do:
Design and implement data pipelines, ETL processes, and data warehousing solutions
Develop and maintain attribution and measurement models for ad campaigns
Perform data matching and segmentation techniques to create customer profiles and behavior patterns
Configure and maintain AWS infrastructure and services to support data engineering processes
Collaborate with cross-functional teams to identify data needs and develop data-driven solutions.
Stay up-to-date with the latest technologies and industry trends related to Data Engineering and Ad Tech.
What You Have:
Proficiency in data modeling, ETL development, and data warehousing
Ability to design and maintain data pipelines for large-scale data sets
Proven experience with Python programming language for data engineering solutions
Knowledge of data matching techniques for identifying duplicate data and inconsistencies
Experience with data deduplication techniques
Ability to work with large datasets to segment and group data
Experience with clustering algorithms and methodologies
Understanding of customer segmentation and persona development
Familiarity with data visualization tools to showcase segmentation results
Strong experience with data engineering in cloud-based environments, primarily in AWS
Experience using AWS data analytics services like EMR, Redshift, Kinesis, and Glue",,,,,,,
"Tech Mahindra / Microsoft
3.2",3.2,Remote,Data Engineer - Cosmos,"Hi,
One of my direct client is looking for Data Engineer in Redmond, WA. If you are interested, please share me your updated resume.
Title: Data Engineer
Location: Redmond, WA
Direct Client: Microsoft
Job Description:
Primary Requirement:
1. Cosmos Scope Scripting
2. Azure Data Lake, Pipelines
3. ADLS, ADLA
4. SQL querying experience
5. Microsoft projects experience
Optional Requirement:
1. ADO
2. PowerShell or Python Scripting language
3. Project manager
Thanks & Regards
K. ManiMegalai
Phone: (206) 337-5702 Ext 241
Zen3 is now a Tech Mahindra Company.
Tech Mahindra is a strategic Microsoft Partner and is a proud partner in implementing the World’s largest Azure Migration program.
Job Type: Full-time
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote",,1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
Alter Tech Solutions,,"Jersey City, NJ",Senior Data Engineer,"Job Title: Data Engineer
Location: Jersey City, NJ
Job Type: Hybrid
Job Description:
Position is more about scale - application monitoring, automation framework, test automation - data engineering - data pipelines - rest api's
system design - experience with distributed systems, design the right components, push out to the AWS cloud, data structures/algorithms
Experience:
10+ years of experience cross functional experience - data engineering and sw engineering background (working with risk, audit teams) need hands on coding but also design
Qualifications & Requirements:
TOP SKILLS java preferred language but would consider strong python - Apache Flink, Kafka, Cassandra, GraphQL, SPARK, MACHINE LEARNING.
Job Types: Full-time, Permanent
Salary: $60.00 - $75.00 per hour
Experience:
Java: 8 years (Preferred)
GraphQL: 8 years (Preferred)
Python: 5 years (Preferred)
Work Location: On the road",$67.50 /hr (est.),,,,,,
"Intertech, Inc
4.4",4.4,Minnesota,Sr. Data Engineer,"Sr. Data Engineer
US Citizenship Required
Contract to Hire Opportunity
Fully Remote

The Senior Data Engineer will oversee the department's data integration work, including developing data models, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. This role will work closely and collaboratively with members of other areas to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of the analytics needs.

Responsibilities
Maintain and build our data warehouse and analytics environment
Design, implement, test, deploy, and maintain stable, secure, and scalable data engineering solutions and pipelines in support of data and analytics projects, including integrating new sources of data into our central data warehouse, and moving data out to applications and affiliates as needed
Make data available for the reporting and analytics teams
Produce scalable, replicable code and engineering solutions that help automate repetitive data management tasks
Implement and monitor best in class security measures in our data warehouse and analytics environment, with an eye towards the evolving threat landscape
Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security
Other duties as assigned
Provide technical assistance in an on-call rotation



Job Qualifications

Required:
Bachelor’s Degree in Computer Science or Management Information Systems (MIS) or Business, Finance or Accounting with an emphasis in MIS
Minimum 5 years experience of developing and supporting enterprise level data warehouse systems
Strong knowledge of relational databases and SQL. Extract, Transform, and Load (ETL) data into a relational database
General data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets together, reformat data between wide and long, etc.
Demonstrated ability to learn new techniques and troubleshoot code without support, ex. find answers to common programming challenges
Strong knowledge of T-SQL language as evidenced by ability to write complex SQL queries, Microsoft SQL Management Studio, SQL Analysis Services and SQL Server Integration Services
Demonstrated ability to work independently and be a self-starter
Demonstrated ability to work effectively in teams, in both a lead and support role
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision

Preferred:
Experience working with Data Vault 2.0
Experience working with cloud infrastructure services like Amazon Web Services and Google Cloud
Experience with advanced data visualization and mapping",,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1991.0,$5 to $25 million (USD)
Accentvision Technology Inc,,"Plano, TX",Senior Data Engineer - Confluent Kafka,"Job Description:
Apply Confluent Kafka API lifecycle development and management.
Administer and improve Kafka use throughout organization including Kafka Producers, Kafka Consumers, Kafka Connect, KsqlDB, KStreams, and custom implementations.
Work with multiple teams to ensure best use of Kafka and data-safe event streaming.
Understand and apply event-driven architecture patterns and Kafka standard processes.
Model system behaviors using standard process methods for communicating architecture and design.
Develop or assist developers in choosing correct patterns, event modeling, and ensuring data integrity.
Work with Kafka APIs to provide pro-active insights and automation.
Acquire a deep understanding of source and sink connector technical details for a variety of platforms including Snowflake, Sales Force, MongoDB, PostgreSQL, MS SQL Server, Azure, and others as required.
Experience:
Senior Data Engineer: 10+ years proven experience in data engineering.
Data Engineer: 5+ years of proven experience in data engineering.
Solid understanding of Kafka (Consumption, publishing, and streaming) architecture and integrations.
Familiar with 3rd Party Confluent Kafka Connectors, Kafka Connect and its connectors for integrating with external systems.
Development experience using Confluent Kafka producers, consumers, and streams.
Experience with building streaming applications with Confluent Kafka
Experience with Java coding, CI/CD processes, microservices is required.
Good exposure to various Azure cloud platforms to play a key role in Application Modernization
Experience in CI/CD, DevOps tool chain, GIT, docker, Jira, and a test-driven approach to agile delivery.
Ability to participate in and contribute to code management in GitHub including actively collaborating in peer-reviews, feature branches, and resolving impediments and commits.
Job Types: Full-time, Contract, Temporary, Permanent
Pay: $110,000.00 - $150,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Vision insurance
Compensation package:
1099 contract
Hourly pay
Yearly pay
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
Data Engineering: 6 years (Required)
Confluent Kafka: 5 years (Required)
KSQL: 5 years (Required)
KStreams: 5 years (Required)
Work Location: Hybrid remote in Plano, TX 75024","$130,000 /yr (est.)",,,,,,
"AMBE Engineering
4.4",4.4,"Rome, GA",Data Engineer,"DATA ENGINEER
FULL TIME
LOCATION: Rome, GA
Works with various stakeholders across the organization to create dashboards and interactive visual reports using Power BI.
Designs, extends and enhance Power BI data models and development of new cubes.
Maintains functionality and relevance of reports and problem solve as issues arise.
Interprets trends and patterns, conduct complex data analysis and report on results.
Develops profiling scripts, data quality reports. Collaborate with business to develop DQ rules to run profiling.
Links Power BI with ERP system for efficient and automated report generation.
Combines raw information from different sources.
Works with users and team members at all levels to elevate existing reports and reporting capabilities and distribution.
Develops, test, and deploy DAX based scripts, queries and functions in Power BI.
Drives continuous improvement in IT processes and increases operational efficiencies
Works closely with project teams in gathering requirements, standing up and configuring the platform.
Other duties can be assigned based on company needs and employee capabilities.
REQUIREMENT
Understands business requirements in BI context and convert data into meaningful insights.
Be able to write and execute DAX measures, queries, and functions in Power BI.
Strong SQL skills (complex queries on relational databases, stored procedures, automation).
Strong Excel skills (experience migrating data from Excel to Power BI strongly preferred)
Understands business requirements in BI context and convert data into meaningful insights
Benefits
Who we are:
Ambe Engineering is a dual-certified W/MBE (woman and minority owned business) Diversity Supplier continuing to grow its ecosystem and extend their expertise globally. We bring the right resource for supplier development, high-impact project management, holistic cost savings, lean manufacturing and quality systems/problem solving solutions.
Quality, Logistics & Production | Crisis Management / Critical Situations | Cost Reduction | HR Services
__
_www.ambeeng.com_
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Schedule:
8 hour shift
Day shift
Experience:
Power BI: 2 years (Required)
Automotive: 2 years (Preferred)
Work Location: In person","$145,000 /yr (est.)",Unknown,Company - Private,Manufacturing,Transportation Equipment Manufacturing,,Unknown / Non-Applicable
"Kroenke Sports Enterprises
3.4",3.4,"Denver, CO",Data Engineer,"Job Title: Data Engineer
Department: Hockey Operations
Business Unit: Colorado Avalanche
Location: Denver, CO or Remote
Reports To: Director of Analytics
Employment Type: Full Time – Salaried - Exempt
Supervisor Position: No
_____________________________________________________________________________________
Kroenke Sports & Entertainment (KSE) is an American Sports and Entertainment holding company based in Denver, Colorado. KSE is committed to providing world class sports and entertainment for both live and broadcast audiences. We are the employer of choice as the owner and operator of Ball Arena, DICK’S Sporting Goods Park, the Paramount Theatre, 1STBANK Center, Denver Nuggets (NBA), the Colorado Avalanche (NHL), Colorado Mammoth (NLL), Colorado Rapids (MLS), KIMN,KXKL, KKSE (FM/AM), Altitude Sports & Entertainment, Major League Fishing/Fishing League Worldwide (MLFLW), Winnercomm, Outdoor Sportsman Group and SkyCam.

Nature of Work:
The Colorado Avalanche are looking to hire a full-time Data Engineer to work within the team’s Hockey Operations Department. This person will be responsible for maintaining and expanding the Avalanche hockey operations database. They will be tasked with importing and integrating data from external providers and interacting with the rest of the hockey operations department to ensure optimal dissemination of information to the appropriate parties. This person will also have the opportunity to analyze data and share insights with members of the Analytics department as well as the broader Hockey Operations department if desired. They will report to the Director of Analytics.

Examples of work performed:
Manage and improve the organization’s data storage, structure, and ETL pipeline while optimizing query performance for large datasets
Design automated processes to oversee data integrity and query performance on a regular basis, as well as being available to spot and resolve data issues that may arise at any time
Ensure that our automated processes run on schedule without issue
Incorporate expansive new datasets from disparate sources into our structure in a seamless fashion

This description is a summary only and is describing the general level of work being performed, it is not intended to be all-inclusive. The duties of this position may change from time to time and/or based on business needs. We reserve the right to add or delete duties and responsibilities at the discretion of the supervisor and/or hiring authority.

Working Conditions & Physical Demands:
Typical Office Conditions
Travel may be required

Qualifications:
Required
Strong knowledge of ETL architecture and development in a cloud-based environment
Academic and/or industry experience in database architecture, back-end software design, and query optimization
Expertise in SQL and Spark
Excellent attention to detail, problem-solving abilities, and work ethic
Preferred
1-3 years of work experience in a database-related position
An advanced degree in software engineering, computer science, information technology, or a related field
Familiarity with R and Python, as well as R Studio / Posit
Experience with databricks and Linux operating system
Knowledge of NHL players and teams as well as a passion for hockey and hockey analytics
Some front-end development and/or data analysis experience is considered a plus

Competencies/Knowledge, Skills & Abilities:
Ability to maintain positive attitude and demonstrate professionalism
Ability to maintain a high level of confidentiality
Ability to complete work accurately and in a timely manner
Ability to work independently & in a group setting and demonstrate good judgment skills
Ability to communicate effectively orally and in writing
Possesses excellent interpersonal skills
Ability to multi-task, prioritize and adapt to changing environments

Compensation:
Salary Range $80,000 - $110,000 per annum

Benefits Include:
12 Paid Company Holidays
Health Insurance (Medical, Dental, Vision)
Paid Time Off (PTO)
Life Insurance
Short and Long-term Disability
Health Savings Account (HSA)
Flexible Spending plans (FSAs)
401K/Employer Match

Equal Employment Opportunity
Kroenke Sports & Entertainment (KSE) provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.","$95,000 /yr (est.)",1001 to 5000 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,,Unknown / Non-Applicable
"Diverse Lynx
3.9",3.9,"Charlotte, NC",Azure Data Engineer,"Production support Engineer
Job summary required skill:
ITIL and Prod Support experience
Powershell • Scripting knowledge
Should be able to check server and service health
Should be able to check certificates
Good to have knowledge of SQL (preferred)
Development experience
Experience: 5+ years
Required Skills:
1. Powershell
2. Any Scripting knowledge
3. SQL
Preferred Skills: Good to have knowledge of SQL
Technical Skills: PowerShell Domain Skills, ITIL Domain Skills, Technology Infrastructure Services
Development experience required skill: PowerShell
ITIL and Prod Support experience
Powershell
Scripting knowledge
Should be able to check server and service health
Should be able to check certificates
Job Type: Full-time
Salary: $78,245.06 - $171,281.86 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28203: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person","$124,763 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2002.0,$100 to $500 million (USD)
"RevolutionParts
4.0",4.0,United States,Data Engineer,"RevolutionParts is dedicated to modernizing the auto industry through our parts e-commerce platform. And we are pretty great at it too! We have enabled thousands of dealerships to sell auto parts online by transforming the way buyers and sellers connect.

And not only are we dedicated to revolutionizing the auto industry; we are also passionate about building a revolutionary team. Our Revolutionaries (as we call ourselves) are talented humans who have a shared goal of delivering an exceptional product and customer experience. Plus, we have fun while doing it!

The Role
RevolutionParts is on a mission to take our data to the next level. In this key role, you will help design and implement the next generation of our ETL pipeline for our parts catalog, pricing, and inventory data, which powers all of our eCommerce solutions. You will also be involved in establishing an enterprise-grade data platform for our largest partners. This is a high-impact role where you will be driving initiatives affecting teams and decisions across the company and setting standards for all our data stakeholders. Does the idea of spearheading a data practice in a high-growth e-commerce business sound exciting? If so, read on.
Responsibilities
We’re pulling in diverse data sources. You’ll need to learn our data and bring a strong grasp of ETL & ELT, workflows, AWS Glue, and data organization via efficient data lake and relational designs.
You will help design and build all stages of data from access to transformation and modeling.
You will build quality into the pipeline from day one using automated tests and data validation.
You will work with stakeholders in Product and data science to run ad hoc analysis of our data to answer questions and help prototype solutions.
You must own business problems through to resolution both individually and as part of a data team.
You will support product engineering teams by performing query analysis and optimization, as well as work with product teams to implement data driven product features.
Requirements
You should have 4+ years experience as a data engineer; or at least 2 years experience as a Data Engineer and 3+ as a software engineer.
You need to show us that you know what good looks like. This means experience implementing automated tests in a multi-stage data pipeline to ensure quality.
You are highly analytical and curious by nature.
You must have the ability to own business problems and the design and solutions that drive business outcomes.
You must be a team player with the ability to work with others and know when to support and when to push.
This role requires strong communication and collaboration skills; comfortable discussing projects with anyone from end users up to executive leadership.
Fluency with the programming language of your trade. Our primary languages for data are Golang and Python, but we use others as well. You must be comfortable learning new skills on the job.
We require fluency with best practices in an object-oriented design and programming; experience as a backend software engineer is necessary. Demonstrable experience with a functional paradigm is also valuable.
The ability to write and optimize complex SQL statements is a base requirement.
Familiarity with ETL/ELT pipelines and modern tools is fundamental to this role. We are building workflows managed in Argo utilizing Docker containers deployed within Kubernetes.
You should have experience working in a cloud-based software development environment, preferably with AWS.
Familiarity with no-SQL databases such as ElasticSearch, DynamoDB or others is helpful.
Bachelor’s Degree in Computer Science or equivalent is required.
RevolutionParts is proud to provide all full-time Revolutionaries with a comprehensive employment package including competitive compensation, career development, benefits, 401K match, parental leave, and many more valuable perks. You can learn more about our core-value driven culture at our career page.

RevolutionParts is an Equal Opportunity Employer; we value diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, gender orientation, gender identity or expression, sexual identity, sexual orientation, age, marital status, family status, genetic information, veteran status, or disability status.",,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2013.0,$5 to $25 million (USD)
"National Conference of Bar Examiners
4.1",4.1,"Madison, WI",Data Engineer,"JOB SUMMARY:
The National Conference of Bar Examiners (NCBE) is a nonprofit organization that provides high-quality assessment products, services, and research for the benefit and protection of the public and the legal profession. We assist state courts and licensing authorities with fulfilling their responsibility to determine minimal competence for entry to the legal profession.

Diversity, fairness, and inclusion are central to NCBE’s mission and to our vision for a competent, ethical, and diverse legal profession. NCBE provides an inclusive and family-friendly environment, flexible schedules, remote work options, and competitive salary and benefits. NCBE’s headquarters is located in Madison, Wisconsin, a vibrant community with excellent municipal services and educational opportunities.

The Data Engineer is responsible for creating, editing, and maintaining SQL functionality in the relational database platforms. The data engineer is responsible for managing and developing code on Oracle, MySQL, and Postgres platforms both on-prem and in the cloud. The data engineer also works to support application developers with database expertise regarding data architecture, SQL construction, application/database integration, as well as developing and maintaining complex procedures for the software backend. The position is also responsible for providing business management information to business leaders via ad-hoc queries and reports. Some traditional DBA server management skills such as creating and restoring data, creating / updating development and test database environments, maintaining backups, tuning for optimal performance and security awareness are also used.
Work is performed under the supervision of the IT Manager of Infrastructure.

ESSENTIAL DUTIES and RESPONSIBILITIES:
Create databases with efficient structures.
Develop, test, maintain and document changes to databases, both internal and public facing.
Develop queries, views and triggers for integration with other applications.
Assist developers and others in the department with database related needs.
Perform customer support for applications as needed.
Monitor health and security of the databases. Maintain high standards of data quality and integrity.
Develop and maintain queries and complex reports as needed.
Develop, debug and maintain procedures, functions and other DDL as needed; understand issues related to performance and security.
Understand and apply the SDLC, specifically Scrum and Agile methodology.
Perform other duties as assigned.

QUALIFICATIONS:
S:
Education and Experience Required:
4+ years of experience working on progressively more complex systems.
PL/SQL, Transact SQL, SQL 92 or other database language required.
Familiarity with scripting languages.
Familiarity with Linux.
Familiarity with Agile development principles.
Strong work ethic, professional, organized, and flexible; able to work independently to solve issues through self-directed research and troubleshooting.
Ability to handle and prioritize multiple tasks.
Bachelor’s Degree or relevant work experience.

Preferred Qualifications:
Design and development of database functions, procedures and packages.
Experience with reporting tools such as Crystal Report.
Experience working in a team environment.
Experience with modern systems development model.

The Data Engineer position may work remotely.

Mission
NCBE promotes fairness, integrity, and best practices in admission to the legal profession for the benefit and protection of the public. We serve admission authorities, courts, the legal education community, and candidates by providing high-quality
assessment products, services, and research;
character investigations; and
informational and educational resources and programs.

EEO Statement
NCBE is proud to be an equal employment opportunity organization. We are committed to providing equal employment opportunity to all applicants and employees regardless of their race, color, religion, age, sex, national origin, disability, military service, protected veteran status, genetic information, sexual orientation, gender identity, or any other characteristic protected by federal, state or local law.

Please note that applicants may be contacted via email throughout the hiring process. We suggest that you add BambooHR (@bamboohr.com and @app.bamboohr.com) to your Approved/Safe Sender list so that email notifications are delivered to your inbox and not marked as spam.","$89,236 /yr (est.)",51 to 200 Employees,Nonprofit Organization,Government & Public Administration,State & Regional Agencies,,$5 to $25 million (USD)
"BuzzClan Private Limited
4.3",4.3,"Dallas, TX",Data Platform Engineer,"Only w2 CANDIDATES
Job Overview
The Data Platform Engineer shall be responsible for maintaining current data platforms, troubleshooting issues, designing new data platforms, ensuring data platform availability, ensuring data integrity, implementing data platform changes, managing security, and providing on-call support. Current data platforms include – SQL Server, Azure SQL Database, Azure SQL Datawarehouse, Azure Data Lake, SQL Server Analysis Services (SSAS), SQL Server Integration Services (SSIS), SQL Server Reporting Services (SSRS), and PowerBI. Teams supported include: Application Engineering, Data Engineering, Decision Science, and Business Analytics.
For a successful candidate for this position, you must -
Have extensive experience of the setup, deployment and hardening of data-intensive environments using cloud-based database and storage technologies
Have experience in creating robust and automated pipelines to ingest and process structured and unstructured data sources into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset
Be able to drive security paradigms for databases and related infrastructure resources
Leverage the right tools for the right job to deliver testable, maintainable, and modern data solutions
Be comfortable with researching data questions, identify root causes, and interact closely with business users and technical resources on various data related decisions
Understand how to profile code, queries, programming objects and optimize performance
Be able to advice on performance optimizations and best practices for scalable data models, pipelines and queries
Aspire to be efficient, thorough and proactive
Responsibilities and Duties
Provides and designs tools to assist in the management of the data platforms
Works to provide a working model of our transaction processing environment for capacity assessment and planning
Develops a methodology for the ongoing assessment of data platform performance and the identification of problem areas
Develops a security scheme for the data platform environment, as well as assisting in disaster recovery, if necessary
Provides leadership during the development and enhancement of the enterprise’s production applications including working with applications, technical support and operations during the design, development and implementation of applications
Recognizes and identifies potential areas where existing policies and procedures require change, or where new ones need to be developed, especially regarding future business expansion
Fulfills departmental requirements in terms of providing work coverage and administrative notification during periods of personnel illness, vacation, or education
Performs at or above the enterprise’s Information Technology performance standards
Participates with vendors in the assessment of advanced transaction processing and data platform productions including beta and field test participation
Special projects as requested
Performs other duties as assigned
Job Type: Full-time
Salary: $80,000.00 - $90,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Experience level:
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: On the road","$85,000 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2013.0,$5 to $25 million (USD)
"DSFederal Inc
4.0",4.0,Remote,Data Engineer*,"Description:
We are seeking an experienced Data Engineer to design, build, and maintain our government client’s data infrastructure. The Data Engineer will work closely with cross-functional teams to develop scalable data solutions that support our client’s business needs.
Requirements:
Design, develop, and maintain data pipelines and data storage systems.
Collaborate with cross-functional teams to design and implement data-driven solutions.
Optimize and tune data systems for performance and scalability.
Implement and maintain data quality and validation processes.
Troubleshoot and debug data-related issues.
Participate in code reviews and contribute to best practices for data engineering.
Technical Skills Required:
Proficiency in one or more programming languages (Python, Java, etc.)
Experience with data modeling, database design, data marts, and data warehousing concepts
Knowledge of ETL tools and techniques
Experience with cloud-based data platforms such as AWS or Azure
Strong problem-solving and analytical skills
Desired Skills:
Familiarity with data visualization tools such as Tableau or Power BI.
Knowledge of machine learning concepts and frameworks.
Experience Required:
3+ years of experience in data engineering or related field
Education Required:
Bachelor's in engineering

Who We Are:
DSFederal is a leader in health IT and data analytics that delivers innovative solutions to improve healthcare delivery and address critical challenges, including cancer surveillance, HIV/AIDS, maternal and child health, and COVID-19 emergency response. Our highly experienced professionals serve over 50 different federal clients on over 80 projects. Our people are united by one mission – to improve human life through transformative solutions. www.dsfederal.com.
What We Can Offer You:
DSFederal attracts the best people in the business with our competitive benefits package that includes medical, dental and vision coverage, 401k plan with employer contribution, paid holidays, vacation, and more. If you enjoy being a part of a high performing, professional services, and technology focused organization, we encourage you to apply!
Our Mission: Improve human life through transformative solutions.
Our Core Values: Integrity, People First, Disciplined Business Practices, Customer Centric, Value Creation.
EEO Statement: DS Federal is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status.
E-Verify Statement
DS Federal utilizes the E-Verify program for every potential new hire. This makes it possible for us to confirm that every employee who works for DS Federal is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website by clicking the logo below. E-Verify® is a registered trademark of the United States Department of Homeland Security.
#LI-LP1",,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007.0,$25 to $100 million (USD)
Artint Knowledge Tech,,"Charlotte, NC",Azure Cloud Data Engineer,"Role: Azure Cloud Data Engineer (Onsite role)
Location: Mount Laurel, NJ / Charlotte, NC (day 1onsite)
Job Duration: Full Time
Below is the JD:
Keys Skills: Azure, Synapse, ADF, DataBricks, PySpark, Informatica Power Centre or SQL Server SSIS or DataStage
Must Have
* More than 12 years of IT experience in Datawarehouse
* Hands-on data experience on Cloud Technologies on Azure, Synapse, ADF, DataBricks, PySpark
* Prior Experience on any of the ETL Technologies like Informatica Power Centre, SSIS, DataStage
* Ability to understand Design, Source to target mapping (STTM) and create specifications documents
* Flexibility & willingness to work on non-cloud ETL technologies as per the project requirements, though main focus of this role is to work on cloud related projects
* Flexibility to operate from client office locations
* Able to mentor and guide junior resources, as needed
* Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Job Type: Full-time
Salary: $100.00 - $115.00 per year
Work Location: One location",,,,,,,
"Dime Community Bank
3.4",3.4,"Hauppauge, NY",Data Engineer,"Summary: Dime Community Bank is looking for a Data Engineer based in Hauppauge, NY.
Salary commensurate with experience, ranging from $100,000 to $135,000 annually. The exact compensation may vary based on relevant experience, skills, education, training, licensure and certifications, and location.
All applicants need to attach a recent resume.
Responsibilities:
Work with relational databases and data warehousing. Add new and modify existing data models to ensure data warehouse is up to date for reporting needs.
Handle data visualization for extract transform and load (ETL).
Handle job pipelining and orchestration. Automate data loads and systems with Powershell scripting and Python. Orchestrate jobs with Dagster to ensure proper data processing, swift execution of tasks, and accurate monitoring. Build ETL pipelines to bring all data sources to the data lake, data warehouse and data marts. Work with python & SQL, and scripting in Powershell for automation.
Build python packages for use by the team for report building, accessing databases, etc.
Develop and maintain operational data stores (ODS) in our datalake and data warehouse architecture. Utilize ETL concepts and handle transformation with data build tool (DBT). 20468.3.6
**Telecommuting may be permitted.
Mail resume to Robin.Derin@dime.com. Must reference job 20468.3.6.
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)","$117,500 /yr (est.)",501 to 1000 Employees,Company - Public,Financial Services,Banking & Lending,2021.0,$100 to $500 million (USD)
"Summit2Sea Consulting
5.0",5.0,"Tampa, FL",Data Engineer,"Summit2Sea Consulting, a cBEYONData company, is a technology consulting firm run by hands on technologists that combines people, process and technology to deliver innovative solutions to our clients. We have been named on The Washington Post's list of top work places for the past 3 years! We invest in our biggest asset - our people! You can be a part of a winning team that will contribute to your career growth.
Have you been looking to shift your career into high gear? This is your opportunity to take your ambitions and convert them into a solid career in a supportive and innovative environment!
Summit2Sea, a cBEYONData company is seeking a Data Engineer to support our federal customer's multi-domain technology platform which offers military and business decision makers, analysts, and users at all levels unprecedented access to authoritative enterprise data and structured analytics in a scalable, reliable, and secure environment.
Responsibilities:
Support data collection, ingestion, validation, and loading of optimized data in the appropriate data stores
Work on a team made up of analyst(s), developer(s), data scientist(s), and a product lead
Working directly with the analyst(s) and the product lead, the data engineer identifies and implements solutions for the data requirements, including building pipelines to collect data from disparate, external sources, implementing rules to validate that expected data is received, cleansed, transformed, massaged and in an optimized output format for the data store
Performs validation and analytics corresponding with client requirements and evolves solutions through automation, optimizing performance with minimal human involvement
As pipelines are executed, the data engineer monitors their status, performance, and troubleshoots issues while working on improvements to ensure the solution is the very best version to address the customer need
This role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis
Apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems
Requirements:
Must Have:
4+ years of experience with SQL
4 years of experience working in a big data and cloud environment
4+ years of experience with a modern programming language such as Python or Java
4+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets.
Active Secret Clearance or higher
Nice to Have:
2 years of experience working in an agile development environment
Possession of excellent verbal and written communication skills
Preferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes.
Ability to display a positive, can-do attitude to solve the challenges of tomorrow
Ability to quickly learn technical concepts and communicate with multiple functional groups
Summit2Sea Consulting, a cBEYONData company, is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity:
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.",,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2003.0,$5 to $25 million (USD)
Resilience Consultancy Services,,Remote,Azure Data Engineer/Tech Lead,"Job Title: Azure Data Engineer and Tech Lead
12+ Months
Contract
Job Summary: We are looking for an experienced Azure Data Engineer and Tech Lead to join our team. The successful candidate will be responsible for designing and implementing data solutions using Azure services such as Data Factory, Databricks, and other related services. Additionally, the candidate will lead and manage the technical team responsible for developing and maintaining these solutions.
Responsibilities:
Design and implement scalable data solutions on Azure platform using Data Factory, Databricks, PySpark, Python and other related services.
Build data pipelines and workflows to ingest, transform, and load data from various sources.
Develop and maintain data models and schemas for efficient data storage and retrieval.
Manage and lead a technical team responsible for data engineering and analysis.
Develop and maintain CI/CD pipelines for automated deployment and testing of data solutions.
Collaborate with cross-functional teams to ensure data solutions meet business requirements.
Continuously evaluate and recommend new data technologies and approaches to improve data solutions.
Requirements:
Bachelor's or Master's degree in Computer Science, Information Systems, or related field.
5+ years of experience in data engineering with expertise in Azure Data Factory, Databricks, PySpark, Python and related services.
Proven experience in leading and managing technical teams in data engineering and analysis.
Experience in designing and developing data models and schemas.
Strong proficiency in programming languages such as Python.
Experience in developing and maintaining CI/CD pipelines for automated deployment and testing.
Good understanding of cloud computing and its services (e.g., Azure, AWS, GCP).
Excellent problem-solving skills and ability to work in a fast-paced environment.
Strong communication and interpersonal skills.
Job Type: Contract
Pay: $75.00 - $80.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Education:
Bachelor's (Preferred)
Experience:
Data Factory: 6 years (Preferred)
SQL: 8 years (Preferred)
Databricks: 6 years (Preferred)
PySpark: 5 years (Preferred)
Python: 5 years (Preferred)
CI/CD: 5 years (Preferred)
Azure: 5 years (Preferred)
Work Location: Remote",$77.50 /hr (est.),Unknown,Company - Public,,,,Unknown / Non-Applicable
"Express Global Solutions LLC
4.3",4.3,Remote,Data Engineer-Databricks Consultant,"5+ years' data engineering experience working with big data.
4+ years' experience developing in Databricks.
Job Types: Full-time, Contract
Pay: $80,631.44 - $97,104.52 per year
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote","$88,868 /yr (est.)",51 to 200 Employees,Company - Public,,,,Unknown / Non-Applicable
"Presidio
4.0",4.0,"Tampa, FL","Engineer, Data Center (FL)","SEIZE THE OPPORTUNITY TO BE A PART OF SOMETHING GREAT!
Presidio is on the leading edge of a technology-driven movement to transform the way business is done, for our customers and our customers' customers. Joining Presidio means immersing yourself in a culture of self-starters, collaborators and innovators who make real, lasting change in the marketplace via cutting-edge technology and business solutions. At Presidio, we know that it’s our people that make the connections happen.
WHY YOU SHOULD JOIN US? You will set your career on track for outstanding achievement with a company that knows no limits. Presidio is a leading a global digital services and solutions provider focused on Digital Infrastructure, Business Analytics, Cloud, Security & Emerging solutions.
THE ROLE: Engineer, Data Center
Job Summary: Presidio is looking for a Data Center Engineer to join our talented Staff Augmented team at the Hillsborough County Courthouse. Primary responsibilities include engineering, design, installation, monitoring and troubleshooting of a complex data center environment.
Travel Requirements: This role is an onsite role with up to 2 days remote work per week. There is no travel required for this opportunity.
Job Responsibilities:
Administration, maintenance and troubleshooting of all servers in the Courts. This includes: monitoring of the servers, verification of all services, upgrades to servers and applications, permission changes, drive shares and folders, and associated other tasks
Review application logs and server resources to identify any preventative maintenance required .
Daily verification of the previous backup jobs
Perform any user-oriented moves, adds and changes to the information in Active Directory. This includes usernames, full names, locations, phone number, accounts and associated information
Keeping up with expiration dates and quotes for all maintenance contracts.
Develop comprehensive graphical and text-based design documentation and effectively manage the implementation process from design to customer acceptance.
Administration, maintenance and troubleshooting of all storage devices in the Courts. This includes: EMC, NetApp, Nimble
Administration, maintenance and troubleshooting of all virtualization infrastructure. This includes: VMware, VCenter, Site Recovery Manager
Administer and Maintain the CrowdStrike server to deploy and monitor CrowdStrike on all servers at the Courts.
Research, testing and staging of the Windows Updates using the WSUS server
Verification and final installation of the server patches. This includes all servers in the Courts.
Required Skills:
Systems Administration
Switches, Routers, Firewalls
Build out, OS install, security scans and remediation of Enterprise servers in a Data Center solution
Build/configure VMware host machines with integration into vCenter
Build/configure windows 2003-2016 virtual servers, patching, security compliance, AD, GPOs and user management
Education and Experience: (List out)
Bachelor's degree or equivalent experience and/or military experience
Over all 5 years of experience in design, installation and management of all Data Center Operations
*****
ABOUT PRESIDIO
Presidio is committed to Diversity, Equity, and Inclusion at the highest levels and has strengthened its drive to build and drive systemic DEI change process across all levels of the organization. Cultivating a culture of inclusion where the expression of all our differences are valued, celebrated, and add to our collective achievements.
Presidio is a global digital services and solutions provider accelerating business transformation through secured technology modernization. Highly skilled teams of engineers and solutions architects with deep expertise across cloud, security, networking and modern data center infrastructure help customers acquire, deploy and operate technology that delivers impactful business outcomes. Presidio is a trusted strategic advisor with a flexible full life cycle model of professional, managed, and support and staffing services to help execute, secure, operationalize and maintain technology solutions. We serve as an extension of our clients' IT teams, providing deep expertise and letting them focus on their core business. Presidio operates in 40+ US offices and offices in Ireland, London, Singapore, and India.
For more information visit: http://presidio.com
*****
Presidio is an Equal Opportunity / Affirmative Action Employer / VEVRAA Federal Contractor. All qualified candidates will receive consideration for this position regardless of race, color, creed, religion, national origin, age, sex, citizenship, ethnicity, veteran status, marital status, disability, sexual orientation, gender identification or any other characteristic protected by applicable federal, state and local statutes, regulations and ordinances.
To read more about discrimination protections under Federal Law, please visit: https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf
If you have any difficulty using our online system and need an accommodation in the job application process due to a disability, please send an email to recruitment@presidio.com for assistance.
Presidio is a VEVRAA Federal Contractor requesting priority referrals of protected veterans for its openings. State Employment Services, please provide priority referrals to recruitment@presidio.com.
RECRUITMENT AGENCIES PLEASE NOTE:
Agencies/3 Parties may not solicit to any employee of Presidio. Any candidate information received from any Agency/3 Party will be considered a gift and property of Presidio, unless the Agency/3 Party is an Authorized Vendor of Presidio with an up-to-date Presidio Contract in hand signed by Presidio Talent Acquisition. No payment will be made to any Agency/3 Party who is not an Authorized Vendor, nor has specific approval in writing from Presidio Talent Acquisition to engage in recruitment efforts for Presidio.","$77,293 /yr (est.)",1001 to 5000 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2006.0,Unknown / Non-Applicable
"FiberSense
5.0",5.0,"Oakland, CA",Data Engineer,"Why join us: Our benefits
Global team and remote role
Inclusive and flexible work culture
Motivated and vibrant team
Small team, large career growth opportunities
Opportunities and support to learn and receive mentoring
Fast growing start-up with unique, highly technical and exciting product
We are making the world safer and more sustainable
Our technology is one of a kind – there is no one else in the world doing what we're doing
About the opportunity: Data Engineer
Hybrid (from Oakland) or remote | Very flexible hours | $141,000 - $155,000 USD per annum
The software team works in a hybrid work environment from an office in Oakland, so ideally you would be based in Oakland too. We will consider candidates who are 100% remote within the USA. We encourage all types of flexible working arrangements.
As FiberSense's first dedicated Data Engineer, you will have a significant role in enhancing our existing data pipelines, while also taking charge of designing, developing, and maintaining new robust and scalable data infrastructure.
You will report directly to the head of ML/AI, located in Oakland, California. However, for the right candidate, we support remote work and offer flexible working hours.
Your responsibilities will include:
Design, build, and maintain data infrastructure focusing on reliability and scalability that can accommodate the growing volume of data from the sensor fleet, using AWS cloud technologies (AWS S3, Lambda, RDS, EMR, and Glue/EventBridge).
Extract and integrate data streaming from our rapidly growing fleet of sensors via RESTful APIs into storage buckets and databases
Ensure data quality, accuracy, and completeness by implementing data validation protocols and testing
Provide input on which APIs and data formats to expose for easier ingestion of streaming data from the sensors
Evaluate infrastructure options that fit the business's needs and optimize costs at scale
Stay up to date with emerging technologies and industry trends
Automate data security and develop data microservices
Build scalable and performant databases using SQL, such as PostgreSQL and MySQL.
Implement data monitoring and alerting, ensure the data infrastructure is running smoothly and that any issues are detected and resolved quickly
Within 90 days, you will:
Ensured the robustness of, or rebuild, the current event-triggered data extraction pipelines to S3/RDS
Have met with key stakeholders to scope needs and priorities both long and short-term
Explored existing data pipelines and contribute significantly to a roadmap for data infrastructure and pipelines
Begun implementing proposed infrastructure and pipelines
About you: The type of person who will be a good fit
If you are excited about this role but not sure if you meet all of the criteria, please apply. Research shows women and minority groups are less likely to apply for roles when they don't meet 100% of the criteria. We'd like to change this.
Essential:
Minimum of 3 years of experience in data engineering
Strong programming skills in relevant languages such as Python, SQL (required) and Golang (or other low-level language and a willingness to learn Go), Java (preferred)
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field
Demonstrated proficiency in AWS cloud technologies, including AWS S3, EMR, EC2, RDS, Glue, and Lambda
Experience using workflow managers and building ETL processes
Experience with data pipeline development using tools such as Apache Airflow, AWS Glue, and AWS Data Pipeline
Expertise in data warehousing, database architecture, and SQL/NoSQL databases, such as PostgreSQL, MySQL, MongoDB, Cassandra, and DynamoDB.
Strong understanding of API development and implementation, including RESTful APIs using tools such AWS API Gateway
Hands-on experience with data ingestion tools such as Apache Flume and Kafka to collect, store and process large volumes of data in real-time or batch mode
Excellent problem-solving and analytical skills
Excellent communication and collaboration skills
Highly desired:
Experience working with IoT or sensor fleets particularly time-series data
Experience working with data stored in protobufs
Experience working with large and rapidly increasing data throughput (10s GB/day)
Experience serving data to be consumed by AI/ML teams
Experience building AI/ML models
Experience designing and building data infrastructure/pipelines end-to-end from inception
Experience working in a rapidly-growing start-up
About FiberSense:
FiberSense is a deep tech company with a remarkable set of capabilities and new service offerings that will transform the way we perceive and respond in public spaces. Our mission is to build the first centralized ubiquitous sensing fabric for all moving objects and events in public spaces in all cities around the world. Our expertise and technology sits at the intersection of optical fiber sensing networks, integrated photonics and machine learning. More than a sensor company, FiberSense is building out a global scale sensing platform using our patented VID+R™ (Vibration Detection and Ranging) technology. The company is Australian founded and already has employees across the globe, which is also true of our customers. FiberSense is implementing a giant vision and a culture built on two key pillars of massive innovation and effective execution. We offer an environment that is exciting, inclusive, challenging, fun and extremely rewarding.
FiberSense is an equal opportunity employer committed to providing a working environment that embraces and values diversity and inclusion. If you have any support or reasonable adjustment requirements, we encourage you to advise us at time of application. All qualified applicants will receive consideration for employment without regard to age, ancestry, colour, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.
Apply now. Join us on our growth journey as we transform the way we perceive and respond in public spaces!","$148,000 /yr (est.)",1 to 50 Employees,Company - Public,,,,Unknown / Non-Applicable
"1SEO Digital Agency
4.2",4.2,"Bristol, PA",Data Integration Engineer,"Based just outside Philadelphia, PA, 1SEO Digital Agency proudly stands as a full-service Digital Marketing Agency and a Google Premier Partner, placing us in the top 1% of all agencies in the US. We prioritize our clients' growth, protection, and inspiration by being solely client-centric. Our team operates on a hybrid schedule, with the majority of us working four days in our Bristol, PA office and one day remotely each week.

We are looking for a skilled Data Engineer to help us solve the problem of multiple software systems with redundant and inconsistent data. The ideal candidate will have experience in integrating disparate data sources and implementing data pipelines to automate data flow between systems. They will work closely with our internal teams to identify pain points and design solutions to streamline our data processes and establish a single source of truth.

As our Data Integration Engineer, you will be responsible for designing, building, and maintaining our data infrastructure, including data warehouses, ETL pipelines, and data integration processes. You will be working with a variety of third-party SaaS applications, as well as helping to integrate with our own SaaS applications (in the future).
Responsibilities:
Design, build, and maintain data infrastructure, including data warehouses, ETL pipelines, and data integration solutions to connect multiple software systems
Collaborate with other teams to ensure data quality, accuracy, and accessibility
Work with a variety of third-party SDKs and APIs to integrate data and streamline workflow processes
Develop and implement data and cybersecurity protocols and procedures
Participate in the design and development of our future SaaS application
Troubleshoot and resolve issues with data integration processes
Document data integration processes and maintain data dictionaries
Requirements:
Bachelor's or Master's degree in Computer Science or Information Systems
Strong knowledge of database architecture, data modeling, and SQL
Experience with ETL tools and data integration techniques
Experience integrating cloud platforms, such as Google WorkSpace, Salesforce.com, NetSuite, QuickBooks, and various SaaS applications
Familiarity with software development best practices, including version control, testing, and continuous integration
Ability to communicate effectively with non-technical stakeholders and users
What You Can Expect From 1SEO Digital Agency:
Open-floor office environment with NO cubicles whatsoever. Basketball, Foosball, Billiards, and ping-pong are in the employee lounge.
A fully-stocked kitchen provided by ownership with catered lunches multiple times per week. There is no shortage of snacks & you could almost eat breakfast, lunch & dinner here every day.
Access to the gym in our building with NO membership fee. Work out before or after work, or during your lunch break.
The office is open from 8 am to 6 pm Monday-Friday. We offer a flexible work schedule for BOTH early and late risers

After 90 days of Full-time employment, we offer our full-time employees:
50% funded healthcare benefits (Medical, Dental & Vision) for the employee. Dependents can be added to the plan AND we offer Supplemental healthcare insurance at a reduced cost
Earn up to 3 weeks of PTO with an additional week given at year 3 AND another week at year 5.
You can join the 401K after your 1st year of employment, with up to a 4% match.
Generous incentive program for each anniversary you celebrate.","$102,793 /yr (est.)",51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2009.0,Unknown / Non-Applicable
"Health Federation Of Philadelphia
3.9",3.9,"Philadelphia, PA",EHR Data Engineer,"Equal Opportunity Employer
The mission of the Health Federation of Philadelphia is to promote health equity for marginalized communities by advancing access to high-quality, integrated, and comprehensive health and human services. Health equity is at the heart of all our work. We believe in and are firmly committed to equal employment opportunity for employees and applicants. We do not discriminate on the basis of race, color, national or ethnic origin, ancestry, age, religion, disability, sex or gender, gender identity and/or expression, sexual orientation, military or veteran status. This commitment applies to all aspects of the Health Federation of Philadelphia’s employment practices, including recruiting, hiring, training, and promotion.
JOB SUMMARY
The Electronic Health Records (EHR) Data Engineer will be responsible for managing all aspects of EHR related data with a focus on database administration, data integration, and data flow design to deliver advanced clinical and operational analytics for the Philadelphia Department of Public Health, Ambulatory Health Services (AHS), a large community health organization. The EHR Data Engineer will utilize a systematic approach and advanced tools to design and support AHS data management services. The position requires skills and knowledge to connect, store, manage, secure, analyze, report, distribute and govern EHR related data. This position works directly with AHS Leadership, EHR Program Management Office (EHR PMO) Program staff, Health IT (HIT), City Office of Innovation and Technology (OIT) and respective partner personnel to support comprehensive planning and tactical delivery of data management services in production operations. In addition, responsibilities include Tier 2 and Tier 3 support for EHR production issues and Helpdesk tickets regarding data, database, and respective services.
Strategic direction, programmatic and operational prioritization will be specified through the authority of AHS Leadership. Accountability is managed through the EHR PMO with AHS oversight.
JOB SPECIFICATIONS
Responsibilities/Duties
Support the design, development, deployment, and ongoing management of AHS data. Work with EHR PMO to integrate data management strategies for projects, programs, and initiatives.
Assist in the design, development, deployment, and support of relational and non-relational database designs in a team-oriented environment using various data visualization tools in accordance with City OIT standards.
Responsible for overseeing patch management and maintenance on reporting infrastructure and data related systems. Develop version and release migration methodologies.
Design, implement and control access to data and databases with/for approved AHS partners and City entities.
Assure systems and data integrity.
Oversee functional configuration components to support monitoring and administering assigned systems and applications.
Serve as the technical intermediary between departments and vendors for various database solutions and datasets within on-premise and cloud hosted systems.
Assist to design, implement, and manage integration points for automated retrieval, sharing and delivery of incoming and outgoing data.
Assist with data conversion, migration, and validation of data to ensure quality and accuracy.
Develop Standard Operating Procedures (SOPs) for EHR data, database, and system administration establishing standards and procedures to maintain consistent practices.
Abide by the EHR PMO change control policies and procedures while ensuring change control methods are congruent with overall Health IT and OIT procedures; Submit and present various changes related to AHS data management.
Support the backup and recovery strategy for database environments. Oversee and administer business continuity and disaster recovery tests and methods.
Perform daily administration and performance monitoring of database related systems.
Monitor and proactively support capacity and performance planning and analysis.
Identify database sizing requirements based on AHS patient population, provider, user and legal retention requirements; monitor space usage and alert appropriate staff members to resolve sizing issues and conditions that may cause application failures.
Perform root cause analyses and troubleshoot internal and external data-related problems.
Aid in the support/writing/configuration of data transformation services (DTS) and extract, transform, load (ETL) packages, Web Services, and SQL database objects to both monitor and maintain the database and support applications.
Actively participate in the creative process to continuously improve architectural designs and implementation to ensure optimal efficiency and cost effectiveness.
Assist AHS in developing and conducting presentations related to data management.
Maintain working relationships with AHS, EHR, HIT, City OIT, and Partner staff including regular status communications using EHR and AHS methods of communication.
Provide off-hours support for upgrades, problem resolution, outages, or disaster recovery events as needed.
Stay current with data management tools as departmental infrastructure evolves.
Assist AHS leadership & staff as needed.
Other data analysis and management duties as required.
Education
Bachelors Degree in Computer Science/Information Systems, Data Analytics, or related experience in the field.
Database and/or Server Administration related Certification
Industry related certificates or certification a plus
Skills/Experience
Demonstrated knowledge of the Systems Development Life Cycle
5-7 years of experience working with SQL Server (2016 or later) and related products; special consideration given to Microsoft Business Intelligence Platform/Development Studio suite (SSRS, SSIS, and SSAS)
5-7 years of experience working with Windows Server (2016 or later) and related products
Five (5) years working in health services related field
HIPAA/HITECH trained
In depth understanding of conceptual, logical, and physical database structures, architectures, and integration
Ability to translate, integrate, and export data from various cross-platform software and databases
Comfortable leading the design of both relational and OLAP databases
Database related expertise working with SQL Server (2012 or later), Oracle SQL developer, and related products; special consideration given to Microsoft Business Intelligence Development Studio suite (SSRS, SSIS, and SSAS)
Experience with working in physical and virtual server environments
Experience with log shipping, SQL back-up restore, and SFTP
Strong analytical skills including experience with tools for: Business Intelligence; Analytics; Dashboards; Reporting
Well-developed computer skills including: Office365, Visio, entity relational diagrams
Effective organizational skills, detail-oriented
Excellent communication and presentation skills, both written and oral
Well-developed interpersonal skills including proper phone etiquette
Ability to work independently, be flexible and handle multiple tasks
Ability to work with a variety of cultures and diverse audiences
Additional proficiencies not required, but preferred
Experience with FQHC, ambulatory, municipal government and/or public health
Knowledge of healthcare, medical, lab and/or public health terminology
Experience with eClinicalWorks, i2i Tracks, Rhapsody and VMware
Experience with Microsoft SQL, Oracle database, Azure Synapse Analytics, and other multi-model database management or data warehouse systems
Experience with Microsoft PowerShell, SharePoint, and Visual Studio
Experience with data visualization software such as Tableau, Power BI, Red Cap, Quickbase, etc.
Web Analytics; batch processing development; HTML; XML; IIS server administration
Work Environment
Standard office setting with extended periods at work station and periodic use of office equipment
Position Type and Work Schedule
Full time position, typical hours are Monday through Friday 8:30 am to 5:00 pm. Flex office schedule options available with supervisor approval.
Travel
Local travel to multiple sites could be required for periodic implementations, upgrades, outages or onsite meetings.
Physical Demands
Ability to manage in an office environment located in a City health center.
Ability to work in a cubicle or shared space environment.
Ability to travel between city locations.
Ability to lift up to 50 pounds.
Ability to work in a fast paced, high pressure setting.
Ability to tolerate extensive use of keyboard, typing, computer.
Salary and Benefits Our employees are our most valuable resource, so we offer a competitive and comprehensive benefits package, which can include:

Medical with vision benefits
Dental insurance
Flexible spending accounts
Life, AD&D and long term care insurance
Short- and long-term disability insurance
403(b) Retirement Plan, with a company contribution
Paid time off including vacation, sick, personal and holiday
Employee Assistance Program
Eligibility and participation is consistent with the plan documents and HFP policy.
DISCLAIMER
The Health Federation reserves the right to modify, interpret, or apply this job description in any way the Company desires. The above statements are intended to describe the general nature and level of work being performed by an employee assigned to this position. This job description in no way implies that these are the only duties, including essential duties, responsibilities and/or skills to be performed by the employee occupying this position. This job description is not an employment contract, implied, or otherwise. The employment relationship remains “at will.” The aforementioned job requirements are subject to change to reasonably accommodate qualified disabled individuals.","$100,000 /yr (est.)",51 to 200 Employees,Nonprofit Organization,Nonprofit & NGO,Civic & Social Services,1983.0,$5 to $25 million (USD)
"World Finance
3.3",3.3,"Greenville, SC",Data Engineer,"**Hybrid role open to local Greenville, SC or Candidates relocating to Greenville, SC**
The Data Engineer implements data-centric solutions to complex business problems. This individual will work alongside a data architect and the data engineering team to expand and optimize data delivery architecture. Providing expertise in data warehousing and data delivery, this individual will work with cross-functional teams including marketing, analytics, operations, and software engineering to build data-aware systems and process flows across the enterprise.
Essential Duties and Responsibilities:
· Protect confidential company information.
· Assist business, technology, and support partners/stakeholders to deliver secure data solutions.
· Design, build, and maintain data delivery solutions in accordance with governing data architecture patterns.
· Model and assemble data sets that meet functional and technical business requirements.
· Implement infrastructure required to optimize ETL and ELT operations across a variety of data sources.
· Process file-based data extracts using data retrieval and management tools to provide timely loading of critical business data.
· Identify, design, and implement process improvements in data flows and data pipelines, focusing on automating data tasks.
· Integrate external systems with internal systems to ensure proper data flow between systems.
· Maintain an accurate and comprehensive inventory of data, data systems, and data storage.
· Communicate with non-technical stakeholders to determine technical solutions to business problems.
· Perform data load, data extraction.
Qualifications:
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required:
· Expertise with relational databases.
· Advanced SQL proficiency.
· Experience working with data warehousing systems for large enterprises with both internal and customer facing applications, preferably with near real time transactional data.
· Expertise in creating and maintaining data structures in SQL.
· Experience structured and non-structured data paradigms, relational databases, data lake and data warehouse technologies, data vault and dimensional data models.
· Ability to define problems, collect data, establish facts, and draw valid conclusions.
· Ability to interpret an extensive variety of technical instructions in mathematical or diagram form and deal with several abstract and concrete variables.
· Ability to conduct research into systems issues and products to determine integration requirements.
· Ability to communicate ideas in both technical and user-friendly language.
· Ability to work with IT operations to quickly come to resolution of open support requests.
Self-motivated and able to work within a project-based environment.
Possesses strong oral and written communication skills, clearly and accurately communicating complex and/or technical information to both technical and nontechnical audiences.
Python for building and redefining data pipelines is required.
Education and/or Experience:
· Bachelor’s Degree in MIS, Computer Science, or related field, or equivalent professional experience.
· Minimum 6 years of designing and implementing Data engineering solution.
· Hands on experience with building productionized data ingestion and processing pipelines.
· Experience working in business intelligence and data warehousing environments.
· MS SQL Server, Integration Services (SSIS) required.
· Strong TSQL scripting abilities and understanding of complex stored procedures, views, data. aggregation/manipulation through table joins/queries, database design, normalization, and de-normalization techniques.
· Experience with Snowflake.
· Experience using Python.
· Experience with ETL tools (Matillion, Fivetran, Stich, etc) is a plus.
· Familiarity with Dev OPS, Airflow and DBT is a plus.
· Industry experience working with large data sets.
· Experience with Microsoft PowerBI or any BI visualization tool is a plus.
Physical Demands:
· Must be able to constantly remain in a stationary position.
· The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, etc.
· Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and computer printer.
· Occasionally may require light lifting to 25 pounds.
Work Environment:
· Office environment.
· Occasional travel may be required.
This job description reflects management’s assignment of essential functions; and nothing in this herein restricts management’s right to assign or reassign duties and responsibilities to this job at any time.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Vision insurance
Compensation package:
Performance bonus
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Greenville, SC 29601: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 4 years (Required)
Data warehouse: 4 years (Required)
Work Location: Hybrid remote in Greenville, SC 29601","$95,216 /yr (est.)",1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,1962.0,$500 million to $1 billion (USD)
"Maven LLC
4.3",4.3,Remote,Senior Data Engineer,"Description:
Please apply to job with this link: https://portal.dynamicsats.com/Application/WebForm/3e11b0af-f443-4cfb-ac80-066fe882c27b?portal=17b1dd1f-7af3-4dae-96aa-8723901631fa&job=f5ac005e-44e8-ed11-a7c6-002248081803
All About Us
Our client is a family community of banking teams that deliver an exceptional depth of resources to individuals and businesses, with a hands-on, personalized approach. Our client offers great benefits including health insurance for you and your whole family, 11 paid holidays, PTO, and tuition reimbursement to name a few.
About the Role
You'll be responsible for building, managing, and optimizing data pipelines and effectively converting into production for key data and analytics consumers. The Senior Data Engineer is also collaborating with Data Engineers, Data Architects, and Data Scientists across the enterprise, as well as Enterprise Architecture, to follow the standardization of data engineering integration architectures.
Job Essentials
Design, create and maintain projects involving data collection and storage systems
Recommend and deploy data models and solutions for existing data systems
Collaborate across teams on the design and maintenance of our client's Operations data mart (ETL, data modeling, metric design, metric design, reporting/dashboarding) to assure a stable reporting infrastructure
Work with data architects and data analysts to determine design needs
Mentor and provide guidance to junior engineers and new team members
Ensure data users and consumers utilize data responsibly through data governance and compliance initiatives
Perform data validation testing to ensure accurate data flows
Promote data and analytics capabilities and awareness to business unit leaders in order to leverage opportunities to achieve strategic business initiatives
Define coding standards, including Object Oriented design standards and design patterns
Implement a standardized library of reusable objects across the enterprise, such as the core banking platform
Maintain awareness of and adherence to Bank’s compliance requirements and risk management concepts, expectations, policies and procedures and apply them to daily tasks
Deliver a consistent, high level of service within our client's standards
Other duties as assigned
Requirements:
Bachelor's degree in computer science, information technology, or related field
Technical Expertise with SQL, PL SQL
Previous knowledge and experience integrating Informatica ETL
Experience with Tableau, Cognos, SSRS or equivalent BI tool
Previous experience using SSIS, Informatica or equivalent ETL
Skilled in Oracle, Snowflake, and SQL servers
Experience in AWS and Azure
Interpersonal/Customer Service Skills
Written and Verbal Communication. Ability to work as part of a team
Adaptable to change. Able to Multi-Task or Juggle Priorities
Analytical Thinking and Problem/Situation Analysis
Experience with Master Data Management (MDM) is preferred
An Equal Opportunity Employer
We do not discriminate based on race, color, religion, national origin, sex, age, disability, genetic information, or any other status protected by law or regulation. It is our intention that all qualified applicants are given equal opportunity and that selection decisions be based on job-related factors.",,1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"Pomeroy
3.1",3.1,"Cincinnati, OH",Data Engineer,"General Function:
The Data Engineer will play an important role in our growing Enterprise Data and Analytics
team. The person in this role will build out a new centralized Analytics Data Lakehouse,
help maintain our existing Operational Data Warehouse, and the infrastructure that
underlies both. We are looking for a candidate with experience creatively solving data
complexities of various sizes and levels of cleanliness - with the goal of enabling data
analysts and business users throughout Pomeroy to make decisions backed by data.
Job Type: Full-time
Pay: $70,000.00 - $80,000.00 per year
Benefits:
401(k)
Dental insurance
Employee assistance program
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
4 years
5 years
6 years
7 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 4 years (Required)
Oracle Cloud Integrator: 4 years (Required)
Data warehouse: 4 years (Required)
Work Location: In person","$75,000 /yr (est.)",1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982.0,Unknown / Non-Applicable
"Direct Line
3.8",3.8,"Ashburn, VA",Data Center Infrastructure Engineer,"SUMMARY:
Direct Line (“DL”) is a high growth global technology services company with primary focus in providing design, integration, installation, maintenance and managed services to well-known data center operators and technology companies. Direct Line deploys decades of experience and knowledge through key partnerships with hyperscale technology companies and multi-tenant data center operators that give its clients a competitive marketplace advantage. Direct Line is committed to continually improving our industry through certified training of cutting-edge technicians that deliver superior results with a passion for detail. Direct Line is headquartered in Fremont, California with additional locations in Virginia, Tennessee, North Carolina, New Mexico, the Pacific Northwest, Asia-Pacific, and Europe.
POSITION: Data Center Infrastructure Engineer
LOCATION: MUST BE LOCATED WITHIN A COMMUTABLE DISTANCE TO SANTA CLARA, CA OR ASHBURN, VA / MUST BE WILLING TO TRAVEL (75-80% travel)
Job description
We are now looking for a Data Center Infrastructure Engineer!
We are looking to grow our company and grow with the hardest working people in the world. Academic and commercial groups around the world are powering a revolution in artificial intelligence using deep learning techniques running on GPUs, enabling breakthroughs in the most complex problems from autonomous driving to medial image processing to natural language processing. Come work on an innovative company's AI technologies!
What You’ll Be Doing:
You will lead all aspects of and personally implement complex architectures in one of several data centers.
Solutions will include network, storage, and compute resources to meet customer requirements, SLAs and high levels of uptime. As a key member of the engineering team you will develop, implement, and lead rack-level elevation designs to ensure velocity and scale while efficiently utilizing space, power, and cooling.
Review, evaluate and improve the design and implementation of structured cable solutions to support network topologies
Establish continuous improvements in the design, implementation, deployment and operation of large-scale cloud-based solutions in power-dense air- and water-cooled environments
Develop and maintain processes and procedures associated with the management and deployment of data center infrastructure including asset management and RMAs
Support and expand data center monitoring applications, with a strong focus on CI/CD automation
You will ensure standards supporting operating procedures and engineering issues for problem incident management are followed, including all safety requirements
Handle network, electrical and mechanical operations at data centers focusing on availability, service delivery, and internal customer relationship management
Analyze and resolve critical engineering issues, often under tight timeframe pressures; Off hours and on-call hours are to be encouraged
What We Need to See:
You love solving hard problems and can work independently or as part of a team under tight timelines
You are passionate about providing outstanding support to customers
Bachelor’s degree in Math, Computer Science, or Engineering subject area. Equivalent background in Military Technical School also acceptable or equivalent experience in datacenter engineering operations.
6+ years’ experience as datacenter operations engineers with critical systems and telecommunications Infrastructure Standards, network certification is very desirable
Deep knowledge of data center operations including network, power, rack layouts, cabling, Raised Floor Systems, HOT/COLD aisle containment. Operational experience with compute, storage and GPU servers in both air- and water-cooled environments
Install, config, and maintain all NW and 3rd party HW
Experience with ERMA, Break-fix, etc.
Reading and understanding P2P cabling, labeling and cable mgt/dressing etc.
Ways to stand out of the crowd:
An obvious passion for getting things done in a fast-paced technology environment
Deep understanding of data center power and cooling infrastructure, of network and cabling infrastructure.
Experience with NetBox, CMMS, SNOW and Inventory Management tools.
You're a self-starter with an attitude for growth, continuous learning, and constantly looking to improve the team.
Attention to detail with superb interpersonal skills and the ability to effectively manage multiple priorities.
A positive attitude with a strategic outlook.
Constantly look to improve the team and build strong business relationships
Direct Line is a proud equal opportunity employer. We are a drug free, EEO employer committed to a diverse workforce. We will consider all qualified candidates regardless of race, color, national origin, sex, age, marital status, personal appearance, sexual orientation, gender identity, family responsibilities, disability, political affiliation, or veteran status.
Job Type: Full-time
Pay: $45.00 - $50.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Life insurance
Paid time off
Referral program
Vision insurance
Schedule:
Monday to Friday
Experience:
CI/CD: 3 years (Required)
Linux: 3 years (Preferred)
Server Support: 3 years (Required)
Break/Fix / ERMA: 3 years (Preferred)
Data center Engineering: 3 years (Required)
Work Location: On the road",$47.50 /hr (est.),501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,1997.0,Unknown / Non-Applicable
"Seamless.AI
3.4",3.4,"Columbus, OH",Senior Data Engineer - Remote US,"Role: Senior Data Engineer
The Opportunity
We are seeking a Senior Data Engineer with a strong background in data ingestion and processing for large data sets. The ideal candidate is maniacal about data accuracy and has a proven track record of building robust and scalable data pipelines. This role requires an experienced professional who can work collaboratively with cross-functional teams to design and implement solutions that meet business requirements and adhere to data security and privacy policies.
If you are a senior data engineer with a passion for data accuracy and a proven track record of building robust and scalable data pipelines, we encourage you to apply for this role. We offer competitive compensation, a flexible work environment, and opportunities for growth and advancement within the organization.
About Seamless
Seamless delivers the world's best sales leads. Through our product, we help sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence; by development of a robust real-time contact and company search engine as well as a suite of technically-advanced tools to support sales and lead generation. We have been recognized as one of Ohio's fastest growing companies and have been recently ranked No. 7 in LinkedIn's Top 50 Startups of 2022, featured in Forbes as #1 Software company in Ohio in 2022, and on G2's ""Top 100 Highest Satisfaction Products for 2022"" list!
The Seamless Team
We have an amazing culture and work environment that anyone would want to be a part of. We encourage a culture of positivity. We thrive off of continuous feedback and do whatever it takes to help our team and customers be successful. You will grow as an individual, professionally, and be able to see and feel the impact you are making to the growth of Seamless every day.
Role Responsibilities
Design and develop data ingestion and processing pipelines that ensure data accuracy and completeness.
Work closely with cross-functional teams to understand business requirements and design solutions that meet those requirements.
Lead efforts to identify and implement data quality control measures to ensure consistency and reliability of data.
Monitor data pipelines to identify issues and proactively resolve them.
Evaluate and recommend new technologies and techniques to improve data pipeline performance and scalability.
Ensure compliance with data security and privacy policies and procedures.
Mentor junior engineers and provide technical leadership in data engineering best practices.

Candidate Requirements
Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.
3+ years of experience in data engineering, with a focus on data ingestion and accuracy.
Strong experience with large-scale data processing frameworks such as Hadoop, Spark, or Flink.
Proficiency in programming languages such as Python, Java, or Scala.
Experience with SQL and NoSQL databases.
Familiarity with cloud-based data processing and storage platforms such as AWS, Google Cloud, or Azure.
Strong experience in designing and implementing data quality control measures.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Experience mentoring and leading teams of data engineers.
Seamless.AI has been delivering the world's best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio's fastest growing companies and won 2020 Best Places to Work and LinkedIn's Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.","$93,575 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015.0,Unknown / Non-Applicable
"Wish
3.2",3.2,"San Francisco, CA",Data Engineer,"Company Description

Wish is a mobile e-commerce platform that flips traditional shopping on its head. We connect hundreds of millions of people with the widest selection of delightful, surprising, and—most importantly—affordable products delivered directly to their doors. Each day on Wish, millions of customers in more than 60 countries around the world discover new products. For our over 250,000 merchant partners, anyone with a good idea and a mobile phone can instantly tap into a global market.
We're fueled by creating unique products and experiences that give people access to a new type of commerce, where all are welcome. If you’ve been searching for a supportive environment to chase your curiosity and use data to investigate the questions that matter most to you, this is the place.

Job Description

Our engineers move extremely fast, while solving unique and challenging problems. Our team is small and nimble. We release every day to ensure that engineers are able to iterate quickly, and make an impact immediately. We’re looking for engineers to work on our massive semi-structured datasets.
You'll develop software to process, transform and analyze the data to identify signals from billions of events we collect every day. You'll provide insights that improve the experience of hundreds of millions of users worldwide. You should be results-driven, highly motivated, and have a track record of using data analytics to drive the understanding, growth, and success of a product.
What you'll be doing:
Design and Develop data collecting and processing systems to handle large data sets. You’ll have the opportunity to design innovative data solutions and solve challenging problems.
Design, Develop and Support highly-parallel, and fault-tolerant applications.
Build and integrate scalable backend systems, services, platforms, and tools
Contribute to the design and code of complex data pipelines operating on production data
Optimize current approaches to efficiently handle ever-increasing volumes of data
Build proof of concept using modern technologies and convert them into production-grade implementation.
Create best-practice reports and dashboards based on data mining, analysis, and visualization

Qualifications
5 + years of experience as a Software Engineer or Data Engineer using Python, java or any other programming language
Expertise with SQL and data storage systems
Experience and knowledge of modern data warehouse, pipeline and reporting/analytic techniques and tools such as Airflow, Presto/Hive, Spark, or any other scheduling frameworks, Tableau or other reporting tools
Experience working on Amazon Web Services or other cloud computing platforms
Bachelor's degree in Computer Science or related field.
Preferred Qualifications:
Experience in data visualization a plus.
Here at Wish, you are joining a team and company at a time of growth and transformation. You will love being surrounded by people who are as passionate as you are about e-commerce, technology, and a data-driven culture. Even if you don't meet 100% of the above, we encourage you to still apply!
The estimated base salary range for this position varies by the candidate's location as follows:
Candidates located in California: $178,000 - $241,000 annually
Candidates located in New York and Washington: $201,000 - $241,000 annually
Candidates located in Colorado: $155,000 - $186,000 annually
For states not listed, the estimated base salary range for this position starts around $155,000 annually

Please note that individual total compensation for this position will be determined at the Company's sole discretion and may vary based on several factors, including but not limited to, location, skill level, years and depth of relevant experience, qualifications and other business considerations.

Additional Information

Wish values diversity and is committed to creating an inclusive work environment. We provide equal employment opportunities for all applicants and employees. We do not discriminate based on any legally-protected class or characteristic. Employment decisions are made based on qualifications, merit, and business needs. If you need assistance or accommodation due to a disability, please let your recruiter know. For job positions in San Francisco, CA, and other locations where required, we will consider employment for qualified applicants with arrest and conviction records.
Individuals applying for positions at Wish, including California residents, can see our privacy policy here.","$155,000 /yr (est.)",501 to 1000 Employees,Company - Public,Information Technology,Internet & Web Services,2010.0,$1 to $5 billion (USD)
"Colorado Community Managed Care Network
3.2",3.2,"Denver, CO",Data Engineer,"CCMCN focuses on improving the lives of Coloradoans through innovative solutions. Our areas of focus include Population Health, Care Coordination, Social Health Information Exchange, Interoperability, and Clinical Quality Improvement. Through working with health centers and community partners, CCMCN provides best-in-class solutions that create a more comprehensive and collaborative network of care for our most needed Coloradoans.
WHY
Work for CCMCN
At CCMCN, we strive to support our employees and their growth within their career life-cycle. We are committed to a diverse and inclusive work environment where new ideas and innovation are not only invited, but celebrated. We value collaboration, innovation, adaptability, and a great work-life balance.
At CCMCN, you will see the impact of your work directly in the improvements that we make with our partners and their communities.
Our Benefits:
Remote work (depending on position) with access to CCMCN's physical office
Friday flextime and volunteer time off
Health, dental, and vision insurance plans
HSA, FSA, DCA, and employer-sponsored HRA
Life, AD&D, and long-term disability insurance plans
401K retirement plan with employer match
Employee Assistance Program (EAP)
Paid leave including vacation, sick, holidays, & 2 floating holidays

Category: Full-time, Exempt
Salary: $70,000 - $85,000 (DOE)
Reports To: Data Engineering Director
At CCMCN, our mission is to provide services that enable our members and their community partners to succeed as efficient, effective, and accountable systems of care.
CCMCN’s vision is that all Coloradans have access to high-quality, integrated, accountable health care. Areas of focus include population health, accountable care, shared services, health information technology, and clinical quality improvement programming. CCMCN is governed by a Board of Directors comprised of organizational representatives from each of its health center members as well as representation from Colorado Community Health Network (CCHN) and clinician representatives. Through working with health centers and community partners, CCMCN provides technological and analytical tools that help create a more comprehensive and collaborative network of care for Coloradans.
Position Description:
The Data Operations department provides data management, integration, and reporting services for multiple external and internal consumers. The Data Engineer will be responsible for all aspects of data management that support CCMCN’s production services. Candidate must have experience in Microsoft SQL database development, data integration, ETL (extract, transform, and load) tools and methods, analytics, reporting, and documentation.
Essential Functions:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures, and code sets as well as applicable data privacy practices and legal requirements.
Required Skills and Experience:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures, and code sets as well as applicable data privacy practices and legal requirements.
Additional Preferred Qualifications:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures, and code sets as well as applicable data privacy practices and legal requirements.","$77,500 /yr (est.)",1 to 50 Employees,Nonprofit Organization,,,,Unknown / Non-Applicable
"DocuSign
3.7",3.7,"San Francisco, CA",Data Engineer,"Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

DocuSign is seeking a talented and results-oriented Data Engineer to focus on delivering trusted data to the business. The Data Engineer delivers data for analytics using our Enterprise Data Warehouse, enabling the global DocuSign analytics community via curated, governed and cleansed data. As a member of the Global Data and Analytics team, the Data Engineer leverages a variety of technologies to accomplish this goal, ranging tools like Airflow, Matillion, dbt, Snowflake and Fivetran to languages like SQL and Python. The successful candidate will develop solutions with innovative cloud technologies, work on a variety of fast-paced assignments, and partner with world-class technical and business teams to maximize the value of data.

This position is an individual contributor role reporting to the Manager, Data Analytics.

Responsibility
Build data pipelines using Fivetran, dbt/Matillion, Snowflake and Airflow
Develop and maintain data documentation including ERD, data dictionaries, data lineage, and metadata
Ensure data quality and integrity by implementing appropriate data validation and cleansing techniques
Monitor and troubleshoot data pipelines to identify and resolve issues in a timely manner
Build POCs to validate new concepts and new technologies
Collaborate with business, engineering, and data science teams to understand their data needs and design efficient solutions to support their requirements


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)


What you bring

Basic
Bachelor’s Degree in Computer Science, Data Analytics, Information Systems or similar, or equivalent work experience
5+ years of relevant experience
2+ years of dimensional and relational data modeling experience
Experience with modern data integration and transformation tools such as Fivetran, Dbt, and Matillion
Experience with workflow orchestration tools such as Airflow
Experience with MPP databases like Snowflake, Redshift and BigQuery
Experience with cloud platforms like AWS, Azure and GCP
Experience with versioning tools like git
Experience working with tools like Jira and Confluence
Experience with SQL and Python
Experience with document and data debugging

Preferred
Good knowledge of database and data warehouse concepts such as facts and dimensions to design and develop data models that support enterprise reporting and analytics needs
Ability to work independently with minimal supervision, as well as in a team environment
Excellent communication skills
Eye for detail, good data intuition, and a passion for data quality
Comfortable working in a rapidly changing environment with ambiguous requirements
Organizational and time management skills, with the ability to prioritize tasks and meet deadlines


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $118,300 - $182,775 base salary

Washington: $111,600 - $162,625 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.","$137,113 /yr (est.)",5001 to 10000 Employees,Company - Public,Information Technology,Information Technology Support Services,2003.0,$1 to $5 billion (USD)
"Spencer Gifts & Spirit Halloween
3.6",3.6,"Egg Harbor Township, NJ",Azure Data Engineer,"We are currently looking for a Data Engineer to optimize our data integration at scale. Inside our Data Integration team, you will be designing pipelines and warehouses to model data from multiple sources that will allow us to derive business insight. Using Azure and other open-source technologies, such as Azure Data Factory and PySpark, you will design and build our next-generation ETL pipelines and data models.
Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data
Develop data models and schemas in our data warehouse that enable performant, intuitive analysis
Handle the challenges that come with managing terabytes of data
Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance
Develop the server applications and APIs that are used by our Data Team
Responsibilities:
Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data
Develop data models and schemas in our data warehouse that enable performant, intuitive analysis
Handle the challenges that come with managing terabytes of data
Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance
Develop the server applications and APIs that are used by our Data Team
Requirements:
Bachelor’s degree or higher in Computer Science, Computer Engineering, Information Technology or related field
Fluent in several programming languages such as Python, R, or Scala
6+ years of work experience in building ETL pipelines in production data processing and analysis
Experience designing SQL tables, choosing indexes, tuning queries, and optimizations across different functional environments.
Hands-on experience writing complex SQL queries and using a BI tool
Experience with data lakes and designing and maintaining data solutions using Spark and Azure serverless services such as ADF
Familiarity with data ingestion APIs, data sharing technologies, and warehouse infrastructure and development
* Proof of vaccination for COVID-19 required for employment, reasonable accommodations considered for medical, pregnancy or sincerely held religious beliefs.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Egg Harbor Township, NJ: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Work Location: In person",,5001 to 10000 Employees,Company - Private,Retail & Wholesale,"Gift, Novelty & Souvenir Stores",1947.0,$100 to $500 million (USD)
"Shutterfly
3.3",3.3,"Eden Prairie, MN",Senior Data Engineer,"At Shutterfly, we make life’s experiences unforgettable. We believe there is extraordinary power in the self-expression. That’s why our family of brands helps customers create products and capture moments that reflect who they uniquely are.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.
What You'll Do as a Sr. Data Engineer II
Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines am improve the operation and performance of the Data Warehouse
The Skills You'll Bring
Expert knowledge Python, Spark, SQL scripting and Java
7+ years of hands-on experience with building data ingress frameworks and experience with dealing with a wide variety of source data formats
Hands-on experience in building data applications, including design, implementation, debugging, and support.
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it’s the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.","$132,354 /yr (est.)",10000+ Employees,Company - Private,Information Technology,Internet & Web Services,1999.0,$1 to $5 billion (USD)
"RelMap Consulting
4.8",4.8,"Addison, TX",Senior Data Engineer (Hybrid),"The data engineer entails:
Advanced SQL Server Development
Ability to create and modify views
Ability to create and modify custom functions
Ability to write complex queries, including joins, temp table processing, and common table expressions (CTE’s)
Ability to create and modify stored procedures using TSQL
Ability to analyze and optimize query performance using indices
Ability to use Red Gate’s SQL Compare to perform a schema (AKA “do a manual roll”) between one server an another, and between revision control and a database
Ability to stand-up new database server VM’s in Linux and Windows using VMWare
Ability to backup and restore a database in a Linux and Windows environment
Ability to manage and script automated database jobs
Ability to use PowerShell / Bash to interact with databases from the command line
Addition skills that would be desired but not required
· Experience with Redis
· Experience with MongoDB / CosmosDB
· Experience with Azure and Azure SQL
· C#, JavaScript (NodeJS / ES6), or Python experience
Job Type: Contract
Pay: $70.00 - $80.00 per hour
Compensation package:
Hourly pay
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Addison, TX: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",$75.00 /hr (est.),1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2010.0,$1 to $5 million (USD)
"Transfr
4.5",4.5,"New York, NY",Sr Data Engineer,"Transfr creates job-training simulations in virtual reality designed by each industry that teaches novices in the same manner that experts master their crafts - through trial and error. Data-driven simulations work like virtual coaches, adapting to every person’s pace and skills while responding to their mistakes. Our immersive Experience Training method helps trainees build confidence in their knowledge, skills, and abilities.
We provide a better way to train young adults for the skills they need to succeed on the job. We focus on developing a pipeline of talent for jobs that are going unfilled, significantly reducing cost and risk.

Job Summary:

Transfr is looking for a highly motivated, self-driven Data Engineer to join the team. This individual will have the opportunity to work with researchers, engineers, and product designers across the company and engage in cutting-edge research to contribute to learning research and analytics.

This position can be 100% remote or work with our team from our headquarters in NYC on a flexible, hybrid schedule.

Responsibilities:
Instrument data solutions from multiple sources that enable fast, data-driven decision making on issues including useability, learning, product engagement, and quality.
Build the infrastructure required for optimal extraction, transformation, and loading of data from multiple data sources using SQL, PostgreSQL, and AWS technologies
Build real-time, scalable data solutions that support A/B product testing and support data and learning science through enabling visualizations and views into complex data sets
Create and maintain data pipeline architecture, configuration and implementation
Work with internal teams to assist with data-related technical issues and support their data infrastructure, access and visualization needs.
Support multiple teams with the creation of data strategies and data guidance documentation and architectures
Adhere to data privacy and security guidelines and regulations analysis
Build data expertise and own data quality for allocated areas
Create efficient processes for acquiring, extracting, integrating, transforming, and modeling data to derive useful information
Qualifications:
Minimum 4+ years of experience in a Data Engineer role
Strong preference for candidates with a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases including Postgres and MySQL.
Strong programming and analytics experience with Python and R
Experience with using Python to build ETL pipelines
Strong knowledge and hands-on experience building data lakes and architecting warehouses
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
Architect and implement CI/CD strategy for Data platform
A successful history of manipulating, processing and extracting value from large datasets
Working knowledge of message queuing, stream processing, and highly scalable big data data stores
Experience supporting and working with cross-functional teams in a dynamic environment

What We Offer:

The base salary range for this position is expected to be between $130K - $150K, with the actual base salary amount dependent on a number of factors, including but not limited to a candidate’s credentials, relevant experience, and primary job location. In addition to salary this role will be eligible for additional company benefits such as stock options, 401(k), paid vacation and sick time, and medical/dental/vision insurance.

In Closing:

We invite you to join us. Be a part of creating pathways to prosperity by helping to develop training simulations to teach skills that lead to well-paying jobs, for all.

At Transfr, we embrace diversity because it breeds innovation. Transfr is an equal opportunity employer that participates in E-Verify committed to providing equal employment opportunities to all applicants, consultants, and employees, and prohibits discrimination and harassment of any type without regard to race, color, religion, age, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

**Must be authorized to work in the United States without restriction**

#LI-Remote

Learn more at https://transfrinc.com/","$125,505 /yr (est.)",51 to 200 Employees,Company - Private,Education,Education & Training Services,2018.0,$1 to $5 million (USD)
"CrossAmerica Partners
3.2",3.2,"Allentown, PA",Microsoft SQL Data Engineer,"We are looking for a talented Microsoft SQL Data Engineer to join our team. The successful candidate will be responsible for designing and implementing scalable, reliable, high-performance data solutions using Microsoft SQL Server. The ideal candidate should have a proven track record of delivering successful projects strong expertise in database design, data modeling, and data integration.
Key Accountabilities:
Design and develop data models, database schemas, and ETL processes using Microsoft SQL Server.
Build and maintain data warehouses and data lakes to support business intelligence and analytics
Collaborate with cross-functional teams to understand data requirements, and design solutions that meet business needs.
Develop efficient SQL queries, stored procedures, and triggers to support data access and analysis.
Perform data profiling and data analysis to identify data quality issues and develop strategies to resolve them.
Optimize database performance by tuning SQL queries, database indexes, and query execution plans.
Implement data security, access controls, and auditing mechanisms to protect sensitive data.
Ensure data quality and consistency by performing regular data validation and testing.
Develop and maintain technical documentation, including data dictionaries, database schemas, and ETL workflows.
Qualifications:
Bachelor's degree in Computer Science, Information Systems, or related field.
At least 5 years of experience in Microsoft SQL Server development, including database design, data modeling, and ETL.
Strong proficiency in T-SQL programming and SQL Server Integration Services (SSIS).
Experience with data warehousing, data integration, and data migration projects.
Working knowledge with Microsoft Azure Cloud Services is a plus.
Strong problem-solving skills and ability to work independently and as part of a team.
Excellent communication and interpersonal skills.
If you are passionate about data and have a strong technical background in Microsoft SQL Server, we encourage you to apply for this exciting opportunity.
We offer:
Competitive compensation
Health insurance (Medical, Dental, Vision)
STD/LTD, Life Insurance
401k with employer match
Vacation/Sick time
Paid holidays
& More!
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: In person","$87,950 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,,$1 to $5 billion (USD)
"Talent Group
2.3",2.3,"Austin, TX",data engineer with ML,"Required Skills :
A background in computer science, engineering, mathematics, or similar quantitative field with a minimum of 2 years professional experience .
Experience in implementing data pipelines using python.
Experience with workflow scheduling / orchestration such as Kubernetes, Airflow or Oozie.
Extract Transform Load (ETL) experience using Spark, Kafka, Hadoop, or similar technologies.
Experience with query APIs using JSON, Protocol Buffers, or XML.
Experience with Unix-based command line interface and Bash scripts
Job Type: Full-time
Salary: Up to $130,000.00 per year
Benefits:
401(k)
Experience level:
11+ years
Ability to commute/relocate:
Austin, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,000 /yr (est.)",1 to 50 Employees,Company - Public,Media & Communication,Broadcast Media,,Less than $1 million (USD)
"Point Predictive, Inc.",,"San Diego, CA",Senior Data Engineer,"Senior Data Engineer, San Diego, CA based OR willing to relocate to San Diego in 60 Days.
Company
PointPredictive is a fast-growing technology start-up that leverages a patented combination of artificial and natural intelligence [Ai+Ni] to provide risk assessments in the auto lending, mortgage, and retail space. The platform has been proven to reduce lender loan losses by 40-60% with review rates of 5-10% of their applications, resulting in higher productivity of lender risk management departments, significantly lower losses to their bottom lines, and improved customer experience. The company was founded in 2013 by a seasoned team of technology entrepreneurs with over 20 years of experience in the startup space (including several acquisitions) and has financial backing from top tier investors.
Role:
The company is looking for an outstanding Senior Data Engineer to focus on its Data Asset, scale the Database and Data Asset for high performance and reliability. You will also serve as an engineering resource for data related questions, issues, and bugs. Core skills include Python, Snowflake, database systems and SQL, and Amazon Web Services (AWS).
Responsibilities:
· Developing new extract-transform-load (ETL) processes and pipelines. Must be able to manage large volumes of data flowing in from a variety of formats and into a variety of location.
· Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Automation experience highly desired.
Write complex SQL; be fluent in relational based systems; have strong analytical and problem-solving skills.
Ability to represent complex algorithms in software; bring strong understanding of database technologies, management systems, data structures, and algorithms; a deep understanding in database architecture testing methodology.
Develop and execute test plan, debugging, and testing scripts and tools.
Building real-time streaming data pipelines; building and deploying data pipelines, data streams, and extract-transform-load (ETL) processes.
Manage Data Governance and Data Cleansing, as well as supporting production issues and customer requests.
Provide engineering support to customer issues and bugs. Research and implement fixes.
About you:
You have 5+ years of relevant software development industry experience building and operating scalable, fault-tolerant, distributed systems.
Database and software development experience with Python, SQL, Redshift, and experience with Amazon Web Services, Snowflake along with pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming, etc.
Experience with container services.
Fluid with Amazon Web Services.
Experience with concurrency, multithreading, and the deployment of distributed system architectures.
Experience leading and shipping large scope technical projects in collaboration with multiple experienced engineers.
You have excellent communication skills and the ability to work well within a team and across engineering teams.
You are a strong problem solver and have solid production debugging skills.
You Thrive in a fast-paced environment and see yourself as a partner with the business with the shared goal of moving the business forward.
You have a high level of responsibility, ownership, and accountability.
Job Type: Full-time
Competitive pay, bonus, equity, and benefits:
Benefits:
401(k)
Health insurance
Dental insurance
Flexible spending account
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Education:
Bachelor's or Master’s (Preferred)
Work Location: San Diego (Del Mar)
Job Type: Full-time
Pay: $105,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Stock options
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
Are you willing to be in the office 5 days a week to influence culture and capabilities ?
Are you willing to move to San Diego within 60 days if not within the commuting region ?
Work Location: Hybrid remote in San Diego, CA 92130","$132,500 /yr (est.)",,,,,,
"plaxonic
4.6",4.6,Remote,Senior Data Engineer,"We are seeking an experienced Senior Data Engineer.
This position is a remote opportunity.
Qualification
Bachelor’s degree in Computer Science or a related field
Minimum of 4 years of experience in building data driven solutions.
Applicants must be authorized to work in the US without requiring employer sponsorship currently or in the future. CSS does not offer H-1B sponsorship for this position.
Specialized Knowledge & Skills
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Attunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Knowledge and understanding of data standards and principles to drive best practices around data management activities and solutions.
Strong understanding of the importance and benefits of good data quality, and the ability to champion results across functions.
Ability to lead collaborative meetings which result in clearly documented outcomes, a concrete understanding of meeting attendee performance/reliability, and ongoing management & follow-up for action items.
Acts with integrity and proactively seeks ways to ensure compliance with regulations, policies, and procedures.
Requirements of this position?
- Python
- AWS (Build data pipeline in AWS environment, Foundational AWS services (s3, VPC, IAM). and Programming in AWS )
- Snowflake or Redshift -Data Integration tool
– PySpark or Glue
- USC (US Citizen) or GC (Green Card holder)
Job Type: Full-time
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
Informatica: 5 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
AWS: 5 years (Preferred)
Python: 5 years (Preferred)
Redshift: 5 years (Preferred)
Snowflake: 5 years (Preferred)
PySpark: 5 years (Preferred)
Glue: 5 years (Preferred)
Work Location: Remote",,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013.0,$1 to $5 million (USD)
"Cushman & Wakefield
3.8",3.8,"Delaware, OH",Data Center Operating Engineer,"Cushman & Wakefield (NYSE: CWK) is a leading global real estate services firm that delivers exceptional value for real estate occupiers and owners. Cushman & Wakefield is among the largest real estate services firms with approximately 53,000 employees in over 400 offices and 60 countries. In 2021, the firm had revenue of $9.4 billion across core services of property, facilities and project management, leasing, capital markets, valuation, and other services.
We believe in a work/life balance and value our employees. We offer:
Weekly Pay
Comprehensive benefits that start on your first day
Competitive time off policy
Advancement opportunities
Training
Company provided safety apparel
Tools provided
Currently, we have an opening for a Data Center Operating Engineer in the Delaware, Ohio area.
JOB DESCRIPTION SUMMARY
Responsible to ensure the efficient operation and maintenance of mechanical, electrical, and plumbing equipment and systems for the assigned property(s). Responsibilities include (but are not limited to) maintaining a clean and safe working environment, performing rounds, conducting routine assessments, performing day-to-day preventive and corrective maintenance, painting, and housekeeping for assigned properties
ESSENTIAL FUNCTIONS AND RESPONSIBILITIES
Perform all plumbing, electrical, or HVAC requirements of the building(s)
Maintain heating equipment, chillers (air and/ or water cooled), DX units, pumps, cooling towers, fan coil units, VAV, air distribution systems, etc.
Monitor and adjust all mechanical/pneumatic equipment, steam stations, control gauges, distributor panels, valves, thermostats, diffusers, and other equipment necessary to provide a comfortable environment for the buildings
Verify field conditions and perform any necessary repairs or adjustments
Monitor Energy Management
Repair doors, ceilings, hand railings, and floors and other general repairs, adjustments, and installations about the property
Perform repairs to plumbing fixtures (water closets, urinals, flush valve assemblies, lavatories, etc.)
Perform preventive maintenance duties in accordance with C&W standards, building protocol, manufacturer recommendations, and industry best practices. including changing filters, cleaning coils, flushing condensers, punching tubes, greasing fan, pump, and motor bearings as required, inspecting and adjusting belts, replacing motor bearings, aligning pulleys and shafts, monitoring condenser, chilled, heating and secondary water chemical treatment and its associated feed equipment, clean and maintain cooling towers, and perform annual inspections and other scheduled routines as directed.
Inspect engine room equipment, fan room equipment, cooling tower, all motors, house pumps, electric rooms, backup generator, fire pump(s), sump pump(s), and ejector pumps. Replace lamps and light fixtures, reinstall or replace signage, verify rooms are clean and clear of obstructions and debris
Check for properly operating emergency exit signs and lights and ensure free and clear access to emergency stairs and exits. Perform additional fire and life safety inspections as per NFPA and local jurisdiction, C&W standards, building protocol, and as directed by superiors and property management
Document and report activities to the supervisor
Respond immediately to emergencies (fire, evacuation, equipment failure, etc.) and customer concerns
Comply with all applicable codes, regulations, governmental agency, and company directives as relates to building operations and practice safe work habits
Complete all required C&W Safety Training as scheduled annually
Comply with C&W Uniform Dress Code while working and maintain a neat and clean appearance while on the property at times other than working hours
KEY COMPETENCIES
Technical Proficiency
Initiative
Flexibility
Multi-Tasking
Sense of Urgency
IMPORTANT EDUCATION
High School Diploma or GED Equivalent
Graduate of apprentice program or trade school preferred
IMPORTANT EXPERIENCE
5+ years of related work experience in operating mechanical, electrical, and plumbing systems in a commercial property setting
ADDITIONAL ELIGIBILITY QUALIFICATIONS
Appropriate license/permit for trade as may be required, i.e. Journeyman or Master Electrician License or City Licenses, such as Refrigeration Certificate of Fitness, High-Pressure Boiler License, High-Pressure Steam Operator, etc.)
May be required to have certification as a Universal Technician for CFCs depending on market licensure requirements
Possess and maintain a valid driver’s license and good driving record with periodic checks (where applicable)
Basic Computing Skills in Outlook, Excel & Word
Experience in operation, maintenance, and basic repair of HVAC, boilers, heaters, pumps, refrigerant systems, compressors, water systems, etc.
Knowledgeable in energy management systems, techniques, and operations.
Thorough knowledge of all building systems operations, maintenance, and repair.
Maybe only maintenance staff member on duty during certain shifts; may be required to work extended periods without relief when responding to priority/emergencies (including overtime type assignments); may require shift work and/or on-call duties
WORK ENVIRONMENT
This job operates in a professional office environment. This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets, and fax machines. Regularly required to travel outside between properties in varying weather conditions.
PHYSICAL DEMANDS
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.
Must have the ability to stoop, stand, climb, frequently lift a minimum of 50 lbs. of equipment (pumps, tools, ladders) and safely install rigging/lifting devices
Regularly required to crouch and reach to install/move equipment by bending forward at the waist or by bending legs and spine
Involves movement between departments, floors, and properties to facilitate work
Ability to speak clearly so others can understand you
Ability to read and understand the information presented orally and in writing
Regularly required to utilize vision abilities, allowing reading of printed material, drawings, and schematics
AAP/EEO STATEMENT
C&W provides equal employment opportunity to all individuals regardless of their race, color, creed, religion, gender, age, sexual orientation, national origin, disability, veteran status, or any other characteristic protected by state, federal, or local law. Further, the company takes affirmative action to ensure that applicants are employed and employees during employment are treated without regard to any of these characteristics. Discrimination of any type will not be tolerated.
OTHER DUTIES
This job description is not designed to cover or contain a comprehensive list of activities, duties, or responsibilities that are required of the employee. Other duties, responsibilities, and activities may change or be assigned at any time with or without notice.
Job Type: Full-time
Pay: $32.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Referral program
Retirement plan
Vision insurance
Experience level:
5 years
Schedule:
12 hour shift
Evening shift
Extended hours
Night shift
Work setting:
In-person
Office
Outdoor work
Warehouse
Ability to commute/relocate:
Delaware, OH: Reliably commute or planning to relocate before starting work (Required)
Education:
High school or equivalent (Preferred)
Experience:
Mechanical knowledge: 5 years (Required)
Electrical: 5 years (Required)
Plumbing: 5 years (Preferred)
Commercial Property Maintenance: 5 years (Required)
License/Certification:
Driver's License (Required)
Work Location: In person",$32.00 /hr (est.),10000+ Employees,Company - Public,Real Estate,Real Estate,1917.0,$5 to $10 billion (USD)
"Glue
3.4",3.4,United States,Senior Data Engineer,"About Glue
Glue is on a mission to make people as happy as they can possibly be at work. To help us get there, we're adding even more of the brightest minds to our team; could that be you?
Glue builds technology empowering distributed teams to stick together. Through an AI-powered connection algorithm HR and People leaders understand and act on the relationships within their organizations. Glue's platform also utilizes Meetups, Events, and Pulse surveys to build connections that further drive retention. Glue is backed by leading investors including Greylock, and Founders' Co-op.
Culture is at the heart of what we do—come join a dynamic and innovative team that's bringing wonder to work!
Job Overview: We are seeking a highly motivated and experienced Senior Data Engineer to join our team. As a Senior Data Engineer, you will be responsible for building and maintaining our data infrastructure, designing and implementing scalable data pipelines, and providing guidance to junior engineers. You will work closely with our data scientists and business analysts to ensure that our data is accurate, reliable, and easily accessible.
Responsibilities:
Design and implement scalable, reliable, and efficient data pipelines to support data ingestion, processing, and delivery
Build and maintain data infrastructure, including data warehouses, databases, and ETL processes
Collaborate with cross-functional teams to ensure that data solutions are aligned with business requirements
Ensure data accuracy, completeness, and consistency by implementing appropriate data quality checks and data governance processes
Provide technical guidance and mentorship to engineers, data engineers, machine learning engineers, and data scientists
Continuously research and evaluate new technologies, tools, and methodologies to improve our data infrastructure and processes
Develop and maintain documentation for data infrastructure, processes, and solutions
Qualifications:
Minimum of 5 years of experience in data engineering, with a proven track record of designing and implementing large-scale data pipelines and data solutions
Experience building and maintaining offline and online machine learning, analytics, and production infrastructure
Strong expertise in data warehousing, ETL, SQL, and technologies (such as Snowflake, PostgreSQL, Airflow, Fivetran, dbt)
Experience with cloud computing platforms and infrastructure-as-code (such as AWS, Docker, Terraform, Kafka, CI/CD)
Experience with event streaming platforms (such as Kafka, Kinesis)
Strong programming and software engineering skills
Excellent communication, collaboration, and problem-solving skills
Strong attention to detail and ability to work as a tech lead and independently in a fast-paced environment
Compensation:
The salary for this role will be between $165,000 and $200,000. In addition to salary this role will include equity in the form of stock options.
Benefits
Glue is proud to provide the following benefits to our employees:
Health, Vision, Dental coverage included
Unlimited PTO with a minimum of 2 weeks a year
Company sponsored 401k
Learning and development Stipend
Home office stipend
Bi-annual all company offsite
Monthly team events
12 weeks paid parental leave","$182,500 /yr (est.)",501 to 1000 Employees,Company - Private,,,,Unknown / Non-Applicable
"The Swift Group
5.0",5.0,"Chantilly, VA",Senior Data Engineer,"Job Description
The Swift Group is seeking Data Analytic expert to support and advance a learning data analytic program that collects and analyzes training data, measures learning related organizational drivers of success and provides information to facilitate data driven decisions.
Essential Job Responsibilities:
Extract, Transform, and Load (ETL) unstructured and structured data.
Transform unstructured and structured data into new formats facilitating analysis and reporting.
Write script (SQL Developer, Python) to ingest, process, clean, transform, and organize data from the Data Lake and other sources.
Assess data quality, verify data accuracy, and correct data issues.
Set up data models, infrastructure, pipelines, and frameworks.
Monitor and troubleshoot scripts for ingestion of business data from source systems.
Create data flow and data model documentation including data dictionaries.
Participate in development/maintenance/management of a data lake.
Work with team to create, update, and performance tune feeds of learning data from source systems.
Work with team to administer, performance tune, and configure cloud and data lake environments.
Work with team to receive, track, prioritize, develop, test, implement, and deploy new requirements for data feeds, web applications, and cloud services.
Troubleshoot solutions to data related issues.
Required Skills
5 to I0 years of data engineering experience and solid educational training in data engineering.
Advanced skills in data engineering and ability to assess data quality and verify data accuracy.
Advanced ability to Extract, Transform, and Load (ETL) unstructured and structured data.
Advanced experience using SQL Developer and Python programming languages to script the extraction and transformation of data.
Advanced experience with data infrastructure development and data lake management.
Advanced skills in data collection, cleaning, and maintenance.
Advanced ability to assess data quality and verify data accuracy.
.
Desired Skills
Preferred experience with data science techniques including data visualization.
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.","$112,276 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2019.0,Unknown / Non-Applicable
"Sadup Softech
4.2",4.2,United States,Data Engineer,"USA ,
Contract to Hire
5 M ago
Experience
3+ Years
Positions opened
11
Job Description
· Strong knowledge of Java, Java based frameworks like Spring · Good knowledge in Python programing & SQL concepts · Experience in Big data technologies such as Hadoop, Spark / Hive and scheduling (UC4) is preferred · Experience with RESTful web services and SOA concepts · Strong communication skills in a collaborative environment · Strong critical thinking skills, Ability to devise innovative solutions · Should be a strong advocate of code craftsmanship, good coding standards
Desired Candidate Profile
Work with the Agile team to clarify the new products and features requested by the Product team.
Collaborate with Data science & other dependent teams to design and implement the required solutions.
Explore new industry standard practices and implement for the project.
Participate in pair programming in the delivery of both POC and targeted features.
Understand and apply our technical architecture to ensure consistent, reliable, and secure deployments.
Enhance and maintain existing product capabilities.
Participate in formal and informal code reviews to ensure code quality.
Actively contribute to the automated test suite to enable continuous integration.",,1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
Everybody Votes Campaign,,United States,Data Engineer,"ABOUT EVERYBODY VOTES CAMPAIGN
Everybody Votes Campaign (EVC) is a national non-partisan, not-for-profit hiring staff for a large-scale coordinated civic engagement campaign active through the 2024 election cycle. The campaign aims to create a more representative democracy by registering millions of underrepresented voters across the country. This effort focuses on voter registration in a targeted fashion by conducting at-scale, effective, efficient, metrics-driven registration work. Through this work, we seek to fundamentally change the make-up of the electorate and to increase the political power of traditionally underrepresented communities in our democracy.
We directly fund organizations who execute voter registration and run quality control operations. We are dedicated to being active participants with the organizations to ensure their programs are effective and promote an investment in the future of emerging communities.


ABOUT THE OPPORTUNITY
EVC seeks a Data Engineer to implement, maintain, and optimize voter registration data pipelines. The Data Engineer will be responsible for large scale reporting automation with a particular focus on standardizing complicated voter registration data across multiple databases, working closely on a multidisciplinary team of engineers, analysts, project managers, and field practitioners. This individual will be at the forefront of the campaign’s effort to develop advanced voter registration dashboards and pipeline solutions, working closely with partner organizations. The Data Engineer will report to the Director of Data Products.

WHAT YOU WILL DO IN YOUR ROLE

Centralize data from disparate sources across multiple databases and use innovative hygiene solutions to clean up traditionally complicated voter registration data
Optimize and automate the campaign’s voter registration data pipeline, integrate new external data sources, and help our data team automate and streamline manual reporting processes
Set best practices for data standardization and refinement
Participate in maintaining a well-documented, consistent codebase
Work as a team: pair-program, review code, co-design and plan, develop a shared vision for and an understanding of the work, document progress in JIRA and Confluence
This is a great opportunity for someone who:

Enjoys coming up with creative solutions to big questions through collaboration, and is able to use immediate challenges as windows into future opportunities.
Notices and fixes errors that others might overlook. Acknowledges mistakes and turns them into learning opportunities. Has a track record of leaving things better than they found them – simplified pipelines, strong documentation, code sharing discipline.
Plans ahead and finds alternative paths, when needed, to get to the finish line. Bounces back from setbacks and rejections. Holds a high bar when things are hectic.
Brings civic engagement experience working for or with groups that serve communities of color.
Stays ahead of the curve in an ever-changing technology environment.
Identifies decisions, policies, or practices that have disparate impacts based on identity. Is driven to make changes in systems and practices to operationalize equity.
CORE COMPETENCIES

Growth mindset: demonstrated ability to take and receive feedback with professionalism and grace from peers and staff as well as supervisors
Relentlessly goal-oriented: enjoys working toward and achieving ambitious goals; willing to go over, under, around, or through any obstacle that gets in the way of meeting goals with a proven track record of creating and executing/managing comprehensive strategic goals
Cultural competency: able to build relationships and collaborate with colleagues, partners, and stakeholders across multiple lines of identity difference
Keeps Calm in Stressful Situations: demonstrated capacity and willingness to work long hours during peak season, rolling up their sleeves and getting the work done
REQUIREMENTS FOR THIS ROLE

Proficiency and experience using APIs to push and pull data
Strong knowledge of SQL and management of relational databases (in particular, Redshift and PostgreSQL)
Experience working with messy data, and creating and maintaining data pipelines
Strong proficiency in programming (preference for Python)
Experience writing and editing clear, clean code and a willingness to use version control systems like git and Github
Interest in being a member of a diverse, multidisciplinary team that communicates technical concepts to non-technical audiences
Helpful but not required:

Previous experience working in the field of voter registration or voting rights
Familiar with cloud infrastructure (e.g. AWS, GCP)
Understanding of Terminal and the command line interface
Experience creating reports using data visualization & business intelligence tools (e.g. Tableau, PowerBI, Periscope, Google Data Studio)
BENEFITS AND CULTURE
We offer flexible remote forward work, and a generous benefits package; including 100% cost coverage of employee health benefits, 401K with an automatic employer contribution regardless of employee contribution level, virtual therapy, stipend for ergonomic office set ups and generous vacation and leave policies.


All employees must be eligible to work lawfully within the United States upon the commencement of employment. The organization does not sponsor visa applications for prospective or current staff.


Salary Range: $88,000-$102,500
Our work is centered on creating a deeply inclusive and significantly more representative electorate. In order to be successful in this role, the candidate must have the cultural competence to successfully work with a diverse group of staff, partners and stakeholders. We especially strongly encourage applicants with close ties to Black, Latinx, Indigenous, non-English-speaking, disability, and LGBTQ+ communities to apply. We are proudly an Equal Opportunity Employer.","$95,250 /yr (est.)",1 to 50 Employees,Nonprofit Organization,,,,Unknown / Non-Applicable
"Bailey Information Technology Consultants, LLC.
5.0",5.0,"Chantilly, VA",TS/SCI Data/ETL Engineer,"Data/ETL Engineer
Location(s): St. Louis, MO; Chantilly, VA; Gaithersburg, MD; Alexandria, VA
Clearance: TS/SCI required.
Required Certifications: DoD 8570 Compliant Certification (Security+)
Level: T3 or T4

JOB DESCRIPTION:

Bailey Information Technology Consultants is recruiting for an experienced ETL Engineer to assist with the development, integration, deployment, and sustainment of critical systems. In this role, the ETL Engineer will work with a large database (Terabyte scale), mission-critical geospatial data, and best-of-breed cloud (AWS) databases and development tools to enable customer mission.

RESPONSIBILITIES:
The primary duties will be developing and maintaining a custom data integration and validation pipeline used by federal mission clients.
Build and support custom Python scripts to transform, standardize, and load an extensive geospatial data archive.
Creating and maintaining a custom data transformation pipeline using Python and Apache NIFI.
Hands-on work with AWS services such as EC2, RDS, Lambda, IAM, and S3
Working and optimizing the performance of an RDS Postgres database
CORE QUALIFICATIONS:
Proactive self-starter demonstrating a positive, willing attitude and excellent oral and written communication skills.
Deep understanding of AWS permission and restrictions
Extensive experience using RDS
Direct experience with Python scripting
Experience using GitLab
Experience with AWS services/technologies (e.g. EC2, Lambda, IAM, S3)
Proficient using Team Collaboration tools (Jira, Confluence, RocketChat)
Must possess an active Top-Secret Security Clearance with SCI (TS/SCI)
Must be a U.S. Citizen
Typically requires a BS in Information Systems, Information Technology, Computer Science or closely related discipline and 8-12 years of prior relevant experience or Masters with 6-10 years of prior relevant experience.
PREFERRED QUALIFICATIONS:
AWS certification (e.g. Database, Big Data)
Experience with ArcGIS or geospatial technologies","$94,089 /yr (est.)",1 to 50 Employees,Contract,,,,Unknown / Non-Applicable
"ZAGG, Inc.
3.6",3.6,"Midvale, UT",Senior Data Engineer,"Position Summary
ZAGG is looking for an individual who can drive business change through insightful development of systems and applications. This position will work closely with business owners to architect, develop, and implement solutions across ERP, CRM, EDW, and other applications. A successful candidate will have excellent communication and technical skills to help implement business requirements into working solutions.
Salary: $120K to $135K
Responsibilities:
Administration – 30%
Work closely with multiple departments locally and overseas contributing and solving complex issues that will directly affect business operations and outcomes
Gather/evaluate requirements for business processes and technology enhancements while uncovering areas for improvement
Manage and prioritize projects and resources to ensure business goals are met and maximum value is created
Evangelize applications through effective process design and user training
Create and maintain documentation for operational and security audits

Development – 60%
Architect and manage solutions across multiple applications ensuring efficient integrations that provide redundancy, visibility, and extensibility
Utilize technology to improve the quality of life by automating and enhancing the ability of users/departments
Enhance cloud based data silos supporting Microsoft Dynamics CRM technologies
Conduct application testing and provide database management support
Create and maintain integrations between core applications, services, databases, etc.
Consolidate multiple data silos into a single EDW used for companywide Reporting and Analytics
Model and build data structures to support multi-dimensional data discovery

Other duties as required – 10%

Qualifications:
7+ years of similar experience in an engineering role utilizing an EDW system
Bachelor’s degree in Computer Science or related area
Exceptional analytical and conceptual thinking skills with a detail oriented and inquisitive personality
Proven experience gathering and interpreting business requirements and converting them to technical blue prints
Knowledgeable in enterprise technology stacks (Servers, Database, Network, EDI, etc)
Strong experience in cloud architecture and development (Azure, AWS)
Strong experience in supporting data structures supporting Microsoft Dynamics Technologies including MSSQL and Cloud CDS
Strong experience with integration tools and web service protocols such as SSIS, Fivetran, Synapse, Jitterbit, Smart Connect, Scribe, SOAP, REST, etc.
Experience in a development/scripting language such as python, powershell, etc.
Strong SQL Skills along with an understanding of data warehouse methodologies
Strong experience with data warehousing platforms (Azure DW, Redshift, Matrix, Snowflake)
Strong experience with visualization tools (PowerBI, Tableau, SSRS, etc)
Strong knowledge of SDLC and Agile/Scrum Framework
Strong understanding of operations in a consumer goods industry – sales forecasting, inventory, etc. is a plus
Be able to communicate with customers and co-workers in an effective, timely and professional manner
Strong interpersonal and meeting facilitation skills with technical project management experience being a plus
Must have a collaborative style and be able to cultivate and maintain an open environment where ideas are shared, questioned and tested","$127,500 /yr (est.)",501 to 1000 Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,2005.0,$500 million to $1 billion (USD)
The Josef Group Inc.,,"Herndon, VA",Data Engineer,"Data Engineer
TS/SCI and Poly is Required
Herndon, VA
We are charged with taking commercial and academic innovation high-side, in domains of Artificial Intelligence / Machine Learning (AI / ML) such as Computer Vision (Image and Video Processing), Natural Language Processing (NLP), and Audio Modeling. We also bring the best ideas, tools, and approaches in technology infrastructure (AWS, DevOps, etc.) to the IC. The tech stack used is extremely broad - anything cutting edge in the commercial market, the open source community, or the academic research community is likely involved: and if something isn't being looked at yet, you can make that happen.
This effort supports ALL missions of the Intelligence Community, including cyber-related data science missions. A seamless group of contractor and customer personnel work to create innovations that supply customer groups with the data sets, models, algorithms, software, and infrastructure they need to increase their mission success. Management is hands off, gives the team the freedom to explore new approaches, and markets the best ideas and results to all the other IC customers.
This project regularly needs various types of people - Data Scientists, Data / ETL Engineers, Analytic Software Engineers, Full Stack Developers, UI/UX Developers, and AWS/DevOps experts. We're particularly interested in people with any of the following experience:
developing AI / ML models (neural networks, tree based algorithms, etc.);
conducting data analysis in the fields of Computer Vision, NLP, or audio signal processing;
doing data ingest and ETL into sponsor environments; or
building data-analytic software systems.
Work on this program takes place throughout the Reston/Herndon/Chantilly, VA area (we cannot support remote work) and requires a TS/SCI + Poly clearance (acceptable to this customer).
THE ROLE
Perform extract, transform, load (ETL) development for data pipelines
Integrate and update lab-to-factory services into data pipeline
Integrate and update workflow orchestration
Parsing
Perform API service development
General Skills
ELK Stack (elastic search, logstash, and kibana) - ingestion, management, and access control
Python
Bash Scripting - ad-hoc processing, cron jobs, etc.
Experience working on cloud environments","$190,000 /yr (est.)",1 to 50 Employees,Company - Public,,,,Unknown / Non-Applicable
"Integrated Technology Strategies, Inc.
4.0",4.0,"New York, NY",Data Engineer,"Integrated Technology Strategies is a provider of information technology consulting. Digital transformation, improving business performance, developing strategies and enhancing value is at the core of what we are.

Integrated Technology Strategies is looking to hire a Data Engineer to be a part of our Consulting team.

We are looking for someone passionate about data is focused on developing the platform for critical data products, including real time business metrics and analytics capabilities. The role requires supporting and collaborating with groups including Data Analytics/BI/Product, as well as our core backend API team. The individual will not be afraid to think out of the box and will play a key role in technical decision making. We are highly focused on giving ownership and responsibility to autonomous teams, using the right tools for the job, and building flexible architectures.","$110,128 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007.0,$5 to $25 million (USD)
"DSMH LLC
5.0",5.0,"Peoria, IL","Sr. Data Engineer (API, Cloud)","Education
- Bachelor degree in Computer Science, Information Systems, or a
related field; a Master degree is preferred
- At least 15 years of professional experience, with a focus on API
design and development, and extensive knowledge of the
Snowflake Data Cloud platform
Technical Skills
(Required)
- Proven experience in leading large-scale API and Snowflake
implementation projects
- Strong knowledge of API technologies, such as REST, GraphQL, or
gRPC
- Experience with ETL and data integration tools, such as Talend,
Informatica, or Matillion
- Deep understanding of data modeling, data warehousing, and data
quality concepts
- Familiarity with cloud platforms, such as AWS, Azure, or GCP, and their
respective services
- Excellent communication, leadership, and problem-solving skills
- Ability to work effectively in a fast-paced, collaborative environment.
- Strong attention to detail and commitment to delivering high-quality work
- Experience with agile methodologies and project management tools,
such as JIRA or Trello
Job Type: Contract
Pay: $55.00 - $60.00 per hour
Experience level:
11+ years
Schedule:
Monday to Friday
Experience:
APIs: 10 years (Required)
AWS: 5 years (Required)
Snowflake: 10 years (Required)
Work Location: In person",$57.50 /hr (est.),1 to 50 Employees,Company - Private,,,,$1 to $5 million (USD)
"GTA (Global Technology Associates)
4.6",4.6,"San Francisco, CA",Data Communications Engineer - III,"Looking for a Data Communications Engineer - III
What you will be doing as a Data Communications Engineer – III
This member of technical staff will be responsible for managing the standalone IT network and 4G/5G infrastructure and mobile edge cloud environments for a test and development lab program.
This is a technical position focused on supporting the development of technology solutions testing and incubation of new platforms and the product engineering of greenlit projects.
In this role the candidate will work with internal technology and product groups and external partners to support the development of new solutions.
He/She will be responsible for the architecture design and deployment of new 4G/5G and MEC platforms and features and will work very closely with Device testing Product engineering Business groups and peer TPD teams.
The candidate will be a member of a small team of Sr. Network engineers who manage remote lab access configure and manage virtual environments to support projects monitor and troubleshoot problems and make recommendations on how to streamline development with internal and external partners.
What you will bring to the table as a Data Communications Engineer - III
BA in IT/EE or Computer Engineering
The ideal candidate needs to have a strong technical background with experience in wireless and wireline networks hands on integration experience with MEC/Cloud/IT
Experience working in a customer facing environment Expert understanding of computer technologies and architectures including O/S Database NFV Cloud and SDN.
Passion for working with cutting edge technology Ability to work well in a team environment.
What you didn’t know about us:
Competitive salary
Health, Dental and Vision Benefits
Short/Long Term Disability and Critical Care/Illness Protection
Life Insurance and Retirement Plans
Employee Assistance Program
With this position, you will get the opportunity to work with our game changing clients and further advance your already valuable experience in the telecom industry!
We are Connectors. We thrive on ‘quality over quantity’ and put in the work building strong relationships. We create connections, discover qualities, uncover skills, and place people with accuracy. We are your true partner!
We are Collaborators. You’ll be working with a wholly-owned subsidiary of Kelly and part of the Kelly Telecom division. It allows us to be as nimble and fiercely competitive as a startup while having the backing of a multibillion dollar publicly traded company which has been in business for 75 years. With direct access to hiring managers, services don’t stop at standard recruiting processes. We use our expertise to improve your application skills and provide ongoing career support.
We give 24/7 Support. We are in this together. We provide around the clock availability, competitive employee benefits, and continuously check-in to make sure things are going smoothly. Check out our Glassdoor page!
Kelly Telecom is an equal opportunity employer and will consider all applications without regard to race, genetic information, sex, age, color, religion, national origin, veteran status, disability, or any other characteristic protected by law. For more information click Equal Employment Opportunity is the law.
You should know: Your safety matters! Vaccination against COVID-19 may be a requirement for this job in compliance with current client and governmental policies. A recruiter will confirm and share more details with you during the interview process.
#JobsAtKellyTelecom
Job Types: Full-time, Contract
Salary: $91,144.17 - $180,998.90 per year
Work Location: In person","$136,072 /yr (est.)",Unknown,Company - Private,,,,Unknown / Non-Applicable
"DSFederal Inc
4.0",4.0,Remote,Junior Data Engineer*,"Description:
We are seeking a Junior Data Engineer to support the design, development, and maintenance of our government client’s data infrastructure. The Junior Data Engineer will work under the guidance of senior team members to implement and maintain data solutions to support business needs.
Requirements:
Develop, maintain, and optimize data pipelines.
Collaborate with cross-functional teams to design and implement data-driven solutions.
Assist in the development and maintenance of data storage systems.
Troubleshoot and debug data-related issues.
Participate in code reviews and contribute to best practices for data engineering.
Technical Skills Required:
Proficiency in at least one programming language (Python, Java, etc.).
Understanding of data modeling, database design, data marts, and data warehousing concepts.
Familiarity with ETL tools and techniques.
Experience with cloud-based data platforms such as AWS or Azure.
Strong problem-solving and analytical skills.
Desired Skills:
Familiarity with data visualization tools such as Tableau or Power BI.
Knowledge of machine learning concepts and frameworks.
Experience Required:
1-2 years of experience in data engineering or related field.
Education Required:
Bachelors in Engineering

Who We Are:
DSFederal is a leader in health IT and data analytics that delivers innovative solutions to improve healthcare delivery and address critical challenges, including cancer surveillance, HIV/AIDS, maternal and child health, and COVID-19 emergency response. Our highly experienced professionals serve over 50 different federal clients on over 80 projects. Our people are united by one mission – to improve human life through transformative solutions. www.dsfederal.com.
What We Can Offer You:
DSFederal attracts the best people in the business with our competitive benefits package that includes medical, dental and vision coverage, 401k plan with employer contribution, paid holidays, vacation, and more. If you enjoy being a part of a high performing, professional services, and technology focused organization, we encourage you to apply!
Our Mission: Improve human life through transformative solutions.
Our Core Values: Integrity, People First, Disciplined Business Practices, Customer Centric, Value Creation
EEO Statement: DS Federal is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status.
E-Verify Statement
DS Federal utilizes the E-Verify program for every potential new hire. This makes it possible for us to confirm that every employee who works for DS Federal is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website by clicking the logo below. E-Verify® is a registered trademark of the United States Department of Homeland Security.
#LI-LP1",,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007.0,$25 to $100 million (USD)
United Digestive,,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs",,Unknown,Company - Public,,,,Unknown / Non-Applicable
"Swish Analytics
5.0",5.0,United States,Data Engineer,"Company Overview
Swish Analytics is a sports analytics, betting and fantasy startup building the next generation of predictive sports analytics data products. We believe that oddsmaking is a challenge rooted in engineering, mathematics, and sports betting expertise; not intuition. We're looking for team-oriented individuals with an authentic passion for accurate and predictive real-time data who can execute in a fast-paced, creative, and continually-evolving environment without sacrificing technical excellence. Our challenges are unique, so we hope you are comfortable in uncharted territory and passionate about building systems to support products across a variety of industries and consumer/enterprise clients.
Job Description
The Swish Analytics team is seeking Data Engineers to have direct impact on the infrastructure and delivery of our core consumer and enterprise data offerings. We're a team passionate about accurate predictions and real-time data, and hope you find satisfaction in building new products with the latest and greatest technologies. This is a remote position.
Duties
Architect low-latency, real-time analytics systems including raw data collection, feature development and endpoint production
Build new sports betting data products and predictions offerings Integrate large and complex real-time datasets into new consumer and enterprise products
Develop production-level predictive analytics into enterprise-grade APIs
Contribute to the design and implementation of new, fully-automated sports data delivery frameworks
Requirements
BS/BA degree in Mathematics, Computer Science, or related STEM field
Experience writing production level code
Proficiency in Python
Proficiency in SQL (preferably MySQL)
Experience building end-to-end ETL pipelines
Experience utilizing REST APIs
Experience in SQL database management, shema design, index structuring
Experience with version control (git), continuous integration and deployment, shell scripting, and cloud-computing infrastructures (AWS)
Experience with web scraping and cleaning unstructured data
Knowledge of data science and machine learning concepts
A strong interest for US sports and sports betting. You love sports, particularly the NFL, NBA, MLB, NHL, College Football, College Basketball, and Tennis, and can use your knowledge of the sport to inform your work with complex datasets
Base salary:$90-145,000
Swish Analytics is an Equal Opportunity Employer. All candidates who meet the qualifications will be considered without regard to race, color, religion, sex, national origin, age, disability, sexual orientation, pregnancy status, genetic, military, veteran status, marital status, or any other characteristic protected by law. The position responsibilities are not limited to the responsibilities outlined above and are subject to change. At the employer's discretion, this position may require successful completion of background and reference checks.","$117,500 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2014.0,Unknown / Non-Applicable
"CareFirst BlueCross BlueShield
3.5",3.5,"Washington, DC",Principal Data Engineer (Remote),"Resp & Qualifications
PURPOSE:
Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources. Creates data collection frameworks for structured and unstructured data. Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent. Employee is recognized as an expert within the organization and has in-depth and/or breadth of expertise in own discipline and broad knowledge of other disciplines within the function. Anticipates internal and/or external business challenges and/or regulatory issues; recommends process, product or service improvements. Solves unique and complex problems that have a broad impact on the business. Contributes to the development of functional strategy. Leads project teams to achieve milestones and objectives.

ESSENTIAL FUNCTIONS:
Leads project teams to achieve milestones and objectives. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources. Develops data models by studying existing data warehouse architecture; evaluating alternative logical data models including planning and execution tables; applying metadata and modeling standards, guidelines, conventions, and procedures; planning data classes and sub-classes, indexes, directories, repositories, messages, sharing, hiding, replication, back-up, retention, and recovery.
Solves the most complex problems; takes a new perspective on existing solutions. Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Recommends process, product or service improvements. Solves unique and complex problems that have a broad impact on the business. Contributes to the development of functional strategy. Acts as a mentor for colleagues with less experience.
Manage the data collection process providing interpretation and recommendations to management. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using various technologies (e.g. Hadoop or equivalent MapReduce platform). Defines, designs and build dimensional databases and data pipelines to support analytics projects.
Oversees the delivery of engineering data initiatives and projects. Supports long term data initiatives as well as Ad-Hoc analysis and ETL activities. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies. Provide technical guidance and support to developers, data engineers and data administrators. Develop strategies for data acquisitions, recovery and implementations.

SUPERVISORY RESPONSIBILITY:
Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.

QUALIFICATIONS:

Education Level: Bachelor's Degree in Information Technology or Computer Science OR inlieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.
Licenses/Certifications Upon Hire:
Data Management\Certified Analytics Professional (CAP)
Experience:
10 years Experience leading database design and developing modeling tools.
Experience in leading data engineering and cross functional teams to implement scalable and fine tuned ETL/ELT solutions for optimal performance.
Experience developing and updating ETL/ELT scripts.
Hands-on experience with application development, relational database layout, development, data modeling.
Knowledge, Skills and Abilities (KSAs)
Expert in at least one programming language (i.e., SQL, NoSQL, Python).
Knowledge of multiple database technologies - structured and un-structured.
Ability to quick learn new technology and take direction.
Strong customer service orientation.
Provide direction to and lead technical teams.
Requires strong organizational with the ability to handle multiple priorities.
Excellent communication skills both written and verbal.
Must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. Must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging.

Department
Department: CBIW Mandates Development
Equal Employment Opportunity
CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Hire Range Disclaimer
Actual salary will be based on relevant job experience and work history.
Where To Apply
Please visit our website to apply: www.carefirst.com/careers
Federal Disc/Physical Demand
Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
PHYSICAL DEMANDS:
The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
Sponsorship in US
Must be eligible to work in the U.S. without Sponsorship
#LI-LD1","$125,053 /yr (est.)",5001 to 10000 Employees,Company - Private,Insurance,Insurance Carriers,1942.0,$100 to $500 million (USD)
PENTAFOUR GROUP,,"New York, NY",Sr. Data Integration Engineer,"Job description:
Major Responsibilities:
· Experience designing and developing Enterprise Data Warehouse solutions.
· Demonstrated proficiency with Data Analytics, Data Insights
· Proficient writing SQL queries and programming including stored procedures and reverse engineering existing process
· Leverage SQL, programming language (Python or similar) and/or ETL Tools (Azure Data Factory, Data Bricks, Talend and SnowSQL) to develop data pipeline solutions to ingest and exploit new and existing data sources.•
· Perform code reviews to ensure fit to requirements, optimal execution patterns and adherence to established standards.
Skills
· 12+ years - Enterprise Data Management
· 10+ years - SQL Server based development of large datasets
· 8+ years with Data Architecture
· 5+ years’ experience in Finance / Banking industry – some understanding of Securities and Banking products and their data footprints.
· 2+ years Python coding experience
· Proficient with Data Visualization tools
· Hands-on experience with Snowflake utilities such as SnowSQL and SnowPipe
· Working knowledge of MS Azure configuration items with respect to Snowflake.
· Hands-on experience with Tasks, Streams, Time travel, Optimizer, Metadata Manager, data sharing
· Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.
· Previous experience leading an enterprise-wide Cloud Data Platform migration with strong architectural and design skills
· Capable of discussing enterprise level services independent of technology stack
· Experience with Cloud based data architectures, messaging, analytics
· Superior communication skills
· Cloud certification(s)
· Any experience with Regulatory Reporting is a Plus
Education
Minimally a BA degree within an engineering and/or computer science discipline
Master’s degree strongly preferred
Job Type: Full-time
Salary: $88,089.11 - $106,085.80 per year
Experience level:
10 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road","$97,087 /yr (est.)",,,,,,
"Crystal Management
4.8",4.8,Remote,Big Data Platform Engineer,"About
Since 2005, Crystal Management provides information technology (IT) infrastructure, systems integration, cybersecurity, facility design and transition, and professional services to customers in the defense, civilian federal agencies, homeland security, intelligence, and commercial sectors. We understand the mission demands innovative approaches, technology, and people. With talented professionals deployed worldwide, Crystal Management delivers IT enterprise solutions, systems engineering, and management consulting services for the largest transformation and restationing programs in defense history. Crystal Management is a service-disabled veteran-owned small business.

Position Summary
DCSA is taking steps to strengthen its position as a key provider of cloud-based and big data services to various Consumers. Transformation of its large-scale Big Data infrastructure is a key element of the overall information technology strategy. The Big Data Platform is comprised of an integrated set of platform technologies that provide industry-leading data persistence and analysis capabilities. As a member of the infrastructure team, the Senior Big Data Platform Engineer will play a significant role in helping DCSA transform its Big Data ecosystem that supports applications including Data Analytics Services and other key initiatives.
This telework position is fully remote with occasional travel to the client worksite.
Responsibilities
Deploy, configure, upgrade, and sustain the development, testing, and production of BDP environments.
Work with the applications team to deploy and upgrade applications running on the Big Data Platform.
Provide subject matter expertise to the organization and will perform hands-on engineering and configuration tasks in complex, interdependent environments, and will lead triage, diagnosis, and remediation of platform-related issues.
Work with our key partners and industry peers to understand and influence industry technical direction in order that big data requirements are accommodated.

Education/Certification Requirements
High School diploma or equivalent education
DoD 8570 compliant IAT Level 2 baseline certification (e.g., CCNA-Security, CySA+, GICSP, GSEC, Security+ CE, CND, or SSCP)

Required Qualifications
9+ years (or commensurate experience) of demonstrated expertise in technology selection, architecture design, engineering and configuration, deployment, operational support, and issue resolution.
Full stack technical expertise from OS and configuration management through to Big Data ecosystem components such as:
Data processing and streaming analytics using Kafka and Storm
Platform management plan using RDAs, Containers, Consul, and Airflow
Data storage and analytics using Hadoop with YARN, Accumulo, and Apache Spark
Manage user accounts and authentication using Citadel, Kerberos, LDAP, and SAML
Proxy services using NGINX
Deployment automation and configuration management using Puppet and Ansible
Expertise with integration and administration of PostgreSQL databases
Experience operating in an AWS Cloud environment, including administration of EC2 resources, and understanding of key networking concepts such as VPC, subnet, and security groups.
Understanding of storage concepts such as EBS volumes and S3.
Strong understanding of security vulnerability detection and remediation using ACAS and DISA STIGs.
Strong understanding of various encryption technologies.
Strong experience in Linux-based scripting and systems administration.
Demonstrated ability to develop sizing and capacity planning for large-scale big data platforms and to periodically evaluate and optimize performance and platform throughput.
Excellent communications skills, including the ability to handle detailed technical communications and distill complex technical details into executive-level oral and written communications.

Preferred Qualifications
Cloudera Certified Data Engineer (CCDE) certification
Hortonworks Certified Associate (HCA) certification
AWS Certified Big Data - Specialty certification
Experience with Atlassian tools such as Jira and Confluence
Experience working in an Agile Framework environment

Clearance Requirement
Active Secret clearance

COVID-19 Safety Protocols: To protect the health and safety of its employees and to comply with customer requirements, employees in certain positions may be required to be fully vaccinated against COVID-19 or subject to facility entry safety protocols (e.g., testing, masking, physical distancing), subject to the status of the federal contractor mandate and customer site requirements.

Crystal Management, LLC provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.

This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
#NowHiring
#CoronaVirusHiring",,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2005.0,$5 to $25 million (USD)
"Purple Drive Technologies
4.2",4.2,"Chicago, IL",AWS Data Engineer,"Looking for a strong AWS Data Engineer who has hands on experience with AWS data stack (AWS Glue, Glue Catalog, S3, AWS Hudi, EMR, Appflow and airflow). Successful candidate will be responsible for the following activities:
i. Identify, Build the foundational blocks of the Data Lake Platform such as Processing layer, CI/CD pipelines, Orchestration, template pipeline etc.
ii. Work closely with Infrastructure/Architects/key stakeholders to finalize the foundational blocks on various aspects of the platform
iii. Automate pipeline for Moxie data source by using the foundation that is built on.
iv. Work with Business stakeholders to understand the data needs and build data tech debt.
v. Build reusable data pipelines/framework as a solution accelerator
vi. Configure automated pipelines using the framework for the data sources (CC1, MedForce, Nice).
vii. Build Data Quality/Governance framework to make data complete and trustworthy.
viii. Reference Architecture, Documentation of Data Platform
ix. Configure automated pipelines for the data sources identified from tech debt.
x. Production roll-out, knowledge sharing and hyper-care
Job Type: Full-time
Schedule:
Monday to Friday
Work Location: One location
Speak with the employer
+91 (609) 796-2792","$97,051 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"OpenGov
4.5",4.5,Remote,Senior Data Engineer,"Imagine yourself here!

OpenGov is a mission driven fast-growth, Series D, venture backed startup (includes Andreessen Horowitz, Formation 8, and Emerson Collective). Our Board of Directors includes iconic Silicon Valley executives John Chambers (former Cisco Chairman and CEO) and Marc Andreessen (Time Magazine’s list of the 100 most influential people in the world).

OpenGov is the leader in modern cloud software for local governments and state agencies. We have surpassed 1,600+ governments (and growing fast!) using our products in our mission to power more effective and accountable government.

OpenGov is a 2022 Top Workplaces USA award winner and a Forbes 2022 America's Best Startup Employer!

The Senior Data Engineers primary responsibility will be to balance the health of our data platform with the development of new integrations, pipelines, and data models. In this role, you will also manage the production support and enhancement of our existing BI and Data Warehouse tech stack. This role will be expected to learn and support Enterprise Data processes and software practices to proactively manage platform processes and ensure the highest quality of data. Developing strong relationships with business partners in the GTM, G&A, and R&D organizations is essential to success in this role. Prior experience in Data Warehousing and Data Integration/ELT technologies, coupled with your ability to understand complex data models and flows, will enable the incumbent to excel in this role.

Responsibilities:

Design and implement data solutions and related integrations with industry best practices
Design and implement efficient ETL and ELT processes to manage the data flowing into and out of the Data Warehouse in accordance with data governance and security standards
•Monitor and maintain data pipelines and system integrations proactively to ensure high service availability. •Systems may include Salesforce, Jira, Intacct, Netsuite, Zuora, Mavenlink (PSA), Zendesk, Pendo, ProductBoard, MixMax, Gainsight, Gong (CI), and Marketo
Help to define and improve our internal standards for style, maintainability, and best practices for a reliable data infrastructure
Build and leverage internal and external partnerships across GTM, G&A and R&D organizations, turning business goals into technical solutions
Assist in driving instrumentation and optimization of the data warehouse and systems integrations processes
Ensure data accuracy by following standardized structures and practices for dissemination and communication with appropriate departments
Comfortable breaking down complex data and integrations to technical and non-technical staff members
Drive business stakeholder meetings to gather requirements and make system integration, pipeline and reporting/BI recommendations
Conduct analysis and ongoing monitoring of available data, refinement, data and BI roadmap and feedback, scrubbing to improve existing data integrity
Understand what good looks like and the change management required to get there
Develop a deep understanding of the various business processes, data sets and data team deliverables

Requirements and Preferred Experience:

Bachelor’s degree in Information Technology, Computer Sciences or related field (equivalent experience may be substituted for formal education)
8+ years of experience in Business Systems, IT, Data Engineering or similar technical role
5+ years of experience in Data Engineering, Warehousing and Systems Integrations; 2+ years of experience with DBT
Experience with Salesforce, Support Platforms (Zendesk or Service Cloud), Mixmax, Outreach or Salesloft, Gainsight, JIRA, Intacct or Netsuite, Zuora, Professional Services Platforms (Mavenlink, FinancialForce, etc.)
Knowledge of enterprise SaaS applications and integrations
Experience working with
business intelligence tools and concepts (Domo, Tableau, Looker, etc.)
data warehouses (Redshift or Snowflake)
working with ETL and/or ELT tools (Snaplogic, Workato, and Fivetran)
iPasS experience maintaining and building integrations (Mulesoft, Snaplogic, Dell Boomi, etc.)
Demonstrated proficiency working with SQL, Python, GIT and CI/CD pipeline
An “end-to-end” data owner who believes in owning the whole stack from data source to cleaning to dashboard and stakeholder communication
Propensity and passion for problem solving, both conceptually and analytically, focused on optimizing strategy and process to continually provide valuable insights.
Organized professional, with the ability to manage multiple projects for multiple stakeholders with varying specifications
Ability to remain focused and flexible, working in a fast-paced environment with rapid change
Strong process and documentation skills, with the ability to connect process gaps with data integrity needs and to manage across teams to refine workflows
Strong written and verbal communication skills
Experience working with SaaS applications built on AWS or Azure platforms is preferred
#LI-DNI
What makes OpenGov unique

» Leadership: CEO Zac Bookman (MPA from Harvard and JD from Yale) is truly a mission-driven CEO. He was named one of the 100 most Intriguing Entrepreneurs by Goldman Sachs, a Tech Pioneer by the World Economic Forum, and SF and Silicon Valley Business Times' 40 under 40 class of 2018!

» Funding: Over $250 million, Series D company, from top tier investors including Andreessen Horowitz, 8VC, Cox Enterprises, and Emerson Collective.

» Board of Directors: Includes iconic executives John Chambers (former Cisco Chairman and CEO), Marc Andreessen (Time Magazine’s list of the 100 most influential people in the world), Katherine August-deWilde (Vice Chair of First Republic Bank), and Amy Pressman (co-founder, former president, and a current board member of Medallia).

» Growth: Record breaking growth with 1,600+ governments (and counting) using our products and seven acquisitions in the past six years! Click here to read more.

» Culture: Winner of Forbes 2022 Best Startup Employers, Winner of 2022 Top Workplaces USA award, 50 Best Workplaces award. Check out our Careers Video!

» Perks: 90% paid Medical/Dental/Vision premium for employees, fully paid Life and Short/Long term disability insurance, Unlimited PTO, Parental Leave policy, monthly fitness stipend, anniversary awards, and more!

» Product: Named to the GovTech 100 (seven consecutive years), we are the leader in cloud software for our nation's cities, counties, and state agencies.

» Mission Driven: We are a technology company with a passion for the mission. We're powering more effective and accountable government.

Come join us and make a positive social impact!

OpenGov is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.",,201 to 500 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2012.0,Unknown / Non-Applicable
"JTEC Consulting
5.0",5.0,"Crystal City, VA",Data Engineer,"Data Engineer
Crystal City, VA 22202

JTEC Consulting LLC focuses on successfully delivering solutions to meet our clients’ most critical needs. Our founding members have decades of experience delivering a wide range of solutions to Air Force and DOD clients. We are a Veteran-Owned Small Business.

Security Clearance Requirement: Current TS/SCI
Location Note: Must reside in Wash DC Metro area and provide on-site support, relocation will be considered



Position Description:
JTEC Consulting is hiring a Mid-Senior level Data Engineer to work in support of a large-scale data analytics platform. This position requires excellent analytical skills and the ability to work directly with the government customer in a dynamic mission driven environment. Qualified candidates will have experience developing solutions for high volume, low latency applications and abilities to operate in a fast-paced environment.

Duties and responsibilities may include (but are not limited to):
Work as part of an enterprise-wide team collaborating with other data engineers, data scientists and product leads to develop innovative solutions for data requirements.
Build pipelines to collect data from disparate external sources.
Implement rules and perform validation and analytics to ensure expected data is received, cleansed, transformed, and in an optimized output format.
Support automation and performance optimization.
Monitor pipeline status and performance.
Support troubleshooting and issue resolution.
Other duties as assigned.
Education and qualifications include (but are not limited to):
U.S. citizenship
Current TS/SCI security clearance
Understanding of distributed computer systems
Experience working in big data environments and understanding of data challenges (DoD command experience preferred)
Experience working in cloud environments
4+ years of experience in the following areas:
Experience with modern programming languages such as Java and Spark
Experience using SQL and ETL
Experience working in a distributed environment using Apache Kafka and Apache Airflow
Experience with stream processing using Apache Flink or StreamSets
Experience working in an Agile development environment is preferred
Proficiency working across MS Office Suite including advanced Excel skills
Experience working with data visualization tools and dashboards is a plus
Effective written and verbal communication skills to work in a collaborative environment with government and contractor teams
Availability to work on-site
JTEC Consulting LLC provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.

Salary:
JTEC offers competitive compensation and benefits to all employees. The estimated salary range displayed in this posting is dependent upon position requirements, individual qualifications, education, experience, location, and other job related factors. Our recruiting team will review candidates' related experience and salary targets during the evaluation process. All qualified candidates will be considered.","$145,000 /yr (est.)",1 to 50 Employees,Company - Private,,,,Unknown / Non-Applicable
"Unilever
4.1",4.1,Remote,Senior Data Engineer (Multiple Openings),"We have multiple Data Engineer roles open across the US to be located preferably in Eastern Standard Timezone; near our HQ office in New Jersey; or near satellite offices in Rogers, AR; Minneapolis, MN and Cincinnati, OH.
Background & Purpose of the Job
The North America Data Engineer will work with peer data engineers to develop data solutions to support product analytics managers and data scientists. This role will be partnering with internal data scientists, product managers and external stakeholders across the business to deliver advanced analytics. The ideal candidate is an experienced data engineer with vast hands-on experience and leadership experience in leading projects and teams from the ground up.
Who You Are & What You’ll Do
Facilitate the team’s delivery of scalable, maintainable, performant data engineering solutions. Work through your teams to define/design solution options, evaluate technical feasibility, and provide estimates on effort and risk. Drive the design and implementation of new data projects and the optimization of existing solutions. Work with architecture and engineers to ensure quality solutions are implemented, and engineering best practices are defined and followed. Drive collaborative reviews of designs, code, and test plans. Define the team’s technical roadmap and influence its adoption. Anticipate, identify and solve issues concerning data management to improve data quality. Work across teams to resolve operational & performance issues. Identify and remove technical bottlenecks for your engineering teams.
In this role you’ll be working closely with passionate and driven Product Management team, Architects, Engineers, Data Analysts, Internal partners and other departments. They will span across various functional groups across the enterprise. You will be in contact with a wide range of great people. In all interactions, you will find success in teamwork, a positive attitude, and hard work. As a Data Engineer, in partnership with diverse team of smart, intelligent, and creative people. This allows us to be a driving force for building first-class solutions for North America Data & Analytics team and its business partners, working on development projects related to customer development, supply chain, commerce, consumer behavior and web analytics among others.
You’re a dot connector and storyteller: You like to unravel complex data, - building analytical tools that utilize the data pipeline to provide actionable insights, operational efficiency, and other key metrics.
You’re a changemaker: You are a self-starter you can work independently to identify, design and implement internal process improvement; driving change with new innovative solutions, optimizing data delivery, re-designing for greater scalability.
You’re a paradox navigator: Enable data driven decision-making across customer development, supply chain, consumer marketing insights, and disrupting data silos.
You’re a culture & change champion: Be an advocate and set an example of digital cultural.
You love to win, and have fun doing it: Transform the way Unilever operates with a proactive, informed risk-taking, winning attitude!
What You Will Bring
Bachelor’s Degree in Computer Science, Information Systems, Business or related field or equivalent combination of education and experience with 3+ years of experience in large-scale software development and data engineering
Ability to define/design solution options, evaluate technical feasibility, and provide estimates on effort and risk
Experience working with data sources such as IRI, Nielsen, Retailer POS systems, Panel data such as Numerator and Nielsen/IRI Panel required, in support of retail such as Walmart, Target, Amazon, Dollar General, Kroger a huge plus!
Solid foundation in data structures, algorithms, and architecture patterns
Experience building distributed / cloud scalable, high-performance data solutions
Experience with batch processing frameworks and programming in tools such as Python, SQL, Spark and Hive
Experience with messaging/streaming/complex event processing tooling and frameworks with an emphasis on Spark Streaming or Structured Streaming or Apache Nifi
Experience with data warehousing related concepts - e.g. SQL and SQL Analytical functions
Experience in Agile/Scrum application development
Familiarity with the principles of Domain Driven Design
The following skills and experience are also relevant to our overall environment, and nice to have:
Experience working in a public cloud environment, particularly Microsoft Azure
Experience building RESTful API’s to enable data consumption
Experience in developing Power BI dashboards using DAX, Power Automate flows, Web Applications
Experience with practices like Continuous Development, Continuous Integration and Automated Testing
Pay: The pay range for this position is $83,200 to $124,700. Unilever takes into consideration a wide range of factors that are utilized in making compensation decisions including, but not limited to, skill sets, experience and training, licensure and certifications, qualifications and education, and other business and organizational needs.
Bonus: This position is bonus eligible.
Long-Term Incentive (LTI): This position is LTI eligible.
Benefits: Unilever employees are eligible to participate in our benefits plan. Should the employee choose to participate, they can choose from a range of benefits to include, but is not limited to, health insurance (including prescription drug, dental, and vision coverage), retirement savings benefits, life insurance and disability benefits, parental leave, sick leave, paid vacation and holidays, as well as access to numerous voluntary benefits. Any coverages for health insurance and retirement benefits will be in accordance with the terms and conditions of the applicable plans and associated governing plan documents.
-
Unilever is an organization committed to diversity and inclusion to drive our business results and create a better future every day for our diverse employees, global consumers, partners, and communities. We believe a diverse workforce allows us to match our growth ambitions and drive inclusion across the business. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

Employment is subject to verification of pre-screening tests, which may include drug screening, background check, credit check and DMV check.

If you are an individual with a disability in need of assistance at any time during our recruitment process, please contact us at
NA.Accommodations@unilever.com
. Please note: This email is reserved for individuals with disabilities in need of assistance and is not a means of inquiry about positions or application statuses.
#LI-Remote","$103,950 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1872.0,$10+ billion (USD)
"Intone Networks
4.5",4.5,"Dallas, TX",Data Engineer,"Walmart Dallas, TX (Must be onsite from Day - Hybrid Schedule onsite a couple times per month) JOB DESCRIPTION: Looking for a Data Engineer who is very strong with Spark, Scala, Hive, GCP , Bit query. (They are commercializing data. Lots of pipeline work. Nice to have are - Druid, Azure, on GCP currently","$97,101 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,,$5 to $25 million (USD)
"SEFCU
3.4",3.4,"Albany, NY",AWS Cloud Data Engineer,"If you are ready to join a company that truly cares about its employees, our members, and our community then you have come to the right place!
Summary of Role:
Broadview is looking for an outstanding Cloud Data Engineer to join the Data team. This is your opportunity to be a core part of the team that has direct impact on the organization’s day-to-day and strategic decision-making processes. As a Cloud Data engineer, you will be responsible for helping to accelerate the organization’s migration to the cloud, moving data from source systems to the AWS environment, managing data into a conformed data model, developing business KPIs and integrating them with front-end BI dashboards. You will get the exciting opportunity to work on large data sets in a cutting-edge data warehouse cloud environment. You will work closely with the business and technical teams to solve many non-standard and unique business problems and use creative problem solving to deliver actionable output.
Our team is serious about great design and redefining best practices with a cloud-based approach to scalability and automation. A successful candidate will be a self-starter, comfortable with ambiguity, with strong attention to detail, an ability to learn and work in a fast-paced environment, and an ability to work effectively with cross-functional teams
Essential Job Functions/Responsibilities:
Design, develop, test, and maintain cloud-based data solutions and infrastructure
Collaborate with cross-functional teams, including business analysts, data engineers, analysts and Cloud infrastructure team to ensure data solutions meet business requirements
Develop data ingestion pipeline to load data from various data sources
Develop and maintain scalable ETL pipelines to extract, transform, and load data
Implement data governance policies and ensure compliance with data security regulations of Broadview
Build and maintain data warehouses and data lakes on AWS
Develop insights into existing data warehouses and optimization approaches
Automate data processes using scripts and workflows to improve efficiency and reduce errors
Optimize data storage and retrieval processes to ensure high performance and scalability
Troubleshoot data-related issues and provide solutions to resolve them
Continuously monitor and improve data quality and data governance processes
Stay up-to-date with emerging technologies in cloud computing, data engineering, and data architecture and recommend best practices to the team.
Minimum Job Qualifications:
Bachelor's degree in Computer Science, Information Technology, or related field
Minimum of 3 years of experience in cloud computing and data engineering
3+ years of related work experience in a similar role. Preferred experience in banking, capital markets, insurance or asset management sectors
Experience with cloud-based data solutions like AWS S3, DMS, Redshift, Snowflake, Athena, EMS, API gateway
Hands-on experience with ETL tools like Apache Airflow or AWS Glue
Hands-on experience working with Quicksight
Knowledge of data modeling and database design principles
Familiarity with data governance and data security policies on AWS
Excellent problem-solving and analytical skills
Strong communication and collaboration skills to work with cross-functional teams
Practical experience in a data architecture/integration engagement across the end to end product life cycle – strategy, roadmap, requirements, design, development, testing, deployment, production support
Starting Compensation: $97,371 - $126,582 plus a competitive benefits package
Bilingual individuals who are fluent in a second language in addition to English are highly encouraged to apply.
We are an equal opportunity employer. We do not discriminate on the basis of race, creed, color, national origin, religion, sex, age, veteran status, disability, genetic information, gender identity, or any other protected class.
Broadview FCU is committed to ensuring individuals with disabilities and/or those who have special needs participate in the workforce and are afforded equal opportunity to apply and compete for jobs. If you would like to contact us regarding the accessibility of our Website or need assistance completing the application process, please contact us at
bv-talentacquisition@sefcu.com
.","$111,977 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1953.0,Unknown / Non-Applicable
Exo Therapeutics,,"Cambridge, MA","Co-op, Data Engineer","Exo Therapeutics (https://exo-therapeutics.com) is a small molecule drug discovery and development company with a pioneering technology to address intractable pharmaceutical targets. By leveraging the company’s ExoSight™ platform, Exo is developing a deep pipeline of potent drug candidates that bind exosites, distal and unique binding pockets that have the potential to reprogram enzyme activity for precise and robust therapeutic effect. Through this specific and selective approach to challenging targets, the company’s team of world-class researchers is unlocking breakthrough therapeutics in oncology, inflammation and a broad range of other diseases.
EXO Therapeutics
Co-op, Data Engineer
Cambridge, Massachusetts, United States
About the company
Exo Therapeutics (https://exo-therapeutics.com/) is a privately held small molecule drug discovery and development company with a pioneering technology to address intractable pharmaceutical targets. By leveraging the company’s ExoSightTM platform, Exo is developing a deep pipeline of potent drug candidates that bind exosites, distal and unique binding pockets that have the potential to reprogram enzyme activity for precise and robust therapeutic effect. Through this specific and selective approach to challenging targets, the company’s team of world-class researchers is unlocking breakthrough therapeutics in oncology, inflammation and a broad range of other diseases.

Overview
Exo Therapeutics is seeking a highly motivated data engineer co-op to join our bioinformatics team. You will work in mining and organizing data sources, building the data analytical workflows and applications to enable oncology/immunology target discovery efforts.
The data engineer co-op will work under the supervision of the Director of Bioinformatics. The ideal candidate should have solid scientific, engineering, and statistical background with strong curiosity of applying knowledge and expertise in addressing problems in the biotech drug R&D setting.

Responsibilities
Explore and curate public data resources to continue building an exosite focused knowledgebase to empower novel target identification and evaluation.
Work with bioinformatics and chemoinformatics peers on internal datasets processing and formatting, data models definition.
Design and establish the computational framework to build/integrate the knowledgebase and the computational pipeline to support new target nomination and prioritization.
Help maintain and upgrade the current data processing pipelines wherever necessary.
Work with the manager to establish the data ingestion, data analysis and application development SOP
Required Qualifications
MS/PhD candidate in a relevant field, such as bioinformatics, data science, data engineering
Experience of working with large scale biomedical datasets and databases, such as uniprot, PDB database, Depmap, TCGA
Experience with building and maintaining data processing and data analytical workflows
Experience with setting up and using database or data warehouse, such as SQL, Redshift and snowflake is a plus
Passionate about applying your skills and knowledge in solving problems in the drug R&D setting
Job Duration
Jun – Dec, 2023

All fully qualified applicants who are authorized to work in the US at the time of application will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, ancestry, disability, veteran status, as protected under law.","$131,330 /yr (est.)",1 to 50 Employees,Company - Public,,,,Unknown / Non-Applicable
"Barclays
4.0",4.0,"Whippany, NJ",Data Engineer,"Data Engineer
Whippany, NJ
As a Barclays Data Engineer, you will be contributing directly to the execution of the business strategy and play a key role in development of future state data science platform. You will also work collaboratively with a cross- functional team of data scientists and database developers, business intelligence designers, architects, business analysts and infrastructure engineers.
Barclays is one of the world's largest and most respected financial institutions, with 329 years of success, quality, and innovation behind us. We've helped millions of individuals and businesses thrive, creating financial and digital solutions that the world now takes for granted. An important and growing presence in the USA, we offer careers providing endless opportunity.
We are currently in the early stages of implementing a hybrid working environment, which means that many colleagues spend part of their working hours at home and part in the office, depending on the nature of the role they are in. We’re flexible on how this works and it may continue to change and evolve. Depending on your team, typically this means that colleagues spend a minimum of between 20% to 60% of their time in the office, which could be over a week, a month or a quarter. However, some colleagues may choose to spend more time in the office over a typical period than their role type requires. We also have a flexible working process where, subject to business needs, all colleagues globally are able to request work patterns to reflect their personal circumstances
Please discuss the detail of the working pattern options for the role with the hiring manager.
What will you be doing?
Designing scalable and secure engineering solutions that will be leveraged by Banking colleagues and customers
Working collaboratively with cross-functional team of data scientists & database developers, business Intelligence designers, architects, business analysts and infrastructure engineers
Being responsible for full life cycle development and design of new data science platform with Python and AWS based applications and components
Working as a team player in a global development group, and participating in requirements and data analysis, design as well as development
Communicating and collaborating between the infrastructure, development, and business groups
Having excellent analytical skills, being self-motivated and capable of working in a dynamic environment that demands multi-tasking
Having the ability to generate ideas and efficiently mock-up proposals and demos
What we’re looking for:
B.S. degree in computer science or related field with emphasis on technology
Five years of experience in Python
At least two years of AWS experience
Two plus years of SQL experience
Skills that will help you in the role:
Working experience in financial industry, especially IB applications is a plus
Prior experience with AWS Services such as Lambda, Glue, Athena, ECS, Cloud Formation
Working Experience in developing REST APIs using Python
Experience in building and deploying services using any container orchestration tools such as Kubernetes, ECS, Docker Swarm, OpenShift
Where will you be working?
At Barclays, we are proud to be redefining the future of finance and here at Whippany we are defining the future of the workplace and the future of the way we work and live. We are creating a unique community, one of four strategic tech-enabled hubs that will redefine opportunity for everyone who works here. Whatever you do at Whippany, you’ll have every chance to build a world-class career in this world-class environment.

#LI-Hybrid
#data","$96,821 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1690.0,$10+ billion (USD)
"The Washington Post
4.3",4.3,"Washington, DC","Data Engineer, Election Platforms (all-levels)","Job Description
The Washington Post is hiring a data engineer for our Election Platforms team, to work on our data pipelines, tools, and infrastructure ahead of the 2024 presidential election.
The Election Platforms team focuses on leveraging data, partnering with stakeholders, and pioneering serviceable tools to facilitate The Post’s election coverage. This role is part of Elections Engineering, a cross-functional group of full-stack engineers and data scientists, with varied technical and academic backgrounds, responsible for delivering live election results across The Post’s platforms.
The role will be focused on contributing to our technical stack and developing new technology to facilitate robust and lively election coverage. In particular, we’re looking for candidates who would be comfortable working on deploying high-impact code in production and have an interest in data and newsroom tooling. We also work on other data engineering projects related to elections, such as voter registration data and campaign finance, and occasionally write
open
source
software
. Most of our work is in Python and TypeScript/React/Node.
We typically work at a higher level of abstraction than a single story, and aim to create general purpose tools that are reusable and easy to deploy. Our ideal candidates are creative problem-solvers, who are comfortable writing production level code to solve real-world newsroom problems. This position is remote-friendly for work in the US.
Motivation
You have a desire to contribute to The Post’s journalism through technology.
You are passionate about refining and improving team process, culture, documentation and communication while tackling complex data science and engineering challenges.
You want to work at a deeper level of abstraction than a single story or beat.
You are excited about learning new technologies and applying them to the practice of journalism.
Responsibilities
Work on the software components of our live election architecture including deployment, observability, participating in code reviews, and more.
Work closely with our engineers and data scientists to develop and maintain pipelines and administrative apps/dashboards for election data.
Work on tooling to make data preparation easier and more efficient for our live election model.
Work with newsroom stakeholders to assess needs, build useful and reusable tooling, and contribute to storytelling.
Foster effective team collaboration and communication by improving and documenting internal tools, processes and guidelines.
Assist in attracting and hiring team members who want to work at the intersection of journalism and technology by actively participating in recruiting – and contributing feedback on our overall hiring process.
Qualifications
A minimum of one year of experience building and maintaining backend and data engineering applications.
Proficiency across some of the languages and frameworks used in our engineering stack such as Python, TypeScript/React/Node, and AWS services.
A desire to work on cutting edge, cross-functional tools to help with features, data science, and machine learning/big data work in a team setting.
Experience with election data is not necessary, but is a plus.
Experience developing and deploying newsroom tools and applications is not necessary, but is a plus.
Levels
If you have less than three years, but at least one year, we’d love to evaluate you for a data engineer role.
If you have more than three years but less than about seven years, we’d love to evaluate you for a senior data engineer role.
If you have more than about seven years, we’d love to talk to you about a principal data engineer role.
Wherever you are in your life or career, The Washington Post offers comprehensive and inclusive benefits for every step of your journey:
Competitive medical, dental and vision coverage
Company-paid pension and 401(k) match
Three weeks of vacation and up to three weeks of paid sick leave
Nine paid holidays and two personal days
20 weeks paid parental leave for any new parent
Robust mental health resources
Backup care and caregiver concierge services
Gender affirming services
Pet insurance
Free Post digital subscription
Leadership and career development programs
Benefits may vary based on the job, full-time or part-time schedule, location, and collectively bargained status.
The Post strives to provide its readers with high-quality, trustworthy news and information while constantly innovating. That mission is best served by a diverse, multi-generational workforce with varied life experiences and perspectives. All cultures and backgrounds are welcomed.
The innovation doesn’t end in the Newsroom – dozens of teams power The Washington Post. We are now hiring the next innovator – how will you Impact Tomorrow?
#washpostlife","$83,249 /yr (est.)",1001 to 5000 Employees,Company - Private,Media & Communication,Publishing,1877.0,Unknown / Non-Applicable
"CareFirst BlueCross BlueShield
3.5",3.5,"Reston, VA","Senior Data Engineer, (Hybrid)","Resp & Qualifications
PURPOSE:
The Senior Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on developing solutions and helping transform the company's platforms deliver data-driven, meaningful insights and value to company and clients.

ESSENTIAL FUNCTIONS:
Develops and maintains auditing systems (e.g., data warehouses, data lakes) using Ab Initio. Prepares and manipulates data using multiple technologies. Creates data collection frameworks for structured and unstructured data.
Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
Supports Production and PLT environments; addresses issues in a timely fashion.
Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.
Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
Applies and implements best practices for data auditing, scalability, reliability and application performance.

SUPERVISORY RESPONSIBILITY:
Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.

QUALIFICATIONS:

Education Level: Bachelor's Degree, Details: Computer Science, Information Technology or Engineering or related field OR In lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.

Experience: 5 years Experience with database design and developing modeling tools. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.
The incumbent must have expereince in Ab Initio, Unix shell scripting, SQL, Axway, Cognos. Exposure to Big Data technologies (Cloudera) and AWS is a big plus. Must be willing to support off hours (weekends) implementation to Production and Plan Test environments and address issues as needed.
Preference will be given to candidates having hands-on experience with FEPOC and FEPDO systems such as HEDIS, FEDVIP Dental and Vision reports, Truven, IBM Watson Health etc.
Knowledge, Skills and Abilities (KSAs)
Knowledge and understanding of at least one programming language (i.e., Ab Initio, SQL, NoSQL, Python).
Knowledge and understanding of database design and implementation concepts.
Knowledge and understanding of data exchange formats.
Knowledge and understanding of data movement concepts.
Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.
Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.
Must be able to effectively work in a fast-paced environment with frequently changing priorities, deadlines, and workloads that can be variable for long periods of time. Must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. Must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging.

Department
Department: (EIS Technical Delivery)
Equal Employment Opportunity
CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Hire Range Disclaimer
Actual salary will be based on relevant job experience and work history.
Where To Apply
PeopleSoft/Self Service/Recruiting
Federal Disc/Physical Demand
Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
PHYSICAL DEMANDS:
The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
Sponsorship in US
Must be eligible to work in the U.S. without Sponsorship
#LI-KT1","$128,662 /yr (est.)",5001 to 10000 Employees,Company - Private,Insurance,Insurance Carriers,1942.0,$100 to $500 million (USD)
"Amway Inc.
3.8",3.8,"Ada, MI",Associate Data Engineer I/II,"Job title: Associate Data Engineer I/II
Department / Division: Global Supply Chain Analytics / Supply Chain
Location: Ada, MI
Salary Range (13): $61,093 - $79,062
Salary Range: (14): $71,540 - $92,581

This role is a hybrid in-office/remote role. You must be available to report to the Ada, MI office on Tuesdays & Wednesdays.

What do we need:
We are looking for an Associate Data Engineer with a desire to work in AWS or Google Cloud Platform to step into this newly created role to build and scale next generation data platforms for our Supply Chain organization. We need a high energy data wrangler who is ready to support our global team.

What’s special about this team:
The Supply Chain Analytics and Demand Planning Analytics business group provides the organization with high quality, innovative data solutions in a focused, fast and fun atmosphere. The team is seeking a tech savvy Data Engineer to work on building, operating, and scaling next generation data platforms and tools that will power data-driven analytics capabilities throughout the entire organization. This newly created role will report to the Manager of Supply Chain Analytics.

What’s special about this role:
As an Associate Data Engineer, you will be responsible for expanding and optimizing our data access and data pipeline analytics processes, as well as optimizing data flow and collection for cross functional teams. You will be a data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.

You will support our global community of data scientists and analysts on enterprise initiatives and will ensure optimal data delivery is consistent throughout ongoing projects. If you are excited by the prospect of optimizing or even re-designing our company’s analytics processes to support our next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain our competitive edge in the industry, this may be the opportunity for you.

Required qualifications:
0-1 years of experience preferred in Data Engineering, Business Analytics processes
Preferred Experience with AWS and Google Cloud Platform “big data” technologies
Familiarity or proficiency with technologies like Hadoop (and related ecosystem), Spark, Kafka, EC2, EMR, RDS, and Redshift in supporting data transformation, data structures, metadata, dependency and workload management
BA/BS degree in Computer Science, Finance, Supply Chain or related field

Skills to be successful in the role:
Self-directed and comfortable supporting the data needs of multiple teams, systems, and products within the Amway’s data eco-system
Understanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing and extracting value from large disconnected datasets
Advanced working database knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases such as Microsoft and Oracle databases to advance analytics capabilities

This role is “Not” eligible for sponsorship.","$70,078 /yr (est.)",10000+ Employees,Company - Private,Manufacturing,Consumer Product Manufacturing,1959.0,$5 to $10 billion (USD)
"Brillio
3.8",3.8,"Austin, TX",AWS Data Engineer – R01525162,"AWS Data Engineer - R01525162
About Brillio:
Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded ‘Great Place to Work’ in 2021 and 2022

Consultant
Primary Skills

Specialization

Job requirements
Job Description: We are looking for an experienced AWS Data Engineer to join our team and help us build and maintain our data infrastructure on AWS. As an AWS Data Engineer, you will work closely with data Architect, Business and Data analysts, BI Developer, Customer Business and engineering team, and other stakeholders to design, implement, and manage data pipelines and systems that support our business needs.

Key Job Responsibilities:
Design, implement, and maintain data pipelines and data processing systems, ETL Infrastructure on AWS using technologies like Apache Spark, AWS Glue, AWS Lambda, and AWS S3 and strong Python and Pandas hands on experience.
Collaborate with data Architect and analysts/BI Developer to understand their data requirements and design data models that meet their needs.
Work with DevOps engineers to ensure that data pipelines are reliable, scalable, and secure.
Monitor and troubleshoot data pipelines to ensure that data is flowing correctly and on time.
Develop automation scripts to streamline deployment, testing, and maintenance of data pipelines and systems.
Document technical designs, standard operating procedures, and best practices for data engineering on AWS.
Keep up to date with emerging technologies and best practices in data engineering and recommend new tools and technologies as appropriate.
Excellent communication and collaboration skills.
Need strong commitment to project, Problem solving, Team Player

Preferred qualifications:
Bachelor’s or master’s degree in computer science, Information Technology, or a related field.
3+ years of experience in data engineering with a focus on AWS technologies.
Strong knowledge of AWS services such as S3, EC2, Glue, Lambda, and Athena.
Experience designing and building data pipelines using Apache Spark.
Experience working with SQL and NoSQL databases.
Familiarity with ETL processes and tools such as Talend or Informatica.
Strong scripting skills in Python or Java.
#LI-CH1
Know what it’s like to work and grow at Brillio: Click here","$88,650 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2014.0,$100 to $500 million (USD)
"Johnson Controls
3.6",3.6,"Milwaukee, WI",PLM Data Engineer (Remote),"Job Details
What you will do
As the PLM Data Engineer, you are a leader within the Engineering IT team. Working under the Engineering Services IT leader, you will partner with peers in the Engineering Services IT organization as well as business and IT leaders globally. You will provide thought leadership on PLM data along with associated technologies and drive successful delivery of IT investments and services. You will develop, manage, and maintain relationships with business and IT leaders across the organization and participate in the development of IT strategies in support of business plans.
In this role you will support the data quality and the data health of the organization. This includes supporting the launch and operationalization of a PLM Data Factory enabling a large multi-year Windchill PLM deployment. You will support the development engineering data standards and data transformation as part of the Engineering Data Factory.
You will participate in the successful planning and execution of all IT related aspects of assigned projects and services as well as contributing to the business and project management aspects. You will coordinate and lead teams consisting of, but not limited to, Global Infrastructure, Enterprise Architecture, Application Delivery, Solution Development, Testing, Performance, Business, and IT Subject-Matter Experts, and including direct and indirect resources.
How you will do it
Serve as a liaison and project leader in partnership between the Business Unit (BU) IT department, Corporate IT department, BU Engineering Services department, Product Business teams, and cross-Product Business functions. Effectively collaborate with teams that span IT and the business.
Express a clear understanding of business structures, hierarchies, products, and services supported. Demonstrate knowledge outside of the technology arena with the Product Business supported.
Partner with other IT leaders, communicate and discuss technology trends, specific changes, and technology roadmaps.
Understand short- and long-term Engineering Services business plans and objectives, IT investments and assets. Participate in development of an IT strategy that enables Engineering Services business strategies. Lead the creation and continuous improvement of a subset of a business capability and IT enablement roadmap. Revise roadmap on at least an annual basis and support investment proposal development.
Support capture, aggregation, and prioritization of a portfolio of IT investments to enable needs as defined on the Engineering Services capability roadmaps. Champion Engineering Services priorities through IT investment portfolio governance process to ensure alignment of projects with business objectives, leverage of assets and practices, and synthesis with broader corporate-wide priorities. Champion understanding, support for, and value of Engineering Services IT investment portfolios.
Lead the ideation and high-level planning of IT projects including specification of high-level business requirements, and formulation of business process and technology solution options. Support creation and vetting of business cases, and high-level project planning (scope, schedule, & budget).
Project responsibilities include analysis, documentation, system design, development of project plans, time and cost estimates, and Capital Appropriation Requests.
Incorporate process standardization and simplification improvements in high-level designs to enable Global Products-wide leverage and streamline the IT applications landscape over time.
Gather requirements, perform analysis, and make recommendations for design of data models and software specifications; create project schedules; provide training; and coordinate implementation and support of developed applications.
Spearhead trade-off analyses and assessments (project timing, technology approaches, requirements prioritization, etc.), support solution design and testing activities, assess overall quality of solution and fit with business capability needs, ensure go-live readiness of Business and IT communities, and measure business results after solution stabilization.
Evaluate emerging technologies or methodologies and develop knowledge in these areas to apply to company projects.
Work closely with and drive accountability and results from internal and external service providers and support teams.
May provide third level support for applications, including working with technical teams, solving issues, routing to IT management, and providing documentation for other team members to use to respond to support issues.
Make decisions and achieve results while being a role model for company values.
Set challenging SMART personal goals that improve both personal skills and team deliverables.
What we look for
Required
Bachelor's Degree in Information Technology, Computer Science, Engineering, or a closely related subject area.
Minimum of 3 years' experience successfully delivering sophisticated Business Technology projects and achieving quantified business outcomes for a related business/industry.
Minimum 3 years' experience in technical data architect role.
Minimum 5 years' experience in product development domain.
Must have a thorough understanding of a broad cross-section of modern Information Technology concepts.
Experience developing solutions with large sophisticated global organizations.
Experience with implementation and roll-out of scalable out-of-the-box solutions as well as custom application development.
A foundational understanding of Agile concepts.
Demonstrated abilit to produce clear, concise, and accurate documentation detailing business processes and requirements.
Position requires knowledge of product development principles, processes, and techniques.
Position requires thorough understanding of Engineering tools including PLM, Informatica tools and PTC Windchill. Working knowledge of PTC Windchill required.
Ability to work collaboratively with a global community including Business Unit and Corporate IT, horizontal and vertical business functions, and end users.
Must have strong ability to communicate effectively with technology, business, and leadership audiences.
Travel varies from 25% to as much as 50% (at peak) including both domestic and international. Typical travel is 1-2 weeks.
Work with global teams, requiring flexibility of working hours.
Proficient English required.
Preferred:
PMI or SAFe certification
BRMi certification.
#DICE",,10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,1885.0,$10+ billion (USD)
"Parker Wellbore
3.9",3.9,"Houston, TX","IT Data Engineer - (Houston, TX)","Company Description Parker Wellbore helps energy companies accomplish their drilling and production goals efficiently, reliably, and safely. Our global team supports oil and gas operators with innovative land and offshore drilling services, premium rental tools and well services, and advanced operations and management support. Founded in 1934, Parker Wellbore helps customers manage their costs and mitigate their risks, to achieve their operational goals in a safe and efficient manner. With experience in both harsh-environment regions and complex drilling situations, you can trust Parker Wellbore to get the job done.
Job Description

The IT Data Engineer will work closely with the business and IT team to build high-quality data pipelines supporting both our corporate and product divisions and will be key in supporting our current and future reporting and analytics solutions. This role requires a deep understanding of data architecture, data engineering, data analysis, reporting, and a basic understanding of data science techniques and workflows. This role will work with software developers, architects, and data analysts on expanding and optimizing our data and data pipeline architecture, as well as streamlining data flow and collection in support of our cross-functional teams.
Essential functions
Work with business to gather requirements and translate business needs to technical specifications
Builds and maintains Azure data platform including enterprise data lake and data warehouse in alignment with our strategic objectives and organizational goals
Create and maintain optimal data pipeline architecture
Collaborate with Cloud Solution Architects in implementing complex end-to-end Enterprise solutions on Microsoft Azure platform.
Develop policies and implement mechanisms for data ingestion into Azure data platform
Configure, validate, and implement various Azure tools such as but not limited to Databricks, Data Factory, Data Lake, Synapse Analytics, and Data Catalog as appropriate
Works with the Enterprise Information management manager, and enterprise architects to define data architecture and high-level solution design.
Actively collaborates with the business intelligence team, business teams, and project teams to understand data requirements, integration needs, constraints, and performance requirements
Works with Parker’s technology team to understand mathematical models and optimize data solutions accordingly.
Developing and maintaining Data Lake and data warehouse schematics, layouts, architectures, and relational/non-relational databases for data access and Advanced Analytics.

Qualifications

Necessary qualifications, skills and abilities
Bachelor’s Degree in Computer Science, or 3-5 years of equivalent work experience
Analytic Problem-Solving: Approaching high-level challenges with a clear eye on what is important; employing the right approach/methods to make the maximum use of time and human resources.
Effective Communication
Explore new territories to find creative and unusual ways to solve problems.
Data Analysis Knowledge: Understanding how data is collected, analyzed and utilized, Data Ingestion and Orchestration from on premise to Azure, Data Ingestion from Azure Blob Storage to Azure SQL DW
3-5 years of experience in Azure Data Factory, Azure Data Lake Analytics (USQL), Data processing in Azure, Azure DevOps CICD Pipelines, Azure SQL DW, Azure Blob Storage or Azure Data Lake Store, Azure Logic App/Functions, Azure Event Hub/IoT
Data Modeling, architecture and storage experience
3-5 years of experience in Power BI – DAX, SSAS tabular, PySpark/scripting, PowerShell, Stream Analytics
3-5 years of experience in Microsoft traditional data warehousing/BI (SQL Server, SSIS, SSAS, SSRS)
Exposure to Azure Services and big data processing solutions

Position competencies
Initiating & Driving Change • Acts as a catalyst for and takes responsibility for leading, directing, and managing organizational change • Develops new insights into situations and applies innovative solutions • Creates work environment that encourages creative thinking and innovation • Drives step changes in how the company operates • Understands how to change and addresses not only systems and processes, but also cultural aspects of change • Is good at bringing the creative ideas of others to market • Develops a change strategy that includes milestones and timelines • Accurately assesses the potential barriers and resources necessary for change initiatives • Understands and supports the need for change • Envisions and articulates the intended result of the change process • Provides direction and focus during the change process • Helps to generate support of the changes throughout the organization • Identifies and enlists allies who support the change process • Provides resources, removes barriers, and acts as an advocate for those initiating change
Result Focused • Establishes clear, specific performance goals, expectations, and priorities • Can be counted on to exceed goals successfully • Is constantly and consistently one of the top performers • Very bottom-line oriented • Steadfastly pushes self and others for results • Navigates quickly and effectively to resolve problems and obstacles • Persists to complete tasks / responsibilities, even in the face of difficulties • Develops a sense of urgency in others to complete tasks • Operates with personal ownership and looks for ways to improve performance all the time • Challenges him- or herself and others to raise the bar on performance • Focuses people on critical activities that yield a high impact • Holds self and others accountable for delivering high-quality results on time and within budget (e.g., models high work standards and demands the same from others)
Team Work • Blends people into teams when they are needed • Creates strong morale and spirit in his/her team • Shares wins and successes • Fosters open dialogue • Lets people finish and be responsible for their work • Seeks consensus among diverse viewpoints as a means of building group commitment • Defines success in terms of the whole team • Creates a feeling of belonging in the team • Values the contributions of all team members • Creates an environment that encourages open communication amongst team members • Creates an environment that encourages collective problem solving amongst team members
Customer Focus • Is dedicated to meeting the expectations and requirements of internal and external customers • Gets first-hand customer information and uses it to understand customers' business issues and needs for improvements in products and services • Acts with customer in mind • Establishes and maintains effective relationships with customers and gains their trust and respect • Genuinely enjoys working with customers to build long-term partnerships • Creates a sense of customer focus throughout their team/ department/ business unit
Physical demands and work environment
Ability to gather, analyze, and interpret data.
Ability to perform under stress, under pressure, and/or in emergency situations.
Ability to multitask, work in a fast-paced environment, meet deadlines, reason logically, and make sound decisions.
Ability to comprehend, remember, and follow verbal and written directions and comply with Company policies, procedures and standard.
Use repetitive wrist, hand or finger movements at a computer.
Ability to work as a team, communicate and interact with others in a professional manner, and consider alternative and diverse perspectives.

Additional Information

All your information will be kept confidential according to EEO guidelines.
Parker Wellbore provides equal opportunity for all people and will not discriminate on the basis of race, color, religion, sex, gender, sexual orientation, pregnancy, age, marital status, national origin, citizenship status, disability, genetic information, military service, veteran’s status or any other characteristic protected by applicable law.
If an applicant has a disability, the applicant may request accommodations when needed to enable that person to perform their essential job functions or to allow that person to participate in employment.","$55,584 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1934.0,Unknown / Non-Applicable
"Hy-Vee, Inc.
3.4",3.4,"Grimes, IA",Software Engineer II - Data,"At Hy-Vee our people are our strength. We promise “a helpful smile in every aisle” and those smiles can only come from a workforce that is fully engaged and committed to supporting our customers and each other.
Job Description:
Job Title: Software Engineer II
Department: Information Technology
FLSA : Exempt
General Function:
A developing professional participating in development, implementation, and support of applications with limited guidance.
Core Competencies
Partnerships
Growth mindset
Results oriented
Customer focused
Professionalism
Reporting Relations:
Accountable and Reports to: TBD
Positions that Report to you: TBD
Primary Duties and Responsibilities
Understand and apply foundational organization and industry engineering principles and core competencies; start to identify opportunities to improve.
Implements tasks of moderate scope with mostly defined specifications sometimes aided by direction or guidance from more experienced engineers. Demonstrates consistency, dependability, and confidence in work delivered.
Understands team's product, domain, and vision and how it fits into the overall business.
Understands team practices and processes and beings to discuss improvements with the team.
Willingly enters areas of ambiguity and unfamiliarity, often assisted by more experienced engineers when needed.
Manages risk by trying to unblock themselves first before seeking help. Can sometimes spot potential problems before they become problems. Starts to evaluate possible solutions by factoring in implications of each option.
Begin to identify tech debt, start to identify opportunities to improve, and sometimes make recommendations on how to implement.
Knowledge, Skills, Abilities, and Worker Characteristics:
Desire to grow as an individual through continuously learning new techniques.
Experience working within an environment with a continuous delivery mindset. Comfortable contributing in and to this kind of environment following existing patterns. Starting to identify opportunities to improve the process.
A few years experience developing and creating applications.
Aware of the importance of security.
Experience and Education
Bachelor degree preferred, or relevant experience.
Supervisory Responsibilities (Direct Reports)
None
Physical Requirements
Visual requirements include: ability to see detail at near range with or without correction.
Must be physically able to perform sedentary work: operating a computer, occasionally lifting or carrying objects of no more than 10 pounds, and occasionally standing or walking.
Must be able to perform the following physical activities: meeting with customers, kneeling, reaching, handling, grasping, feeling, talking, hearing, and repetitive motions.
Working Conditions
The duties for this position are performed in a general or remote office setting. There is weekly pressure to meet deadlines and handle multiple tasks in a day.
Equipment Used to Perform Job
Laptop and desktop computer, telephone, copier, Fax, printer, PC with Microsoft Office programs and other software relevant to specific position.
Financial Responsibility
None
Contacts
Has frequent contact with office personnel in other departments related to the position as well as occasional contact with users and customers.
Confidentiality
Has access to confidential information.
Are you ready to smile, apply today.","$73,262 /yr (est.)",10000+ Employees,Company - Private,Retail & Wholesale,Grocery Stores,1930.0,$5 to $10 billion (USD)
"Land O'Lakes
3.9",3.9,"Arden Hills, MN",Data Operations Engineer,"Join Land O’Lakes, Inc., and help us bring food from farmer to fork. We’re a global, Fortune 250 company and a farmer-owned cooperative. While benefits can vary by location and role, some offer the following. Please speak with your local HR Representative regarding benefit eligibility.

Medical, dental, vision, life and AD&D; short- and long-term disability insurance
Retirement savings plan and profit-sharing plan, both with company match
Paid time off and paid holidays, plus paid benefits for maternity and parental leaves
Data Operations Engineer
This position is remote (virtual) and can be located anywhere in the United States.
The Data Ops Engineer has an understanding that data is the foundation of any information-driven operating environment and is critical to the success of business outcomes. The Data Ops Engineer will work with the Data Architecture and Data Engineering teams to learn/assist in the design of solutions that ensure consistent, reliable, efficient, and sustainable data are delivered using best practices, standards and processes. The result of these efforts is highly accurate and trusted data assets for use in the Enterprise Data Platform and across the organization. Additionally, the Data Ops Engineer will also learn/assist in the effort to support the tools used to build the data platform for DevOps processes. The Data Operations Engineer will work to enhance the platform and utilities to drive efficiencies in existing data pipelines as well as the process to engineer new ones. Areas of responsibility:
Automate administration tasks using Python and SQL against API’s and CLI’s to nurture and develop the data platform
Work closely with technical staff to learn and optimize their technical environment needs
Administer cloud native platforms including Snowflake, Databricks, Qlik, Event Hubs, Power BI and other Azure services
Build notification solutions to ensure cloud platforms are operational and alerts are sent when they are not using methods such as pub/sub
Install patches and upgrades
Support issue resolution and escalation
Facilitate reporting of business metrics of platform (adoption, alignment to business goals, alignment to product team needs, etc.)
Build custom DevOps capabilities where tools or platforms do not have them built-in
Engage with software vendors to understand new capabilities of their platforms and tools
Establish platform-wide standards
Measure data pipeline alignment to standards
Measure uptime of platforms
Participate in MVP and PoC activities to understand new technical capabilities of the technology platform
Perform root cause analysis when there are issues
Required Experience/Education:
College or Vocation Training in Computer Science, Programming, or similar technical areas.
Or 1+ years work experience in data processing, programming languages (SQL, Python, SAS, R, etc.), infrastructure, dev ops, etc.. This could experience could be in school or a work environment.
Required Competencies/Skills:
Basic technical knowledge and understanding of data and processing
Ability to collaborate across all levels and areas of the data platform team
Ability to participate in PoC efforts to research emerging technologies
Ability to work closely with technical and business people, as well as perform hands-on implementation
CI/CD
Knowledge of modern data platform tools (Snowflake, Redshift/AWS, Big Query/Google, etc)
Knowledge of Python, SQL, Linux, Docker, or similar tools
Preferred Experience/Education:
Post High School college or tech training
Preferred Competencies/Skills:
Infrastructure as code
Desire to work in a fast paced, changing environment
Travel: This position requires quarterly travel to the corporate office.
Salary: $65,360-$98,040
Land O'Lakes, Inc. is an Equal Opportunity Employer (EOE) M/F/Vets/Disabled. The company maintains a drug-free workforce, including pre- and post-employment substance abuse testing pursuant to a Drug and Alcohol Policy.
Neither Land O’Lakes, nor its search firms, will ever contact you and ask for confidential information over the phone or in email. If you receive a call or email like this, please do not provide the information being requested.","$81,700 /yr (est.)",5001 to 10000 Employees,Company - Private,Manufacturing,Food & Beverage Manufacturing,1921.0,$10+ billion (USD)
"Press Ganey Associates, Inc.
3.4",3.4,Remote,Senior Data Engineer,"About Press Ganey:
Press Ganey , the leading Human Experience (HX) healthcare performance improvement company, offers an integrated suite of solutions that address safety, clinical excellence, patient experience and workforce engagement. The company works with more than 41,000 healthcare facilities in its mission to reduce patient suffering and enhance caregiver resilience to improve the overall safety, quality, and experience of care. Press Ganey is a PG Forsta company.
Position Description:
Our company is looking for a Senior Data Engineer who will be responsible for designing and building scalable and robust data systems. The ideal candidate will be an expert in data engineering technologies, have a strong understanding of data architecture, and be able to work with large and complex data sets. The Senior Data Engineer will also work closely with the data science and analytics teams to ensure data integrity and develop data pipelines.
Duties & Responsibilities:
· Design and build large scale data pipelines to process and analyze large volumes of data.
· Build and maintain efficient data infrastructure, ensuring data quality and consistency.
· Work with the data science and analytics teams to ensure data accuracy and completeness.
· Collaborate with other engineers to build scalable and maintainable data systems.
· Develop and implement data governance and security policies.
· Continuously evaluate and improve data processes to optimize system performance.
· Keep up to date with the latest data engineering technologies and trends.
Technical Skills:
· Expertise in SQL and NoSQL databases, data warehousing, and data modeling.
· Fluency in Python and SQL. Additional data or system languages (e.g. Java, Scala, Go, R) a plus.
· Experience using data pipeline frameworks such as Apache Beam or Apache Spark at scale.
· Experience using data orchestration / automation frameworks such as Airflow, Databricks and MLFlow.
· Hands on experience with one or more of the major cloud providers (GCP, AWS, Azure). Experience with infrastructure-as-code (e.g. Cloud Formation, Terraform) a plus.
· Experience with data visualization tools such as Tableau or Power BI.
Minimum Qualifications:
· Bachelor’s degree in Computer Science, Engineering, or a related field.
· 5+ years of experience in data engineering or related field.
· Excellent problem-solving skills and ability to work independently.
· Strong communication and collaboration skills.
Preferred Qualifications:
· Master’s degree in Computer Science, Engineering, or a related field.
· Experience in machine learning or data science.
All positions at Press Ganey require an applicant who has accepted an offer to undergo a background check. The specific checks are based on the nature of the position. Background checks may include some or all of the following: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, fingerprint verification, credit check, and/or drug test. By applying for a position with Press Ganey, you understand that you will be required to undergo a background check should you be made an offer. You also understand that the offer is contingent upon successful completion of the background check and results consistent with Press Ganey's employment policies. You will be notified during the hiring process which checks are required for the position.
Press Ganey Associates LLC is an Equal Employment Opportunity/Affirmative Action employer and well committed to a diverse workforce. We do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, veteran status, and basis of disability or any other federal, state or local protected class.
Pay Transparency Non-Discrimination Notice – Press Ganey will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information.
The expected base salary for this position ranges from $96,000 - $150,000. It is not typical for offers to be made at or near the top of the range. Salary offers are based on a wide range of factors including relevant skills, training, experience, education, and, where applicable, licensure or certifications obtained. Market and organizational factors are also considered. In addition to base salary and a competitive benefits package, successful candidates are eligible to receive a discretionary bonus or commission tied to achieved results.
#LI-Remote","$123,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,1985.0,$500 million to $1 billion (USD)
"Watauga Group
3.6",3.6,"Atlanta, GA",Associate Data Engineer,"WHO WE ARE:
At Watauga Group, we leverage two decades of specialized media expertise and our love for the outdoors and entertainment to help outdoor recreation & attraction brands maximize their sales and elevate advertising ROI.Our unique blend of marketplace intelligence and deep insights into the media behaviors and preferences of outdoor participants and attraction visitors enables us to surgically target untapped consumer audiences and connect brands with the greatest number of potential customers at every step of their buying journeys.Watauga’s brand and performance advertising experts navigate today’s complex media landscape to create fully integrated strategies encompassing the most effective mix of digital and traditional media channels, platforms, data, and technologies. Our end-to-end media solutions include Broadcast TV & Radio, OTT & Streaming Audio, Out-of-Home, Digital Display, Paid Search, Paid Social, Sponsorships, and more.
Certified by the WBENC, Watauga is one of the largest women-owned media agencies in North America with offices in Orlando, Atlanta, and Birmingham.
WHY JOIN US:
Generous health benefits package including employer contribution to medical insurance, employer-paid life insurance and disability insurance
Employer match to 401(k) retirement plan
Flexible PTO
Flexible Hybrid Schedule
Paid Parental Leave
Health Savings Account
Tuition Reimbursement
Career progression
Bonus and incentive plans
ABOUT THE JOB:
PRIMARY DUTIES:
The duties and responsibilities of this position include but are not limited to those listed below. These duties and responsibilities may be modified at any time by Management. Modifications will be in writing and will be acknowledged by both parties.
Assist with maintenance of Python/SQL ETL pipelines
Assist with administration of PostgreSQL databases
Assist with design of database architecture to support business analysts
Assist with maintenance of Azure Cloud environment
Create and maintain internal and external dashboards for reporting & data visualization in Power BI
Transform, improve, and integrate data from multiple sources, into accessible, understandable, and usable datasets.
Assist in building and maintaining DataMart tables to optimize BI performance.
Maintain thorough documentation of dashboard data requirements.
Provide quality assurance of imported data.
Assist with pipeline and database development and maintenance.
Work with Digital Media team to ensure the proper data needs are delivered with focus on accuracy and attention to detail.
CORE COMPETENCIES:
Achievement and Results Orientation
Adaptability and Flexibility
Analytical and Strategic Thinking
Attention to Detail with Accuracy
Communication – Written, Oral, Presenting
Learning Support and Continuous Learning
Process Orientation
Problem Solving
Teamwork, Cooperation, and Working with Others
EDUCATION AND EXPERIENCE REQUIREMENTS:
Bachelor’s degree (Computer Science, Data Analytics, Accounting, or Finance a plus)
Strong data analysis skills
Strong data visualization skills
Proficient in Microsoft Excel
Working knowledge of MS Power BI Development, Deployment, and Integration
SQL and Python experience is a plus
Familiarity with CM360 or other tag management systems is a plus
1 year relevant work experience with digital marketing or related industry is a plus
PHYSICAL CRITERIA:
Able to lift and carry 20 pounds
Able to sit for prolonged periods of time at a computer
Industry
Marketing & Advertising
Employment Type
Full-time","$74,258 /yr (est.)",1 to 50 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2004.0,$5 to $25 million (USD)
"Parkland Health and Hospital System
3.7",3.7,"Dallas, TX",Data Engineer - PCHP,"Interested in a career with both meaning and growth? Whether your abilities are in direct patient care or one of the many other areas of healthcare administration and support, everyone at Parkland works together to fulfill our mission: the health and well-being of individuals and communities entrusted to our care. By joining Parkland, you become part of a diverse healthcare legacy that’s served our community for more than 125 years. Put your skills to work with us, seek opportunities to learn and join a talented team where patient care is more than a job. It’s our passion.

Primary Purpose
The Parkland Community Health Plan’s (PCHP’s) Data Engineer is responsible for maintaining the data systems including business intelligence, ETL, and supporting backup strategies to provide PCHP with secure, dependable, and accurate data including data transfer, data integrity, and data storage responsibilities. The Data Engineer will collaborate with Database Administrators, server team, storage team, and other teams to plan maintenance activities and with PHCP’s analytics team for report or universe deployments. The Data Engineer will also be involved in dashboard and report development activities.

Minimum Specifications

Education
Bachelor’s degree in computer science, management information systems, information technology, statistics, mathematics, or related discipline.

Experience
Seven years of experience in maintaining business intelligence, data warehouse solutions, or ETL in a Run or Production environment.
Six years of experience troubleshooting ETL load related issues (SSIS or Data Solutions).
Six years of experience with ETL development and maintenance experience in a data warehouse environment.
Experience with systems engineering (hardware / software) capacity.
Experience with database or report portal tool administration is preferred.
Experience at a healthcare or managed care organization is preferred.

Certification/Registration/Licensure
System Administration or Reporting Tool Administrative Certification is preferred. (i.e., Epic Cogito or Clarity, SAP Business Objects, Tibco Composite, Microsoft Certified Solutions Engineer (MCSE), Oracle Certified Professional (OCP), etc.)
PMP or other project management certificate or training is preferred.

Skills or Special Abilities
Proficiency with ETL tool Build or Run activities.
Ability to create reports and/or build virtual data environments.
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
Proficiency with Microsoft Office Excel, Word and Outlook is required; Access and PowerPoint are preferred.
Demonstrated critical thinking and troubleshooting skills accompanied by a high level of detail.
Demonstrated ability to plan and manage multiple processes and projects simultaneously.
High level of attention to detail.
Strong verbal and written communication skills.
Demonstrated ability to collaborate effectively and work as part of a team.
Independent worker and self-starter, having the ability to provide internal motivation and drive.
Proficiency with server or application patching, backups, scripting is preferred.
Understanding of SSIS and Apache NiFi is preferred.
Proficiency with Business Objects Administration is preferred.

Responsibilities
Implements and maintains high-value business intelligence environments.
Maintains the data systems including business intelligence, ETL, and supporting backup strategies.
Has a strong understanding of all the tools within the environment, regardless of vendor, and quickly and efficiently triages, troubleshoots, and restores services during outages or service degradation.
Responsible for being on-call for Business Intelligence and ETL cycles.
Works with the Database Analyst and storage teams to ensure proper backups are taken, test back-ups periodically, and ensures that the system can be restored in the time of a disaster.
Proactively identifies areas for improvement in our Business Intelligence environment.
Documents all routine processes and cross-trains other team members.
Improves function, speed, and accuracy of data distribution methods.
Develops automated reports and dashboards.

Job Accountabilities
Identifies ways to improve work processes and improve customer satisfaction. Makes recommendations to supervisor, implements, and monitors results as appropriate in support of the overall goals of PCHP.
Stays abreast of the latest developments, advancements, and trends in the field by attending seminars/workshops, reading professional journals, actively participating in professional organizations, and/or maintaining certification or licensure. Integrates knowledge gained into current work practices.
Maintains knowledge of applicable rules, regulations, policies, laws, and guidelines that impact the area. Develops effective internal controls designed to promote adherence with applicable laws, accreditation agency requirements, and customer requirements. Seeks advice and guidance as needed to ensure proper understanding.

Parkland Health and Hospital System prohibits discrimination based on age (40 or over), race, color, religion, sex (including pregnancy), sexual orientation, gender identity, gender expression, genetic information, disability, national origin, marital status, political belief, or veteran status. As part of our commitment to our patients and employees’ wellness, Parkland Health is a tobacco and smoke-free campus.","$97,618 /yr (est.)",5001 to 10000 Employees,Hospital,Healthcare,Health Care Services & Hospitals,1894.0,$500 million to $1 billion (USD)
"Atos
3.7",3.7,"Troy, MI",DATA ENGINEER,"Publication Date:
May 10, 2023

Ref. No:
480012

Location:
Troy, MI, US, 48083

The future is our choice

At Atos, as the global leader in secure and decarbonized digital, our purpose is to help design the future of the information space. Together we bring the diversity of our people’s skills and backgrounds to make the right choices with our clients, for our company and for our own futures.

Position : Cloud Data Engineer

Location : Westlake Village, CA

Job Description

Design and Build scalable, secure, low latency, resilient and cost-effective solutions for enabling predictive and prescriptive analytics across the organization
Design/ Architect frameworks to Operationalize ML models through serverless architecture and support unsupervised continuous training models
Take over and scale our data models (Tableau, Dynamo DB, Kibana)
Experience in shipping low-latency massive scale systems to production
Communicate data-backed findings to a diverse constituency of internal and external stakeholders
Build frameworks for data ingestion pipeline both real time and batch using best practices in data modeling, ETL/ELT processes and hand off to data engineers
Participate in technical decisions and collaborate with talented peers.
Review code, implementations and give meaningful feedback that helps others build better solutions.
Helps drive technology direction and choices of technologies by making recommendations based on experience and research. MINIMUM
REQUIREMENTS & SPECIAL ATTRIBUTES

5 or more years of experience working directly with enterprise data solutions
Hands on experience working in a public cloud environment and on-prem infrastructure.
Specialty on Columnar Databases like Redshift Spectrum, Time Series data stores like Apache Pinot and the AWS cloud infrastructure
Experience with in-memory, serverless, streaming technologies and orchestration tools such as Docker, Spark, Kafka, Airflow, Kubernetes is needed
Excellent SQL skills and Python coding is a must
Current hands-on implementation experience required, possessing 5or more years of IT platform implementation experience.
AWS Certified Big Data - Specialty desirable
Experience designing and implementing AWS big data and analytics solutions in large digital and retail environments is desirable
Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases, data lakes, and schemas.
Experience with AWS Cloud Data Lake Technologies and operational experience of Kinesis/Kafka, S3, Glue and Athena.
Experience with any of the message / file formats: Parquet, Avro, ORC
Design and development experience on subscribing to a Streaming Service, EMS, MQ, Java, XSD, File Adapter, and ESB based applications
Experience in distributed architectures such as Microservices, SOA, RESTful APIs and data integration architectures is a plus
Hands on experience migrating On-Prem Data solutions to cloud
Prior experience managing On-prem Enterprise Data Warehouse solutions like Netezza is a plus
Experience with a wide variety of modern data processing technologies, including
Big Data Stack (Spark, spectrum, Flume, Kafka, Kinesis etc.)
Data streaming (Kafka, SQS/SNS queuing, etc)
Expert in Columnar databases primarily, Redshift or like technologies lile Snowflake, Firebolt
Expert in Commonly used AWS services (S3, Lambda, Redshift, Glue, EC2, etc)
Expertise in Python, pySpark or similar programming languages is a must have
BI tools (Tableau, Domo, MicroStrategy) is a plus
Skilled in AWS Compute such as EC2, Lambda, Beanstalk, or ECS
Skilled in AWS Management and Governance suite of products such as CloudTrail, CloudWatch, or Systems Manager
Skilled in Amazon Web Services (AWS) offerings, development, and networking platforms
Skilled in AWS Analytics such as Athena, EMR, or Glue
Proficiency in Oracle, MYSQL and Microsoft SQL Server Databases is a plus
Understanding Continuous Integration / Continuous Delivery with experience in Jenkins
Here at Atos, diversity and inclusion are embedded in our DNA. Read more about our commitment to a fair work environment for all.

Atos is a recognized leader in its industry across Environment, Social and Governance (ESG) criteria. Find out more on our CSR commitment.

Choose your future. Choose Atos.

Nearest Major Market: Troy

Nearest Secondary Market: Detroit","$90,916 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Information Technology Support Services,1997.0,$10+ billion (USD)
"HORNE LLP
3.5",3.5,"Ridgeland, MS",Data Engineer,"The Data Engineer will develop, optimize, and support Horne’s data integrations, APIs and ETL processes. The Data Engineer should know how to examine new data system requirements and implement processes. The ideal candidate will also have proven experience in cloud data management, with excellent analytical and problem-solving abilities.
Responsibilities:
Creates data pipelines, big data platforms and data integrations in databases, data warehouses and data lakes, and works with various cloud and on-premises technologies.

Gathers and analyzes data from databases and other source systems, runs machine learning algorithms and predictive models, creates data visualizations for business users.

Extracts data from business systems, cleanses, analyzes, and creates reports and dashboards to highlight trends and other business information for end users.

Create complex functions, scripts, stored procedures, and triggers to support application development.

Work on multiple, small to large projects as a team member, or independently on small projects.

Troubleshoot applications and provide corrective measures.

Monitor the system performance by performing regular tests, troubleshooting, and integrating new features.

Offer support by responding to system problems in a timely manner.

Requirements:
Bachelor’s degree in computer science, Computer Engineering, or relevant field.

Strong knowledge of relational databases, data warehouses.

Strong knowledge in developing complex SQL scripts.

Proficiency in Python and/or Scala, machine learning.

Experience in data visualization, and data analytics.

Knowledge of data integration and ETL tools Informatica IICS is a preferred skill.

Experience with Business Intelligence tools such as Tableau, Power BI is a preferred skill.

Experience with cloud computing environments, particularly Microsoft Azure, is a preferred skill.

Knowledge of big data computing on Spark framework.

Some knowledge of data modeling

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)","$89,047 /yr (est.)",501 to 1000 Employees,Company - Private,Financial Services,Accounting & Tax,1962.0,$25 to $100 million (USD)
"Hollstadt & Associates
4.5",4.5,Minnesota,Senior Data Engineer,"Minnesota - Developer
Hollstadt Overview
Hollstadt Consulting is a management and technology consulting firm dedicated to placing professionals at engagements where they will excel. When you work with us, you'll work with a refreshingly real company led and staffed by seasoned experts who are also down-to-earth, good people. We're committed to treating you with respect and helping you achieve your career aspirations.

Since 1990, Hollstadt has been a trusted partner to more than 150 domestic and global companies and has successfully completed over 2,000 projects. Our continued growth has created challenging and rewarding opportunities for accomplished IT and Business Consultants. Hollstadt Consulting is an equal opportunity employer including disability/veteran.

Job Description

The Senior Data Engineer will oversee the department's data integration work, including developing data models, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. This role will work closely and collaboratively with members of other areas to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of the analytics needs.

Requirements
Bachelor’s Degree in Computer Science or Management Information Systems (MIS) or Business, Finance or Accounting with an emphasis in MIS
Minimum 5 years experience of developing and supporting enterprise level data warehouse systems
Strong knowledge of relational databases and SQL. Extract, Transform, and Load (ETL) data into a relational database
General data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets together, reformat data between wide and long, etc.
Demonstrated ability to learn new techniques and troubleshoot code without support, ex. find answers to common programming challenges
Strong knowledge of T-SQL language as evidenced by ability to write complex SQL queries, Microsoft SQL Management Studio, SQL Analysis Services and SQL Server Integration Services
Demonstrated ability to work independently and be a self-starter
Demonstrated ability to work effectively in teams, in both a lead and support role
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision
Cloud knowledge or exposure
Experience working with cloud infrastructure services like Amazon Web Services and Google Cloud
Agile LeSS (seems like this would make the candidate stand out and they'd be very interested in interviewing)
Mentorship experience
Preferred
Experience working with Data Vault 2.0
Experience with advanced data visualization and mapping




Benefits + Perks


Comprehensive Benefit Plan
Hollstadt offers a competitive and comprehensive benefit package which includes Medical, Dental, Vision, Long Term/Short Term Disability, and Life Insurance. With three different medical plans to choose from, you can enroll in the coverage you need from single to family, or anywhere in between!

Remarketing Process
Hollstadt is based on retention and relationships. We get to know your strengths and career wishes throughout your assignment and then start remarket discussions 6-8 weeks prior to your end date. By being proactive, we are able to keep your down time between assignments as short as possible, unless you choose otherwise.

Professional Development
Hollstadt offers free bi-weekly training courses for our consultants as well as on-demand access to past sessions through our consultant portal. Trainings give our consultants the continuing education they need to excel on their projects.

401k + Matching
One popular benefit is our 401(k) match on the first 4% of your contributions. Hollstadt wants to help you reach your long-term financial goals and understands that planning for your future is critical. Consultants also have access to support from a Financial Advisor.

Bonus Opportunities
We appreciate and reward loyalty. Join Hollstadt, stay for 5 years, and we’ll give you a $5,000 Longevity Award bonus! Additionally, we know great talent knows other great talent. If you are on contract with Hollstadt and refer one of your connections who gets placed, we’ll pay you $1,000!

Ongoing Support & Networking
We have made a significant investment in building a support program for our consultant team - so you never have to feel like you are going it alone. We also have a Consultant Coach program which acts like a 'work buddy' to provide a safe ear for questions or concerns at your client site.",,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1990.0,$25 to $100 million (USD)
"Syntelligent Analytic Solutions
3.9",3.9,"Washington, DC",Data Engineer,"Syntelligent Analytic Solutions, LLC provides uniquely qualified personnel with the expertise and tools needed to fulfill our customers’ management and technical requirements in the intelligence, defense, homeland security and commercial market space.
Our customers’ and Syntelligent’s success are built upon the core values of People First, Integrity & Accountability, Mission Driven, Community Focus and Team Oriented.
Syntelligent is seeking a Data Engineer with an active TS/SCI clearance for upcoming work with a federal client in Washington, DC.
Description
Syntelligent is seeking a data engineer to work in a variety of settings to develop and design data pipelines to support end-to-end solutions. Candidate will build systems that collect, manage, and convert raw data into usable information for data scientists and business analysts to interpret.
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Qualifications:
Active TS/SCI clearance ONLY
Bachelor’s degree
Three (3) to five (5) years of relevant experience.
Clearance level required: TS/SCI
Location:
For any unclassified development efforts, remote work is available. Position does require client site work at DHS HQs on Nebraska Ave NW, DC
Online applications, please.
The salary ranges for these positions will be set based on experience, geographic location and possibly contractual requirements and could fall outside of this range. Other rewards may include annual bonuses, Spot bonuses, and program-specific awards. In addition, Syntelligent provides a variety of benefits to all our Full-Time employees.
When we review candidates' information, we are looking for the best matches for the position based on the qualifications listed in the job posting. If your skills and experience appear to match an open position, a recruitment services professional or a hiring manager may contact you.
Syntelligent Analytic Solutions, LLC is an Equal Employment Opportunity and Affirmative Action employer. It is the policy of the company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation and gender identity or expression, national origin or protected veteran status and will not be discriminated against on the basis of disability. If you are a qualified disabled veteran or individual with a disability and need reasonable accommodation to use or access our online system, please contact our Human Resources department at 540-736-4570, Extension #2","$100,358 /yr (est.)",1 to 50 Employees,Company - Public,Telecommunications,"Cable, Internet & Telephone Providers",,Unknown / Non-Applicable
"WEX Inc.
3.7",3.7,Remote,Data Engineer,"We are seeking a Data Engineer to play a critical role in the development of WEX's enterprise data & analytics capabilities. You will be part of an organization focusing on the development and delivery of data solutions and capabilities for WEX’s data platform. The successful candidate is motivated by thinking big data, technically proficient, and enjoys working in a fast paced environment.
The base pay range represents the anticipated low and high end of the pay range for this position. Actual pay rates will vary and will be based on various factors, such as your qualifications, skills, competencies, and proficiency for the role. Base pay is one component of WEX's total compensation package. Most sales positions are eligible for commission under the terms of an applicable plan. Non-sales roles are typically eligible for a quarterly or annual bonus based on their role and applicable plan. WEX's comprehensive and market competitive benefits are designed to support your personal and professional well-being. Benefits include health, dental and vision insurances, retirement savings plan, paid time off, health savings account, flexible spending accounts, life insurance, disability insurance, tuition reimbursement, and more. For more information, check out the ""About Us"" section.
Salary Pay Range: $80,500.00 - $108,000.00","$94,250 /yr (est.)",1001 to 5000 Employees,Company - Public,Financial Services,Investment & Asset Management,1983.0,$1 to $5 billion (USD)
TekValue IT Solutions,,"Houston, TX",Data Engineer with PL/SQL,"Data Engineer
Day 1 Onsite
Loc Houston Texas
Required Skills:
Requirements:
Recent PL/SQL development work experience with Oracle 19.
Must have working experience with tools like Airflow, Git.
Must have python coding experience and REST API development experience.
Plus would be experience with microservices, docker, Kubernetes.
Plus would be experience with accounting and financial data
Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.
Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL.
Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations. Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts.
Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
2+ years’ experience in the design, implementation and support of full life-cycle data engineering projects.
Bachelor's degree in Computer Science, Engineering or other equivalent degree, with 5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree with 2+ years data engineering preferred.
High school diploma or GED is required.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Houston, TX 77003: Reliably commute or planning to relocate before starting work (Required)
Experience:
PL/SQL: 9 years (Preferred)
Python: 8 years (Preferred)
Database development: 8 years (Preferred)
Work Location: One location",$72.50 /hr (est.),Unknown,Company - Public,,,,Unknown / Non-Applicable
"OM Group Inc.
3.5",3.5,Remote,Senior Data Engineer,"Data Engineer
OM Group, Inc. is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal & DoD clients. We are currently expanding support for the Enterprise Data Warehouse to continue and evolve on-prem/cloud/hybrid data migration and enterprise reporting platforms.
We are hiring a Data Engineer with experience creating and maintaining complex shell and SQL scripts used to expand and populate data into the enterprise data warehouse. The successful candidate will be responsible for managing and maintaining the Extract, Transform and Load (ETL) processes to populate the Enterprise Data Warehouse/ Enterprise Virtual Viewer (EDW/EVV). The EDW/EVV is powered by a multi-tier environment encapsulation of Windows/Unix and Linux Environments. The primary data repository, as well as host to the virtualization/governance and catalog layers live on the IBM Cloud Pak for Data System (CP4D) and Netezza Performance Server (NPS) PostgreSQL based database. The CP4D system is Linux-based on a combined OpenShift and Redhat 7 platform. The EDW/EVV also utilizes SAP Business Objects (BOBJ) to do reporting based on a Windows Virtual Machine (VM) architecture. The Oracle 19c database is on Oracle Solaris VMs and acts as repository databases for SAP BOBJ.
This position is remote on Eastern Time zone schedule.
Responsibilities
Work with stakeholders to evaluate business needs and develop tasks to meet requirements and objectives
Provide quality assurance and identify bugs or required fixes and communicate to respective teams. Ensure data being migrated to the EDW/EVV production environment is documented
Enforce EDW/EVV standards / policies and provide quality assurance for universe, star schema/build, report, and dashboard migrations from the EDW/EVV development environment into the EDW/EVV production environment (see Appendix A for migration metrics)
Maintain existing Unix Shell and Structured Query Language (SQL) script based ETL processes. Design, Develop, document, and maintain new and/or expanding script based ETL processes e.g., Unix Shell and SQL scripts
Coordinate, test, implement, document, and manage required upgrades and capability enhancements for the EDW/EVV. Perform tasks for data repository expansion as more data content is transitioned to the warehouse
Work with infrastructure team to ensure that all the required monitoring, exception handling and fault tolerance is in place for a production-quality data platform
Team up with analysts, product managers, and other stakeholders to understand evolving business needs and translate reporting capabilities accordingly
Assist with process improvement with a customer-focused, progressive mindset
Understand data classification and adhere to the information protection and privacy restrictions
Troubleshoot and provide technical support for staff and back-end system users
Document and support code migrations and provide quality assurance / control of EDW/EVV star schemas/builds, universes, local data, and reports

Requirements
Active DoD Secret Security clearance
Hold DoD IAT-III, IAM-II and IASAE-II certifications and Data related industry certifications
5+ years experience with Linux Shell Script development, testing, debugging and deployment
5+ years experience with SQL, relational database and data warehouse technologies
3+ years experience developing and maintaining Python or similar scripting language
3+ years experience in multiple subversion technologies such as Subversion, GitHub or Tortoise
Knowledge of AWS technologies such as S3, EC2, Redshift, Glue, Athena and Step Functions preferred
Experience in data mining and development of ETL processes, distributed data architectures and big data processing technologies
Knowledge of production / environment control
Knowledge of Agile software development lifecycle
Excellent problem-solving skills and attention to detail
Strong documentation and training skills
OM Group, Inc. recently voted one of the top workplaces in the Washington, DC area, is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal & DoD clients. We are a growing company that values your skills, training, and ideas and strives to foster a welcoming, diverse, and inclusive environment. OM Group provides competitive compensation and benefits including health insurance coverage, 401(k), paid time off, as well as support for continuous education and training.
OM Group, Inc. is an equal opportunity employer (EEO) and does not discriminate on the basis of race, color, religion, sex, national origin, age, disability, veteran status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive workplace where all employees are treated with respect and dignity",,1 to 50 Employees,Company - Public,Management & Consulting,Business Consulting,2001.0,Unknown / Non-Applicable
vebyond corp,,"Charlotte, NC",AWS Data Engineer,"AWS Data Engineer Lead, Core Technical Skills
5+ years of AWS experience
Experience in a regulated environment
AWS services - S3, EMR, Glue Jobs, Lambda, Athena, CloudTrail, SNS, SQS, CloudWatch, Step Functions
Experience with Kafka/Messaging preferably Confluent Kafka
Experience with databases such as DocumentDB, MySQL, Postgres, Glue Catalog, Lake Formation, Redshift, DynamoDB and Aurora and SQL
Tools and Languages – Python, Spark, PySpark
Experience with Secrets Management Platform like Vault and AWS Secrets manager
Experience with Event Driven Architecture
Experience with Rest APIs and API gateway
Experience with AWS workflow orchestration tool like Airflow or Step Functions
AWS Data Engineer Lead Additional Technical Skills (nice to have, but not required for the role)
Experience with native AWS technologies for data and analytics such as Kinesis, OpenSearch
Databases - Document DB, MongoDB Atlas
Data Lake platform (Hive, Druid, Apachi Hudi/Apache Iceberg/Databricks Delta)
Java, Scala, Node JS, Pandas
Workflow Automation
Experience transitioning on premise big data platforms into cloud-based platforms such as AWS
Strong Background in Kubernetes, Distributed Systems, Microservice architecture and containers
Day to Day Responsibilities/project specifics:
Provides technical direction, guides the team on key technical aspects and responsible for product tech delivery
Lead the Design, Build, Test and Deployment of components
i. Where applicable in collaboration with Lead Developers (Data Engineer, Software Engineer, Data Scientist, Technical Test Lead)
Understand requirements / use case to outline technical scope and lead delivery of technical solution
Confirm required developers and skillsets specific to product
Provides leadership, direction, peer review and accountability to developers on the product (key responsibility)
Works closely with the Product Owner to align on delivery goals and timing
Assists Product Owner with prioritizing and managing team backlog
Collaborates with Data and Solution architects on key technical decisions
i. The architecture and design to deliver the requirements and functionality
Mentor other developers in development of components and related processes
Job Type: Full-time
Salary: $80.00 - $85.00 per hour
Experience level:
10 years
11+ years
9 years
Schedule:
8 hour shift
Experience:
AWS Data Engineer: 5 years (Preferred)
Work Location: On the road",$82.50 /hr (est.),,,,,,
