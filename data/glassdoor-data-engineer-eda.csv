company,company_rating,location,job_title,job_description,salary_estimate,company_size,company_type,company_sector,company_industry,company_founded,company_revenue,job_state,company_age,job_simp,seniority,job_languages,job_cloud,job_viz,job_bigdata,job_data,job_education
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
Steward Health Care,2.7,"Westwood, MA",Data Engineer,"Position Purpose:
Reporting to the Manager of the Data Warehouse team, part of the larger Health Informatics group, the data engineer applies their technical expertise to meet the needs of the department and Steward Health Care Network (SHCN).

Key Responsibilities:
ETL/Automation
Design configurable data process flows with full automation
Develop ETL processes for data loading and data extraction
Schedule ETL processes for full process automation
Data Engineering
Responsible for data analysis to support building data processes and reporting
Design useful and accurate data marts that meet requirements
Apply SQL skills when designing and building data marts and data flows
Quality
Establish and utilize QC processes to ensure data integrity
Incorporate standard error logging and alerts to ensure data is loaded as expected
Documentation
Create and maintain clear documentation

Education / Experience / Other Requirements


Education:
Bachelor's degree in Computer Science, Mathematics, Statistics or related experience


Years of Experience:
5+ years of database related work
2+ years of focus on healthcare data

Specialized Knowledge:
Knowledge of healthcare data
Experience using relational databases, SQL Server experience preferred
Experience using ETL tools (SSIS, Informatica, etc.)
Strong SQL programming skills
Experience with scripting languages (PowerShell, R, Python, etc.)
Experience automating data flows
Experience with Health Catalyst tools preferred, but not required
Deep understanding of database structures and data design.
Creative, flexible, and self-motivated with sound judgment
Strong communication skills




Location: Steward Health Care Network · 1301.72330 Steward Health Care Network
Schedule: Full Time, Day Shift, 40 hours",94536.0,10000+ Employees,Hospital,Healthcare,Health Care Services & Hospitals,1998,Unknown / Non-Applicable,MA,25,data engineer,na,"['sql', 'r', 'python']",[],[],[],[],bachelor
"Twitch Interactive, Inc.",3.8,"San Francisco, CA",Data Engineer,"3+ years of experience in data engineering, software engineering, or other related roles. 3+ years in relational database concepts with a solid knowledge of star schema, SQL, SQL Tuning, OLAP, Big Data technologies 3+ years of experience in generating and maintaining data pipelines from various data sources, in collaboration with diverse stakeholders. 3+ years of experience working with Amazon Webservices, S3, EMR, Redshift etc. Experience with best practices for development including query optimization, version control, code reviews, and documentation. Experience with coding languages like Python/Java/Scala
About Us: Twitch is the world's biggest live streaming service, with global communities built around gaming, entertainment, music, sports, cooking, and more. It's where millions of people come together to chat, interact, and make their own entertainment. We're about community, inside and out. You'll find coworkers who are eager to team up, collaborate, and smash (or elegantly solve) problems together. We're on a quest to empower live communities, so if this sounds good to you, see what we're up to on LinkedIn and Twitter, get interviewing tips on Instagram, and discover projects we're solving on our Blog. About the Role: Data is central to Twitch's decision-making process, and data engineers operate at the forefront of this by creating authoritative datasets that drives analysis and decision-making across all of Twitch. In this role you will be shaping the way that business performance is measured, defining how we transform our data, and scaling analytics methods and tools to support our growing business, leading the way for high quality, high velocity decisions. For this role, we're looking for an experienced data engineer to join our Content Data Science team, which is focused on empowering staff throughout Twitch to use and trust our business data. Your responsibilities may range from developing and enhancing our data warehouse which act as authoritative sources of truth across the company, driving data quality and trustworthiness across product verticals and business areas, building self-service business intelligence infrastructure for analysts, as well as connecting into data interfaces that enable everyone in Twitch to discover and analyze the data. In the process, you will have the opportunity to interact with technical and non-technical staff members throughout the company, and will report to the Director of Content Data Science. This position can be located in San Francisco, CA; Irvine, CA; Seattle, WA; New York, NY; and Salt Lake City, UT. You Will: Define and own team level data architecture for trusted, governed, dimensionally-modeled repository of data that enables Twitch staff to quickly and reliably answer their business questions. Keep existing data sources fresh against data quality issues, design, develop and maintain data quality assurance framework and continuously improve the processes for developing new ones raising the level of quality expected from our work. Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost. Improve search, discovery and literacy: Create exploration and visualization interfaces in our BI tools and evangelize the adoption of these sources across the company through education and training programs. Improve business and engineering team processes via data architecture, engineering, test, and operational excellence best practices. Make enhancements that improve data processes.
Bonus Points
A passion for data science and interest in growing / learning data science, machine learning at scale.
A passion for games and the gaming industry
Perks
Medical, Dental, Vision & Disability Insurance
401(k)
Maternity & Parental Leave
Flexible PTO
Amazon Employee Discount
Monthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages, etc.)
We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
Pursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $105,700/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. Applicants should apply via our internal or external career site.",105700.0,10000+ Employees,Company - Public,Information Technology,Internet & Web Services,1994,$10+ billion (USD),CA,29,data engineer,na,"['sql', 'scala', 'java', 'python']",[],[],[],[],na
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
Nike,4.1,"Boston, MA",Data Engineer,"Become part of the Converse Team

Converse is a place to explore potential, break barriers and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Converse, it’s about each person bringing skills and passion to a challenging and constantly evolving world to make things better as a team.
Converse, Inc. Boston, MA. Work closely with Project Management and Business teams to completely define specifications to ensure the project acceptance. Involved in preparation of functional and technical specifications with different cross teams. Lead team, defining solution options, providing estimates on effort and risk, and evaluating technical feasibility in Agile development process, including Scrum and Kanban. Work on troubleshooting data and analytics issues and perform root cause analysis to proactively resolve issues. Develop data extracts and feeds from the full spectrum of systems in the Converse ecosystem, including transactional ERP systems, POS data, product and merchandising systems. Engineer data products for a variety of Operations analytics use cases, ranging from reporting and data visualization to advanced analytics/machine learning use cases. Support designing technical specifications and data transformation models for junior developers. Ensure development is on track and meets specifications as defined by product management and the business. Responsible for data integrity of current platform and QA of new releases. Support the development and maintenance of backlog items and solution feature. Participate in sprint planning activities from a development perspective. Responsible for designing cloud-based data architecture using AWS stacks. Design and develop Python data science and data engineering libraries dealing with structured and unstructured data. Work with a variety of database types (SQL/NoSQL, columnar, object-oriented) and diverse data formats. Responsible for ETL with Spark and building data pipelines/orchestrations in Airflow and working on ETL tools like Matillion. Responsible for DevOps toolchain and Continuous Development, Continuous Integration and Automated Testing using Jenkins. Ensure and use data engineering for advanced analytics/data science and Software development skills.
Applicant must have a Bachelor’s degree in Computer Science, Information Systems, or Information Technology and 5 years of progressive post-baccalaureate experience in the job offered or a related occupation. Experience must include:
Data warehousing;
ETL or ELT;
Amazon Web Service (AWS) Cloud Services, including AWS S3, AWS Lambda, AWS EC2, AWS EMR or AWS DynamoDB;
Relational Database Management Systems (RDBMS), such as Oracle, Teradata, SQL Server or Snowflake;
Database Development with writing stored procedures, functions, triggers, cursors or SQL queries;
Hadoop, HDFS, Hive or Spark;
Programming languages, including Java or Python;
Business Intelligence Tools, such as Tableau;
Unix Shell scripting; and
Version control systems, such as Git, Bitbucket or Github
#LI-DNP
Converse is more than a company; it’s a worldwide advocate for self-expression. This belief motivates our employees, permeates our working environment and inspires our products. No two of us look or think exactly alike. We are each one-of-a-kind. Individually and as a culture, we have the freedom to create and grow professionally. Generous benefits packages only sweeten the experience. From Boston to Shanghai, from Brand Design to Finance, Converse is a brand that celebrates the unique and creative people of the world. Together, we’re different.",115797.0,10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1972,$10+ billion (USD),MA,51,data engineer,na,"['nosql', 'sql', 'java', 'python']",['aws'],['tableau'],"['hadoop', 'spark']","['hive', 'snowflake', 'airflow']",bachelor
Aretec Inc,1.0,Remote,Junior Data Engineer,"POSITION TITLE: Junior Data Engineer YEARS OF EXPERIENCE: 1-3
LOCATON: 100% remote
*****Please Note: Aretec, Inc. does not offer Corp - 2 - Corp (C2C) employment. *****
Aretec is looking for a Junior Data Engineer. The Junior Data Engineer will be primarily responsible for design, development, support and enhancement of the data pipelines developed in AWS.

RESPONSIBILITIES:
You'll write clean and functional code on the front- and back-end
You'll write reusable and maintainable code
Coordinate with data migration plans
Ability to communication and collaborate with various teams and vendors.
Participates in functional and technical design.
Participation in Agile activities Scrum, Kanban.
Ensure coding, testing, debugging and implementation activities completed as required.
Flexible and adaptable with the ability to align to changing priorities
The developer should have great communication skills and be able to discuss and develop requirements with multiple levels of staff from corporate and field locations
An interest in and ability to understand financial reporting, accounting concepts and related accounting data
Participate in data flow diagramming and/or process modeling (code architecture)
Documents work and steps to completion as required
Follows AWS best practices to integrate with ecosystem and infrastructure
Ability to partner with domain architects to implement the defined solution architecture including application, infrastructure, data, integration, and security domains

REQUIRED SKILLS:
1-3 years of software engineering experience
1+ years of real industry experience
Experience with website development, web services and API development
Hands-on experience performing data engineering and transformation tasks using Python
Experience implementing backend in Python using frameworks such as Django or Flask
Knowledge of web technologies - both back and front-end development including, but not limited to JavaScript, React, CSS, HTML, T-SQL, and Python
Understand log monitoring and analytics
Experience Meeting both technical and consumer needs
Experience testing software to ensure responsiveness and efficiency
A general knowledge of index migrations, debugging and researching concepts are major pluses
Must be aware of CI/CD pipelines and well-versed in using GitLab for creating required pipelines for CI/CD

EDUCATION: Bachelors Degree in Mathematics/Statistics/Technology/Science/Engineering/Applied Mathematics or related field

CERTIFICATIONS: N/A",102500.0,51 to 200 Employees,Contract,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],[],[],na
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
Adobe,4.4,"New York, NY",Data Engineer,"Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

Job Description
Adobe Customer Solutions is looking for a full time Data Engineer with experience in building data integrations using AWS technology stack as part of the team's Data as a Service portfolio for Adobe’s Digital Experience enterprise customers.
Customer facing Engineers who enjoy tackling complex technical challenges, have a passion for delighting customers and who are self-motivated to push themselves in a team oriented culture will thrive in our environment
What you'll Do
Collaborate with Data architects, Enterprise architects, Solution consultants and Product engineering teams to gather customer data integration requirements, conceptualize solutions & build required technology stack
Collaborate with enterprise customer's engineering team to identify data sources, profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating customer data sources and third party data sources with Adobe solutions
Develop new features and improve existing data integrations with customer data ecosystem
Encourage team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Collaborate with a Project Manager to bill and forecast time for customer solutions
What you need to succeed

Proven experience in building/operating/maintaining fault tolerant and scalable data processing integrations using AWS
Proven track record in Python programming language
Software development experience working with Apache Airflow, Spark, MongoDB, MySQL
Experience using Docker or Kubernetes is a plus
BS/MS degree in Computer Science or equivalent proven experience
Ability to identify and resolve problems associated with production grade large scale data processing workflows
Excellent interpersonal skills
Experience crafting and maintaining unit tests and continuous integration.
Passion for crafting I ntelligent data pipelines that customers love to use
Strong capacity to handle numerous projects are a must
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists. You will also be surrounded by colleagues who are committed to helping each other grow through our outstanding Check-In approach where feedback flows freely.
If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the significant benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age, sexual orientation, gender identity, disability or veteran status.

Our compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $101,500 -- $194,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.",147900.0,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1982,$5 to $10 billion (USD),NY,41,data engineer,na,['python'],['aws'],[],"['spark', 'mongodb']",['airflow'],na
Glow Networks,3.5,"Dallas, TX",Data Engineer,"Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.",151840.0,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD),TX,20,data engineer,na,"['sql', 'scala', 'java', 'python']",['aws'],[],"['hadoop', 'spark']","['kafka', 'hive', 'snowflake', 'airflow']",bachelor
Enterprise Knowledge LLC,4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”",89080.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD),VA,10,data engineer,na,"['sql', 'r', 'python']",['aws'],[],['spark'],['airflow'],na
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876.0,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,"['sql', 'c']",['azure'],['power bi'],[],[],bachelor
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],na
Small Batch Standard,4.1,Remote,Junior Data Engineer,"We're the premier, remote accounting, tax, and consulting firm built exclusively to serve the craft brewing industry.
Our mission is to help craft breweries grow profits and build deep successful relationships. And our team is filled with expert, autonomous, adaptable, technology-driven high performers.
Are you up for the challenge?
We're looking for a full-time, remote Junior Data Engineer to join our specialized team. The main objective of this role is to design, develop, implement, and improve both internal and external applications to support our brewery clients and team in accordance with the SBS Core Values.
About The Role
This role will report to our Technology/Product Manager and is accountable for fulfilling the following responsibilities:
Building our data pipeline and analysis applications. A key aspect of the consulting service we provide to clients involves the collection, aggregation, analysis, modeling, and usage of financial data and benchmarks. We use this data both internally to develop and inform strategy, as well as externally through our Benchmarks Assessment (https://sbstandard.com/assessment/) and Compass analysis product (https://sbstandard.com/levelup-compass/). You'll be responsible for working with our team to build out our data pipeline for these tools, and progressively increasing our ability to aggregate, analyze, query, and feed back this data into our reporting, analysis, and consulting work. Platforms we're building with include: SQL, Airflow, Excel Visual Basic for Applications (VBA), Google Apps Script, Intuit/QuickBooks Online.
New process and technology R&D. We're always looking for new opportunities to provide both our team and our clients access to additional tools that give them leverage, automate and streamline processes, and overall make work more efficient. Part of your time will be dedicated to researching, testing, and prototyping new tech and application options.
Participate and contribute to the overall success of our team. Each week the team meets to share wins, progress, and knowledge, as well as identify and solve issues at multiple levels (company, team, individual). Your full participation in this process is critical to ensure that we are operating as a cohesive, high-performance unit.
About You
We're looking for an individual who:
Is a problem solver through-and-through. Everywhere you look, you both (a) see problems to solve, and (b) see solutions and new ways of doing things that just haven't been done yet. You know how to think outside of the box, are willing to “go there” with new ideas and solutions that haven't been done before, and have the confidence to start building, testing, iterating, and making sh*t work.
Is a systems thinker. You understand both the big picture and how the functional components fit together, and have the ability to take a specific analysis outcome and generalize it to fit a wide range of scenarios through structure and sound system design.
Can fail fast, iterate, and learn. You're an independent, self-directed, learner who isn't afraid to “move fast and break stuff” knowing that failure is a prerequisite to success, ESPECIALLY in product development. You may not have traditional credentials, but what you do have is the ability to rapidly learn, adopt, test, and understand new languages, platforms, tools, and solutions.
Is a manager of one. Unlike working within a traditional firm, in this role you'll be in the driver's seat, managing your workflow and workload in order to meet the standard set of deliverables required for each client.
About Our Culture
We're fully remote, with team members and clients located all across the U.S. and have developed our own unique culture we call The SBS Way, within which we operate, evaluate performance, and make decisions using our core values as a guide:
Be Antifragile. Everything we do, good or bad, makes us better. And every experience is an opportunity for learning and continuous improvement.
Play The Long Game. We make decisions, to the best of our ability, in the long-term interest of our firm, our team, our clients, and our broader industry and community.
Embrace Technology. We welcome new technologies with open arms, and are always exploring, testing, and implementing them in the interest of enhancing both our internal capabilities and our client's outcomes.
Build and Trust The Process. Each member of the team is committed to building, following, and improving the processes we use to deliver exceptional results for our clients.
Act as A Team of Expert Knowledge Workers. We openly and willingly collaborate, communicate, and provide rapid, direct feedback in the interest of learning, improving and developing ourselves.
Working At SBS
What it's like working at our firm:
High flexibility. We believe in the ability of our team to determine the best way to complete their work. We measure outputs, not inputs. We don't have time sheets. We don't track hours. We don't pay attention to when and where our team works. Your schedule is yours to make.
High accountability. What we care about most is that we deliver on what we promise to our clients. In this respect, we measure and manage to our deliverable performance metrics and ensure each team member takes ownership over their accomplishment with a high level of quality that aligns with our core values
Great pay for great work. We pay based on the characteristics that matter: position (and its market value), level of mastery, and longevity with the firm. All of which aim to ensure each member of the team feels they are compensated well and can focus on great work.
Merit-based career progression. We have clearly established career tracks, performance benchmarks, and mastery levels set for all of our core positions. How quickly you progress is entirely under your control, with a quarterly review and bi-annual promotion consideration cycle in place to evaluate your progress.
Generous benefits. We offer a generous benefits package that includes medical, dental, and vision insurance enrollment; as well as an IRA match, tech stipend, 3 weeks of paid time off, and entry into our profit share bonus program after two years of service.
Personal and and team development. In addition to our overall continuous learning focus, we also provide support for personal development in the form of expense coverage for continuing education (books, courses, training, certifications, etc.) as well as experiential learning (brewery visits, industry events and conferences, etc.). Each year we also meet in person for an all-expenses-paid annual retreat as a team. No work. Lots of fun. Lots of client beer.
Job Requirements
The following basic requirements must be met:
Previous experience in SQL development and database management.
Previous experience building useful applications in scripting languages like VBA, Google Apps Script, Python, PHP, etc.
Can do effective cross-functional work in a remote environment.
Have crystal clear professional written and verbal communication skills.
Have exacting organizational standards and a calm and friendly attitude.
Available and responsive during normal business hours (9am-5pm Eastern Time, Monday-Friday).
Have a strong, consistent internet connection and a work environment conducive to video calls.
Preferred qualifications include:
Direct previous experience building data pipelines.
Direct previous experience building Airflow workflows and applications.
Experience building out and managing API connections.
Experience working with Quickbooks Online or similar accounting or finance platforms.
Experience using Podio or similar remote project management tools (e.g. Trello, Asana, etc.).
Next Steps
If the position, culture, values, and mission at Small Batch Standard sound like they're the right fit for you, please apply here.",64000.0,1 to 50 Employees,Company - Private,Financial Services,Accounting & Tax,2010,Unknown / Non-Applicable,Remote,13,data engineer,na,"['sql', 'r', 'go', 'python']",[],[],[],['airflow'],na
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['sql', 'python']",[],[],['spark'],[],bachelor
Tripoint Solutions,4.5,Remote,Data Engineer (Remote),"Tripoint Solutions is seeking a Data Engineer to join our team.
The Data Engineer will be part of a team responsible for ensuring the success of a highly visible, results-driven federal client through the development of a cloud-based next generation system.
This position requires the applicant to parse disparate data sources, including structured and unstructured elements, to find the patterns and meaning in large quantities of data. The successful candidate will leverage machine learning as well as best of breed pipeline technology to process and store a variety of data elements.
Location: This position is eligible for fully remote work. Selected candidates living within a 25 miles radius of the NITAAC office in Rockville, MD will be required to come into the office once a week. The selected candidate must be currently located in, or willing to relocate to, a state supported by Tripoint Solutions corporate offices (AL, DC, FL, IL, LA, MD, MI, MN, MS, NJ, NC, PA, TN, TX, or VA).
The successful candidate will be accountable to:
Creating and maintaining optimal data pipeline architecture.
Assembling large, complex data sets that meet functional / non-functional business requirements.
Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Building the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Keeping data separated and secure across national boundaries through multiple data centers and AWS regions.
Strong interest to learn and stay up to date on relevant technologies, trends, industry standards and identify new ones to implement.
What you bring
Experience, Education & Training:
Bachelor's degree in computer science, Math, Analytics, Statistics, Informatics, Information Technology or equivalent quantitative field.
5 years of experience working in a Data Engineer or Data Scientist role.
Experience with cloud data services (AWS preferred).
Experience solutioning and applying Natural Language Processing (NLP) and or Machine Learning (ML) technologies
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience with Microsoft SQL, database development and design.
Experience building processes supporting data integration, transformation, data structures, metadata, dependency and workload management.
Demonstrated success in manipulating, processing and extracting value from large disconnected data sets.
Demonstrated accomplishments in designing, coding, testing and supporting data analytics and reporting solutions in a cloud environment.
Experience with object-oriented/object function scripting languages: Python, Java
Concept experience; information retrieval, search engine, document data extraction
Preferred experience with AWS cloud services: Textract, Comprehend, GlueMaker, Athena, Notebook
Working knowledge of message queueing, stream processing, and highly scalable ‘big data’ data stores.
Clearance Requirements:
Applicants selected may be subject to a government security investigation and must meet eligibility requirements for potential access to classified information. Accordingly, US Citizenship or Green Card is required.
What we offer
About Tripoint Solutions
We are technology innovators, partnered with state-of-the-art providers, such as AWS, ServiceNow, and UiPath, to drive digital transformation in the federal space. TPS teams are bringing automation and data science into areas of the government that are crying out for fresh tech—making positive impacts felt by tens of thousands of users, countless citizens, and all six branches of the military each day. Our Agile teams are responsible for envisioning, launching, and operating the massive data systems and analytics platforms used to manage $14.5B in government procurements and $200B in military real estate assets globally. At TPS, we apply the power of cloud technologies to help the government think smarter and function better—for everyone.
TPS Company Values
We value and respect each employee's dedicated work and unique contributions; as they directly impact who we are and what we do.
Your talent and innovative thinking bring leading-edge solutions to our customers.
Our success is driven by the dedication of our employees.
Employee-generated solutions have sustained our continued success and customer satisfaction
Benefit Offerings
Tripoint Solutions builds flexibility into health benefit plan choices, covers most of the monthly premiums, and helps employees build a career with impact through our generous professional development program.
We offer all full-time employees:
Medical, Dental, Vision benefits with a national provider network (company pays 100% of Vision and Dental premiums)
Flexible Spending and Health Savings Accounts (FSA & HSA)
Company-paid Life and Disability insurance including Short-Term, Long-Term, and Accidental
Paid-time off (PTO), accruing with each year of service, up to 20 days, plus 11 paid holidays
401(k) Retirement Plan - No waiting period to contribute and company makes 3% contribution of eligible pay in addition to annual profit-sharing contribution option
Eligibility to receive impact bonuses each quarter
Referral Program
Professional Development Reimbursement Program to pursue undergraduate, graduate, training, and certifications
Monthly transportation, parking, and cell phone service reimbursement
COVID-19 Related Information
Tripoint Solutions does not have a vaccination mandate applicable to all employees. However, to protect the health and safety of its employees and to comply with customer requirements, Tripoint Solutions may require employees in certain positions to be fully vaccinated against COVID-19. Vaccination requirements will depend on the status of the federal contractor mandate and customer site requirements. Furthermore, remote work arrangements are subject to change based on customer site requirements.
Tripoint Solutions is an Equal Opportunity Employer/Veterans/Disabled
Job Type: Full-time
Pay: $145,000.00 - $155,000.00 per year
Benefits:
401(k)
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Retirement plan
Tuition reimbursement
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
What cloud services have you worked with?
Do you have experience with Machine Learning or NLP?
Does the advertised salary align to your expectations?
US citizenship or green card is required. Do you meet this requirement?
This is a remote position (See description for details and requirements). Where are you located?
Are you willing to undergo a federal background check?
Education:
Bachelor's (Required)
Experience:
data scientist or data engineer role: 5 years (Required)
cloud services: 2 years (Required)
Microsoft SQL (development and design)?: 2 years (Required)
optimizing ‘big data’ pipelines, architectures and data sets: 2 years (Required)
AWS: 1 year (Preferred)
Python: 1 year (Required)
Java: 1 year (Required)
Work Location: Remote",150000.0,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,Unknown / Non-Applicable,Remote,10,data engineer,na,"['sql', 'java', 'python']",['aws'],[],[],[],bachelor
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
LOVEFOODIES INC,4.4,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",62400.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['sql', 'python']",[],['tableau'],[],[],na
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800.0,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['sql', 'python']",[],[],[],[],master
"Double Line, Inc.",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",85882.0,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['sql', 'python']","['aws', 'azure']","['power bi', 'tableau']",[],[],na
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451.0,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['sql', 'python', 'nosql', 'java']","['azure', 'google cloud', 'aws']",[],"['hadoop', 'spark']","['kafka', 'hive']",na
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
"Dovenmuehle Mortgage, Inc.",2.6,"San Francisco, CA",Data Engineer,"Data Engineer
DMI Software, the San Francisco branch of Dovenmuehle Mortgage, Inc, the leading sub-servicer of mortgage loans in the United States, is looking for a talented and enthusiastic data engineer. We work exclusively in Software Development. Our growing office offers the feel of a startup with the backing and security of a long-established company. We aspire to create elegantly scalable products while fostering the continued growth of each team member. The ideal candidate will have 5+ years relevant experience, including Hadoop Ecosystem or similar, and with a scripting language.

Here we believe that the best software is created by an eclectic set of voices, and we strive to nurture an environment rich in differing opinion, belief, and background. Only in this way can we develop revolutionary products capable of meeting the varied needs of an increasingly interconnected world.

What You’ll Be Doing:
Design, implement, automate, and maintain large-scale enterprise ETL processes
Evolve data model and schema based on business and engineering needs
Oversee systems tracking data quality and consistency
Collaborate with data analysts to bridge business goals with data delivery

Requirements:
5+ years data engineering experience
Highly experienced using Python, SQL and and Hadoop
Excellent communication, analytical and problem-solving skills
Keen attention to detail while keeping an eye toward the big picture
You are comfortable with the nuts and bolts of systems programming in the Linux environment (shell/bash scripting)
Experience working in an Agile environment
Excellent presentation and communication skills
Experience profiling, debugging, tracing, and or parallelizing/optimizing Python code
Ideal candidate is one who can adapt and adopt to our existing architectures while also making impactful improvements and suggestions.

Job Type: Full-time",135927.0,1001 to 5000 Employees,Company - Private,Financial Services,Banking & Lending,1844,Unknown / Non-Applicable,CA,179,data engineer,na,"['sql', 'python']",[],[],['hadoop'],[],na
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",['aws'],[],"['hadoop', 'spark']",['airflow'],na
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
BOTG LLC,4.4,"Chicago, IL",Data Engineer,"We are looking for a Data Engineer in Chicago, IL (Hybrid) for a direct-hire position.
Job Description:
Position: Data Engineer - Centralized Data Science and Analytics (CDSA)
Location: Chicago, IL (Hybrid)
Duration: Direct-hire position
Client: Direct Client
Note: This is a W2 direct-hire role. Looking for candidates who are open to work independently on W2.
Requirements:
· Experience building and optimizing ""big data"" data pipelines, architectures and data sets.
· Working knowledge of message queuing, stream processing and highly scalable ""big data"" data stores.
· Advanced working SQL knowledge and experience working with cloud and relational databases.
· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
· Experience building processes supporting data transformation, data structures, metadata, dependency and workload management.
· A successful history of manipulating, processing and extracting value from large, disconnected datasets.
· Experience using the following software/tools:
· Relational SQL and NoSQL databases, including Postgres.
· Data pipeline and workflow management tools.
· Azure cloud services.
· Object-oriented/object function scripting languages: Python, PySpark Java, C++, R/RStudio/RSpark.
· CI/CD systems.
· Strong understanding across cloud and infrastructure components (server, storage, network, data, and applications) and ability to deliver end to end cloud infrastructure, architectures, and designs.
· Knowledge and implementation of enterprise scale cloud security platforms and tooling.
· Experience with enterprise applications, solutions, and data center infrastructures.
· Bachelor's degree in computer science or similar field; master's degree a plus.
· Exceptional product, project and client management skills.
· Azure, AWS or any other cloud/data engineering certifications are preferred.
Job Type: Full-time
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Big data: 5 years (Required)
Advanced SQL: 5 years (Required)
Cloud: 3 years (Required)
CI/CD: 3 years (Preferred)
NoSQL: 2 years (Required)
Work Location: Hybrid remote in Chicago, IL 60606",85894.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,"['python', 'nosql', 'sql', 'r', 'c', 'java']","['azure', 'aws']",[],[],[],bachelor
TheHive,4.4,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",72800.0,,,,,-1,,MN,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],"['kafka', 'snowflake']",na
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
CapitalTech Solutions,4.5,Remote,Data Engineer,"Job Description:
12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
Complete Description:
Require the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals:
(1) Establish a data governance program,
(2) Perform a comprehensive data gap analysis,
(3) Design a master data architecture,
(4) Create a data warehouse for all data assets,
(5) Develop a front-end for program staff to quickly access workforce information and visualize program status,
(6) Create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities
(7) Foster relations with other agencies and improve inter-agency data integration.
The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff.
Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.
Develop and maintain an understanding of the data landscape including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.
Support the Data Management Project team to develop and maintain data quality controls.
Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.
Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.
Support the data stewards to troubleshoot and resolve data issues.
Support business users to obtain requirements for enhancements and/or new analytic assets.
Assist in the Development of data asset training and documentation.
Participate in the development and implementation of a data standard.
Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.
Job Types: Full-time, Contract
Pay: $66.00 - $74.00 per hour
Schedule:
8 hour shift
Experience:
in SQL, Python, R, JavaScript, JSON: 10 years (Preferred)
Agile Testing, Automation Testing, Black-box Testing: 10 years (Preferred)
Windows and Linux: 10 years (Preferred)
of BI tool architecture, Tableau: 9 years (Required)
Work Location: Remote",145600.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1999,$25 to $100 million (USD),Remote,24,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755.0,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['sql', 'python']",['aws'],[],['gcp'],['airflow'],na
Invictus Data,4.4,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500.0,,,,,-1,,CA,-1,data engineer,senior,"['sql', 'scala', 'python']","['aws', 'azure']",[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
ITExpertUS,2.8,"Chicago, IL",Data Engineer,"Client: Agilisium
Role: Data Engineer
Location: Chicago, IL – Hybrid role – Need to be onsite once/twice in a month.
Type: Long term contract
Job Description
Title: Data Engineer
We are looking for a savvy Data Engineer to join our growing team. The hire will be responsible for expanding and optimizing our data and pipeline architecture and data flow and collection. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. In addition, the Data Engineer will collaborate and support software developers, implementation architecture, and data engineers on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. In addition, they must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
Responsibilities:
Because we work on a programmatic leading edge of many technologies, we need someone who is a creative problem solver, resourceful in getting things done, and can shift productively to or from working independently and collaboratively. This person would also take on the following responsibilities:
Process unstructured data into a form suitable for analysis.
Support the business with ad hoc data analysis and build reliable data pipelines.
Implementation of best practices and IT operations in mission-critical tighter SLA data
pipelines using Airflow
Query Engine Migration from Dremio to Redshift.
We leverage: Multiple AWS Data & Analytic Services(e.g.,Glue, Kinesis, S3) , SQL (e.g.,
PostgreSQL, Redshift, Athena); NoSQL (e.g., DocumentDB, MongoDB); Kafka, Docker, Spark(AWS EMR and DataBricks), Airflow, Dremio, Qubole, etc.
We use AWS extensively, so experience with AWS cloud and AWS Data & Analytics certification will help you hit the ground running.
Business Skills for Data Engineers:
Creative Problem-Solving: Approaching data organization challenges with a clear eye on what is important; employing the suitable approach/methods to make the maximum use of time and human resources.
Effective Collaboration: Carefully listening to management, data engineers, and architects to establish their needs.
Intellectual Curiosity: Exploring new territories and finding creative and pragmatic ways to solve data problems.
Industry Knowledge: Understanding and perspective of how industry functions and how data can be collected, analyzed, and utilized for value creation; maintaining flexibility in the face of big data developments.
Skills and Qualifications:
3 to 5 years of real-world Data Engineering experience.
Programming experience, ideally in Python and other data engineering languages like Scala
Programming knowledge to clean structure and semi-structure datasets.
Experience processing large amounts of structured and unstructured data. Streaming data experience is a plus.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Background in Linux
Build the infrastructure required for optimal extraction, transformation, and loading of data
from various data sources using SQL and other cloud ‘big data’ technologies like DataBricks, Snowflake, Dremio, and Qubole.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience creating a platform on which complex data pipelines are built using orchestration tools like Airflow, and Astronomer.
Experience with real-time sync between OLTP and OLAP using AWS technologies like real- time sync between AWS Aurora and AWS Redshift.
Job Type: Full-time
Salary: Up to $68.00 per hour
Schedule:
8 hour shift
Work Location: On the road
Speak with the employer
+91 6305048577",141440.0,501 to 1000 Employees,Company - Private,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,"['nosql', 'sql', 'scala', 'python']",['aws'],[],"['spark', 'mongodb']","['kafka', 'snowflake', 'airflow']",na
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277.0,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master
Ascent Solutions,4.4,Remote,Hadoop Big Data Engineer,"Hadoop Big Data Engineer
Remote
Must Haves • Strong experience in Big Data, Cloudera Distribution 7.x, Cloud migration, RDBMS • Strong Experience with Amazon EMR/Databricks/Cloudera CDP • 4-5 experience building data pipelines using Hadoop components Sqoop, Hive, Solr, MR, Impala, Spark, Spark SQL., HBase. • 4-5 years of programming experience in Python, Java and Scala is must. • Strong experience with REST API development using Python frameworks (Django, Flask etc.) • Micro Services/Web service development experience using Spring framework is highly desirable.
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'scala', 'java', 'python']",[],[],"['hadoop', 'spark']",['hive'],na
IntelliBridge LLC,3.9,"McLean, VA",Data Engineer,"Title: Data Engineer
Location: Permanent remote role
Clearance: Not required: Start date not contingent on a having or completion of a clearance, however one could be offered upon starting for future programs
Overview:
IntelliBridge is seeking a Data Engineer to collaborate with technical and non-technical data and development team members to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of analytics that provide help ensure national security. You'll be able to gain experience in designing cloud architectures while providing critical support to the client's mission. You will be responsible for designing and building smart data pipelines that are secure, robust, and alerting. You will also create innovative ways to combine disparate data sources and build integrated datasets for advanced analytics.
As a direct employee of IntelliBridge, you would receive a benefit package that includes health/dental/vision insurance coverage, 401K with company match, PTO & paid holidays, and annual tuition/training assistance. For more information, please visit our website.
Responsibilities/Duties:
Build and maintain the infrastructure to support integration, extraction, transformation, and loading (ETL) of data from a wide variety of data sources, such as relational SQL and NoSQL databases, and other platform APIs
Design data pipelines that are robust and secure including pipeline monitoring and alerting mechanisms
Create innovative ways to orchestrate data ETL processes
Guide and support the implementation of new data engineering solutions to enable adoption and growth
Integrate disparate data sources into powerful datasets for advanced analytics
Recommend tools and capabilities based on understanding the current environment and knowledge of various on-premises, cloud based, and hybrid capabilities/technologies
Monitor existing metrics, analyze data, and lead partnership with other Data and Analytics personnel to identify and implement system and process improvements
Develop processes to convert aggregated data from teams, collection tools, and dashboards
Configure and manage data analytic frameworks and pipelines using databases and tools
Develop Python packages to improve application capabilities
Apply distributed systems concepts and principles such as consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms
Administrate cloud computing and CI/CD pipelines to include Amazon Web Service (AWS)
Investigate legacy code to determine areas of improvement and automation
Required Qualifications:
Excellent verbal and written communications
Bachelor’s Degree in a STEM filed or Master’s Degree in Operations Research, Industrial Engineering, Applied Mathematics, Statistics, Physics, Computer Science, or related fields
5+ years of experience with Python, SQL, Unix(Linux), and handling semi-structured data (JSON)
3+ years of experience with Elasticsearch, Logstash, and Kibana (ELK stack)
3+ years of experience with Amazon Web Services (AWS) or other cloud provider
Proficient in Docker
Proficient in Agile Development
Proficient in Git Operations
Experience understanding requirements, analyzing data, discovering opportunities, addressing gaps and communicating them to multiple individuals and stakeholders
Demonstrated expertise in technical data engineering on integrating complex applications, systems, software, and project activities and integrating them into cloud-based resources
General knowledge in machine learning for building efficient and accurate data pipelines that occur for downstream users, such as for data scientists to create the models and analytics that produce insight
Preferred Qualifications:
Organizational skills and a love of documentation
Experienced in Airflow
Experience with demonstrated strength in data lake/warehouse technical architecture, infrastructure components, and ETL/ELT pipelines
Experience with geo-spatial data
Experience with deployments via Kubernetes
Experience with configuring and aggregating logs for data analysis using Splunk or ELK solutions
Experience with developing and managing machine images or templates to automate cloud deployments
About Us:
IntelliBridge delivers IT strategy, cloud, cybersecurity, application, data and analytics, enterprise IT, intelligence analysis, and mission operation support services to accelerate technical performance and efficiency for Defense, Civilian, and National Security & Federal Law Enforcement clients.",92898.0,501 to 1000 Employees,Company - Private,Government & Public Administration,National Agencies,-1,Unknown / Non-Applicable,VA,-1,data engineer,na,"['nosql', 'sql', 'python']",['aws'],[],[],['airflow'],bachelor
MARVEL TECHNOLOGIES INC,3.7,Remote,Data Engineer,"Primary Skills
SCALA, SPARK, SQL, HADOOP, AWS, HIVE, Spark SQL, HIVE QL, CICD (Continuous Integration/Continuous Delivery), VCS (GIT HUB)
Coding in Scala
Designing in of HADOOP ecosystem
Hands-on experience on AWS tools like EMR, EC2
Hands-on experience of SQL in Big Data: SQL, Spark SQL, Hive QL
Proficient in working with large data sets and pipelines
Proficient with workflow scheduling / orchestration tools
Well versed with CICD process and VCS
Databricks is a big PLUS
Job Types: Full-time, Contract
Pay: $50.00 - $58.00 per hour
Schedule:
8 hour shift
Monday to Friday
Experience:
Spark: 4 years (Required)
Scala: 4 years (Required)
Hadoop: 3 years (Required)
Aws: 3 years (Required)
Hive: 3 years (Required)
CI/CD, VCS: 3 years (Required)
Databricks: 1 year (Required)
Work Location: Remote",112320.0,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,$5 to $25 million (USD),Remote,-1,data engineer,na,"['sql', 'scala']",['aws'],[],"['hadoop', 'spark']",['hive'],na
Grid,4.4,"Oakland, CA",Data Engineer,"Today’s financial system is built to favor those with money. Grid’s mission is to level that playing field by building financial products that help users better manage their financial future. The Grid app lets users access cash, build credit, spend money, file their taxes and lots, lots more.

Grid is a fast growing team that’s deeply passionate about making a difference in the lives of millions. We’re solving huge problems and believe that a merit driven culture allows every team member to play a big role. Join our growing team in our Bay Area headquarters or additional offices across the US.

Our Data Engineers are responsible for converting the data surrounding a customer's financial life into tools that make our customers smarter and better equipped financially for the future. We value making intelligent decisions backed by good data and tools. We're looking for people who share our values, particularly if you have experience analyzing, processing, and learning from large data sets.

We’re looking for a seasoned data engineer who can help us lay the foundation of an exceptional data engineering practice. The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions. You will be writing code, setting style guides and collaborating cross-functionally with product, engineering and leadership.
Problems we work on:
Analytics: Collect all the data for a user into tools that help our customers
Machine Learning: Using a variety of techniques to reach better insights
Data Processing: Managing data & statistics using scalable and efficient technologies
Visualization: Envision our data as beautiful graphs and tools that allow customers to explore their data & ask their own questions
Risk: Analyze data for anomalous patterns and build tools that allow us to find bad actors quickly
We Practice:
Open collaboration
Code reviews
Testing
Agile development
We Use:
Go
Python
MySQL
Google Cloud Platform
Kubernetes
Kubeflow
Docker
Google Pubsub
BigQuery
Firebase
We’re looking for Engineers to:
Design and implement platform services, frameworks and ecosystems
Build a scalable, reliable, operable and performant big data workflow platform for data scientists/engineers, AI/ML engineers, and product/operation team members
Drive efficiency and reliability improvements through design and automation: performance, scaling, observability, and monitoring
Requirements:
Strong programming skills with Python
Strong programming skills with a typed programming language, such as Java, Scala, Go, etc.
Disciplined approach to development, testing, and quality assurance
Excellent communication skills, capable of explaining highly technical problems in English
Understand data processing and ETL, hands on experience building pipelines and using frameworks such as Hive hdfs, Presto, Spark, etc.
Really strong candidates may have:
Really strong candidates may have:
Actively contribute to open source software
Worked with a strong, lean-based development environment
Previous work experience in a start-up environment
Ability to recognize the right tool for the right situation/problem.
Will have strong programming skill in Go",112815.0,,,,,-1,,CA,-1,data engineer,na,"['java', 'scala', 'go', 'python']",['google cloud'],[],['spark'],['hive'],na
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537.0,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],[],na
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831.0,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],[],[],na
FlexIT Inc,4.0,"Beaverton, OR",Data Engineer,"FlexIT client is looking for a Data Engineer 12 months contract in Beaverton, Oregon.
Looking for local candidates to work on site.
Top skills: Python, SQL , AWS, Spark",106334.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],na
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755.0,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['sql', 'python']",['aws'],[],['gcp'],['airflow'],na
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100714.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['nosql', 'scala', 'java', 'python']",[],[],"['flink', 'hadoop', 'spark']","['kafka', 'hive']",na
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
Manufacturers Bank,3.2,"Los Angeles, CA",Data Engineer,"GENERAL SUMMARY:
Manufacturers bank is looking for a Data Engineer with strong software engineering backgrounds, a ""platform engineering mindset"" and deep passion for data. This Engineer will help to shape the technical direction of the data domain within the organization. The Engineer will also partner closely with internal business units and external vendors to develop a high performance, reliable and scalable data environment for the bank.

PRINCIPAL DUTIES & RESPONSIBILITIES:
Develop and automate high-performance data processing systems and visualization to ensure reliability and meet critical business requirements.
Design, develop, implement, document and support Data & Analytics systems which adhere to the industry best practices.
Lead data engineering projects, and overall strategy for data governance, security, privacy, quality, and retention.
Triage and resolve issues reported by Risk, Operations, and other business stakeholders.
Mentor junior engineers and continue promoting data engineering and analytics tooling & standards.",115908.0,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),CA,-1,data engineer,na,[],[],[],[],[],na
Grid,4.4,"Oakland, CA",Data Engineer,"Today’s financial system is built to favor those with money. Grid’s mission is to level that playing field by building financial products that help users better manage their financial future. The Grid app lets users access cash, build credit, spend money, file their taxes and lots, lots more.

Grid is a fast growing team that’s deeply passionate about making a difference in the lives of millions. We’re solving huge problems and believe that a merit driven culture allows every team member to play a big role. Join our growing team in our Bay Area headquarters or additional offices across the US.

Our Data Engineers are responsible for converting the data surrounding a customer's financial life into tools that make our customers smarter and better equipped financially for the future. We value making intelligent decisions backed by good data and tools. We're looking for people who share our values, particularly if you have experience analyzing, processing, and learning from large data sets.

We’re looking for a seasoned data engineer who can help us lay the foundation of an exceptional data engineering practice. The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions. You will be writing code, setting style guides and collaborating cross-functionally with product, engineering and leadership.
Problems we work on:
Analytics: Collect all the data for a user into tools that help our customers
Machine Learning: Using a variety of techniques to reach better insights
Data Processing: Managing data & statistics using scalable and efficient technologies
Visualization: Envision our data as beautiful graphs and tools that allow customers to explore their data & ask their own questions
Risk: Analyze data for anomalous patterns and build tools that allow us to find bad actors quickly
We Practice:
Open collaboration
Code reviews
Testing
Agile development
We Use:
Go
Python
MySQL
Google Cloud Platform
Kubernetes
Kubeflow
Docker
Google Pubsub
BigQuery
Firebase
We’re looking for Engineers to:
Design and implement platform services, frameworks and ecosystems
Build a scalable, reliable, operable and performant big data workflow platform for data scientists/engineers, AI/ML engineers, and product/operation team members
Drive efficiency and reliability improvements through design and automation: performance, scaling, observability, and monitoring
Requirements:
Strong programming skills with Python
Strong programming skills with a typed programming language, such as Java, Scala, Go, etc.
Disciplined approach to development, testing, and quality assurance
Excellent communication skills, capable of explaining highly technical problems in English
Understand data processing and ETL, hands on experience building pipelines and using frameworks such as Hive hdfs, Presto, Spark, etc.
Really strong candidates may have:
Really strong candidates may have:
Actively contribute to open source software
Worked with a strong, lean-based development environment
Previous work experience in a start-up environment
Ability to recognize the right tool for the right situation/problem.
Will have strong programming skill in Go",112815.0,,,,,-1,,CA,-1,data engineer,na,"['java', 'scala', 'go', 'python']",['google cloud'],[],['spark'],['hive'],na
Edrstaffing,4.4,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000.0,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],na
Govini,3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.",88151.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable,PA,12,data engineer,na,['sql'],[],[],[],[],master
Airbus Americas,4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.",92046.0,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD),GA,53,data engineer,na,"['r', 'java', 'python']",[],[],['spark'],[],bachelor
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451.0,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['sql', 'python', 'nosql', 'java']","['azure', 'google cloud', 'aws']",[],"['hadoop', 'spark']","['kafka', 'hive']",na
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000.0,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],['azure'],['power bi'],[],[],na
Arthur Grand Technologies Inc,4.8,"Atlanta, GA",AWS Data Engineer,"Role: AWS Data Engineer
Location: Atlanta, GA (Meet customer Monthly once post client selection & onboarding)
JD for AWS Data Engineer
Experience with the core AWS services, plus the specifics mentioned in this job description.
Experience with Docker . Having used ECS , SNS/SQS, Athena, Lambda,S3, RDS and layers in the past is a plus.
Proficiency in at least in Python, Java
Strong notions of security best practices (e.g. using IAM Roles, KMS, etc.).
Experience with monitoring solutions such as CloudWatch, Cloud Trail.
Previous exposure to large-scale systems design.
Knowledge of writing infrastructure as code (IaC) using CloudFormation or Terraform.
Experience with building or maintaining cloud-native applications.
Past experience with the serverless approaches using AWS Lambda is a plus. For example, the Serverless Application Model (AWS SAM).
Job Types: Full-time, Contract
Schedule:
10 hour shift
8 hour shift
Work Location: Remote",94994.0,51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable,GA,11,data engineer,na,"['java', 'python']",['aws'],[],[],[],na
Nursa,4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!",89485.0,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable,UT,5,data engineer,na,"['sql', 'python']",['aws'],[],[],[],na
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851.0,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['azure', 'aws']",[],[],['airflow'],na
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537.0,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],[],na
Globaleur,4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information
Deploy sophisticated analytics programs, machine learning, and statistical methods
Ensure compliance with data governance and security policies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java
Have working experiences in e-commence, travel, marketing domain is a plus
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.",119136.0,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017,Unknown / Non-Applicable,CA,6,data engineer,senior,"['python', 'nosql', 'sql', 'go', 'java']","['aws', 'azure']",['tableau'],['mongodb'],[],bachelor
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277.0,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",['aws'],[],"['hadoop', 'spark']",['airflow'],na
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']",['aws'],[],['spark'],['snowflake'],na
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100714.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['nosql', 'scala', 'java', 'python']",[],[],"['flink', 'hadoop', 'spark']","['kafka', 'hive']",na
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451.0,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['sql', 'python', 'nosql', 'java']","['azure', 'google cloud', 'aws']",[],"['hadoop', 'spark']","['kafka', 'hive']",na
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
Airbus Americas,4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.",92046.0,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD),GA,53,data engineer,na,"['r', 'java', 'python']",[],[],['spark'],[],bachelor
PSRTEK,4.6,"Mount Laurel, NJ",Lead Data Engineer,"AR# 226054
Role: Lead Data Engineer /Databricks (On-site)
Location: Mt. Laurel, NJ
Full-time
Visa status: GC/USC
Must have skills:
Databricks, Python, RDBMS, PowerShell scripting, data warehouse
Detailed JD:
Experience in ETL/Pipeline Development using tools such as Azure Databricks/Apache Spark and Azure
Data Factory with development expertise on batch and real-time data integration
Experience in programming using Python
RDBMS knowledge and experience in writing the Store Procedures
Experience in writing bash and Power shell scripting.
Experience in data ingestion, preparation, integration, and operationalization techniques in optimally addressing the data requirements
Experience in Cloud data warehouse like Azure Synapse, Snowflake analytical warehouse
Experience with Orchestration tools, Azure DevOps, and GitHub
Experience in building end to end architecture for Data Lakes, Data Warehouses and Data Marts
Experience in relational data processing technology like MS SQL, Delta Lake, Spark SQL, SQL Server
Experience to own end-to-end development, including coding, testing, debugging and deployment
Extensive knowledge of ETL and Data Warehousing concepts, strategies, methodologies
Experience working with structured and unstructured data
Familiarity with Azure services like Azure functions, Azure Data Lake Store, Azure Cosmos
Ability to provide solutions that are forward-thinking in data and analytics
Job Type: Full-time
Salary: $120.00 - $130.00 per year
Schedule:
8 hour shift
Experience:
Data Warehouse: 10 years (Required)
Python: 10 years (Required)
PowerShell: 10 years (Required)
Data Bricks: 10 years (Required)
RDBMS: 10 years (Required)
Work Location: On the road
Speak with the employer
+91 609-917-9952",121211.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,NJ,-1,data engineer,senior,"['sql', 'python']",['azure'],[],['spark'],['snowflake'],na
"Pomeroy Technologies, LLC.",3.0,"Cleveland, OH",Data Engineer,"The Data Engineer is responsible for the development, integration, and implementation of ETL Feeds in support of various modern architecture projects. This role requires hands-on knowledge of modern Integrations Platform solutions, file, and database ETL, and XML transformation. The incumbent will act as a technical resource, participating in the successful design, development, unit testing, implementation, and support of new and existing ETL integrations.
Essential Functions
Must be able to acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design.
Working with manager, gathering business requirements for the development of new applications.
Perform programming assignments based on established standards, methods, and best practices.
Develop ETL solutions as assigned in supporting new or updated business solutions and requirements.
Help business users develop functional requirements for integration.
Provide support during user acceptance testing.
Prepare documentation for code changes.
Code ETL feeds, and conduct tests to verify correct program functionality, reliability, and data integrity.
Maintain and modify ETL feeds; make approved changes by amending application documentation, developing detailed programming logic, and coding changes.
Provide support and resolution for application problems and issues.
Provide user and system documentation for operational and technical support.
Assist in the discovery and investigation of production problems as required.
Support ongoing daily use of ETL environments and Integrations platforms.
Provide on-call support for critical application problems and issues.
Analyze program performance and take action to correct deficiencies based on consultation with clients and approval of supervisor.
required.Work outside the standard office 7.5 hour workday may be
Experience Requirements
2+ years of IT experience.
1+ years of experience building ETL feeds using any ETL tool (Talend, Informatica, etc.)
Knowledge of the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment, and post-production support.
Understanding of XML, JSON, XSD, WSDL, REST, and SOAP APIs.
Experience in developing SQL programs.
Job Type: Full-time
Pay: From $80,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Work Location: Hybrid remote in Cleveland, OH 44115",80000.0,1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable,OH,41,data engineer,na,['sql'],[],[],[],[],na
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755.0,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['sql', 'python']",['aws'],[],['gcp'],['airflow'],na
ITExpertUS,2.8,"Chicago, IL",Data Engineer,"Client: Agilisium
Role: Data Engineer
Location: Chicago, IL – Hybrid role – Need to be onsite once/twice in a month.
Type: Long term contract
Job Description
Title: Data Engineer
We are looking for a savvy Data Engineer to join our growing team. The hire will be responsible for expanding and optimizing our data and pipeline architecture and data flow and collection. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. In addition, the Data Engineer will collaborate and support software developers, implementation architecture, and data engineers on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. In addition, they must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
Responsibilities:
Because we work on a programmatic leading edge of many technologies, we need someone who is a creative problem solver, resourceful in getting things done, and can shift productively to or from working independently and collaboratively. This person would also take on the following responsibilities:
Process unstructured data into a form suitable for analysis.
Support the business with ad hoc data analysis and build reliable data pipelines.
Implementation of best practices and IT operations in mission-critical tighter SLA data
pipelines using Airflow
Query Engine Migration from Dremio to Redshift.
We leverage: Multiple AWS Data & Analytic Services(e.g.,Glue, Kinesis, S3) , SQL (e.g.,
PostgreSQL, Redshift, Athena); NoSQL (e.g., DocumentDB, MongoDB); Kafka, Docker, Spark(AWS EMR and DataBricks), Airflow, Dremio, Qubole, etc.
We use AWS extensively, so experience with AWS cloud and AWS Data & Analytics certification will help you hit the ground running.
Business Skills for Data Engineers:
Creative Problem-Solving: Approaching data organization challenges with a clear eye on what is important; employing the suitable approach/methods to make the maximum use of time and human resources.
Effective Collaboration: Carefully listening to management, data engineers, and architects to establish their needs.
Intellectual Curiosity: Exploring new territories and finding creative and pragmatic ways to solve data problems.
Industry Knowledge: Understanding and perspective of how industry functions and how data can be collected, analyzed, and utilized for value creation; maintaining flexibility in the face of big data developments.
Skills and Qualifications:
3 to 5 years of real-world Data Engineering experience.
Programming experience, ideally in Python and other data engineering languages like Scala
Programming knowledge to clean structure and semi-structure datasets.
Experience processing large amounts of structured and unstructured data. Streaming data experience is a plus.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Background in Linux
Build the infrastructure required for optimal extraction, transformation, and loading of data
from various data sources using SQL and other cloud ‘big data’ technologies like DataBricks, Snowflake, Dremio, and Qubole.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience creating a platform on which complex data pipelines are built using orchestration tools like Airflow, and Astronomer.
Experience with real-time sync between OLTP and OLAP using AWS technologies like real- time sync between AWS Aurora and AWS Redshift.
Job Type: Full-time
Salary: Up to $68.00 per hour
Schedule:
8 hour shift
Work Location: On the road
Speak with the employer
+91 6305048577",141440.0,501 to 1000 Employees,Company - Private,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,"['nosql', 'sql', 'scala', 'python']",['aws'],[],"['spark', 'mongodb']","['kafka', 'snowflake', 'airflow']",na
TY Software,4.4,"Dallas, TX",Data Engineer,"Job Title Data Engineer,
Location: Dallas, TX
Type of work- Onsite , C2C
Job Description
Experience 3-5 years
At least 3+ years of enterprise experience in working with data bricks and highly proficient in SQL, Spark, Scala/Python.
Skilled in Big Data Technologies like Spark, Spark SQL, PySpark
Experience with one or more of the major cloud platforms & cloud services such as - Azure/AWS/GCP, Databricks
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Strong analytic skills related to working with unstructured datasets
Working knowledge of highly scalable ‘big data’ data stores
A successful history of manipulating, processing and extracting value from large, disconnected datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Experience developing enterprise software products
Experience with at least one of these object-oriented/object function scripting languages: PySpark/Python, Scala, Java
Build monitoring and automated testing to ensure data consistency and availability
Experience supporting and working with cross-functional teams in a dynamic environment
Experience working in an AGILE environment
Job Types: Full-time, Contract
Salary: $42.15 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75201: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",106246.4,,,,,-1,,TX,-1,data engineer,na,"['sql', 'scala', 'java', 'python']","['azure', 'aws']",[],"['gcp', 'spark']",[],na
Nursa,4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!",89485.0,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable,UT,5,data engineer,na,"['sql', 'python']",['aws'],[],[],[],na
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000.0,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],['azure'],['power bi'],[],[],na
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851.0,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['azure', 'aws']",[],[],['airflow'],na
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537.0,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],[],na
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",[],[],[],[],na
Globaleur,4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information
Deploy sophisticated analytics programs, machine learning, and statistical methods
Ensure compliance with data governance and security policies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java
Have working experiences in e-commence, travel, marketing domain is a plus
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.",119136.0,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017,Unknown / Non-Applicable,CA,6,data engineer,senior,"['python', 'nosql', 'sql', 'go', 'java']","['aws', 'azure']",['tableau'],['mongodb'],[],bachelor
Sky Consulting Inc,4.4,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.",103312.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,['python'],[],[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
BCVS Group INC,5.0,"Oakland, CA",Data engineer lead,"Role: Data Engineer Lead
Location: Oakland, CA
Description
Qualitest is looking for a Data Architect to join our team.
Must Have-
Seasoned data professional with experience implementing solutions across Data Quality, Data Assurance,
Data Governance and Test Data Management
Ability to assess current state, recommend solutions and build a rollout strategy
Analyze existing data landscape, business processes and create knowledge articles (architectures, data flows, tools)Design, Build and Deliver quality gates across data lifecycle for automated and continuous data quality assurance.
Integrate data testing solutions with Automation, CI/CD Pipelines for Continuous Testing
Working experience on various database technologies such as Oracle, SQL Server, MongoDB, Snowflake, Teradata
Good understanding about QA Test Management, Defect Management and Testing methodologies. Evaluate tools, build proof of concepts, and implement NextGen solutions
Work with customer executives and cross functional teams for seamless project implementation and delivery
Mentor onshore/offshore teams to deliver projects in time and assist in case of delays/clarifications
Experience working on data quality validation tools such as RightData, Icedq, Informatica, Tosca DI, Datagaps
Experience working on TDM tools such as Informatica TDM , IBM Optim, Delphix, CA TDM/Broadcom TDM
Proficient in AWS Cloud Services such as Glue, Lambda, RedShift, Dynamo DB and Databricks
Experience working on Data Warehouse, Data Lake, Big Data platforms
Requirements
Good to Have-
Knowledge about Enterprise Applications such as Salesforce, SAP, Veeva CRM
AI/ML knowledge on building Intelligent data validations and eliminate data anomalies
Job Types: Full-time, Contract
Salary: From $41.80 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Oakland, CA 94619: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 +12019071788",86944.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,['sql'],['aws'],[],['mongodb'],['snowflake'],na
ASCENDING,4.2,"Rockville, MD",Data Engineer,"Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.
This role is only available for W2 or individual contracts. Please no C2C.
100% Remote Work.

Responsibilities:
Analyze system requirements and design responsive algorithms and solutions.
Use big data and cloud technologies to produce production quality code.
Engage in performance tuning and scalability engineering.
Work with team, peers and management to identify objectives and set priorities.
Perform related SDLC engineering activities like sprint planning and estimation.
Work effectively in small agile teams.
Provide creative solutions to problems.
Identify opportunities for improvement and execute.

Requirements:
Minimum 5 years of proven professional experience working in the IT industry.
Degree in Computer Science or related domains.
Experience with cloud based Big Data technologies.
Experience with big data technologies like Hadoop, Spark and Hive.
AWS experience is a big plus.
Proficiency in Hive / Spark SQL / SQL. Experience with Spark.
Experience with one or more programming languages like Scala & Python & Java.
Ability to push the frontier of technology and independently pursue better alternatives.
Kubernetes or AWS EKS experience will be a plus.

Thanks for applying!
U3GJMKlbkr",96611.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,"['sql', 'scala', 'java', 'python']",['aws'],[],"['hadoop', 'spark']",['hive'],na
Edrstaffing,4.4,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000.0,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],na
Savvy Technology Solutions,4.4,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location",107339.0,,,,,-1,,DC,-1,data engineer,senior,"['nosql', 'sql']",['aws'],['tableau'],[],[],master
Govini,3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.",88151.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable,PA,12,data engineer,na,['sql'],[],[],[],[],master
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100714.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['nosql', 'scala', 'java', 'python']",[],[],"['flink', 'hadoop', 'spark']","['kafka', 'hive']",na
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
Invictus Data,4.4,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500.0,,,,,-1,,CA,-1,data engineer,senior,"['sql', 'scala', 'python']","['aws', 'azure']",[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451.0,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['sql', 'python', 'nosql', 'java']","['azure', 'google cloud', 'aws']",[],"['hadoop', 'spark']","['kafka', 'hive']",na
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000.0,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],['azure'],['power bi'],[],[],na
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851.0,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['azure', 'aws']",[],[],['airflow'],na
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",['aws'],[],"['hadoop', 'spark']",['airflow'],na
Manufacturers Bank,3.2,"Los Angeles, CA",Data Engineer,"GENERAL SUMMARY:
Manufacturers bank is looking for a Data Engineer with strong software engineering backgrounds, a ""platform engineering mindset"" and deep passion for data. This Engineer will help to shape the technical direction of the data domain within the organization. The Engineer will also partner closely with internal business units and external vendors to develop a high performance, reliable and scalable data environment for the bank.

PRINCIPAL DUTIES & RESPONSIBILITIES:
Develop and automate high-performance data processing systems and visualization to ensure reliability and meet critical business requirements.
Design, develop, implement, document and support Data & Analytics systems which adhere to the industry best practices.
Lead data engineering projects, and overall strategy for data governance, security, privacy, quality, and retention.
Triage and resolve issues reported by Risk, Operations, and other business stakeholders.
Mentor junior engineers and continue promoting data engineering and analytics tooling & standards.",115908.0,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),CA,-1,data engineer,na,[],[],[],[],[],na
Nursa,4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!",89485.0,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable,UT,5,data engineer,na,"['sql', 'python']",['aws'],[],[],[],na
"Pomeroy Technologies, LLC.",3.0,"Cleveland, OH",Data Engineer,"The Data Engineer is responsible for the development, integration, and implementation of ETL Feeds in support of various modern architecture projects. This role requires hands-on knowledge of modern Integrations Platform solutions, file, and database ETL, and XML transformation. The incumbent will act as a technical resource, participating in the successful design, development, unit testing, implementation, and support of new and existing ETL integrations.
Essential Functions
Must be able to acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design.
Working with manager, gathering business requirements for the development of new applications.
Perform programming assignments based on established standards, methods, and best practices.
Develop ETL solutions as assigned in supporting new or updated business solutions and requirements.
Help business users develop functional requirements for integration.
Provide support during user acceptance testing.
Prepare documentation for code changes.
Code ETL feeds, and conduct tests to verify correct program functionality, reliability, and data integrity.
Maintain and modify ETL feeds; make approved changes by amending application documentation, developing detailed programming logic, and coding changes.
Provide support and resolution for application problems and issues.
Provide user and system documentation for operational and technical support.
Assist in the discovery and investigation of production problems as required.
Support ongoing daily use of ETL environments and Integrations platforms.
Provide on-call support for critical application problems and issues.
Analyze program performance and take action to correct deficiencies based on consultation with clients and approval of supervisor.
required.Work outside the standard office 7.5 hour workday may be
Experience Requirements
2+ years of IT experience.
1+ years of experience building ETL feeds using any ETL tool (Talend, Informatica, etc.)
Knowledge of the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment, and post-production support.
Understanding of XML, JSON, XSD, WSDL, REST, and SOAP APIs.
Experience in developing SQL programs.
Job Type: Full-time
Pay: From $80,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Work Location: Hybrid remote in Cleveland, OH 44115",80000.0,1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable,OH,41,data engineer,na,['sql'],[],[],[],[],na
Airbus Americas,4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.",92046.0,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD),GA,53,data engineer,na,"['r', 'java', 'python']",[],[],['spark'],[],bachelor
Infinity Quest,4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote",105128.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable,NJ,17,data engineer,na,"['sql', 'python']",['azure'],[],[],[],na
Business Integra Inc,3.5,"San Francisco, CA",Data Engineer,"Position can be 100% remote but preferred to have candidates who can periodically (2 x month) work at headquarters.
(Data Engineer)
Job Description:
Assigned Personnel to provide data analytic support to the Data Analytics/Data Integration Project for Judicial Branch Statistical Information System (JBSIS) data reporting.
This position will perform high level data engineering and data analytics on a variety of agency data sources, but primarily on the Judicial Branch Statistical Information System (JBSIS).
Partnering with IT staff, this position will reengineer JBSIS to create new technical documentation for JBSIS; create mappings for the Court Statistics Report and other JBSIS products, make policy recommendations, create and/or implement new governance standards, enhance data auditing and data quality controls, and create data visualizations.
These same tasks may be performed with additional agency datasets.
Specific Skills/Qualifications Required
Technical project management and documentation skills.
Ability to analyze issues from system documentation and recommend solutions.
Experience managing technical projects, including conflict resolution, issue escalations, status reporting and resource management.
Experience creating and executing data mappings and scripts to clean, compile and analyze data
Ability to assess and maintain data pipeline, data quality in the database, and address data reporting issues.
Experience developing and implementing testing protocols for data and system quality
Experience in R and Stata.
Experience with data visualization and software such as Tableau and Power BI.
Excellent oral, written, analytical and communication skills with the ability to lead a technical discussion to both technical and non-technical staff.
Excellent analytical, verbal and conflict resolution skills.
Additional Skills/Qualifications Desired:
General:
Understanding of courtroom operations and workflow.
Experience in government (State) setting
Excellent presentation skills for both technical and non-technical audiences, including creating and presenting executive summaries to management and technical committees.
Technical:
Exposure and experience with Cloud computing.
Conceptual understanding of Amazon Web Services, Microsoft Azure, Google Cloud, IBM and Oracle Cloud Platforms.
Prior experience using Snowflake
Experience using Python or other database query languages.
Job Types: Full-time, Contract
Pay: $112,604.02 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Vision insurance
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco, CA 94102: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data science: 9 years (Required)
Work Location: Hybrid remote in San Francisco, CA 94102",131302.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2001,$25 to $100 million (USD),CA,22,data engineer,na,"['r', 'python']","['azure', 'google cloud']","['power bi', 'tableau']",[],['snowflake'],na
Softcrylic,4.0,Remote,Senior Data Engineer,"Who We Are
For more than 20 years, we have been working with organizations large and small to help solve business challenges through technology. We bring a unique combination of engineering and strategy to Make Data Work for organizations.
Our clients range from the travel and leisure industry to publishing, retail and banking. The common thread between our clients is their commitment to making data work as seen through their investment in those efforts.
In our quest to solve data challenges for our clients, we work with large enterprise, cloud based and marketing technology suites. We have a deep understanding of these solutions so we can help our clients make the most of their investment in an efficient way to have a data driven business.
Why Work at Softcrylic?
Softcrylic provides an engaging, team-focused, and rewarding work environment where people are excited about the work they do and passionate about delivering creative solutions to our clients.
We are looking to add a Senior Data Engineer to our team! This is a 100% Remote role and preference will be given to candidates from Atlanta, NJ or Texas regions.
Job Description:
Softcrylic is looking for a Senior Data Engineerwith strong design, development, and team leadership skills. The person should be working with Clients / Customer and with our internal (onshore and offshore) members to design, develop and rollout data projects. The person to be hands on and have strong leadership/communication and interpersonal skills.
Requirement:
· 5 to 7 years of experience in working as a Data Engineer.
· Strong experience in Python.
· Experience in working on GCP.
· Good experience in Airflow.
· Should have good experience in ETL pipeline design and development.
· Very good experience in SQL
· Experience in working on Redshift.
· Excellent designing and documentation (diagrams) and presentation skills.
· Data Quality Concepts are must have.
· Must know design and development of any of industry leading graph databases.
· Good communication skills.
· Independent thinker, good team player with Data Engineering Design skills.
· Work with minimum guidelines.
Plus:
GCP - Big Query
Agile background
Microsoft Power BI
Graph Database
Job Types: Full-time, Contract
Pay: $130,000.00 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Compensation package:
Performance bonus
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
GCP: 3 years (Preferred)
Python: 4 years (Preferred)
Work Location: Remote
Speak with the employer
+91 609.241.9641",135000.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$5 to $25 million (USD),Remote,23,data engineer,senior,"['sql', 'python']",[],['power bi'],['gcp'],['airflow'],na
"MOBE, LLC",3.7,"Minneapolis, MN",Data Engineer (ETL),"MOBE
MOBE guides people to better health and more happiness. Behind our innovative health solutions is uniquely human philosophy. We believe that person-to-person connections and understanding can make a difference in a world where self-care can be complicated, and health care is ever-evolving and complex.
MOBE works with health plans and large employers to identify individuals who are frequent users of health care but aren?t finding resolutions for their underlying health issues. We use a whole-person approach and guidance to impact health outcomes positively.
Supporting people is at the core of our business, employees included. MOBE is a high-growth organization with a culture built on trust and collaboration. Consistent across our teams and offerings is a belief in the power of people doing good together. We genuinely care about people and consider our workforce the most significant asset.
Your Role at MOBE
This is an exciting time at MOBE and we are growing fast. At MOBE, we have a lot of data: eligibility, medical and pharmacy claims, marketing campaign impressions, transcripts from participant interactions, etc.
This position is responsible for providing technical and project expertise to enable MOBE analytics and operations with structured and unstructured data. Responsibilities includes executing and/or leading user story development, data design and architecture, data pipeline development, testing and deployment in the Analytic Data Framework. This role will partner with internal and external business and technology teams to drive project deliverables and ensure high quality delivery of data architecture and integration.
Responsibilities
The Data Engineer ensures the following capabilities and functions:
Translate high level business processes into logical data processing steps
Design data structures and pipelines that are flexible and scalable for MOBE analytics and operational requirements
Support Analytic partners through collaborative and transparent development, information delivery, problem resolution, shared insights, and training
Data processing definition, execution, and documentation, in a time appropriate way, to meet business priorities and requirements
Data quality and maintenance consistent within the Analytic Data Framework
Lead small to moderate sized projects and initiatives, following through on execution of chosen strategies and demonstrating the ability to work through obstacles and changing priorities.
Demonstrate ability and willingness to play multiple roles for different projects (e.g. planning/architecture, project development, hands-on technical resource/support for others, analysis and resolution of data issues)
Identify and constructively communicate the need for improvements or enhancements in MOBE technology assets
All other duties as assigned to help fulfill our Mission and abide by MOBE?s Guiding Principles",97357.0,51 to 200 Employees,Company - Private,Personal Consumer Services,Beauty & Wellness,2014,Unknown / Non-Applicable,MN,9,data engineer,na,[],[],[],[],[],na
Sky Consulting Inc,4.4,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.",103312.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,['python'],[],[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
Savvy Technology Solutions,4.4,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location",107339.0,,,,,-1,,DC,-1,data engineer,senior,"['nosql', 'sql']",['aws'],['tableau'],[],[],master
Globaleur,4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information
Deploy sophisticated analytics programs, machine learning, and statistical methods
Ensure compliance with data governance and security policies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java
Have working experiences in e-commence, travel, marketing domain is a plus
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.",119136.0,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017,Unknown / Non-Applicable,CA,6,data engineer,senior,"['python', 'nosql', 'sql', 'go', 'java']","['aws', 'azure']",['tableau'],['mongodb'],[],bachelor
BCVS Group INC,5.0,"Oakland, CA",Data engineer lead,"Role: Data Engineer Lead
Location: Oakland, CA
Description
Qualitest is looking for a Data Architect to join our team.
Must Have-
Seasoned data professional with experience implementing solutions across Data Quality, Data Assurance,
Data Governance and Test Data Management
Ability to assess current state, recommend solutions and build a rollout strategy
Analyze existing data landscape, business processes and create knowledge articles (architectures, data flows, tools)Design, Build and Deliver quality gates across data lifecycle for automated and continuous data quality assurance.
Integrate data testing solutions with Automation, CI/CD Pipelines for Continuous Testing
Working experience on various database technologies such as Oracle, SQL Server, MongoDB, Snowflake, Teradata
Good understanding about QA Test Management, Defect Management and Testing methodologies. Evaluate tools, build proof of concepts, and implement NextGen solutions
Work with customer executives and cross functional teams for seamless project implementation and delivery
Mentor onshore/offshore teams to deliver projects in time and assist in case of delays/clarifications
Experience working on data quality validation tools such as RightData, Icedq, Informatica, Tosca DI, Datagaps
Experience working on TDM tools such as Informatica TDM , IBM Optim, Delphix, CA TDM/Broadcom TDM
Proficient in AWS Cloud Services such as Glue, Lambda, RedShift, Dynamo DB and Databricks
Experience working on Data Warehouse, Data Lake, Big Data platforms
Requirements
Good to Have-
Knowledge about Enterprise Applications such as Salesforce, SAP, Veeva CRM
AI/ML knowledge on building Intelligent data validations and eliminate data anomalies
Job Types: Full-time, Contract
Salary: From $41.80 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Oakland, CA 94619: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 +12019071788",86944.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,['sql'],['aws'],[],['mongodb'],['snowflake'],na
Boston Globe Media Partners,4.1,"Boston, MA",Data Engineer,"Boston Globe Media is New England's largest newsgathering organization - and much more. We are committed to being an indispensable, trusted, reliable source of round-the-clock information. Through the powerful journalism from our newsroom, engaging content from our content marketing studio, or through targeted advertising solutions, brands and marketers rely on us to reach highly engaged, educated and influential audiences through a variety of media and experiences.
Responsibilities:
Collect, organize, and document often-used data resources (maps, APIs, etc).
Create scripts to scrape data from websites for stakeholders.
With guidance, start creation of a data style guide.
Technology:
Basic knowledge of HTML, CSS, and JavaScript.
Basic familiarity with PHP, Groovy, or another server side scripting language.
Basic familiarity of build tools such as Grunt, Gulp, or Webpack.
Basic familiarity with version control systems such as SVN or Git.
Qualifications:
Understands and follows the team’s agile process.
Adheres to defined coding standards.
Participates in code reviews.
A willingness to adapt and be audience focused, with a curious mindset and a commitment to creating an inclusive work environment
Vaccination Statement:
We require that all BGMP employees (including temporary employees, co-ops, interns, and independent contractors) be vaccinated from COVID-19, unless an exemption from this policy has been granted as an accommodation or otherwise. All BGMP employees, regardless of vaccination status or work location, must provide proof of vaccination status as instructed by the employee's designated Human Resources contact. Employees may request a reasonable accommodation or other exemption from this policy by contacting their designated Human Resources contact. Failure to comply with or enforce any part of this policy, or misrepresentation of compliance with this policy, may result in discipline, up to and including termination of employment, subject to reasonable accommodation and other requirements of applicable federal, state, and local law.
EEO Statement:
Boston Globe Media Partners is an equal employment opportunity employer, and does not discriminate on the basis of race, color, religion, gender, sexual orientation, gender identity or expression, age, disability, national origin, ancestry, genetic information, military or veteran status, pregnancy or pregnancy-related condition or any other protected characteristic. Boston Globe Media Partners is committed to diversity in its most inclusive sense.
wcZyZ7QvrB",110394.0,1001 to 5000 Employees,Company - Private,Media & Communication,Publishing,-1,$100 to $500 million (USD),MA,-1,data engineer,na,[],[],[],[],[],na
NIVID Technologies,3.7,Remote,Sr. Data Engineer,"We hope this Job meets your skills and expectations. If you are available and interested, please contact me at your earliest convenience. You will be working with a highly skilled team of IT Professionals in a high pace corporate environment. This opportunity will move quickly and candidates will be interviewed in the order they apply. Job Description/ Required Skills: (i) Strong hands-on programming experience in Python (ii) Hands-on experience of API development (from application / software engineering perspective) (iii) AWS Lambda and data streaming ingestion (Kinesis) (iv) AWS tech stack from data engineering stand-point
Job Types: Full-time, Contract, Permanent
Salary: $39.76 - $86.23 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",131040.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$5 to $10 billion (USD),Remote,11,data engineer,senior,"['sql', 'python']",['aws'],[],[],[],na
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100714.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['nosql', 'scala', 'java', 'python']",[],[],"['flink', 'hadoop', 'spark']","['kafka', 'hive']",na
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
"Fiserv, Inc.",3.2,"Bridgewater, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451.0,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['sql', 'python', 'nosql', 'java']","['azure', 'google cloud', 'aws']",[],"['hadoop', 'spark']","['kafka', 'hive']",na
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",['aws'],[],"['hadoop', 'spark']",['airflow'],na
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
Sky Consulting Inc,4.4,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.",103312.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,['python'],[],[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
Plaxonic Technologies,4.6,"New York, NY",GCP Data Engineer,"Bachelor’s Degree in Computer Science or a related discipline
5+ years of applicable engineering experience
Strong proficiency in Python with an emphasis in building data pipelines
Ability to write complex SQL to perform common types of analysis and aggregations
Experience with Apache Airflow or Google Composer
Detail-oriented and document all the work
Ability to work with others from diverse skill-sets and backgrounds
GCP solution architect - certified
Experience in GCP, Big Query
Working experience in Databricks, Spark is expected
Job Types: Full-time, Contract
Benefits:
401(k)
Health insurance
Paid time off
Schedule:
8 hour shift
Work Location: One location
Speak with the employer
+91 (727) 216-7989",117952.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),NY,10,data engineer,na,"['sql', 'python']",[],[],"['gcp', 'spark']",['airflow'],bachelor
Softcrylic,4.0,Remote,Senior Data Engineer,"Who We Are
For more than 20 years, we have been working with organizations large and small to help solve business challenges through technology. We bring a unique combination of engineering and strategy to Make Data Work for organizations.
Our clients range from the travel and leisure industry to publishing, retail and banking. The common thread between our clients is their commitment to making data work as seen through their investment in those efforts.
In our quest to solve data challenges for our clients, we work with large enterprise, cloud based and marketing technology suites. We have a deep understanding of these solutions so we can help our clients make the most of their investment in an efficient way to have a data driven business.
Why Work at Softcrylic?
Softcrylic provides an engaging, team-focused, and rewarding work environment where people are excited about the work they do and passionate about delivering creative solutions to our clients.
We are looking to add a Senior Data Engineer to our team! This is a 100% Remote role and preference will be given to candidates from Atlanta, NJ or Texas regions.
Job Description:
Softcrylic is looking for a Senior Data Engineerwith strong design, development, and team leadership skills. The person should be working with Clients / Customer and with our internal (onshore and offshore) members to design, develop and rollout data projects. The person to be hands on and have strong leadership/communication and interpersonal skills.
Requirement:
· 5 to 7 years of experience in working as a Data Engineer.
· Strong experience in Python.
· Experience in working on GCP.
· Good experience in Airflow.
· Should have good experience in ETL pipeline design and development.
· Very good experience in SQL
· Experience in working on Redshift.
· Excellent designing and documentation (diagrams) and presentation skills.
· Data Quality Concepts are must have.
· Must know design and development of any of industry leading graph databases.
· Good communication skills.
· Independent thinker, good team player with Data Engineering Design skills.
· Work with minimum guidelines.
Plus:
GCP - Big Query
Agile background
Microsoft Power BI
Graph Database
Job Types: Full-time, Contract
Pay: $130,000.00 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Compensation package:
Performance bonus
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
GCP: 3 years (Preferred)
Python: 4 years (Preferred)
Work Location: Remote
Speak with the employer
+91 609.241.9641",135000.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$5 to $25 million (USD),Remote,23,data engineer,senior,"['sql', 'python']",[],['power bi'],['gcp'],['airflow'],na
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851.0,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['azure', 'aws']",[],[],['airflow'],na
Infinity Quest,4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote",105128.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable,NJ,17,data engineer,na,"['sql', 'python']",['azure'],[],[],[],na
NIVID Technologies,3.7,Remote,Sr. Data Engineer,"We hope this Job meets your skills and expectations. If you are available and interested, please contact me at your earliest convenience. You will be working with a highly skilled team of IT Professionals in a high pace corporate environment. This opportunity will move quickly and candidates will be interviewed in the order they apply. Job Description/ Required Skills: (i) Strong hands-on programming experience in Python (ii) Hands-on experience of API development (from application / software engineering perspective) (iii) AWS Lambda and data streaming ingestion (Kinesis) (iv) AWS tech stack from data engineering stand-point
Job Types: Full-time, Contract, Permanent
Salary: $39.76 - $86.23 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",131040.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$5 to $10 billion (USD),Remote,11,data engineer,senior,"['sql', 'python']",['aws'],[],[],[],na
"Pomeroy Technologies, LLC.",3.0,"Cleveland, OH",Data Engineer,"The Data Engineer is responsible for the development, integration, and implementation of ETL Feeds in support of various modern architecture projects. This role requires hands-on knowledge of modern Integrations Platform solutions, file, and database ETL, and XML transformation. The incumbent will act as a technical resource, participating in the successful design, development, unit testing, implementation, and support of new and existing ETL integrations.
Essential Functions
Must be able to acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design.
Working with manager, gathering business requirements for the development of new applications.
Perform programming assignments based on established standards, methods, and best practices.
Develop ETL solutions as assigned in supporting new or updated business solutions and requirements.
Help business users develop functional requirements for integration.
Provide support during user acceptance testing.
Prepare documentation for code changes.
Code ETL feeds, and conduct tests to verify correct program functionality, reliability, and data integrity.
Maintain and modify ETL feeds; make approved changes by amending application documentation, developing detailed programming logic, and coding changes.
Provide support and resolution for application problems and issues.
Provide user and system documentation for operational and technical support.
Assist in the discovery and investigation of production problems as required.
Support ongoing daily use of ETL environments and Integrations platforms.
Provide on-call support for critical application problems and issues.
Analyze program performance and take action to correct deficiencies based on consultation with clients and approval of supervisor.
required.Work outside the standard office 7.5 hour workday may be
Experience Requirements
2+ years of IT experience.
1+ years of experience building ETL feeds using any ETL tool (Talend, Informatica, etc.)
Knowledge of the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment, and post-production support.
Understanding of XML, JSON, XSD, WSDL, REST, and SOAP APIs.
Experience in developing SQL programs.
Job Type: Full-time
Pay: From $80,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Work Location: Hybrid remote in Cleveland, OH 44115",80000.0,1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable,OH,41,data engineer,na,['sql'],[],[],[],[],na
BCVS Group INC,5.0,"Oakland, CA",Data engineer lead,"Role: Data Engineer Lead
Location: Oakland, CA
Description
Qualitest is looking for a Data Architect to join our team.
Must Have-
Seasoned data professional with experience implementing solutions across Data Quality, Data Assurance,
Data Governance and Test Data Management
Ability to assess current state, recommend solutions and build a rollout strategy
Analyze existing data landscape, business processes and create knowledge articles (architectures, data flows, tools)Design, Build and Deliver quality gates across data lifecycle for automated and continuous data quality assurance.
Integrate data testing solutions with Automation, CI/CD Pipelines for Continuous Testing
Working experience on various database technologies such as Oracle, SQL Server, MongoDB, Snowflake, Teradata
Good understanding about QA Test Management, Defect Management and Testing methodologies. Evaluate tools, build proof of concepts, and implement NextGen solutions
Work with customer executives and cross functional teams for seamless project implementation and delivery
Mentor onshore/offshore teams to deliver projects in time and assist in case of delays/clarifications
Experience working on data quality validation tools such as RightData, Icedq, Informatica, Tosca DI, Datagaps
Experience working on TDM tools such as Informatica TDM , IBM Optim, Delphix, CA TDM/Broadcom TDM
Proficient in AWS Cloud Services such as Glue, Lambda, RedShift, Dynamo DB and Databricks
Experience working on Data Warehouse, Data Lake, Big Data platforms
Requirements
Good to Have-
Knowledge about Enterprise Applications such as Salesforce, SAP, Veeva CRM
AI/ML knowledge on building Intelligent data validations and eliminate data anomalies
Job Types: Full-time, Contract
Salary: From $41.80 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Oakland, CA 94619: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 +12019071788",86944.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,['sql'],['aws'],[],['mongodb'],['snowflake'],na
PepsiCo,4.0,"Plano, TX",Azure Data Engineer,"As a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems.
Responsibilities:
Active contributor to code development in projects and services.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.
Responsible for implementing best practices around systems integration, security, performance and data management.
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.
Develop and optimize procedures to “productionalize” data science models.
Define and manage SLA’s for data products and processes running in production.
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale.
Research in state-of-the-art methodologies.
Create documentation for learnings and knowledge transfer.
Create and audit reusable packages or libraries.
Requirements:
2+ years of overall technology experience that includes at least 2+ years of hands-on software development, data engineering, and systems architecture.
2+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
1+ years in cloud data engineering experience in Azure Certification is a plus.
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.
Experience with version control systems like Github and deployment & CI tools.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus
Understanding of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI)
Covid-19 vaccination may be a condition of employment dependent on role and location. For specific information, please discuss role requirements with the recruiter
Education
BA/BS in Computer Science, Math, Physics, or other technical fields
Skills, Abilities, Knowledge
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.
Proven track record of leading, mentoring data teams.
Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.
Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.
Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.
Foster a team culture of accountability, communication, and self-management.
Proactively drives impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals
Ability to lead others without direct authority in a matrixed environment.
Job Type: Full-time
Pay: $85,000.00 - $90,000.00 per year
Schedule:
Monday to Friday
Work Location: Hybrid remote in Plano, TX 75024",87500.0,10000+ Employees,Company - Public,Manufacturing,Food & Beverage Manufacturing,1965,$10+ billion (USD),TX,58,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],[],[],na
Ascendion,4.5,Remote,Senior Data Engineer,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote",135200.0,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable,Remote,1,data engineer,senior,"['python', 'nosql', 'sql', 'scala', 'c', 'java']",['azure'],[],['mongodb'],[],na
Arthur Grand Technologies Inc,4.8,"Jersey City, NJ",Azure Tech Lead/ Sr Data Engineer,"Role: Azure Tech Lead/ Sr Data Engineer – Onsite role – Preferred locals
Location: Jersey City, New Jersey / Fort Mill, South Carolina.
Full-time
Mandatory Skills: MS Azure using Azure Data Factory, MS Synapse, Scala, Spark, Data Warehousing
Skills:
Over all 12 to 15 years of experience with Data Management, Data Warehousing and Analytics.
At least 4 to 5 years of experience in Architecting and Implementing Data Solutions.
At least 3 years of experience in implementing the data solutions on MS Azure using Azure Data Factory, MS Synapse.
One to two years of experience in Azure Synapse Analytics is plus.
Installing and configuring ADF integration runtimes and linked services.
At least one hands on experience with Big data platform tool selection POC.
Two years of experience in data migrations to Azure by using data box or Data migration Services.
Apache Spark experience using Scala or PySpark or pre-packaged tools like Databrick is must.
Extensive hands-on experience in data warehousing design, tuning and ETL/ELT process development by using cloud native technologies.
At least one year experience with unified data governance solution using MS Purview.
Developing the CICD pipeline for Azure Infrastructure, version control strategy and Integrate source control ( Azure repos)
In-depth understanding of various storage services offered by Azure.
Experience with implementation of data security, encryption, PII/PSI legislation, identity and access management across sources and environments.
Experience with data process Orchestration, end-to-end design and build process of Near-Real Time and Batch Data Pipelines.
Certification in Azure data engineering and solution architecture Azure is must.
Strong client-facing communication and facilitation skills.
Job Type: Full-time
Salary: $81,075.29 - $186,473.81 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Experience:
Azure: 4 years (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road",133775.0,51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable,NJ,11,data engineer,senior,"['sql', 'scala']",['azure'],[],['spark'],[],na
ConnectiveRx,3.0,"Hanover, NJ",Sr. Data Engineer,"ConnectiveRx is a leading, technology-enabled healthcare services company. We work strategically with hundreds of biopharmaceutical manufacturers to help commercialize and maximize the benefits of specialty and branded medications. Our mission is to simplify how patients get on and stay on therapy. We fulfill our mission by providing our customers with innovative services such as patient and provider messaging, the design and operation of copay, vouchers and patient affordability programs, and hub services, all of which accelerate speed-to-therapy and help improve outcomes for manufacturers, healthcare providers and patients.

ConnectiveRx was formed in 2015 by bringing together the industry-leading business of PSKW, PDR/LDM, Careform (2017) and The Macaluso Group (2018) to advance our technology-driven expertise in providing state-of-the-art commercialization solutions. To learn more about our company, visit ConnectiveRx.com

Job Description

What you will do:
Looking for a seasoned Senior Data Engineer to help us continue to build out our new Enterprise Data Platform. This person must have a strong understanding and demonstrated experience with data streaming architectures that leverage microservice & message-oriented integration patterns and practices within AWS cloud native technologies. This person will help to scale our data ingestion pipelines which are at the core of our Enterprise Data Platform which supports our client reporting as well as our internal analytics & operational teams.

The successful candidate will:
Work with senior leadership, architects, engineers, data analysts, product managers and cloud infrastructure teams to deliver a new features and capabilities.
Write clean, robust, and well-thought-out code with an emphasis on quality, performance, scalability, and maintainability.
Demonstrate strong end to end ownership & craftsmanship - analysis, design, code, test, debug, and deploy
Your ability to traverse the full stack within AWS server-less technologies will be an asset to us as we evaluate the tradeoffs inherent in software engineering. You have the product driven development mindset and can work closely with BA’s and Product teams to breakdown requirements and translate business workflows into scalable technical solutions.

What we’d like from you:
Strong Python & strong SQL
Extensive relational DB experience (Redshift, SQL Server, PostgresSQL) with exposure to document DBs such as DynamoDB. ElasticSearch.
Experience with designing solutions that run in AWS cloud technologies (Lambda, ECS, DynamoDB etc), docker containers
Message oriented architectures, patterns and tools, CQRS, event streaming, Kafka, SQS
Change data capture concepts, Database Triggers, AWS DMS
Data lake concepts, data catalogs, meta data etc
CICD Pipelines
Event store processing, data validation, operational logging via AWS Cloud Watch
Why work with us?
Excellent company culture, fun events, and volunteer opportunities
Competitive benefits (medical, dental, vision & more)
401k package with dollar-for-dollar match-up
Generous PTO and paid holidays days offered
Opportunities to grow professionally and personally
Team-oriented atmosphere
#LI-BJ1

Equal Opportunity Employer: This employer (hereafter the Company) is an equal opportunity employer and does not discriminate in recruitment, hiring, training, promotion, or other employment policies on the basis of age, race, sex, color, religion, national origin, disability, veteran status, genetic information, or any other basis that is prohibited by federal, state, or local law. No question in this application is intended to secure information to be used for such discrimination. In addition, the Company makes reasonable accommodation to the needs of disabled applicants and employees, so long as this does not create an undue hardship on the Company or threaten the health or safety of others at work. This application will be given every consideration, but its receipt does not imply that the applicant will be employed.",115021.0,1001 to 5000 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2015,Unknown / Non-Applicable,NJ,8,data engineer,senior,"['sql', 'python']",['aws'],[],[],['kafka'],na
Airbus Americas,4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.",92046.0,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD),GA,53,data engineer,na,"['r', 'java', 'python']",[],[],['spark'],[],bachelor
Savvy Technology Solutions,4.4,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location",107339.0,,,,,-1,,DC,-1,data engineer,senior,"['nosql', 'sql']",['aws'],['tableau'],[],[],master
Manufacturers Bank,3.2,"Los Angeles, CA",Data Engineer,"GENERAL SUMMARY:
Manufacturers bank is looking for a Data Engineer with strong software engineering backgrounds, a ""platform engineering mindset"" and deep passion for data. This Engineer will help to shape the technical direction of the data domain within the organization. The Engineer will also partner closely with internal business units and external vendors to develop a high performance, reliable and scalable data environment for the bank.

PRINCIPAL DUTIES & RESPONSIBILITIES:
Develop and automate high-performance data processing systems and visualization to ensure reliability and meet critical business requirements.
Design, develop, implement, document and support Data & Analytics systems which adhere to the industry best practices.
Lead data engineering projects, and overall strategy for data governance, security, privacy, quality, and retention.
Triage and resolve issues reported by Risk, Operations, and other business stakeholders.
Mentor junior engineers and continue promoting data engineering and analytics tooling & standards.",115908.0,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),CA,-1,data engineer,na,[],[],[],[],[],na
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),CA,10,data engineer,na,['sql'],"['aws', 'azure']",[],[],['snowflake'],na
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",107952.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['sql', 'python']",[],[],[],['airflow'],na
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451.0,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['sql', 'python', 'nosql', 'java']","['azure', 'google cloud', 'aws']",[],"['hadoop', 'spark']","['kafka', 'hive']",na
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800.0,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['sql', 'python']",[],[],[],[],master
LOVEFOODIES INC,4.4,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",62400.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['sql', 'python']",[],['tableau'],[],[],na
Invictus Data,4.4,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500.0,,,,,-1,,CA,-1,data engineer,senior,"['sql', 'scala', 'python']","['aws', 'azure']",[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']",['aws'],[],['spark'],['snowflake'],na
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",['aws'],[],"['hadoop', 'spark']",['airflow'],na
Edrstaffing,4.4,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000.0,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],na
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],na
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000.0,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],['azure'],['power bi'],[],[],na
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",107952.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['sql', 'python']",[],[],[],['airflow'],na
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
Invictus Data,4.4,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500.0,,,,,-1,,CA,-1,data engineer,senior,"['sql', 'scala', 'python']","['aws', 'azure']",[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
LOVEFOODIES INC,4.4,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",62400.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['sql', 'python']",[],['tableau'],[],[],na
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876.0,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,"['sql', 'c']",['azure'],['power bi'],[],[],bachelor
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
"Double Line, Inc.",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",85882.0,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['sql', 'python']","['aws', 'azure']","['power bi', 'tableau']",[],[],na
TheHive,4.4,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",72800.0,,,,,-1,,MN,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],"['kafka', 'snowflake']",na
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']",['aws'],[],['spark'],['snowflake'],na
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800.0,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['sql', 'python']",[],[],[],[],master
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
Sky Consulting Inc,4.4,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.",103312.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,['python'],[],[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],na
Infinity Quest,4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote",105128.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable,NJ,17,data engineer,na,"['sql', 'python']",['azure'],[],[],[],na
Teamware Solutions (quantum leap consulting).,4.6,"South San Francisco, CA",Data Engineer - Onsite,"Hi,
Data Engineer
Bay Area, CA – Onsite(Hybrid)
Client: Decision Minds/PANW
Duration: Contract
Exp Level: 10+ Years
Must have skill: Google cloud exp
Job Responsibilities:
Expert in data engineering and GCP data technologies.
Work with client teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
Work with Agile and DevOps techniques and implementation approaches in the delivery
Key responsibilities: Architecture, Design and Development
Required Skills:
10+ Year experience in BI and Analytics
Hands on and deep experience ( at least 2 years) working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, ]etc.).
Experience in Spark (Scala/Python/Java) and Kafka, Airflow
Data Engineering and Lifecycle (including non-functional requirements and operations) management.
E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.
Experience with SQL and NoSQL modern data stores.
Thanks & Regards
Jagadeesh
Teamware Solutions Inc |2838 E. Long Lake Road,Suite# 210, TROY, MI 48085
Job Type: Full-time
Salary: $60.00 - $65.00 per hour
Experience level:
10 years
11+ years
9 years
Schedule:
On call
Ability to commute/relocate:
South San Francisco, CA 94080: Reliably commute or planning to relocate before starting work (Required)
Experience:
Google Cloud Platform: 4 years (Preferred)
Data Engineer: 9 years (Preferred)
Spark: 4 years (Preferred)
Work Location: One location",130000.0,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,Unknown / Non-Applicable,CA,20,data engineer,na,"['python', 'nosql', 'sql', 'scala', 'java']",['google cloud'],[],"['gcp', 'spark']","['kafka', 'airflow']",na
"MOBE, LLC",3.7,"Minneapolis, MN",Data Engineer (ETL),"MOBE
MOBE guides people to better health and more happiness. Behind our innovative health solutions is uniquely human philosophy. We believe that person-to-person connections and understanding can make a difference in a world where self-care can be complicated, and health care is ever-evolving and complex.
MOBE works with health plans and large employers to identify individuals who are frequent users of health care but aren?t finding resolutions for their underlying health issues. We use a whole-person approach and guidance to impact health outcomes positively.
Supporting people is at the core of our business, employees included. MOBE is a high-growth organization with a culture built on trust and collaboration. Consistent across our teams and offerings is a belief in the power of people doing good together. We genuinely care about people and consider our workforce the most significant asset.
Your Role at MOBE
This is an exciting time at MOBE and we are growing fast. At MOBE, we have a lot of data: eligibility, medical and pharmacy claims, marketing campaign impressions, transcripts from participant interactions, etc.
This position is responsible for providing technical and project expertise to enable MOBE analytics and operations with structured and unstructured data. Responsibilities includes executing and/or leading user story development, data design and architecture, data pipeline development, testing and deployment in the Analytic Data Framework. This role will partner with internal and external business and technology teams to drive project deliverables and ensure high quality delivery of data architecture and integration.
Responsibilities
The Data Engineer ensures the following capabilities and functions:
Translate high level business processes into logical data processing steps
Design data structures and pipelines that are flexible and scalable for MOBE analytics and operational requirements
Support Analytic partners through collaborative and transparent development, information delivery, problem resolution, shared insights, and training
Data processing definition, execution, and documentation, in a time appropriate way, to meet business priorities and requirements
Data quality and maintenance consistent within the Analytic Data Framework
Lead small to moderate sized projects and initiatives, following through on execution of chosen strategies and demonstrating the ability to work through obstacles and changing priorities.
Demonstrate ability and willingness to play multiple roles for different projects (e.g. planning/architecture, project development, hands-on technical resource/support for others, analysis and resolution of data issues)
Identify and constructively communicate the need for improvements or enhancements in MOBE technology assets
All other duties as assigned to help fulfill our Mission and abide by MOBE?s Guiding Principles",97357.0,51 to 200 Employees,Company - Private,Personal Consumer Services,Beauty & Wellness,2014,Unknown / Non-Applicable,MN,9,data engineer,na,[],[],[],[],[],na
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",107952.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['sql', 'python']",[],[],[],['airflow'],na
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
LOVEFOODIES INC,4.4,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",62400.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['sql', 'python']",[],['tableau'],[],[],na
Arthur Grand Technologies Inc,4.8,"Atlanta, GA",AWS Data Engineer,"Role: AWS Data Engineer
Location: Atlanta, GA (Meet customer Monthly once post client selection & onboarding)
JD for AWS Data Engineer
Experience with the core AWS services, plus the specifics mentioned in this job description.
Experience with Docker . Having used ECS , SNS/SQS, Athena, Lambda,S3, RDS and layers in the past is a plus.
Proficiency in at least in Python, Java
Strong notions of security best practices (e.g. using IAM Roles, KMS, etc.).
Experience with monitoring solutions such as CloudWatch, Cloud Trail.
Previous exposure to large-scale systems design.
Knowledge of writing infrastructure as code (IaC) using CloudFormation or Terraform.
Experience with building or maintaining cloud-native applications.
Past experience with the serverless approaches using AWS Lambda is a plus. For example, the Serverless Application Model (AWS SAM).
Job Types: Full-time, Contract
Schedule:
10 hour shift
8 hour shift
Work Location: Remote",94994.0,51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable,GA,11,data engineer,na,"['java', 'python']",['aws'],[],[],[],na
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
TheHive,4.4,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",72800.0,,,,,-1,,MN,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],"['kafka', 'snowflake']",na
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876.0,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,"['sql', 'c']",['azure'],['power bi'],[],[],bachelor
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800.0,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['sql', 'python']",[],[],[],[],master
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451.0,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['sql', 'python', 'nosql', 'java']","['azure', 'google cloud', 'aws']",[],"['hadoop', 'spark']","['kafka', 'hive']",na
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],na
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']",['aws'],[],['spark'],['snowflake'],na
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000.0,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],['azure'],['power bi'],[],[],na
Edrstaffing,4.4,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000.0,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],na
"Double Line, Inc.",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",85882.0,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['sql', 'python']","['aws', 'azure']","['power bi', 'tableau']",[],[],na
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",107952.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['sql', 'python']",[],[],[],['airflow'],na
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
Invictus Data,4.4,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500.0,,,,,-1,,CA,-1,data engineer,senior,"['sql', 'scala', 'python']","['aws', 'azure']",[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],na
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876.0,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,"['sql', 'c']",['azure'],['power bi'],[],[],bachelor
"Double Line, Inc.",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",85882.0,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['sql', 'python']","['aws', 'azure']","['power bi', 'tableau']",[],[],na
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
TheHive,4.4,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",72800.0,,,,,-1,,MN,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],"['kafka', 'snowflake']",na
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831.0,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],[],[],na
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']",['aws'],[],['spark'],['snowflake'],na
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800.0,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['sql', 'python']",[],[],[],[],master
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['sql', 'python']",[],[],['spark'],[],bachelor
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000.0,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],['azure'],['power bi'],[],[],na
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",107952.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['sql', 'python']",[],[],[],['airflow'],na
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),CA,10,data engineer,na,['sql'],"['aws', 'azure']",[],[],['snowflake'],na
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",['aws'],[],"['hadoop', 'spark']",['airflow'],na
Arthur Grand Technologies Inc,4.8,"Mount Laurel, NJ",Lead Informatica / Data Engineer,"Role: Lead Informatica / Data Engineer – On Prem –ETL (Onsite role) / Senior Informatica / Mid-Level Informatica
Location: Mount Laurel, NJ / Charlotte, NC
Duration : FTE
Client :: Hexaware / TD Bank
Key Skills: Informatica Power Centre, Autosys, Unix
Must Have
More than 12+ years of IT experience in Datawarehouse and ETL
Hands-on Experience on ETL Informatica Power Centre
Experience on Autosys, Unix and scripting knowledge on Python, Shell Scripts
Experience on Oracle Database
Ability to understand ETL Design, Source to target mapping (STTM) and create ETL specifications documents
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have
Any cloud experience on Azure or AWS or Informatica cloud connector
Any relevant certifications
Job Type: Full-time
Salary: $69,919.38 - $166,922.18 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 3 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road",118421.0,51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable,NJ,11,data engineer,senior,"['sql', 'python']","['azure', 'aws']",[],[],[],na
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],na
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
Invictus Data,4.4,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500.0,,,,,-1,,CA,-1,data engineer,senior,"['sql', 'scala', 'python']","['aws', 'azure']",[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']",['aws'],[],['spark'],['snowflake'],na
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000.0,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],['azure'],['power bi'],[],[],na
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451.0,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['sql', 'python', 'nosql', 'java']","['azure', 'google cloud', 'aws']",[],"['hadoop', 'spark']","['kafka', 'hive']",na
Edrstaffing,4.4,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000.0,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],na
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831.0,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],[],[],na
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851.0,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['azure', 'aws']",[],[],['airflow'],na
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800.0,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['sql', 'python']",[],[],[],[],master
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),CA,10,data engineer,na,['sql'],"['aws', 'azure']",[],[],['snowflake'],na
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876.0,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,"['sql', 'c']",['azure'],['power bi'],[],[],bachelor
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],na
TheHive,4.4,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",72800.0,,,,,-1,,MN,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],"['kafka', 'snowflake']",na
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']",['aws'],[],['spark'],['snowflake'],na
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",107952.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['sql', 'python']",[],[],[],['airflow'],na
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['sql', 'python']",[],[],['spark'],[],bachelor
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",['aws'],[],"['hadoop', 'spark']",['airflow'],na
Savvy Technology Solutions,4.4,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location",107339.0,,,,,-1,,DC,-1,data engineer,senior,"['nosql', 'sql']",['aws'],['tableau'],[],[],master
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['nosql', 'scala', 'java', 'python']",[],[],"['flink', 'hadoop', 'spark']","['kafka', 'hive']",na
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831.0,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],[],[],na
Predict Health,4.4,"Arlington, VA",Data Engineer,"Overview of the Role
We are looking for an exceptional Data Engineer who is interested in joining a small and rapidly growing Product and Services team at our company. We view our data-driven products and services as strategic assets, and you will be a key part of our team and company. The successful candidate will have 5+ years of experience designing and implementing data solutions and services. This role will report to our VP, Product Engineering and will be an instrumental member of our product development team.
The Data Engineer will participate in the design of new information intensive product and service offerings. They will be responsible for the development and implementation of the data pipelines and services underlying these new products. They will deepen their understanding of the political, policy, financial and business aspects of the health plan industry so as to be able to provide insight into the best manner to collect and manage information related to these businesses.
The Data Engineer will work closely with the Product Management function to understand customer needs and help identify capabilities for new offerings. They will also partner with the Data Science team to understand the analytical needs of the business and assist in the design and implementation of data solutions to support these efforts.
About Predict Health, Inc.
Predict Health is transforming how health plans, providers and their partners increase retention, quality, and lifetime value of their customer or patient relationships. The core competency of the organization is the curation, analysis, and presentation of information to support targeted operational improvements in healthcare enterprises. The company is an early-stage enterprise based in Washington, D.C. which focuses on three solution categories: Member Acquisition, Quality Management, and Retention Management.
Our company, Predict Health was founded in 2019 and is led by an experienced team of healthcare experts, data scientists and healthcare entrepreneurs. We are focused on building services and solutions designed to improve the healthcare experience of our nation’s seniors. We founded our company after seeing first-hand how challenging it is for the seniors in our lives - including our parents, grandparents and friends - to deal with the complexities of Medicare with little or no real help.
Detailed Job Description
The Data Engineer will initially support the deployment of an Azure solution to help collect, aggregate, transform, and distribute information related to Predict Health’s current service offerings. The Data Engineer is responsible for designing the solution, building the data integration and associated pipelines, design and deploying data structures required to support current products and analytical efforts, and automating the operational aspects of the data platform. The Data Engineer responsibilities will include:
· Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as, but not limited to, Azure Synapse, Azure Data Factory and Azure pipelines.
· Translate business requirements from product owners and analysts into technical code.
· Identify, fix and document bugs, bottlenecks in workflows and pipelines.
· Enforce security compliance, fine tune performance and promote code quality standards.
· Identify and curate new data sources required to support business requirements
· Explore, learn the latest Azure data management technologies to add to current competencies.
· Support development of data intensive solutions in a fast-paced, dynamic environment
· Support Business Intelligence Applications (e.g. Power BI) as required.
Qualifications
Requires minimum of five years relevant experience in design, development, implementation, and testing of data services
· Experience in data structures, data modeling, data warehousing and data pipeline development on Azure
· 3+ years of hands-on experience in the design, implementation and performance tuning of Azure Data Factory, Synapse, or other Azure data services
· Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure
· Experience developing software code in one or more programming languages (Java, Python, etc.)
· Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform
· Experience with data migration
· Experience defining system architectures and exploring technical feasibility trade-offs.
· Ability to prototype and evaluate applications and interaction methodologies.
Other Knowledge, Skills, and Abilities Required
· Demonstrated capability for problem solving, decision making, sound judgement and assertiveness.
· Ability to change direction to meet demand in a high-paced work environment
· Ability to focus on details and technicalities while simultaneously retaining a comprehensive vision of organizational goals
· Excellent collaborator and able to influence and direct work contributions from others to meet critical program deliverables and timelines
· Advanced interpersonal, written, and oral communication skills.
· Proficiency in Microsoft Office including Excel and PowerPoint
What we Offer
This is a full-time position and compensation, and benefits will be highly competitive.
Predict Health is committed to bringing together people from different backgrounds and perspectives to deliver real results for our clients and end-users. We recruit, employ, compensate, and promote regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.
This position will be based in our Arlington, VA office. We offer a flexible work schedule with a minimum of three days in the office each week.
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Employee discount
Flexible schedule
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Arlington, VA: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",115000.0,,,,,-1,,VA,-1,data engineer,na,"['python', 'c', 'java']",['azure'],['power bi'],[],[],na
"Double Line, Inc.",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",85882.0,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['sql', 'python']","['aws', 'azure']","['power bi', 'tableau']",[],[],na
IBR (Imagine Believe Realize),4.5,"Melbourne, FL",Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",118866.0,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,FL,16,data engineer,senior,"['r', 'java', 'c', 'python']","['aws', 'azure']",['tableau'],"['hadoop', 'spark']",[],bachelor
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),CA,10,data engineer,na,['sql'],"['aws', 'azure']",[],[],['snowflake'],na
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876.0,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,"['sql', 'c']",['azure'],['power bi'],[],[],bachelor
Predict Health,4.4,"Arlington, VA",Data Engineer,"Overview of the Role
We are looking for an exceptional Data Engineer who is interested in joining a small and rapidly growing Product and Services team at our company. We view our data-driven products and services as strategic assets, and you will be a key part of our team and company. The successful candidate will have 5+ years of experience designing and implementing data solutions and services. This role will report to our VP, Product Engineering and will be an instrumental member of our product development team.
The Data Engineer will participate in the design of new information intensive product and service offerings. They will be responsible for the development and implementation of the data pipelines and services underlying these new products. They will deepen their understanding of the political, policy, financial and business aspects of the health plan industry so as to be able to provide insight into the best manner to collect and manage information related to these businesses.
The Data Engineer will work closely with the Product Management function to understand customer needs and help identify capabilities for new offerings. They will also partner with the Data Science team to understand the analytical needs of the business and assist in the design and implementation of data solutions to support these efforts.
About Predict Health, Inc.
Predict Health is transforming how health plans, providers and their partners increase retention, quality, and lifetime value of their customer or patient relationships. The core competency of the organization is the curation, analysis, and presentation of information to support targeted operational improvements in healthcare enterprises. The company is an early-stage enterprise based in Washington, D.C. which focuses on three solution categories: Member Acquisition, Quality Management, and Retention Management.
Our company, Predict Health was founded in 2019 and is led by an experienced team of healthcare experts, data scientists and healthcare entrepreneurs. We are focused on building services and solutions designed to improve the healthcare experience of our nation’s seniors. We founded our company after seeing first-hand how challenging it is for the seniors in our lives - including our parents, grandparents and friends - to deal with the complexities of Medicare with little or no real help.
Detailed Job Description
The Data Engineer will initially support the deployment of an Azure solution to help collect, aggregate, transform, and distribute information related to Predict Health’s current service offerings. The Data Engineer is responsible for designing the solution, building the data integration and associated pipelines, design and deploying data structures required to support current products and analytical efforts, and automating the operational aspects of the data platform. The Data Engineer responsibilities will include:
· Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as, but not limited to, Azure Synapse, Azure Data Factory and Azure pipelines.
· Translate business requirements from product owners and analysts into technical code.
· Identify, fix and document bugs, bottlenecks in workflows and pipelines.
· Enforce security compliance, fine tune performance and promote code quality standards.
· Identify and curate new data sources required to support business requirements
· Explore, learn the latest Azure data management technologies to add to current competencies.
· Support development of data intensive solutions in a fast-paced, dynamic environment
· Support Business Intelligence Applications (e.g. Power BI) as required.
Qualifications
Requires minimum of five years relevant experience in design, development, implementation, and testing of data services
· Experience in data structures, data modeling, data warehousing and data pipeline development on Azure
· 3+ years of hands-on experience in the design, implementation and performance tuning of Azure Data Factory, Synapse, or other Azure data services
· Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure
· Experience developing software code in one or more programming languages (Java, Python, etc.)
· Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform
· Experience with data migration
· Experience defining system architectures and exploring technical feasibility trade-offs.
· Ability to prototype and evaluate applications and interaction methodologies.
Other Knowledge, Skills, and Abilities Required
· Demonstrated capability for problem solving, decision making, sound judgement and assertiveness.
· Ability to change direction to meet demand in a high-paced work environment
· Ability to focus on details and technicalities while simultaneously retaining a comprehensive vision of organizational goals
· Excellent collaborator and able to influence and direct work contributions from others to meet critical program deliverables and timelines
· Advanced interpersonal, written, and oral communication skills.
· Proficiency in Microsoft Office including Excel and PowerPoint
What we Offer
This is a full-time position and compensation, and benefits will be highly competitive.
Predict Health is committed to bringing together people from different backgrounds and perspectives to deliver real results for our clients and end-users. We recruit, employ, compensate, and promote regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.
This position will be based in our Arlington, VA office. We offer a flexible work schedule with a minimum of three days in the office each week.
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Employee discount
Flexible schedule
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Arlington, VA: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",115000.0,,,,,-1,,VA,-1,data engineer,na,"['python', 'c', 'java']",['azure'],['power bi'],[],[],na
TheHive,4.4,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",72800.0,,,,,-1,,MN,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],"['kafka', 'snowflake']",na
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],na
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['nosql', 'scala', 'java', 'python']",[],[],"['flink', 'hadoop', 'spark']","['kafka', 'hive']",na
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831.0,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],[],[],na
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",['aws'],[],"['hadoop', 'spark']",['airflow'],na
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['sql', 'python']",[],[],['spark'],[],bachelor
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']",['aws'],[],['spark'],['snowflake'],na
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755.0,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['sql', 'python']",['aws'],[],['gcp'],['airflow'],na
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
MARVEL TECHNOLOGIES INC,3.7,Remote,Data Engineer,"Primary Skills
SCALA, SPARK, SQL, HADOOP, AWS, HIVE, Spark SQL, HIVE QL, CICD (Continuous Integration/Continuous Delivery), VCS (GIT HUB)
Coding in Scala
Designing in of HADOOP ecosystem
Hands-on experience on AWS tools like EMR, EC2
Hands-on experience of SQL in Big Data: SQL, Spark SQL, Hive QL
Proficient in working with large data sets and pipelines
Proficient with workflow scheduling / orchestration tools
Well versed with CICD process and VCS
Databricks is a big PLUS
Job Types: Full-time, Contract
Pay: $50.00 - $58.00 per hour
Schedule:
8 hour shift
Monday to Friday
Experience:
Spark: 4 years (Required)
Scala: 4 years (Required)
Hadoop: 3 years (Required)
Aws: 3 years (Required)
Hive: 3 years (Required)
CI/CD, VCS: 3 years (Required)
Databricks: 1 year (Required)
Work Location: Remote",112320.0,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,$5 to $25 million (USD),Remote,-1,data engineer,na,"['sql', 'scala']",['aws'],[],"['hadoop', 'spark']",['hive'],na
"Double Line, Inc.",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",85882.0,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['sql', 'python']","['aws', 'azure']","['power bi', 'tableau']",[],[],na
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",107952.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['sql', 'python']",[],[],[],['airflow'],na
Nursa,4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!",89485.0,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable,UT,5,data engineer,na,"['sql', 'python']",['aws'],[],[],[],na
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),CA,10,data engineer,na,['sql'],"['aws', 'azure']",[],[],['snowflake'],na
TheHive,4.4,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",72800.0,,,,,-1,,MN,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],"['kafka', 'snowflake']",na
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],na
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876.0,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,"['sql', 'c']",['azure'],['power bi'],[],[],bachelor
Plaxonic Technologies,4.6,"Dallas, TX",Data Engineer,"Job Description – Sr. Data Engineer:
Minimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark
Ability to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark
Hands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies
Strong skill in understanding database architecture, data models and writing complex SQL queries/code
Hands on experience with integration of different data sources (Files, DBs, APIs)
Ability to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation
Knowledge of various ETL techniques
Experience with data pipeline and workflow management tools [Azure DevOps]
Strong analytic skill to work with unstructured data
Experience working with Agile teams scrums.
Strong customer handling skills and communication skills
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75287: Reliably commute or planning to relocate before starting work (Required)
Experience:
Microsoft Excel: 1 year (Preferred)
Microsoft Office: 1 year (Preferred)
Compliance management: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: One location
Speak with the employer
+91 7272167989",98641.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),TX,10,data engineer,na,['sql'],['azure'],[],[],[],na
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851.0,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['azure', 'aws']",[],[],['airflow'],na
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",107952.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['sql', 'python']",[],[],[],['airflow'],na
Edrstaffing,4.4,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000.0,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],na
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755.0,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['sql', 'python']",['aws'],[],['gcp'],['airflow'],na
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']",['aws'],[],['spark'],['snowflake'],na
Predict Health,4.4,"Arlington, VA",Data Engineer,"Overview of the Role
We are looking for an exceptional Data Engineer who is interested in joining a small and rapidly growing Product and Services team at our company. We view our data-driven products and services as strategic assets, and you will be a key part of our team and company. The successful candidate will have 5+ years of experience designing and implementing data solutions and services. This role will report to our VP, Product Engineering and will be an instrumental member of our product development team.
The Data Engineer will participate in the design of new information intensive product and service offerings. They will be responsible for the development and implementation of the data pipelines and services underlying these new products. They will deepen their understanding of the political, policy, financial and business aspects of the health plan industry so as to be able to provide insight into the best manner to collect and manage information related to these businesses.
The Data Engineer will work closely with the Product Management function to understand customer needs and help identify capabilities for new offerings. They will also partner with the Data Science team to understand the analytical needs of the business and assist in the design and implementation of data solutions to support these efforts.
About Predict Health, Inc.
Predict Health is transforming how health plans, providers and their partners increase retention, quality, and lifetime value of their customer or patient relationships. The core competency of the organization is the curation, analysis, and presentation of information to support targeted operational improvements in healthcare enterprises. The company is an early-stage enterprise based in Washington, D.C. which focuses on three solution categories: Member Acquisition, Quality Management, and Retention Management.
Our company, Predict Health was founded in 2019 and is led by an experienced team of healthcare experts, data scientists and healthcare entrepreneurs. We are focused on building services and solutions designed to improve the healthcare experience of our nation’s seniors. We founded our company after seeing first-hand how challenging it is for the seniors in our lives - including our parents, grandparents and friends - to deal with the complexities of Medicare with little or no real help.
Detailed Job Description
The Data Engineer will initially support the deployment of an Azure solution to help collect, aggregate, transform, and distribute information related to Predict Health’s current service offerings. The Data Engineer is responsible for designing the solution, building the data integration and associated pipelines, design and deploying data structures required to support current products and analytical efforts, and automating the operational aspects of the data platform. The Data Engineer responsibilities will include:
· Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as, but not limited to, Azure Synapse, Azure Data Factory and Azure pipelines.
· Translate business requirements from product owners and analysts into technical code.
· Identify, fix and document bugs, bottlenecks in workflows and pipelines.
· Enforce security compliance, fine tune performance and promote code quality standards.
· Identify and curate new data sources required to support business requirements
· Explore, learn the latest Azure data management technologies to add to current competencies.
· Support development of data intensive solutions in a fast-paced, dynamic environment
· Support Business Intelligence Applications (e.g. Power BI) as required.
Qualifications
Requires minimum of five years relevant experience in design, development, implementation, and testing of data services
· Experience in data structures, data modeling, data warehousing and data pipeline development on Azure
· 3+ years of hands-on experience in the design, implementation and performance tuning of Azure Data Factory, Synapse, or other Azure data services
· Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure
· Experience developing software code in one or more programming languages (Java, Python, etc.)
· Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform
· Experience with data migration
· Experience defining system architectures and exploring technical feasibility trade-offs.
· Ability to prototype and evaluate applications and interaction methodologies.
Other Knowledge, Skills, and Abilities Required
· Demonstrated capability for problem solving, decision making, sound judgement and assertiveness.
· Ability to change direction to meet demand in a high-paced work environment
· Ability to focus on details and technicalities while simultaneously retaining a comprehensive vision of organizational goals
· Excellent collaborator and able to influence and direct work contributions from others to meet critical program deliverables and timelines
· Advanced interpersonal, written, and oral communication skills.
· Proficiency in Microsoft Office including Excel and PowerPoint
What we Offer
This is a full-time position and compensation, and benefits will be highly competitive.
Predict Health is committed to bringing together people from different backgrounds and perspectives to deliver real results for our clients and end-users. We recruit, employ, compensate, and promote regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.
This position will be based in our Arlington, VA office. We offer a flexible work schedule with a minimum of three days in the office each week.
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Employee discount
Flexible schedule
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Arlington, VA: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",115000.0,,,,,-1,,VA,-1,data engineer,na,"['python', 'c', 'java']",['azure'],['power bi'],[],[],na
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831.0,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],[],[],na
Ascent Solutions,4.4,Remote,Hadoop Big Data Engineer,"Hadoop Big Data Engineer
Remote
Must Haves • Strong experience in Big Data, Cloudera Distribution 7.x, Cloud migration, RDBMS • Strong Experience with Amazon EMR/Databricks/Cloudera CDP • 4-5 experience building data pipelines using Hadoop components Sqoop, Hive, Solr, MR, Impala, Spark, Spark SQL., HBase. • 4-5 years of programming experience in Python, Java and Scala is must. • Strong experience with REST API development using Python frameworks (Django, Flask etc.) • Micro Services/Web service development experience using Spring framework is highly desirable.
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'scala', 'java', 'python']",[],[],"['hadoop', 'spark']",['hive'],na
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),CA,10,data engineer,na,['sql'],"['aws', 'azure']",[],[],['snowflake'],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],na
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876.0,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,"['sql', 'c']",['azure'],['power bi'],[],[],bachelor
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851.0,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['azure', 'aws']",[],[],['airflow'],na
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
IBR (Imagine Believe Realize),4.5,"Melbourne, FL",Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",118866.0,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,FL,16,data engineer,senior,"['r', 'java', 'c', 'python']","['aws', 'azure']",['tableau'],"['hadoop', 'spark']",[],bachelor
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831.0,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],[],[],na
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
TheHive,4.4,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",72800.0,,,,,-1,,MN,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],"['kafka', 'snowflake']",na
Plaxonic Technologies,4.6,"Dallas, TX",Data Engineer,"Job Description – Sr. Data Engineer:
Minimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark
Ability to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark
Hands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies
Strong skill in understanding database architecture, data models and writing complex SQL queries/code
Hands on experience with integration of different data sources (Files, DBs, APIs)
Ability to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation
Knowledge of various ETL techniques
Experience with data pipeline and workflow management tools [Azure DevOps]
Strong analytic skill to work with unstructured data
Experience working with Agile teams scrums.
Strong customer handling skills and communication skills
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75287: Reliably commute or planning to relocate before starting work (Required)
Experience:
Microsoft Excel: 1 year (Preferred)
Microsoft Office: 1 year (Preferred)
Compliance management: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: One location
Speak with the employer
+91 7272167989",98641.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),TX,10,data engineer,na,['sql'],['azure'],[],[],[],na
Invictus Data,4.4,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500.0,,,,,-1,,CA,-1,data engineer,senior,"['sql', 'scala', 'python']","['aws', 'azure']",[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
Ascent Technologies,4.4,Remote,Data Engineer,"Job Description: Involved in resource / volume ramp-up plan/ backfilling resources planned for rotation and resignation cases Tracking Performance of individual Projects within the delivery/SIPs. Participate / presenting SD performance during client visit Publish Daily, Weekly and Monthly reports on team productivity Reviewing the weekly / monthly dashboards Publishing dashboards to customer People Management Drive the team to reach the defined targets Ensure training and development of the team members Mentor team members on technical and communication aspects. onsite anni data enginneer-pheonix,az python,charlottee,nc ba-houston tx
Job Types: Full-time, Contract, Permanent
Salary: $39.87 - $86.24 per hour
Benefits:
Dental insurance
Compensation package:
1099 contract
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91 7842516676",131144.0,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'python']",[],[],[],[],na
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['sql', 'python']",[],[],['spark'],[],bachelor
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",['aws'],[],"['hadoop', 'spark']",['airflow'],na
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),CA,10,data engineer,na,['sql'],"['aws', 'azure']",[],[],['snowflake'],na
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277.0,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851.0,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['azure', 'aws']",[],[],['airflow'],na
Invictus Data,4.4,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500.0,,,,,-1,,CA,-1,data engineer,senior,"['sql', 'scala', 'python']","['aws', 'azure']",[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",107952.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['sql', 'python']",[],[],[],['airflow'],na
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['nosql', 'scala', 'java', 'python']",[],[],"['flink', 'hadoop', 'spark']","['kafka', 'hive']",na
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876.0,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,"['sql', 'c']",['azure'],['power bi'],[],[],bachelor
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['sql', 'python']",[],[],['spark'],[],bachelor
Enterprise Knowledge LLC,4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”",89080.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD),VA,10,data engineer,na,"['sql', 'r', 'python']",['aws'],[],['spark'],['airflow'],na
LOVEFOODIES INC,4.4,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",62400.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['sql', 'python']",[],['tableau'],[],[],na
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537.0,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],[],na
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']",['aws'],[],['spark'],['snowflake'],na
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),CA,10,data engineer,na,['sql'],"['aws', 'azure']",[],[],['snowflake'],na
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851.0,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['azure', 'aws']",[],[],['airflow'],na
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277.0,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537.0,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],[],na
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",['aws'],[],"['hadoop', 'spark']",['airflow'],na
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']",['aws'],[],['spark'],['snowflake'],na
Enterprise Knowledge LLC,4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”",89080.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD),VA,10,data engineer,na,"['sql', 'r', 'python']",['aws'],[],['spark'],['airflow'],na
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['sql', 'python']",[],[],['spark'],[],bachelor
Invictus Data,4.4,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500.0,,,,,-1,,CA,-1,data engineer,senior,"['sql', 'scala', 'python']","['aws', 'azure']",[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),CA,10,data engineer,na,['sql'],"['aws', 'azure']",[],[],['snowflake'],na
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
IBR (Imagine Believe Realize),4.5,"Melbourne, FL",Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",118866.0,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,FL,16,data engineer,senior,"['r', 'java', 'c', 'python']","['aws', 'azure']",['tableau'],"['hadoop', 'spark']",[],bachelor
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277.0,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851.0,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['azure', 'aws']",[],[],['airflow'],na
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537.0,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],[],na
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",['aws'],[],"['hadoop', 'spark']",['airflow'],na
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']",['aws'],[],['spark'],['snowflake'],na
Enterprise Knowledge LLC,4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”",89080.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD),VA,10,data engineer,na,"['sql', 'r', 'python']",['aws'],[],['spark'],['airflow'],na
Invictus Data,4.4,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500.0,,,,,-1,,CA,-1,data engineer,senior,"['sql', 'scala', 'python']","['aws', 'azure']",[],"['gcp', 'hadoop', 'spark']","['kafka', 'hive']",na
Kaizen Dynamics,4.4,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",176800.0,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],master
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['sql', 'python']",[],[],['spark'],[],bachelor
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",107952.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['sql', 'python']",[],[],[],['airflow'],na
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),CA,10,data engineer,na,['sql'],"['aws', 'azure']",[],[],['snowflake'],na
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851.0,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['azure', 'aws']",[],[],['airflow'],na
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",['aws'],[],"['hadoop', 'spark']",['airflow'],na
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537.0,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],[],na
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277.0,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
Enterprise Knowledge LLC,4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”",89080.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD),VA,10,data engineer,na,"['sql', 'r', 'python']",['aws'],[],['spark'],['airflow'],na
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845.0,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']",['aws'],[],['spark'],['snowflake'],na
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['sql', 'python']",[],[],['spark'],[],bachelor
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",107952.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['sql', 'python']",[],[],[],['airflow'],na
Ascent Technologies,4.4,Remote,Data Engineer,"Job Description: Involved in resource / volume ramp-up plan/ backfilling resources planned for rotation and resignation cases Tracking Performance of individual Projects within the delivery/SIPs. Participate / presenting SD performance during client visit Publish Daily, Weekly and Monthly reports on team productivity Reviewing the weekly / monthly dashboards Publishing dashboards to customer People Management Drive the team to reach the defined targets Ensure training and development of the team members Mentor team members on technical and communication aspects. onsite anni data enginneer-pheonix,az python,charlottee,nc ba-houston tx
Job Types: Full-time, Contract, Permanent
Salary: $39.87 - $86.24 per hour
Benefits:
Dental insurance
Compensation package:
1099 contract
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91 7842516676",131144.0,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'python']",[],[],[],[],na
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800.0,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['sql', 'python']",[],[],[],[],master
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",135200.0,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],na
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",140400.0,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],[],master
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",160160.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'python', 'java']","['aws', 'azure']",[],"['hadoop', 'spark']",['hive'],na
Data Crunch Corp,4.4,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",124800.0,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],na
Futuretech Consultants LLC,4.4,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",88400.0,,,,,-1,,MS,-1,data engineer,na,"['sql', 'c']",[],[],[],['snowflake'],bachelor
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),CA,10,data engineer,na,['sql'],"['aws', 'azure']",[],[],['snowflake'],na
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",130000.0,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],"['gcp', 'hadoop']",['airflow'],na
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000.0,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'python', 'java']",[],[],[],[],na
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'scala', 'java']",['azure'],[],['spark'],[],bachelor
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",98800.0,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],na
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",135200.0,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],[],[],bachelor
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851.0,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['azure', 'aws']",[],[],['airflow'],na
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",114400.0,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['hadoop'],[],na
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500.0,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",['aws'],[],"['hadoop', 'spark']",['airflow'],na
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277.0,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']",['aws'],[],['spark'],['kafka'],bachelor
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000.0,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],['azure'],['power bi'],[],[],na
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989.0,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'scala', 'python', 'java']",[],[],['spark'],[],na
IBR (Imagine Believe Realize),4.5,"Melbourne, FL",Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",118866.0,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,FL,16,data engineer,senior,"['r', 'java', 'c', 'python']","['aws', 'azure']",['tableau'],"['hadoop', 'spark']",[],bachelor
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",119600.0,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['azure', 'google cloud', 'aws']","['power bi', 'tableau']","['gcp', 'hadoop', 'spark']",[],bachelor
Enterprise Knowledge LLC,4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”",89080.0,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD),VA,10,data engineer,na,"['sql', 'r', 'python']",['aws'],[],['spark'],['airflow'],na
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537.0,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],[],na
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500.0,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['sql', 'python']",[],[],['spark'],[],bachelor
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840.0,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['nosql', 'scala', 'java', 'python']",[],[],"['flink', 'hadoop', 'spark']","['kafka', 'hive']",na
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927.0,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],['spark'],[],na
