company,company_rating,location,job_title,job_description,salary_estimate,company_size,company_type,company_sector,company_industry,company_founded,company_revenue,job_state,company_age,job_simp,seniority,job_languages,job_cloud,job_viz,job_databases,job_bigdata,job_devops,job_education,job_experience
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'java', 'python']",[],[],[],[],[],,0-2 years
Futuretech Consultants LLC,4.0,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],['snowflake'],[],[],bachelor,2-5 years
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']","['databricks', 'aws']",[],[],['spark'],[],master,0-2 years
Apple,4.2,"Cupertino, CA",Data Engineer,"Summary
Posted: Dec 22, 2021
Weekly Hours: 40
Role Number:200327520
As part of our Video Engineering group, you’ll help deliver creative solutions to various problems that could impact the people all over the world. This Data Engineer will work closely with other members of the Video Engineering group to mine data, implement model evaluation pipeline, analyze large scale data, visualize data, and ensure the delivery is of the highest quality. This position will also require strong coding skills, presentation skills, and collaborating with multiple teams (ex: machine learning, cloud infrastructure support).
Key Qualifications
A curious mind
An obsession for quality
Background in Data science, Data mining, Multivariate statistics, Computer vision, Machine learning
Experience working with large scale data sets
Solid programming skills including:
Python
C/C++
Experience with data visualization and presentation, familiar with data analysis tools such as Tableau
Excellent problem solving and communication skills
Description
The responsibilities of this position includes the following for current and future products: - Implement algorithm evaluation methods - Analyze data and build data analysis tools - Deep-dive failure analysis - Discover new perspectives for old data - Produce / Present meaningful data visualization to higher-ups and across various involved teams
Education & Experience
Masters in Computer Science or relevant experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $50.72 and $76.44/hr, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",112889,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1976,$10+ billion (USD),CA,47,data engineer,na,['python'],[],['tableau'],[],[],[],,
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],[],['tableau'],['oracle'],[],[],bachelor,5-10 years
Steward Health Care,2.7,"Westwood, MA",Data Engineer,"Position Purpose:
Reporting to the Manager of the Data Warehouse team, part of the larger Health Informatics group, the data engineer applies their technical expertise to meet the needs of the department and Steward Health Care Network (SHCN).

Key Responsibilities:
ETL/Automation
Design configurable data process flows with full automation
Develop ETL processes for data loading and data extraction
Schedule ETL processes for full process automation
Data Engineering
Responsible for data analysis to support building data processes and reporting
Design useful and accurate data marts that meet requirements
Apply SQL skills when designing and building data marts and data flows
Quality
Establish and utilize QC processes to ensure data integrity
Incorporate standard error logging and alerts to ensure data is loaded as expected
Documentation
Create and maintain clear documentation

Education / Experience / Other Requirements


Education:
Bachelor's degree in Computer Science, Mathematics, Statistics or related experience


Years of Experience:
5+ years of database related work
2+ years of focus on healthcare data

Specialized Knowledge:
Knowledge of healthcare data
Experience using relational databases, SQL Server experience preferred
Experience using ETL tools (SSIS, Informatica, etc.)
Strong SQL programming skills
Experience with scripting languages (PowerShell, R, Python, etc.)
Experience automating data flows
Experience with Health Catalyst tools preferred, but not required
Deep understanding of database structures and data design.
Creative, flexible, and self-motivated with sound judgment
Strong communication skills




Location: Steward Health Care Network · 1301.72330 Steward Health Care Network
Schedule: Full Time, Day Shift, 40 hours",94536,10000+ Employees,Hospital,Healthcare,Health Care Services & Hospitals,1998,Unknown / Non-Applicable,MA,25,data engineer,na,"['sql', 'r', 'python']",[],['ssis'],['sql server'],[],[],bachelor,+10 years
"Twitch Interactive, Inc.",3.8,"San Francisco, CA",Data Engineer,"3+ years of experience in data engineering, software engineering, or other related roles. 3+ years in relational database concepts with a solid knowledge of star schema, SQL, SQL Tuning, OLAP, Big Data technologies 3+ years of experience in generating and maintaining data pipelines from various data sources, in collaboration with diverse stakeholders. 3+ years of experience working with Amazon Webservices, S3, EMR, Redshift etc. Experience with best practices for development including query optimization, version control, code reviews, and documentation. Experience with coding languages like Python/Java/Scala
About Us: Twitch is the world's biggest live streaming service, with global communities built around gaming, entertainment, music, sports, cooking, and more. It's where millions of people come together to chat, interact, and make their own entertainment. We're about community, inside and out. You'll find coworkers who are eager to team up, collaborate, and smash (or elegantly solve) problems together. We're on a quest to empower live communities, so if this sounds good to you, see what we're up to on LinkedIn and Twitter, get interviewing tips on Instagram, and discover projects we're solving on our Blog. About the Role: Data is central to Twitch's decision-making process, and data engineers operate at the forefront of this by creating authoritative datasets that drives analysis and decision-making across all of Twitch. In this role you will be shaping the way that business performance is measured, defining how we transform our data, and scaling analytics methods and tools to support our growing business, leading the way for high quality, high velocity decisions. For this role, we're looking for an experienced data engineer to join our Content Data Science team, which is focused on empowering staff throughout Twitch to use and trust our business data. Your responsibilities may range from developing and enhancing our data warehouse which act as authoritative sources of truth across the company, driving data quality and trustworthiness across product verticals and business areas, building self-service business intelligence infrastructure for analysts, as well as connecting into data interfaces that enable everyone in Twitch to discover and analyze the data. In the process, you will have the opportunity to interact with technical and non-technical staff members throughout the company, and will report to the Director of Content Data Science. This position can be located in San Francisco, CA; Irvine, CA; Seattle, WA; New York, NY; and Salt Lake City, UT. You Will: Define and own team level data architecture for trusted, governed, dimensionally-modeled repository of data that enables Twitch staff to quickly and reliably answer their business questions. Keep existing data sources fresh against data quality issues, design, develop and maintain data quality assurance framework and continuously improve the processes for developing new ones raising the level of quality expected from our work. Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost. Improve search, discovery and literacy: Create exploration and visualization interfaces in our BI tools and evangelize the adoption of these sources across the company through education and training programs. Improve business and engineering team processes via data architecture, engineering, test, and operational excellence best practices. Make enhancements that improve data processes.
Bonus Points
A passion for data science and interest in growing / learning data science, machine learning at scale.
A passion for games and the gaming industry
Perks
Medical, Dental, Vision & Disability Insurance
401(k)
Maternity & Parental Leave
Flexible PTO
Amazon Employee Discount
Monthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages, etc.)
We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
Pursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $105,700/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. Applicants should apply via our internal or external career site.",105700,10000+ Employees,Company - Public,Information Technology,Internet & Web Services,1994,$10+ billion (USD),CA,29,data engineer,na,"['sql', 'java', 'scala', 'python']",['redshift'],[],[],[],[],,+10 years
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'java', 'python']","['databricks', 'azure', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",[],,5-10 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],['aws'],[],[],['hadoop'],[],,0-2 years
Nike,4.1,"Boston, MA",Data Engineer,"Become part of the Converse Team

Converse is a place to explore potential, break barriers and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Converse, it’s about each person bringing skills and passion to a challenging and constantly evolving world to make things better as a team.
Converse, Inc. Boston, MA. Work closely with Project Management and Business teams to completely define specifications to ensure the project acceptance. Involved in preparation of functional and technical specifications with different cross teams. Lead team, defining solution options, providing estimates on effort and risk, and evaluating technical feasibility in Agile development process, including Scrum and Kanban. Work on troubleshooting data and analytics issues and perform root cause analysis to proactively resolve issues. Develop data extracts and feeds from the full spectrum of systems in the Converse ecosystem, including transactional ERP systems, POS data, product and merchandising systems. Engineer data products for a variety of Operations analytics use cases, ranging from reporting and data visualization to advanced analytics/machine learning use cases. Support designing technical specifications and data transformation models for junior developers. Ensure development is on track and meets specifications as defined by product management and the business. Responsible for data integrity of current platform and QA of new releases. Support the development and maintenance of backlog items and solution feature. Participate in sprint planning activities from a development perspective. Responsible for designing cloud-based data architecture using AWS stacks. Design and develop Python data science and data engineering libraries dealing with structured and unstructured data. Work with a variety of database types (SQL/NoSQL, columnar, object-oriented) and diverse data formats. Responsible for ETL with Spark and building data pipelines/orchestrations in Airflow and working on ETL tools like Matillion. Responsible for DevOps toolchain and Continuous Development, Continuous Integration and Automated Testing using Jenkins. Ensure and use data engineering for advanced analytics/data science and Software development skills.
Applicant must have a Bachelor’s degree in Computer Science, Information Systems, or Information Technology and 5 years of progressive post-baccalaureate experience in the job offered or a related occupation. Experience must include:
Data warehousing;
ETL or ELT;
Amazon Web Service (AWS) Cloud Services, including AWS S3, AWS Lambda, AWS EC2, AWS EMR or AWS DynamoDB;
Relational Database Management Systems (RDBMS), such as Oracle, Teradata, SQL Server or Snowflake;
Database Development with writing stored procedures, functions, triggers, cursors or SQL queries;
Hadoop, HDFS, Hive or Spark;
Programming languages, including Java or Python;
Business Intelligence Tools, such as Tableau;
Unix Shell scripting; and
Version control systems, such as Git, Bitbucket or Github
#LI-DNP
Converse is more than a company; it’s a worldwide advocate for self-expression. This belief motivates our employees, permeates our working environment and inspires our products. No two of us look or think exactly alike. We are each one-of-a-kind. Individually and as a culture, we have the freedom to create and grow professionally. Generous benefits packages only sweeten the experience. From Boston to Shanghai, from Brand Design to Finance, Converse is a brand that celebrates the unique and creative people of the world. Together, we’re different.",115797,10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1972,$10+ billion (USD),MA,51,data engineer,na,"['python', 'shell', 'java', 'sql', 'nosql']","['snowflake', 'aws']",['tableau'],"['dynamodb', 'snowflake', 'oracle', 'sql server', 'hive']","['hadoop', 'spark']",[],bachelor,
Aretec Inc,1.0,Remote,Junior Data Engineer,"POSITION TITLE: Junior Data Engineer YEARS OF EXPERIENCE: 1-3
LOCATON: 100% remote
*****Please Note: Aretec, Inc. does not offer Corp - 2 - Corp (C2C) employment. *****
Aretec is looking for a Junior Data Engineer. The Junior Data Engineer will be primarily responsible for design, development, support and enhancement of the data pipelines developed in AWS.

RESPONSIBILITIES:
You'll write clean and functional code on the front- and back-end
You'll write reusable and maintainable code
Coordinate with data migration plans
Ability to communication and collaborate with various teams and vendors.
Participates in functional and technical design.
Participation in Agile activities Scrum, Kanban.
Ensure coding, testing, debugging and implementation activities completed as required.
Flexible and adaptable with the ability to align to changing priorities
The developer should have great communication skills and be able to discuss and develop requirements with multiple levels of staff from corporate and field locations
An interest in and ability to understand financial reporting, accounting concepts and related accounting data
Participate in data flow diagramming and/or process modeling (code architecture)
Documents work and steps to completion as required
Follows AWS best practices to integrate with ecosystem and infrastructure
Ability to partner with domain architects to implement the defined solution architecture including application, infrastructure, data, integration, and security domains

REQUIRED SKILLS:
1-3 years of software engineering experience
1+ years of real industry experience
Experience with website development, web services and API development
Hands-on experience performing data engineering and transformation tasks using Python
Experience implementing backend in Python using frameworks such as Django or Flask
Knowledge of web technologies - both back and front-end development including, but not limited to JavaScript, React, CSS, HTML, T-SQL, and Python
Understand log monitoring and analytics
Experience Meeting both technical and consumer needs
Experience testing software to ensure responsiveness and efficiency
A general knowledge of index migrations, debugging and researching concepts are major pluses
Must be aware of CI/CD pipelines and well-versed in using GitLab for creating required pipelines for CI/CD

EDUCATION: Bachelors Degree in Mathematics/Statistics/Technology/Science/Engineering/Applied Mathematics or related field

CERTIFICATIONS: N/A",102500,51 to 200 Employees,Contract,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['aws'],[],[],[],['gitlab'],,+10 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['sql', 'r', 'python']",[],['ssis'],[],['hadoop'],[],,
Adobe,4.4,"New York, NY",Data Engineer,"Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

Job Description
Adobe Customer Solutions is looking for a full time Data Engineer with experience in building data integrations using AWS technology stack as part of the team's Data as a Service portfolio for Adobe’s Digital Experience enterprise customers.
Customer facing Engineers who enjoy tackling complex technical challenges, have a passion for delighting customers and who are self-motivated to push themselves in a team oriented culture will thrive in our environment
What you'll Do
Collaborate with Data architects, Enterprise architects, Solution consultants and Product engineering teams to gather customer data integration requirements, conceptualize solutions & build required technology stack
Collaborate with enterprise customer's engineering team to identify data sources, profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating customer data sources and third party data sources with Adobe solutions
Develop new features and improve existing data integrations with customer data ecosystem
Encourage team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Collaborate with a Project Manager to bill and forecast time for customer solutions
What you need to succeed

Proven experience in building/operating/maintaining fault tolerant and scalable data processing integrations using AWS
Proven track record in Python programming language
Software development experience working with Apache Airflow, Spark, MongoDB, MySQL
Experience using Docker or Kubernetes is a plus
BS/MS degree in Computer Science or equivalent proven experience
Ability to identify and resolve problems associated with production grade large scale data processing workflows
Excellent interpersonal skills
Experience crafting and maintaining unit tests and continuous integration.
Passion for crafting I ntelligent data pipelines that customers love to use
Strong capacity to handle numerous projects are a must
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists. You will also be surrounded by colleagues who are committed to helping each other grow through our outstanding Check-In approach where feedback flows freely.
If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the significant benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age, sexual orientation, gender identity, disability or veteran status.

Our compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $101,500 -- $194,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.",147900,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1982,$5 to $10 billion (USD),NY,41,data engineer,na,['python'],['aws'],[],"['mongodb', 'mysql']",['spark'],['docker'],,
Glow Networks,3.5,"Dallas, TX",Data Engineer,"Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.",131400,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD),TX,20,data engineer,na,"['sql', 'java', 'scala', 'python']","['databricks', 'snowflake', 'aws', 'redshift']",[],"['snowflake', 'hive', 'dbt']","['kafka', 'hadoop', 'spark']",['docker'],bachelor,+10 years
Enterprise Knowledge LLC,4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”",89080,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD),VA,10,data engineer,na,"['sql', 'r', 'python']",['aws'],[],[],['spark'],[],,2-5 years
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,['sql'],['azure'],"['power bi', 'ssis']","['sql server', 'oracle']",[],[],bachelor,
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],[],,0-2 years
Small Batch Standard,4.1,Remote,Junior Data Engineer,"We're the premier, remote accounting, tax, and consulting firm built exclusively to serve the craft brewing industry.
Our mission is to help craft breweries grow profits and build deep successful relationships. And our team is filled with expert, autonomous, adaptable, technology-driven high performers.
Are you up for the challenge?
We're looking for a full-time, remote Junior Data Engineer to join our specialized team. The main objective of this role is to design, develop, implement, and improve both internal and external applications to support our brewery clients and team in accordance with the SBS Core Values.
About The Role
This role will report to our Technology/Product Manager and is accountable for fulfilling the following responsibilities:
Building our data pipeline and analysis applications. A key aspect of the consulting service we provide to clients involves the collection, aggregation, analysis, modeling, and usage of financial data and benchmarks. We use this data both internally to develop and inform strategy, as well as externally through our Benchmarks Assessment (https://sbstandard.com/assessment/) and Compass analysis product (https://sbstandard.com/levelup-compass/). You'll be responsible for working with our team to build out our data pipeline for these tools, and progressively increasing our ability to aggregate, analyze, query, and feed back this data into our reporting, analysis, and consulting work. Platforms we're building with include: SQL, Airflow, Excel Visual Basic for Applications (VBA), Google Apps Script, Intuit/QuickBooks Online.
New process and technology R&D. We're always looking for new opportunities to provide both our team and our clients access to additional tools that give them leverage, automate and streamline processes, and overall make work more efficient. Part of your time will be dedicated to researching, testing, and prototyping new tech and application options.
Participate and contribute to the overall success of our team. Each week the team meets to share wins, progress, and knowledge, as well as identify and solve issues at multiple levels (company, team, individual). Your full participation in this process is critical to ensure that we are operating as a cohesive, high-performance unit.
About You
We're looking for an individual who:
Is a problem solver through-and-through. Everywhere you look, you both (a) see problems to solve, and (b) see solutions and new ways of doing things that just haven't been done yet. You know how to think outside of the box, are willing to “go there” with new ideas and solutions that haven't been done before, and have the confidence to start building, testing, iterating, and making sh*t work.
Is a systems thinker. You understand both the big picture and how the functional components fit together, and have the ability to take a specific analysis outcome and generalize it to fit a wide range of scenarios through structure and sound system design.
Can fail fast, iterate, and learn. You're an independent, self-directed, learner who isn't afraid to “move fast and break stuff” knowing that failure is a prerequisite to success, ESPECIALLY in product development. You may not have traditional credentials, but what you do have is the ability to rapidly learn, adopt, test, and understand new languages, platforms, tools, and solutions.
Is a manager of one. Unlike working within a traditional firm, in this role you'll be in the driver's seat, managing your workflow and workload in order to meet the standard set of deliverables required for each client.
About Our Culture
We're fully remote, with team members and clients located all across the U.S. and have developed our own unique culture we call The SBS Way, within which we operate, evaluate performance, and make decisions using our core values as a guide:
Be Antifragile. Everything we do, good or bad, makes us better. And every experience is an opportunity for learning and continuous improvement.
Play The Long Game. We make decisions, to the best of our ability, in the long-term interest of our firm, our team, our clients, and our broader industry and community.
Embrace Technology. We welcome new technologies with open arms, and are always exploring, testing, and implementing them in the interest of enhancing both our internal capabilities and our client's outcomes.
Build and Trust The Process. Each member of the team is committed to building, following, and improving the processes we use to deliver exceptional results for our clients.
Act as A Team of Expert Knowledge Workers. We openly and willingly collaborate, communicate, and provide rapid, direct feedback in the interest of learning, improving and developing ourselves.
Working At SBS
What it's like working at our firm:
High flexibility. We believe in the ability of our team to determine the best way to complete their work. We measure outputs, not inputs. We don't have time sheets. We don't track hours. We don't pay attention to when and where our team works. Your schedule is yours to make.
High accountability. What we care about most is that we deliver on what we promise to our clients. In this respect, we measure and manage to our deliverable performance metrics and ensure each team member takes ownership over their accomplishment with a high level of quality that aligns with our core values
Great pay for great work. We pay based on the characteristics that matter: position (and its market value), level of mastery, and longevity with the firm. All of which aim to ensure each member of the team feels they are compensated well and can focus on great work.
Merit-based career progression. We have clearly established career tracks, performance benchmarks, and mastery levels set for all of our core positions. How quickly you progress is entirely under your control, with a quarterly review and bi-annual promotion consideration cycle in place to evaluate your progress.
Generous benefits. We offer a generous benefits package that includes medical, dental, and vision insurance enrollment; as well as an IRA match, tech stipend, 3 weeks of paid time off, and entry into our profit share bonus program after two years of service.
Personal and and team development. In addition to our overall continuous learning focus, we also provide support for personal development in the form of expense coverage for continuing education (books, courses, training, certifications, etc.) as well as experiential learning (brewery visits, industry events and conferences, etc.). Each year we also meet in person for an all-expenses-paid annual retreat as a team. No work. Lots of fun. Lots of client beer.
Job Requirements
The following basic requirements must be met:
Previous experience in SQL development and database management.
Previous experience building useful applications in scripting languages like VBA, Google Apps Script, Python, PHP, etc.
Can do effective cross-functional work in a remote environment.
Have crystal clear professional written and verbal communication skills.
Have exacting organizational standards and a calm and friendly attitude.
Available and responsive during normal business hours (9am-5pm Eastern Time, Monday-Friday).
Have a strong, consistent internet connection and a work environment conducive to video calls.
Preferred qualifications include:
Direct previous experience building data pipelines.
Direct previous experience building Airflow workflows and applications.
Experience building out and managing API connections.
Experience working with Quickbooks Online or similar accounting or finance platforms.
Experience using Podio or similar remote project management tools (e.g. Trello, Asana, etc.).
Next Steps
If the position, culture, values, and mission at Small Batch Standard sound like they're the right fit for you, please apply here.",64000,1 to 50 Employees,Company - Private,Financial Services,Accounting & Tax,2010,Unknown / Non-Applicable,Remote,13,data engineer,na,"['sql', 'r', 'python', 'go']",[],['excel'],[],[],[],,
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['sql', 'python']",[],[],[],['spark'],[],bachelor,0-2 years
Tripoint Solutions,4.5,Remote,Data Engineer (Remote),"Tripoint Solutions is seeking a Data Engineer to join our team.
The Data Engineer will be part of a team responsible for ensuring the success of a highly visible, results-driven federal client through the development of a cloud-based next generation system.
This position requires the applicant to parse disparate data sources, including structured and unstructured elements, to find the patterns and meaning in large quantities of data. The successful candidate will leverage machine learning as well as best of breed pipeline technology to process and store a variety of data elements.
Location: This position is eligible for fully remote work. Selected candidates living within a 25 miles radius of the NITAAC office in Rockville, MD will be required to come into the office once a week. The selected candidate must be currently located in, or willing to relocate to, a state supported by Tripoint Solutions corporate offices (AL, DC, FL, IL, LA, MD, MI, MN, MS, NJ, NC, PA, TN, TX, or VA).
The successful candidate will be accountable to:
Creating and maintaining optimal data pipeline architecture.
Assembling large, complex data sets that meet functional / non-functional business requirements.
Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Building the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Keeping data separated and secure across national boundaries through multiple data centers and AWS regions.
Strong interest to learn and stay up to date on relevant technologies, trends, industry standards and identify new ones to implement.
What you bring
Experience, Education & Training:
Bachelor's degree in computer science, Math, Analytics, Statistics, Informatics, Information Technology or equivalent quantitative field.
5 years of experience working in a Data Engineer or Data Scientist role.
Experience with cloud data services (AWS preferred).
Experience solutioning and applying Natural Language Processing (NLP) and or Machine Learning (ML) technologies
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience with Microsoft SQL, database development and design.
Experience building processes supporting data integration, transformation, data structures, metadata, dependency and workload management.
Demonstrated success in manipulating, processing and extracting value from large disconnected data sets.
Demonstrated accomplishments in designing, coding, testing and supporting data analytics and reporting solutions in a cloud environment.
Experience with object-oriented/object function scripting languages: Python, Java
Concept experience; information retrieval, search engine, document data extraction
Preferred experience with AWS cloud services: Textract, Comprehend, GlueMaker, Athena, Notebook
Working knowledge of message queueing, stream processing, and highly scalable ‘big data’ data stores.
Clearance Requirements:
Applicants selected may be subject to a government security investigation and must meet eligibility requirements for potential access to classified information. Accordingly, US Citizenship or Green Card is required.
What we offer
About Tripoint Solutions
We are technology innovators, partnered with state-of-the-art providers, such as AWS, ServiceNow, and UiPath, to drive digital transformation in the federal space. TPS teams are bringing automation and data science into areas of the government that are crying out for fresh tech—making positive impacts felt by tens of thousands of users, countless citizens, and all six branches of the military each day. Our Agile teams are responsible for envisioning, launching, and operating the massive data systems and analytics platforms used to manage $14.5B in government procurements and $200B in military real estate assets globally. At TPS, we apply the power of cloud technologies to help the government think smarter and function better—for everyone.
TPS Company Values
We value and respect each employee's dedicated work and unique contributions; as they directly impact who we are and what we do.
Your talent and innovative thinking bring leading-edge solutions to our customers.
Our success is driven by the dedication of our employees.
Employee-generated solutions have sustained our continued success and customer satisfaction
Benefit Offerings
Tripoint Solutions builds flexibility into health benefit plan choices, covers most of the monthly premiums, and helps employees build a career with impact through our generous professional development program.
We offer all full-time employees:
Medical, Dental, Vision benefits with a national provider network (company pays 100% of Vision and Dental premiums)
Flexible Spending and Health Savings Accounts (FSA & HSA)
Company-paid Life and Disability insurance including Short-Term, Long-Term, and Accidental
Paid-time off (PTO), accruing with each year of service, up to 20 days, plus 11 paid holidays
401(k) Retirement Plan - No waiting period to contribute and company makes 3% contribution of eligible pay in addition to annual profit-sharing contribution option
Eligibility to receive impact bonuses each quarter
Referral Program
Professional Development Reimbursement Program to pursue undergraduate, graduate, training, and certifications
Monthly transportation, parking, and cell phone service reimbursement
COVID-19 Related Information
Tripoint Solutions does not have a vaccination mandate applicable to all employees. However, to protect the health and safety of its employees and to comply with customer requirements, Tripoint Solutions may require employees in certain positions to be fully vaccinated against COVID-19. Vaccination requirements will depend on the status of the federal contractor mandate and customer site requirements. Furthermore, remote work arrangements are subject to change based on customer site requirements.
Tripoint Solutions is an Equal Opportunity Employer/Veterans/Disabled
Job Type: Full-time
Pay: $145,000.00 - $155,000.00 per year
Benefits:
401(k)
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Retirement plan
Tuition reimbursement
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
What cloud services have you worked with?
Do you have experience with Machine Learning or NLP?
Does the advertised salary align to your expectations?
US citizenship or green card is required. Do you meet this requirement?
This is a remote position (See description for details and requirements). Where are you located?
Are you willing to undergo a federal background check?
Education:
Bachelor's (Required)
Experience:
data scientist or data engineer role: 5 years (Required)
cloud services: 2 years (Required)
Microsoft SQL (development and design)?: 2 years (Required)
optimizing ‘big data’ pipelines, architectures and data sets: 2 years (Required)
AWS: 1 year (Preferred)
Python: 1 year (Required)
Java: 1 year (Required)
Work Location: Remote",150000,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,Unknown / Non-Applicable,Remote,10,data engineer,na,"['sql', 'java', 'python']",['aws'],[],[],[],[],bachelor,0-2 years
Angle Health,4.0,Remote,Data Engineer,"Changing Healthcare For Good
At Angle Health, we believe the healthcare system should be accessible, transparent, and easy to navigate. As a digital-first, data-driven health plan, we are replacing legacy systems with modern infrastructure to deliver our members the care they need when they need it. If you want to build the future of healthcare, we'd love for you to join us.

The Role
As a Data Engineer on the Strategy team at Angle Health, you will be part of an elite team of problem solvers tackling some of the hardest operational challenges across the business. You will be working closely with Technical Product Managers and other engineers in small teams embedded within functions across the company—from sales to operations to finance and more.

In partnership with operational stakeholders, your job will be to quickly understand business workflows, and design and implement technical, data-driven solutions to achieve the intended business outcomes. That may include architecting backend micro-services, building and maintaining robust data pipelines, standing up health metrics and automated alerts, and developing scalable technical solutions to support Angle Health's day-to-day operations.

Every day will be a new learning experience in this role and may span discussing architecture with fellow engineers, wrangling large-scale data, coding a custom web app or micro-service, or speaking with customers and executives.

This role is ideal for entrepreneurial engineers, applied data scientists, and creative, intellectually curious thinkers that want to solve high-value problems in healthcare.

This position may be based in San Francisco, New York City, Salt Lake City, or Remote.

We are currently trialing various titles for the same role. Please consider the following posted roles as the same position: Deployed Engineer, Software Engineer, Business Operations, and Data Engineer. Please note these are the same, so if you have already applied for one position, there is no need to reapply for the others.
What We Value
A strong engineering background in computer science, software engineering, data science, mathematics, or similar technical field is required for this role
Proficiency in programming languages (e.g. Python, SQL, Java, TypeScript/JavaScript, or similar) and data engineering frameworks
A highly analytical mindset and an eagerness to build technical solutions to complex business problems
High attention to detail and intellectual curiosity—you're not satisfied with surface-level answers. You want to dive into the data, the ""how,"" and the ""why"" because ""the way it's always been done"" is not always the way it should be done
Low ego—the outcome matters more than who gets the credit
Demonstrated ability to collaborate effectively in teams of technical and non-technical individuals
Highly organized with an ability to multitask, problem solve, and balance competing priorities in a rapidly changing environment
Because We Value You:

Competitive compensation and stock options
100% company paid comprehensive health, vision & dental insurance for you and your dependents
Supplemental Life, AD&D and Short Term Disability coverage options
Discretionary time off
Opportunity for rapid career progression
Relocation assistance (if relocation is required)
3 months of paid parental leave and flexible return to work policy (after 10 months of employment)
Work-from-home stipend for remote employees
Company provided lunch for in-office employees
401(k) account
Other benefits coming soon!

Backed by a team of world class investors, we are a healthcare startup on a mission to make our health system more effective, accessible, and affordable to everyone. From running large hospitals and health plans to serving on federal healthcare advisory boards to solving the world's hardest problems at Palantir, our team has done it all. As part of this core group at Angle Health, you will have the right balance of support and autonomy to grow both personally and professionally and the opportunity to own large parts of the business and scale with the company.

Angle Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. Angle Health is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities.",112889,1 to 50 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2019,Unknown / Non-Applicable,Remote,4,data engineer,na,"['sql', 'java', 'python']",[],[],[],[],[],,
Data Crunch Corp,4.0,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'python']",['aws'],['tableau'],[],[],['docker'],,
LOVEFOODIES INC,4.0,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",54000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['sql', 'python']",[],['tableau'],[],[],[],,2-5 years
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['sql', 'python']",[],[],[],[],[],master,
"Double Line, Inc.",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",85882,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['sql', 'python']","['azure', 'aws']","['tableau', 'power bi']",[],[],[],,
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],[],,
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'java', 'scala', 'python']","['databricks', 'azure']",[],[],['spark'],[],bachelor,+10 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['nosql', 'java', 'scala', 'python']",[],[],[],['spark'],['docker'],,
Kaizen Dynamics,4.0,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],"['tableau', 'excel']",[],[],[],master,0-2 years
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['sql', 'java', 'nosql', 'python']","['azure', 'aws', 'google cloud']",[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",[],,+10 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['sql', 'scala', 'python']","['databricks', 'azure', 'aws', 'google cloud']","['tableau', 'power bi']",[],"['spark', 'hadoop']",['docker'],bachelor,5-10 years
AgileEngine,5.0,Remote,Senior/Lead Data Engineer,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Work collaboratively with other engineers, architects, data scientists, analytics teams, and business product owners in an agile environment
Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools
Design and build robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Must haves
Bachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience
6 years of experience with Python or Java
4+ years of experience in building data lake, cloud data platform leveraging cloud (GCP/AWS) cloud native architecture, ETL/ELT, and data integration
3 years of development experience with cloud services ( AWS,GCP,AZURE) utilizing various support tools (e.g. GCS, Dataproc, Cloud Data flow, Airflow(Composer), Kafka , Cloud Pub/Sub)
Expertise in developing distributed data processing and Streaming frameworks and architectures (Apache Spark, Apache Beam, Apache Flink, )
In-depth knowledge of NoSQL database technologies (e.g. MongoDB, BigTable, DynamoDB)
Expertise in build and deployment tools – (Visual Studio, PyCharm, Git/Bitbucket/Bamboo, Maven, Jenkins, Nexus, )
5 years of experience and expertise in database design techniques and philosophies (e.g. RDBMS, Document, Star Schema, Kimball Model)
5 years of experience with integration and service frameworks (e.g API Gateways, Apache Camel, Swagger API, Zookeeper, Kafka, messaging tools, microservices)
Expertise with containerized Microservices and REST/GraphQL based API development
Experience leveraging continuous integration/development tools (e.g. Jenkins, Docker, Containers, OpenShift, Kubernetes, and container automation) in a Ci/CD pipeline
Advanced understanding of software development and research tools
Attention to detail and results-oriented, with a strong customer focus
Ability to work as part of a team and independently
Problem-solving and technical communication skills
Ability to prioritize workload to meet tight deadlines
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
Our team of data experts and engineers has helped create a supply chain management platform used by Forbes 500 enterprises and featured by Gartner. We’ve accompanied the platform from an early prototype to a full-fledged solution based on big data, AI, and machine learning. Data quality is integral to our current engagement with this project as we’re expanding the platform functionality.
AgileEngine has also been working on data and ML solutions in multiple industries and verticals like telecom, FinTech, energy efficiency, and more. Our work with our clients’ data has contributed to patented software products, Nasdaq-grade fraud detection, and products used by Deloitte and T-Mobile.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",112889,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable,Remote,13,data engineer,senior,"['nosql', 'java', 'python']","['azure', 'aws']",[],"['dynamodb', 'mongodb']","['kafka', 'flink', 'spark']",['docker'],master,2-5 years
"Dovenmuehle Mortgage, Inc.",2.6,"San Francisco, CA",Data Engineer,"Data Engineer
DMI Software, the San Francisco branch of Dovenmuehle Mortgage, Inc, the leading sub-servicer of mortgage loans in the United States, is looking for a talented and enthusiastic data engineer. We work exclusively in Software Development. Our growing office offers the feel of a startup with the backing and security of a long-established company. We aspire to create elegantly scalable products while fostering the continued growth of each team member. The ideal candidate will have 5+ years relevant experience, including Hadoop Ecosystem or similar, and with a scripting language.

Here we believe that the best software is created by an eclectic set of voices, and we strive to nurture an environment rich in differing opinion, belief, and background. Only in this way can we develop revolutionary products capable of meeting the varied needs of an increasingly interconnected world.

What You’ll Be Doing:
Design, implement, automate, and maintain large-scale enterprise ETL processes
Evolve data model and schema based on business and engineering needs
Oversee systems tracking data quality and consistency
Collaborate with data analysts to bridge business goals with data delivery

Requirements:
5+ years data engineering experience
Highly experienced using Python, SQL and and Hadoop
Excellent communication, analytical and problem-solving skills
Keen attention to detail while keeping an eye toward the big picture
You are comfortable with the nuts and bolts of systems programming in the Linux environment (shell/bash scripting)
Experience working in an Agile environment
Excellent presentation and communication skills
Experience profiling, debugging, tracing, and or parallelizing/optimizing Python code
Ideal candidate is one who can adapt and adopt to our existing architectures while also making impactful improvements and suggestions.

Job Type: Full-time",135927,1001 to 5000 Employees,Company - Private,Financial Services,Banking & Lending,1844,Unknown / Non-Applicable,CA,179,data engineer,na,"['sql', 'shell', 'python']",[],[],[],['hadoop'],['bash'],,+10 years
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']","['aws', 'redshift']",[],['dbt'],"['spark', 'hadoop']","['gitlab', 'docker']",,2-5 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],[],,0-2 years
BOTG LLC,4.0,"Chicago, IL",Data Engineer,"We are looking for a Data Engineer in Chicago, IL (Hybrid) for a direct-hire position.
Job Description:
Position: Data Engineer - Centralized Data Science and Analytics (CDSA)
Location: Chicago, IL (Hybrid)
Duration: Direct-hire position
Client: Direct Client
Note: This is a W2 direct-hire role. Looking for candidates who are open to work independently on W2.
Requirements:
· Experience building and optimizing ""big data"" data pipelines, architectures and data sets.
· Working knowledge of message queuing, stream processing and highly scalable ""big data"" data stores.
· Advanced working SQL knowledge and experience working with cloud and relational databases.
· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
· Experience building processes supporting data transformation, data structures, metadata, dependency and workload management.
· A successful history of manipulating, processing and extracting value from large, disconnected datasets.
· Experience using the following software/tools:
· Relational SQL and NoSQL databases, including Postgres.
· Data pipeline and workflow management tools.
· Azure cloud services.
· Object-oriented/object function scripting languages: Python, PySpark Java, C++, R/RStudio/RSpark.
· CI/CD systems.
· Strong understanding across cloud and infrastructure components (server, storage, network, data, and applications) and ability to deliver end to end cloud infrastructure, architectures, and designs.
· Knowledge and implementation of enterprise scale cloud security platforms and tooling.
· Experience with enterprise applications, solutions, and data center infrastructures.
· Bachelor's degree in computer science or similar field; master's degree a plus.
· Exceptional product, project and client management skills.
· Azure, AWS or any other cloud/data engineering certifications are preferred.
Job Type: Full-time
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Big data: 5 years (Required)
Advanced SQL: 5 years (Required)
Cloud: 3 years (Required)
CI/CD: 3 years (Preferred)
NoSQL: 2 years (Required)
Work Location: Hybrid remote in Chicago, IL 60606",85894,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,"['java', 'nosql', 'sql', 'r', 'python']","['azure', 'aws']",[],[],[],[],bachelor,2-5 years
TheHive,4.0,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['sql', 'python']","['snowflake', 'azure']",[],"['sql server', 'snowflake']",['kafka'],[],,
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['sql', 'scala', 'python']","['databricks', 'azure']",[],[],['spark'],[],,
CapitalTech Solutions,4.5,Remote,Data Engineer,"Job Description:
12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
Complete Description:
Require the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals:
(1) Establish a data governance program,
(2) Perform a comprehensive data gap analysis,
(3) Design a master data architecture,
(4) Create a data warehouse for all data assets,
(5) Develop a front-end for program staff to quickly access workforce information and visualize program status,
(6) Create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities
(7) Foster relations with other agencies and improve inter-agency data integration.
The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff.
Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.
Develop and maintain an understanding of the data landscape including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.
Support the Data Management Project team to develop and maintain data quality controls.
Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.
Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.
Support the data stewards to troubleshoot and resolve data issues.
Support business users to obtain requirements for enhancements and/or new analytic assets.
Assist in the Development of data asset training and documentation.
Participate in the development and implementation of a data standard.
Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.
Job Types: Full-time, Contract
Pay: $66.00 - $74.00 per hour
Schedule:
8 hour shift
Experience:
in SQL, Python, R, JavaScript, JSON: 10 years (Preferred)
Agile Testing, Automation Testing, Black-box Testing: 10 years (Preferred)
Windows and Linux: 10 years (Preferred)
of BI tool architecture, Tableau: 9 years (Required)
Work Location: Remote",126000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1999,$25 to $100 million (USD),Remote,24,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],[],[],[],master,5-10 years
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['sql', 'python']","['aws', 'redshift']",[],[],[],[],,
Invictus Data,4.0,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500,,,,,-1,,CA,-1,data engineer,senior,"['sql', 'scala', 'python']","['azure', 'aws']",[],['hive'],"['kafka', 'hadoop', 'spark']",[],,2-5 years
ITExpertUS,2.8,"Chicago, IL",Data Engineer,"Client: Agilisium
Role: Data Engineer
Location: Chicago, IL – Hybrid role – Need to be onsite once/twice in a month.
Type: Long term contract
Job Description
Title: Data Engineer
We are looking for a savvy Data Engineer to join our growing team. The hire will be responsible for expanding and optimizing our data and pipeline architecture and data flow and collection. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. In addition, the Data Engineer will collaborate and support software developers, implementation architecture, and data engineers on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. In addition, they must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
Responsibilities:
Because we work on a programmatic leading edge of many technologies, we need someone who is a creative problem solver, resourceful in getting things done, and can shift productively to or from working independently and collaboratively. This person would also take on the following responsibilities:
Process unstructured data into a form suitable for analysis.
Support the business with ad hoc data analysis and build reliable data pipelines.
Implementation of best practices and IT operations in mission-critical tighter SLA data
pipelines using Airflow
Query Engine Migration from Dremio to Redshift.
We leverage: Multiple AWS Data & Analytic Services(e.g.,Glue, Kinesis, S3) , SQL (e.g.,
PostgreSQL, Redshift, Athena); NoSQL (e.g., DocumentDB, MongoDB); Kafka, Docker, Spark(AWS EMR and DataBricks), Airflow, Dremio, Qubole, etc.
We use AWS extensively, so experience with AWS cloud and AWS Data & Analytics certification will help you hit the ground running.
Business Skills for Data Engineers:
Creative Problem-Solving: Approaching data organization challenges with a clear eye on what is important; employing the suitable approach/methods to make the maximum use of time and human resources.
Effective Collaboration: Carefully listening to management, data engineers, and architects to establish their needs.
Intellectual Curiosity: Exploring new territories and finding creative and pragmatic ways to solve data problems.
Industry Knowledge: Understanding and perspective of how industry functions and how data can be collected, analyzed, and utilized for value creation; maintaining flexibility in the face of big data developments.
Skills and Qualifications:
3 to 5 years of real-world Data Engineering experience.
Programming experience, ideally in Python and other data engineering languages like Scala
Programming knowledge to clean structure and semi-structure datasets.
Experience processing large amounts of structured and unstructured data. Streaming data experience is a plus.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Background in Linux
Build the infrastructure required for optimal extraction, transformation, and loading of data
from various data sources using SQL and other cloud ‘big data’ technologies like DataBricks, Snowflake, Dremio, and Qubole.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience creating a platform on which complex data pipelines are built using orchestration tools like Airflow, and Astronomer.
Experience with real-time sync between OLTP and OLAP using AWS technologies like real- time sync between AWS Aurora and AWS Redshift.
Job Type: Full-time
Salary: Up to $68.00 per hour
Schedule:
8 hour shift
Work Location: On the road
Speak with the employer
+91 6305048577",122400,501 to 1000 Employees,Company - Private,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,"['sql', 'python', 'scala', 'nosql']","['databricks', 'snowflake', 'aws', 'redshift']",[],"['postgresql', 'snowflake', 'mongodb']","['kafka', 'spark']",['docker'],,5-10 years
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],[],master,
Ascent Solutions,4.0,Remote,Hadoop Big Data Engineer,"Hadoop Big Data Engineer
Remote
Must Haves • Strong experience in Big Data, Cloudera Distribution 7.x, Cloud migration, RDBMS • Strong Experience with Amazon EMR/Databricks/Cloudera CDP • 4-5 experience building data pipelines using Hadoop components Sqoop, Hive, Solr, MR, Impala, Spark, Spark SQL., HBase. • 4-5 years of programming experience in Python, Java and Scala is must. • Strong experience with REST API development using Python frameworks (Django, Flask etc.) • Micro Services/Web service development experience using Spring framework is highly desirable.
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'java', 'scala', 'python']",['databricks'],[],['hive'],"['spark', 'hadoop']",[],,5-10 years
IntelliBridge LLC,3.9,"McLean, VA",Data Engineer,"Title: Data Engineer
Location: Permanent remote role
Clearance: Not required: Start date not contingent on a having or completion of a clearance, however one could be offered upon starting for future programs
Overview:
IntelliBridge is seeking a Data Engineer to collaborate with technical and non-technical data and development team members to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of analytics that provide help ensure national security. You'll be able to gain experience in designing cloud architectures while providing critical support to the client's mission. You will be responsible for designing and building smart data pipelines that are secure, robust, and alerting. You will also create innovative ways to combine disparate data sources and build integrated datasets for advanced analytics.
As a direct employee of IntelliBridge, you would receive a benefit package that includes health/dental/vision insurance coverage, 401K with company match, PTO & paid holidays, and annual tuition/training assistance. For more information, please visit our website.
Responsibilities/Duties:
Build and maintain the infrastructure to support integration, extraction, transformation, and loading (ETL) of data from a wide variety of data sources, such as relational SQL and NoSQL databases, and other platform APIs
Design data pipelines that are robust and secure including pipeline monitoring and alerting mechanisms
Create innovative ways to orchestrate data ETL processes
Guide and support the implementation of new data engineering solutions to enable adoption and growth
Integrate disparate data sources into powerful datasets for advanced analytics
Recommend tools and capabilities based on understanding the current environment and knowledge of various on-premises, cloud based, and hybrid capabilities/technologies
Monitor existing metrics, analyze data, and lead partnership with other Data and Analytics personnel to identify and implement system and process improvements
Develop processes to convert aggregated data from teams, collection tools, and dashboards
Configure and manage data analytic frameworks and pipelines using databases and tools
Develop Python packages to improve application capabilities
Apply distributed systems concepts and principles such as consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms
Administrate cloud computing and CI/CD pipelines to include Amazon Web Service (AWS)
Investigate legacy code to determine areas of improvement and automation
Required Qualifications:
Excellent verbal and written communications
Bachelor’s Degree in a STEM filed or Master’s Degree in Operations Research, Industrial Engineering, Applied Mathematics, Statistics, Physics, Computer Science, or related fields
5+ years of experience with Python, SQL, Unix(Linux), and handling semi-structured data (JSON)
3+ years of experience with Elasticsearch, Logstash, and Kibana (ELK stack)
3+ years of experience with Amazon Web Services (AWS) or other cloud provider
Proficient in Docker
Proficient in Agile Development
Proficient in Git Operations
Experience understanding requirements, analyzing data, discovering opportunities, addressing gaps and communicating them to multiple individuals and stakeholders
Demonstrated expertise in technical data engineering on integrating complex applications, systems, software, and project activities and integrating them into cloud-based resources
General knowledge in machine learning for building efficient and accurate data pipelines that occur for downstream users, such as for data scientists to create the models and analytics that produce insight
Preferred Qualifications:
Organizational skills and a love of documentation
Experienced in Airflow
Experience with demonstrated strength in data lake/warehouse technical architecture, infrastructure components, and ETL/ELT pipelines
Experience with geo-spatial data
Experience with deployments via Kubernetes
Experience with configuring and aggregating logs for data analysis using Splunk or ELK solutions
Experience with developing and managing machine images or templates to automate cloud deployments
About Us:
IntelliBridge delivers IT strategy, cloud, cybersecurity, application, data and analytics, enterprise IT, intelligence analysis, and mission operation support services to accelerate technical performance and efficiency for Defense, Civilian, and National Security & Federal Law Enforcement clients.",92898,501 to 1000 Employees,Company - Private,Government & Public Administration,National Agencies,-1,Unknown / Non-Applicable,VA,-1,data engineer,na,"['sql', 'python', 'nosql']",['aws'],[],['elasticsearch'],[],['docker'],bachelor,+10 years
Oddball,4.6,Remote,Data Engineer,"Oddball believes that the best products are built when teams understand and value the things they are working on. We value learning and growth and the ability to make a big impact at a small company. We believe that we can make big changes happen and improve the daily lives of millions of people by bringing quality software to the federal space.
As a Database Engineer you will be responsible for helping to develop production-quality analytics tools, utilizing existing data sources to empower stakeholders to inform research and development. Act as a liaison between engineers and analysts. Implement and enforce standards and procedures to ensure data is managed consistently and properly integrated.
Experience Requirements
5+ years of proven data and performance engineering experience.
A proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs.
Experience with system analysis, data analysis or programming, using a variety of computer languages and procedures.
Experience working in Agile environments.
Experience in developing, analyzing, and presenting data models.
Knowledge of data management, data standardization, and data governance.
Ability to analyze source data for potential data quality issues.
Expert in SQL and/or SQL based languages.
Create supporting documentation, such as metadata and diagrams of entity relationships, business processes, and process flow.
Create plans, test files, and scripts for data warehouse testing, ranging from unit to integration testing.
This is a Remote position. This is a salaried role. Oddball offers both a tech, and a continuing education stipend.
Must be located within the United States.
Clearances
Ability to obtain low-level federal clearance is required.
Oddball is an Equal Opportunity Employer and does not discriminate against applicants based on race, religion, color, disability, medical condition, legally protected genetic information, national origin, gender, sexual orientation, marital status, gender identity or expression, sex (including pregnancy, childbirth or related medical conditions), age, veteran status or other legally protected characteristics. Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Work Location: Remote",112889,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,['sql'],[],[],[],[],[],bachelor,5-10 years
MARVEL TECHNOLOGIES INC,3.7,Remote,Data Engineer,"Primary Skills
SCALA, SPARK, SQL, HADOOP, AWS, HIVE, Spark SQL, HIVE QL, CICD (Continuous Integration/Continuous Delivery), VCS (GIT HUB)
Coding in Scala
Designing in of HADOOP ecosystem
Hands-on experience on AWS tools like EMR, EC2
Hands-on experience of SQL in Big Data: SQL, Spark SQL, Hive QL
Proficient in working with large data sets and pipelines
Proficient with workflow scheduling / orchestration tools
Well versed with CICD process and VCS
Databricks is a big PLUS
Job Types: Full-time, Contract
Pay: $50.00 - $58.00 per hour
Schedule:
8 hour shift
Monday to Friday
Experience:
Spark: 4 years (Required)
Scala: 4 years (Required)
Hadoop: 3 years (Required)
Aws: 3 years (Required)
Hive: 3 years (Required)
CI/CD, VCS: 3 years (Required)
Databricks: 1 year (Required)
Work Location: Remote",97200,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,$5 to $25 million (USD),Remote,-1,data engineer,na,"['sql', 'scala']","['databricks', 'aws']",[],['hive'],"['hadoop', 'spark']",[],,0-2 years
Grid,4.0,"Oakland, CA",Data Engineer,"Today’s financial system is built to favor those with money. Grid’s mission is to level that playing field by building financial products that help users better manage their financial future. The Grid app lets users access cash, build credit, spend money, file their taxes and lots, lots more.

Grid is a fast growing team that’s deeply passionate about making a difference in the lives of millions. We’re solving huge problems and believe that a merit driven culture allows every team member to play a big role. Join our growing team in our Bay Area headquarters or additional offices across the US.

Our Data Engineers are responsible for converting the data surrounding a customer's financial life into tools that make our customers smarter and better equipped financially for the future. We value making intelligent decisions backed by good data and tools. We're looking for people who share our values, particularly if you have experience analyzing, processing, and learning from large data sets.

We’re looking for a seasoned data engineer who can help us lay the foundation of an exceptional data engineering practice. The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions. You will be writing code, setting style guides and collaborating cross-functionally with product, engineering and leadership.
Problems we work on:
Analytics: Collect all the data for a user into tools that help our customers
Machine Learning: Using a variety of techniques to reach better insights
Data Processing: Managing data & statistics using scalable and efficient technologies
Visualization: Envision our data as beautiful graphs and tools that allow customers to explore their data & ask their own questions
Risk: Analyze data for anomalous patterns and build tools that allow us to find bad actors quickly
We Practice:
Open collaboration
Code reviews
Testing
Agile development
We Use:
Go
Python
MySQL
Google Cloud Platform
Kubernetes
Kubeflow
Docker
Google Pubsub
BigQuery
Firebase
We’re looking for Engineers to:
Design and implement platform services, frameworks and ecosystems
Build a scalable, reliable, operable and performant big data workflow platform for data scientists/engineers, AI/ML engineers, and product/operation team members
Drive efficiency and reliability improvements through design and automation: performance, scaling, observability, and monitoring
Requirements:
Strong programming skills with Python
Strong programming skills with a typed programming language, such as Java, Scala, Go, etc.
Disciplined approach to development, testing, and quality assurance
Excellent communication skills, capable of explaining highly technical problems in English
Understand data processing and ETL, hands on experience building pipelines and using frameworks such as Hive hdfs, Presto, Spark, etc.
Really strong candidates may have:
Really strong candidates may have:
Actively contribute to open source software
Worked with a strong, lean-based development environment
Previous work experience in a start-up environment
Ability to recognize the right tool for the right situation/problem.
Will have strong programming skill in Go",112815,,,,,-1,,CA,-1,data engineer,na,"['java', 'go', 'python', 'scala']",['google cloud'],[],"['hive', 'mysql']",['spark'],['docker'],,
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['sql', 'python']","['databricks', 'azure']",[],[],[],[],,
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']","['aws', 'redshift']",[],"['dynamodb', 'oracle']","['kafka', 'spark']",[],bachelor,0-2 years
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],[],,
"Second Wave Delivery Systems, LLC",4.0,Remote,Data Engineer,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker",112889,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020,Unknown / Non-Applicable,Remote,3,data engineer,na,"['sql', 'java', 'scala', 'python']",['google cloud'],['looker'],[],['spark'],[],,
FlexIT Inc,4.0,"Beaverton, OR",Data Engineer,"FlexIT client is looking for a Data Engineer 12 months contract in Beaverton, Oregon.
Looking for local candidates to work on site.
Top skills: Python, SQL , AWS, Spark",106334,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['sql', 'python']",['aws'],[],[],['spark'],[],,
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100714,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['nosql', 'java', 'scala', 'python']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'flink', 'spark']",[],,
Metrohm Spectro,4.0,"Plainsboro, NJ",Data Engineer,"Metrohm Spectro is an advanced mobile spectroscopic instrumentation leader, developing, manufacturing, and servicing state-of-the-art analytical devices, including portable and handheld Raman analyzers.We provide solutions for the pharmaceutical, biomedical, safety and security, chemical, and academic research industries. We are constantly growing with new products and new opportunities, and are always looking for talented, dedicated employees to join us and grow together as a team.
With the fast growth of our business, we have an immediate vacancy for a full-time Data Engineer for our Plainsboro, NJ location.
Job Description
In this role, you will take responsibility for developing and maintaining databases within software products. You will be required to have hands-on problem solving, from the upkeep and generation of database, to data validation as well as the capability of data processing and analysis, and will be able to perform data processing algorithm validation with the knowledge of data science. To excel in this role, you need to be very organized with a fine eye for detail, and openness to learn new skills to meet growing business needs.
Education
· Bachelor’s of Science degree from an accredited university or college in chemistry, physics, mathematics or computer science.
Experience:
· High-level proficiency in Microsoft Excel or other automated data management tool.
· Experienced in database programming and familiar with all popular database types. Good understanding of MySQL is a plus;
· Knowledge in MATLAB, R, Python or SAS tools for data processing and analysis;
· Knowledge in AI/machine learning and data mining basics;
· Knowledge in C/C++ programming for data processing algorithm;
· High-level proficiency in Microsoft Excel or other automated data management tool;
· Knowledge in Network/Cloud infrastructure will be a plus.
ROLE AND JOB RESPONSIBILITIES
· Develop and maintain database for cross-platform software implementation on all BWTEK spectroscopic products.
· Assist in data process and analysis algorithm design and validation.
· Collaboration with entire software team for product enhancement and new product.
Job Type: Full-time
Application Question(s):
What are your salary expectations?
Work Location: Plainsboro, NJ - on site
Can you commute to this location?
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
5x8
8 hour shift
Monday to Friday
Ability to commute/relocate:
Plainsboro, NJ 08536: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Will you need sponsorship to work in US?
Work Location: In person",112889,,,,,-1,,NJ,-1,data engineer,na,"['r', 'python']",[],['excel'],['mysql'],[],[],bachelor,
Manufacturers Bank,3.2,"Los Angeles, CA",Data Engineer,"GENERAL SUMMARY:
Manufacturers bank is looking for a Data Engineer with strong software engineering backgrounds, a ""platform engineering mindset"" and deep passion for data. This Engineer will help to shape the technical direction of the data domain within the organization. The Engineer will also partner closely with internal business units and external vendors to develop a high performance, reliable and scalable data environment for the bank.

PRINCIPAL DUTIES & RESPONSIBILITIES:
Develop and automate high-performance data processing systems and visualization to ensure reliability and meet critical business requirements.
Design, develop, implement, document and support Data & Analytics systems which adhere to the industry best practices.
Lead data engineering projects, and overall strategy for data governance, security, privacy, quality, and retention.
Triage and resolve issues reported by Risk, Operations, and other business stakeholders.
Mentor junior engineers and continue promoting data engineering and analytics tooling & standards.",115908,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),CA,-1,data engineer,na,[],[],[],[],[],[],,
Edrstaffing,4.0,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],['docker'],,2-5 years
Govini,3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.",88151,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable,PA,12,data engineer,na,['sql'],[],[],[],[],[],master,5-10 years
Airbus Americas,4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.",92046,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD),GA,53,data engineer,na,"['java', 'r', 'python']",[],[],[],['spark'],[],bachelor,2-5 years
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],"['databricks', 'azure']",['power bi'],[],[],[],,5-10 years
Arthur Grand Technologies Inc,4.8,"Atlanta, GA",AWS Data Engineer,"Role: AWS Data Engineer
Location: Atlanta, GA (Meet customer Monthly once post client selection & onboarding)
JD for AWS Data Engineer
Experience with the core AWS services, plus the specifics mentioned in this job description.
Experience with Docker . Having used ECS , SNS/SQS, Athena, Lambda,S3, RDS and layers in the past is a plus.
Proficiency in at least in Python, Java
Strong notions of security best practices (e.g. using IAM Roles, KMS, etc.).
Experience with monitoring solutions such as CloudWatch, Cloud Trail.
Previous exposure to large-scale systems design.
Knowledge of writing infrastructure as code (IaC) using CloudFormation or Terraform.
Experience with building or maintaining cloud-native applications.
Past experience with the serverless approaches using AWS Lambda is a plus. For example, the Serverless Application Model (AWS SAM).
Job Types: Full-time, Contract
Schedule:
10 hour shift
8 hour shift
Work Location: Remote",94994,51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable,GA,11,data engineer,na,"['java', 'python']",['aws'],[],[],[],"['terraform', 'docker']",,
Nursa,4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!",89485,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable,UT,5,data engineer,na,"['sql', 'python']","['aws', 'redshift']",[],[],[],[],,+10 years
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['databricks', 'azure', 'aws']",[],[],[],[],,0-2 years
IBR (Imagine Believe Realize),4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",112889,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,Remote,16,data engineer,senior,"['java', 'r', 'python']","['azure', 'aws', 'redshift']",['tableau'],['redis'],"['spark', 'hadoop']",[],bachelor,
Globaleur,4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information
Deploy sophisticated analytics programs, machine learning, and statistical methods
Ensure compliance with data governance and security policies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java
Have working experiences in e-commence, travel, marketing domain is a plus
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.",119136,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017,Unknown / Non-Applicable,CA,6,data engineer,senior,"['java', 'nosql', 'sql', 'go', 'python']","['azure', 'aws']",['tableau'],['mongodb'],[],[],bachelor,
Lendem Solutions,4.0,"Plano, TX",Data Engineer,"LENDEM Solutions is looking at add a Data Engineer to our business!
CORE COMPETENCIES
Ability to thrive in a dynamic and fast-paced environment, drive change, and collaborate effectively with a variety of individuals and teams
Strong analytical and problem-solving skills
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
PRINCIPAL DUTIES
Shredding and parsing data to extract meaningful information
Preparing historical and live data for data studies to identify trends and patterns
Performing adhoc analysis to answer specific business questions and provide insights
Working with relational databases to model and query complex data relationships
Understanding and working with MySQL data in several different data environments
Mining consumer loan data utilizing tools such as SQL, Python, R, or other comparable data mining tools.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Engineering, or a related field Master’s degree preferred
3+ years of experience as a data engineer or similar role
REQUIRED SKILLS, ABILITIES, SOFT SKILL FACTORS
· Strong experience in ETL processes, data modeling, and data warehousing
Experience working with graph databases, such as Neo4j or Apache Cassandra
Expertise in programming languages such as SQL, Python
Familiarity with big data technologies, such as Hadoop, Spark, or Kafka
Ability to analyze and manipulate large and complex data sets
Strong problem-solving skills and the ability to work independently and as part of a team
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
If you are a data engineer with a passion for problem-solving and a strong background in ETL processes, data modeling, and graph databases, we encourage you to apply for this exciting opportunity. We offer a competitive salary, comprehensive benefits, and a dynamic work environment where you can continue to grow and develop your skills.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Plano, TX 75024: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 5 years (Preferred)
SQL: 5 years (Preferred)
Work Location: Hybrid remote in Plano, TX 75024",112889,,,,,-1,,TX,-1,data engineer,na,"['sql', 'r', 'python']",[],[],"['neo4j', 'mysql']","['kafka', 'spark', 'hadoop']",[],bachelor,5-10 years
Okaya Corp,4.0,"Mahwah, NJ",Azure Data Engineer,"Job Title: Azure Data Engineer
Location: Mahwah, NJ
Duration: Full Time
Skills Required:
Azure data factory, data bricks, data lake, automation, and performance optimization of ETL
Strong Hands-on experience in ADF, data bricks, data lake, power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end-to-end CI/CD implementation or Devops process in Data & Analytics context.
Design, plan, develop and update technical docs & BI solutions.
Understand the requirements and define the data load strategy for data refresh.
Create, debug, troubleshoot and deploy solutions.
Work on ETL design
Designing and optimization of ETL Process using ADF
Implementing end-to-end automated ETL processes and monitoring of those processes using various options using Azure
Experience in Azure data factory, data bricks, data lake, automation, and performance optimization of ETL
Job Type: Full-time
Work Location: One location",112889,,,,,-1,,NJ,-1,data engineer,na,[],['azure'],['power bi'],[],[],[],,
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'go', 'python']","['databricks', 'snowflake', 'aws']",['ssis'],['snowflake'],['spark'],[],,
PSRTEK,4.6,"Mount Laurel, NJ",Lead Data Engineer,"AR# 226054
Role: Lead Data Engineer /Databricks (On-site)
Location: Mt. Laurel, NJ
Full-time
Visa status: GC/USC
Must have skills:
Databricks, Python, RDBMS, PowerShell scripting, data warehouse
Detailed JD:
Experience in ETL/Pipeline Development using tools such as Azure Databricks/Apache Spark and Azure
Data Factory with development expertise on batch and real-time data integration
Experience in programming using Python
RDBMS knowledge and experience in writing the Store Procedures
Experience in writing bash and Power shell scripting.
Experience in data ingestion, preparation, integration, and operationalization techniques in optimally addressing the data requirements
Experience in Cloud data warehouse like Azure Synapse, Snowflake analytical warehouse
Experience with Orchestration tools, Azure DevOps, and GitHub
Experience in building end to end architecture for Data Lakes, Data Warehouses and Data Marts
Experience in relational data processing technology like MS SQL, Delta Lake, Spark SQL, SQL Server
Experience to own end-to-end development, including coding, testing, debugging and deployment
Extensive knowledge of ETL and Data Warehousing concepts, strategies, methodologies
Experience working with structured and unstructured data
Familiarity with Azure services like Azure functions, Azure Data Lake Store, Azure Cosmos
Ability to provide solutions that are forward-thinking in data and analytics
Job Type: Full-time
Salary: $120.00 - $130.00 per year
Schedule:
8 hour shift
Experience:
Data Warehouse: 10 years (Required)
Python: 10 years (Required)
PowerShell: 10 years (Required)
Data Bricks: 10 years (Required)
RDBMS: 10 years (Required)
Work Location: On the road
Speak with the employer
+91 609-917-9952",121211,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,NJ,-1,data engineer,senior,"['sql', 'shell', 'python']","['databricks', 'azure', 'snowflake']",[],"['sql server', 'snowflake']",['spark'],['bash'],,0-2 years
"Pomeroy Technologies, LLC.",3.0,"Cleveland, OH",Data Engineer,"The Data Engineer is responsible for the development, integration, and implementation of ETL Feeds in support of various modern architecture projects. This role requires hands-on knowledge of modern Integrations Platform solutions, file, and database ETL, and XML transformation. The incumbent will act as a technical resource, participating in the successful design, development, unit testing, implementation, and support of new and existing ETL integrations.
Essential Functions
Must be able to acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design.
Working with manager, gathering business requirements for the development of new applications.
Perform programming assignments based on established standards, methods, and best practices.
Develop ETL solutions as assigned in supporting new or updated business solutions and requirements.
Help business users develop functional requirements for integration.
Provide support during user acceptance testing.
Prepare documentation for code changes.
Code ETL feeds, and conduct tests to verify correct program functionality, reliability, and data integrity.
Maintain and modify ETL feeds; make approved changes by amending application documentation, developing detailed programming logic, and coding changes.
Provide support and resolution for application problems and issues.
Provide user and system documentation for operational and technical support.
Assist in the discovery and investigation of production problems as required.
Support ongoing daily use of ETL environments and Integrations platforms.
Provide on-call support for critical application problems and issues.
Analyze program performance and take action to correct deficiencies based on consultation with clients and approval of supervisor.
required.Work outside the standard office 7.5 hour workday may be
Experience Requirements
2+ years of IT experience.
1+ years of experience building ETL feeds using any ETL tool (Talend, Informatica, etc.)
Knowledge of the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment, and post-production support.
Understanding of XML, JSON, XSD, WSDL, REST, and SOAP APIs.
Experience in developing SQL programs.
Job Type: Full-time
Pay: From $80,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Work Location: Hybrid remote in Cleveland, OH 44115",80000,1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable,OH,41,data engineer,na,['sql'],[],[],[],[],[],,+10 years
TY Software,4.0,"Dallas, TX",Data Engineer,"Job Title Data Engineer,
Location: Dallas, TX
Type of work- Onsite , C2C
Job Description
Experience 3-5 years
At least 3+ years of enterprise experience in working with data bricks and highly proficient in SQL, Spark, Scala/Python.
Skilled in Big Data Technologies like Spark, Spark SQL, PySpark
Experience with one or more of the major cloud platforms & cloud services such as - Azure/AWS/GCP, Databricks
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Strong analytic skills related to working with unstructured datasets
Working knowledge of highly scalable ‘big data’ data stores
A successful history of manipulating, processing and extracting value from large, disconnected datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Experience developing enterprise software products
Experience with at least one of these object-oriented/object function scripting languages: PySpark/Python, Scala, Java
Build monitoring and automated testing to ensure data consistency and availability
Experience supporting and working with cross-functional teams in a dynamic environment
Experience working in an AGILE environment
Job Types: Full-time, Contract
Salary: $42.15 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75201: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",91944,,,,,-1,,TX,-1,data engineer,na,"['sql', 'java', 'scala', 'python']","['databricks', 'azure', 'aws']",[],[],['spark'],[],,+10 years
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'python']",[],[],[],[],[],,
Sky Consulting Inc,4.0,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.",103312,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,['python'],[],[],['hive'],"['kafka', 'spark', 'hadoop']",[],,
BCVS Group INC,5.0,"Oakland, CA",Data engineer lead,"Role: Data Engineer Lead
Location: Oakland, CA
Description
Qualitest is looking for a Data Architect to join our team.
Must Have-
Seasoned data professional with experience implementing solutions across Data Quality, Data Assurance,
Data Governance and Test Data Management
Ability to assess current state, recommend solutions and build a rollout strategy
Analyze existing data landscape, business processes and create knowledge articles (architectures, data flows, tools)Design, Build and Deliver quality gates across data lifecycle for automated and continuous data quality assurance.
Integrate data testing solutions with Automation, CI/CD Pipelines for Continuous Testing
Working experience on various database technologies such as Oracle, SQL Server, MongoDB, Snowflake, Teradata
Good understanding about QA Test Management, Defect Management and Testing methodologies. Evaluate tools, build proof of concepts, and implement NextGen solutions
Work with customer executives and cross functional teams for seamless project implementation and delivery
Mentor onshore/offshore teams to deliver projects in time and assist in case of delays/clarifications
Experience working on data quality validation tools such as RightData, Icedq, Informatica, Tosca DI, Datagaps
Experience working on TDM tools such as Informatica TDM , IBM Optim, Delphix, CA TDM/Broadcom TDM
Proficient in AWS Cloud Services such as Glue, Lambda, RedShift, Dynamo DB and Databricks
Experience working on Data Warehouse, Data Lake, Big Data platforms
Requirements
Good to Have-
Knowledge about Enterprise Applications such as Salesforce, SAP, Veeva CRM
AI/ML knowledge on building Intelligent data validations and eliminate data anomalies
Job Types: Full-time, Contract
Salary: From $41.80 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Oakland, CA 94619: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 +12019071788",75240,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,['sql'],"['databricks', 'snowflake', 'aws', 'redshift']",['sap'],"['sql server', 'mongodb', 'snowflake', 'oracle']",[],[],,0-2 years
ASCENDING,4.2,"Rockville, MD",Data Engineer,"Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.
This role is only available for W2 or individual contracts. Please no C2C.
100% Remote Work.

Responsibilities:
Analyze system requirements and design responsive algorithms and solutions.
Use big data and cloud technologies to produce production quality code.
Engage in performance tuning and scalability engineering.
Work with team, peers and management to identify objectives and set priorities.
Perform related SDLC engineering activities like sprint planning and estimation.
Work effectively in small agile teams.
Provide creative solutions to problems.
Identify opportunities for improvement and execute.

Requirements:
Minimum 5 years of proven professional experience working in the IT industry.
Degree in Computer Science or related domains.
Experience with cloud based Big Data technologies.
Experience with big data technologies like Hadoop, Spark and Hive.
AWS experience is a big plus.
Proficiency in Hive / Spark SQL / SQL. Experience with Spark.
Experience with one or more programming languages like Scala & Python & Java.
Ability to push the frontier of technology and independently pursue better alternatives.
Kubernetes or AWS EKS experience will be a plus.

Thanks for applying!
U3GJMKlbkr",96611,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,"['sql', 'java', 'scala', 'python']",['aws'],[],['hive'],"['spark', 'hadoop']",[],,
FalconSmartIT,4.5,"Dover, DE",Big Data Engineer,"Job Title: Big Data Engineer
Location: Toronto, Ontario, Canada
Job Type: Full time


Job Description:


Qualifications :
8+ years of software development experience in Big Data technologies (Spark/Hive/Hadoop)
Experience in working on Hadoop Distribution, good understanding of core concepts and best practices
Good experience in building/tuning Spark pipelines in Scala/Python
Good experience in writing complex Hive queries to derive business critical insights
Good Programming experience with Java/Python/Scala
Experience with AWS Cloud, exposure to Lambda/EMR/Kinesis will be good to have
Experience in NoSQL Technologies - MongoDB, Dynamo DB
Roles and Responsibilities :
Design and implement solutions for problems arising out of large-scale data processing
Attend/drive various architectural, design and status calls with multiple stakeholders
Ensure end-to-end ownership of all tasks being aligned
Design, build & maintain efficient, reusable & reliable code
Test implementation, troubleshoot & correct problems
Capable of working as an individual contributor and within team too
Ensure high quality software development with complete documentation and traceability
Fulfil organizational responsibilities (sharing knowledge & experience with other teams/ groups)
Conduct technical training(s)/session(s), write whitepapers/case studies/blogs etc.",112889,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,DE,-1,data engineer,na,"['nosql', 'java', 'scala', 'python']",['aws'],[],"['mongodb', 'hive']","['hadoop', 'spark']",[],,
Monogram Health Renal Services,4.0,"Brentwood, TN",Azure Data Engineer,"Position: Azure Data Engineer

The Azure Data Engineer will work with our business operations and technology teams to build out an enterprise-wide Data Warehouse and related systems in the cloud. Will collaborate with our internal teams and 3rd party vendor in building our future state, business process, system interfaces, governance and technology platforms needed to enable a single source of truth for product and customer data, drive a more sophisticated data environment and be positioned to scale for future business growth.

Roles and Responsibilities
In collaboration with our 3rd party vendor, implement strategic roadmap and operational framework for Enterprise Data Management, in partnership with key stakeholders
Collaborate on the strategy on new cloud data stores and migration of existing data in Azure
Work collaboratively with teams to design, document, and ensure continuous improvement of SDLC best practices for data in the cloud
Be an advocate for quality and security through data quality and accuracy validation and auditing of security best practices
Focus on understanding business problems and determine what aspects require optimization; articulate those aspects in a clear and concise manner
Work collaboratively with cross-functional teams to support Data Models in alignment with business needs
Partner with IT, on design and oversight of APIs and other data sharing capabilities in collaboration with short / long term business needs
Partner with IT to integrate new data management technologies and software engineering tools (automation projects, process improvements, etc.) into existing systems to enable single source of truth for product and customer data
Adhere to identified key performance indicators for quality and compliance metrics to ensure data related policies and standards are followed
Acts as a SME to explain all facets of EDM data exchange and processing, business user involvement, and overall architecture
Support Monogram as necessary with any tasks required to deliver excellent personalized kidney care and perform all other duties as assigned
Uphold the mission and values of Monogram Health in all aspects of your role and activities
Position Requirements
Nashville, TN
BS or equivalent experience in crucial duties and responsibilities
3-5 years’ experience in cloud computing
Solid understanding of enterprise data management in Healthcare Data
Solid understanding of data, databases, data tools (e.g. SQL) and data processing experience
Solid understanding of standard ETL tools and techniques, sourcing, maintaining, and updating data, data warehousing, data cleansing, and other analytical techniques required for data usage.
Experience with database systems, Azure cloud storage, and significant exposure to or experience with modern data and BI platforms (such as Databricks, MSSQL Data Warehouse (Synapse), Snowflake, and Tableau).
Previous experience with data migration to cloud-based environment
Interest and passion in designing and implementing data management, querying, and storage, with a particular focus on purpose-built data stores.
Functional knowledge of data visualization tools such as PowerBI and Tableau.
Strong intuition for business and analytical skills
Team player with excellent communication skills

Benefits

Opportunity to work in a dynamic, fast-paced and innovative value-based provider organization that is transforming the delivery of kidney care
Competitive salary and opportunity to participate in the company’s bonus program
Comprehensive medical, dental, vision and life insurance
Flexible paid leave and vacation policy
401(k) plan with matching contributions

About Monogram Health
Monogram Health is a next-generation, value-based chronic condition risk provider serving patients living with chronic kidney and end-stage renal disease and their related metabolic disorders. Monogram seeks to fill systemic gaps and transform the way nephrology, primary care and chronic condition treatment are delivered. Monogram’s innovative, in-home approach utilizes a national nephrology practice powered by a suite of technology-enabled clinical services, including case and disease management, utilization management and review, and medication therapy management services that improve health outcomes while lowering medical costs across the healthcare continuum. By focusing on increasing access to evidence-based care pathways and addressing social determinants of health, Monogram has emerged as an industry leader in championing greater health equity and improving health outcomes for individuals with chronic kidney and end-stage renal disease.

At Monogram Health we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.
Experience
Required
SQL
healthcare data
cloud computing
Azure
Preferred
Tableau
Snowflake
Data Warehouse
Databricks
Power BI",112889,,,,,-1,,TN,-1,data engineer,na,['sql'],"['databricks', 'azure', 'snowflake']","['tableau', 'power bi']",['snowflake'],[],[],,5-10 years
Savvy Technology Solutions,4.0,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location",107339,,,,,-1,,DC,-1,data engineer,senior,"['sql', 'nosql']","['aws', 'redshift']","['tableau', 'ssis']",['mysql'],[],[],master,2-5 years
Infinity Quest,4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote",105128,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable,NJ,17,data engineer,na,"['sql', 'python']","['databricks', 'azure']",[],[],[],[],,0-2 years
Business Integra Inc,3.5,"San Francisco, CA",Data Engineer,"Position can be 100% remote but preferred to have candidates who can periodically (2 x month) work at headquarters.
(Data Engineer)
Job Description:
Assigned Personnel to provide data analytic support to the Data Analytics/Data Integration Project for Judicial Branch Statistical Information System (JBSIS) data reporting.
This position will perform high level data engineering and data analytics on a variety of agency data sources, but primarily on the Judicial Branch Statistical Information System (JBSIS).
Partnering with IT staff, this position will reengineer JBSIS to create new technical documentation for JBSIS; create mappings for the Court Statistics Report and other JBSIS products, make policy recommendations, create and/or implement new governance standards, enhance data auditing and data quality controls, and create data visualizations.
These same tasks may be performed with additional agency datasets.
Specific Skills/Qualifications Required
Technical project management and documentation skills.
Ability to analyze issues from system documentation and recommend solutions.
Experience managing technical projects, including conflict resolution, issue escalations, status reporting and resource management.
Experience creating and executing data mappings and scripts to clean, compile and analyze data
Ability to assess and maintain data pipeline, data quality in the database, and address data reporting issues.
Experience developing and implementing testing protocols for data and system quality
Experience in R and Stata.
Experience with data visualization and software such as Tableau and Power BI.
Excellent oral, written, analytical and communication skills with the ability to lead a technical discussion to both technical and non-technical staff.
Excellent analytical, verbal and conflict resolution skills.
Additional Skills/Qualifications Desired:
General:
Understanding of courtroom operations and workflow.
Experience in government (State) setting
Excellent presentation skills for both technical and non-technical audiences, including creating and presenting executive summaries to management and technical committees.
Technical:
Exposure and experience with Cloud computing.
Conceptual understanding of Amazon Web Services, Microsoft Azure, Google Cloud, IBM and Oracle Cloud Platforms.
Prior experience using Snowflake
Experience using Python or other database query languages.
Job Types: Full-time, Contract
Pay: $112,604.02 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Vision insurance
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco, CA 94102: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data science: 9 years (Required)
Work Location: Hybrid remote in San Francisco, CA 94102",131302,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2001,$25 to $100 million (USD),CA,22,data engineer,na,"['r', 'python']","['snowflake', 'azure', 'google cloud']","['tableau', 'power bi']","['snowflake', 'oracle']",[],[],,5-10 years
Softcrylic,4.0,Remote,Senior Data Engineer,"Who We Are
For more than 20 years, we have been working with organizations large and small to help solve business challenges through technology. We bring a unique combination of engineering and strategy to Make Data Work for organizations.
Our clients range from the travel and leisure industry to publishing, retail and banking. The common thread between our clients is their commitment to making data work as seen through their investment in those efforts.
In our quest to solve data challenges for our clients, we work with large enterprise, cloud based and marketing technology suites. We have a deep understanding of these solutions so we can help our clients make the most of their investment in an efficient way to have a data driven business.
Why Work at Softcrylic?
Softcrylic provides an engaging, team-focused, and rewarding work environment where people are excited about the work they do and passionate about delivering creative solutions to our clients.
We are looking to add a Senior Data Engineer to our team! This is a 100% Remote role and preference will be given to candidates from Atlanta, NJ or Texas regions.
Job Description:
Softcrylic is looking for a Senior Data Engineerwith strong design, development, and team leadership skills. The person should be working with Clients / Customer and with our internal (onshore and offshore) members to design, develop and rollout data projects. The person to be hands on and have strong leadership/communication and interpersonal skills.
Requirement:
· 5 to 7 years of experience in working as a Data Engineer.
· Strong experience in Python.
· Experience in working on GCP.
· Good experience in Airflow.
· Should have good experience in ETL pipeline design and development.
· Very good experience in SQL
· Experience in working on Redshift.
· Excellent designing and documentation (diagrams) and presentation skills.
· Data Quality Concepts are must have.
· Must know design and development of any of industry leading graph databases.
· Good communication skills.
· Independent thinker, good team player with Data Engineering Design skills.
· Work with minimum guidelines.
Plus:
GCP - Big Query
Agile background
Microsoft Power BI
Graph Database
Job Types: Full-time, Contract
Pay: $130,000.00 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Compensation package:
Performance bonus
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
GCP: 3 years (Preferred)
Python: 4 years (Preferred)
Work Location: Remote
Speak with the employer
+91 609.241.9641",135000,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$5 to $25 million (USD),Remote,23,data engineer,senior,"['sql', 'python']",['redshift'],['power bi'],[],[],[],,2-5 years
"MOBE, LLC",3.7,"Minneapolis, MN",Data Engineer (ETL),"MOBE
MOBE guides people to better health and more happiness. Behind our innovative health solutions is uniquely human philosophy. We believe that person-to-person connections and understanding can make a difference in a world where self-care can be complicated, and health care is ever-evolving and complex.
MOBE works with health plans and large employers to identify individuals who are frequent users of health care but aren?t finding resolutions for their underlying health issues. We use a whole-person approach and guidance to impact health outcomes positively.
Supporting people is at the core of our business, employees included. MOBE is a high-growth organization with a culture built on trust and collaboration. Consistent across our teams and offerings is a belief in the power of people doing good together. We genuinely care about people and consider our workforce the most significant asset.
Your Role at MOBE
This is an exciting time at MOBE and we are growing fast. At MOBE, we have a lot of data: eligibility, medical and pharmacy claims, marketing campaign impressions, transcripts from participant interactions, etc.
This position is responsible for providing technical and project expertise to enable MOBE analytics and operations with structured and unstructured data. Responsibilities includes executing and/or leading user story development, data design and architecture, data pipeline development, testing and deployment in the Analytic Data Framework. This role will partner with internal and external business and technology teams to drive project deliverables and ensure high quality delivery of data architecture and integration.
Responsibilities
The Data Engineer ensures the following capabilities and functions:
Translate high level business processes into logical data processing steps
Design data structures and pipelines that are flexible and scalable for MOBE analytics and operational requirements
Support Analytic partners through collaborative and transparent development, information delivery, problem resolution, shared insights, and training
Data processing definition, execution, and documentation, in a time appropriate way, to meet business priorities and requirements
Data quality and maintenance consistent within the Analytic Data Framework
Lead small to moderate sized projects and initiatives, following through on execution of chosen strategies and demonstrating the ability to work through obstacles and changing priorities.
Demonstrate ability and willingness to play multiple roles for different projects (e.g. planning/architecture, project development, hands-on technical resource/support for others, analysis and resolution of data issues)
Identify and constructively communicate the need for improvements or enhancements in MOBE technology assets
All other duties as assigned to help fulfill our Mission and abide by MOBE?s Guiding Principles",97357,51 to 200 Employees,Company - Private,Personal Consumer Services,Beauty & Wellness,2014,Unknown / Non-Applicable,MN,9,data engineer,na,[],[],[],[],[],[],,
Boston Globe Media Partners,4.1,"Boston, MA",Data Engineer,"Boston Globe Media is New England's largest newsgathering organization - and much more. We are committed to being an indispensable, trusted, reliable source of round-the-clock information. Through the powerful journalism from our newsroom, engaging content from our content marketing studio, or through targeted advertising solutions, brands and marketers rely on us to reach highly engaged, educated and influential audiences through a variety of media and experiences.
Responsibilities:
Collect, organize, and document often-used data resources (maps, APIs, etc).
Create scripts to scrape data from websites for stakeholders.
With guidance, start creation of a data style guide.
Technology:
Basic knowledge of HTML, CSS, and JavaScript.
Basic familiarity with PHP, Groovy, or another server side scripting language.
Basic familiarity of build tools such as Grunt, Gulp, or Webpack.
Basic familiarity with version control systems such as SVN or Git.
Qualifications:
Understands and follows the team’s agile process.
Adheres to defined coding standards.
Participates in code reviews.
A willingness to adapt and be audience focused, with a curious mindset and a commitment to creating an inclusive work environment
Vaccination Statement:
We require that all BGMP employees (including temporary employees, co-ops, interns, and independent contractors) be vaccinated from COVID-19, unless an exemption from this policy has been granted as an accommodation or otherwise. All BGMP employees, regardless of vaccination status or work location, must provide proof of vaccination status as instructed by the employee's designated Human Resources contact. Employees may request a reasonable accommodation or other exemption from this policy by contacting their designated Human Resources contact. Failure to comply with or enforce any part of this policy, or misrepresentation of compliance with this policy, may result in discipline, up to and including termination of employment, subject to reasonable accommodation and other requirements of applicable federal, state, and local law.
EEO Statement:
Boston Globe Media Partners is an equal employment opportunity employer, and does not discriminate on the basis of race, color, religion, gender, sexual orientation, gender identity or expression, age, disability, national origin, ancestry, genetic information, military or veteran status, pregnancy or pregnancy-related condition or any other protected characteristic. Boston Globe Media Partners is committed to diversity in its most inclusive sense.
wcZyZ7QvrB",110394,1001 to 5000 Employees,Company - Private,Media & Communication,Publishing,-1,$100 to $500 million (USD),MA,-1,data engineer,na,[],[],[],[],[],[],,
NIVID Technologies,3.7,Remote,Sr. Data Engineer,"We hope this Job meets your skills and expectations. If you are available and interested, please contact me at your earliest convenience. You will be working with a highly skilled team of IT Professionals in a high pace corporate environment. This opportunity will move quickly and candidates will be interviewed in the order they apply. Job Description/ Required Skills: (i) Strong hands-on programming experience in Python (ii) Hands-on experience of API development (from application / software engineering perspective) (iii) AWS Lambda and data streaming ingestion (Kinesis) (iv) AWS tech stack from data engineering stand-point
Job Types: Full-time, Contract, Permanent
Salary: $39.76 - $86.23 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",113400,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$5 to $10 billion (USD),Remote,11,data engineer,senior,"['sql', 'python']",['aws'],[],[],[],[],,0-2 years
Plaxonic Technologies,4.6,"New York, NY",GCP Data Engineer,"Bachelor’s Degree in Computer Science or a related discipline
5+ years of applicable engineering experience
Strong proficiency in Python with an emphasis in building data pipelines
Ability to write complex SQL to perform common types of analysis and aggregations
Experience with Apache Airflow or Google Composer
Detail-oriented and document all the work
Ability to work with others from diverse skill-sets and backgrounds
GCP solution architect - certified
Experience in GCP, Big Query
Working experience in Databricks, Spark is expected
Job Types: Full-time, Contract
Benefits:
401(k)
Health insurance
Paid time off
Schedule:
8 hour shift
Work Location: One location
Speak with the employer
+91 (727) 216-7989",117952,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),NY,10,data engineer,na,"['sql', 'python']",['databricks'],[],[],['spark'],[],bachelor,
EZOPs Inc,4.0,"New York, NY",Python Data Engineer,"Responsibility:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies.
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems.
Utilize programming languages like Python, ReactJs, JavaScript and Open Source RDBMS and Cloud based data warehousing services such as Snowflake.
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community.
Collaborate with product managers and deliver robust cloud-based solutions.
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply).
At least 1 year of experience in data technologies.
Hands on Experience in application development with Python, Pandas, NumPy, SQL, Docker.
Preferred Qualifications:
2+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud).
1+ year experience working on real-time data and streaming applications like Kafka is big plus.
1+ years of data warehousing experience (Redshift or Snowflake)
1+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
Job Type: Full-time
Benefits:
401(k)
Health insurance
Paid time off
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
New York, NY: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location
Speak with the employer
+91 9599382735",112889,,,,,-1,,NY,-1,data engineer,na,"['scala', 'shell', 'java', 'sql', 'python']","['snowflake', 'google cloud', 'azure', 'aws', 'redshift']",[],['snowflake'],['kafka'],['docker'],bachelor,2-5 years
PepsiCo,4.0,"Plano, TX",Azure Data Engineer,"As a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems.
Responsibilities:
Active contributor to code development in projects and services.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.
Responsible for implementing best practices around systems integration, security, performance and data management.
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.
Develop and optimize procedures to “productionalize” data science models.
Define and manage SLA’s for data products and processes running in production.
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale.
Research in state-of-the-art methodologies.
Create documentation for learnings and knowledge transfer.
Create and audit reusable packages or libraries.
Requirements:
2+ years of overall technology experience that includes at least 2+ years of hands-on software development, data engineering, and systems architecture.
2+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
1+ years in cloud data engineering experience in Azure Certification is a plus.
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.
Experience with version control systems like Github and deployment & CI tools.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus
Understanding of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI)
Covid-19 vaccination may be a condition of employment dependent on role and location. For specific information, please discuss role requirements with the recruiter
Education
BA/BS in Computer Science, Math, Physics, or other technical fields
Skills, Abilities, Knowledge
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.
Proven track record of leading, mentoring data teams.
Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.
Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.
Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.
Foster a team culture of accountability, communication, and self-management.
Proactively drives impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals
Ability to lead others without direct authority in a matrixed environment.
Job Type: Full-time
Pay: $85,000.00 - $90,000.00 per year
Schedule:
Monday to Friday
Work Location: Hybrid remote in Plano, TX 75024",87500,10000+ Employees,Company - Public,Manufacturing,Food & Beverage Manufacturing,1965,$10+ billion (USD),TX,58,data engineer,na,"['sql', 'scala', 'python']",['azure'],[],[],[],[],,+10 years
Ascendion,4.5,Remote,Senior Data Engineer,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote",117000,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable,Remote,1,data engineer,senior,"['sql', 'scala', 'java', 'nosql', 'python']","['databricks', 'azure']",['ssis'],['mongodb'],[],[],,5-10 years
Arthur Grand Technologies Inc,4.8,"Jersey City, NJ",Azure Tech Lead/ Sr Data Engineer,"Role: Azure Tech Lead/ Sr Data Engineer – Onsite role – Preferred locals
Location: Jersey City, New Jersey / Fort Mill, South Carolina.
Full-time
Mandatory Skills: MS Azure using Azure Data Factory, MS Synapse, Scala, Spark, Data Warehousing
Skills:
Over all 12 to 15 years of experience with Data Management, Data Warehousing and Analytics.
At least 4 to 5 years of experience in Architecting and Implementing Data Solutions.
At least 3 years of experience in implementing the data solutions on MS Azure using Azure Data Factory, MS Synapse.
One to two years of experience in Azure Synapse Analytics is plus.
Installing and configuring ADF integration runtimes and linked services.
At least one hands on experience with Big data platform tool selection POC.
Two years of experience in data migrations to Azure by using data box or Data migration Services.
Apache Spark experience using Scala or PySpark or pre-packaged tools like Databrick is must.
Extensive hands-on experience in data warehousing design, tuning and ETL/ELT process development by using cloud native technologies.
At least one year experience with unified data governance solution using MS Purview.
Developing the CICD pipeline for Azure Infrastructure, version control strategy and Integrate source control ( Azure repos)
In-depth understanding of various storage services offered by Azure.
Experience with implementation of data security, encryption, PII/PSI legislation, identity and access management across sources and environments.
Experience with data process Orchestration, end-to-end design and build process of Near-Real Time and Batch Data Pipelines.
Certification in Azure data engineering and solution architecture Azure is must.
Strong client-facing communication and facilitation skills.
Job Type: Full-time
Salary: $81,075.29 - $186,473.81 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Experience:
Azure: 4 years (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road",133775,51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable,NJ,11,data engineer,senior,"['sql', 'scala']",['azure'],[],[],['spark'],[],,0-2 years
ConnectiveRx,3.0,"Hanover, NJ",Sr. Data Engineer,"ConnectiveRx is a leading, technology-enabled healthcare services company. We work strategically with hundreds of biopharmaceutical manufacturers to help commercialize and maximize the benefits of specialty and branded medications. Our mission is to simplify how patients get on and stay on therapy. We fulfill our mission by providing our customers with innovative services such as patient and provider messaging, the design and operation of copay, vouchers and patient affordability programs, and hub services, all of which accelerate speed-to-therapy and help improve outcomes for manufacturers, healthcare providers and patients.

ConnectiveRx was formed in 2015 by bringing together the industry-leading business of PSKW, PDR/LDM, Careform (2017) and The Macaluso Group (2018) to advance our technology-driven expertise in providing state-of-the-art commercialization solutions. To learn more about our company, visit ConnectiveRx.com

Job Description

What you will do:
Looking for a seasoned Senior Data Engineer to help us continue to build out our new Enterprise Data Platform. This person must have a strong understanding and demonstrated experience with data streaming architectures that leverage microservice & message-oriented integration patterns and practices within AWS cloud native technologies. This person will help to scale our data ingestion pipelines which are at the core of our Enterprise Data Platform which supports our client reporting as well as our internal analytics & operational teams.

The successful candidate will:
Work with senior leadership, architects, engineers, data analysts, product managers and cloud infrastructure teams to deliver a new features and capabilities.
Write clean, robust, and well-thought-out code with an emphasis on quality, performance, scalability, and maintainability.
Demonstrate strong end to end ownership & craftsmanship - analysis, design, code, test, debug, and deploy
Your ability to traverse the full stack within AWS server-less technologies will be an asset to us as we evaluate the tradeoffs inherent in software engineering. You have the product driven development mindset and can work closely with BA’s and Product teams to breakdown requirements and translate business workflows into scalable technical solutions.

What we’d like from you:
Strong Python & strong SQL
Extensive relational DB experience (Redshift, SQL Server, PostgresSQL) with exposure to document DBs such as DynamoDB. ElasticSearch.
Experience with designing solutions that run in AWS cloud technologies (Lambda, ECS, DynamoDB etc), docker containers
Message oriented architectures, patterns and tools, CQRS, event streaming, Kafka, SQS
Change data capture concepts, Database Triggers, AWS DMS
Data lake concepts, data catalogs, meta data etc
CICD Pipelines
Event store processing, data validation, operational logging via AWS Cloud Watch
Why work with us?
Excellent company culture, fun events, and volunteer opportunities
Competitive benefits (medical, dental, vision & more)
401k package with dollar-for-dollar match-up
Generous PTO and paid holidays days offered
Opportunities to grow professionally and personally
Team-oriented atmosphere
#LI-BJ1

Equal Opportunity Employer: This employer (hereafter the Company) is an equal opportunity employer and does not discriminate in recruitment, hiring, training, promotion, or other employment policies on the basis of age, race, sex, color, religion, national origin, disability, veteran status, genetic information, or any other basis that is prohibited by federal, state, or local law. No question in this application is intended to secure information to be used for such discrimination. In addition, the Company makes reasonable accommodation to the needs of disabled applicants and employees, so long as this does not create an undue hardship on the Company or threaten the health or safety of others at work. This application will be given every consideration, but its receipt does not imply that the applicant will be employed.",115021,1001 to 5000 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2015,Unknown / Non-Applicable,NJ,8,data engineer,senior,"['sql', 'python']","['aws', 'redshift']",[],"['sql server', 'dynamodb', 'elasticsearch']",['kafka'],['docker'],,
Talent-One,4.0,"Imperial, CA",Data Engineer,"Responsibilities:
Develop predictive models
Develop optimization models
Develop re-activation and retention models
Advanced analytics to drive incremental revenue
Identify performance metrics definition, algorithm development and automation
Reporting and visualization
Complex data analysis tasks
Data anomaly detection and correction modeling
Conversion of data into stories for internal and external consumption
Cross-team support for CRM and Database Marketing Teams.
Qualifications:
Bachelor’s degree in an Analytical field (Business, Marketing) required.
At least three (3) years casino database experience required, or the equivalent combination of education and experience in data analysis.
SAS programming level 1 or higher certification required.
Ability to use data to solve complex business problems.
Advanced SQL skills
Strong industry experience of Microsoft Office Suite, including Excel, Word, Access, required
Job Type: Full-time
Salary: $1.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",112889,,,,,-1,,CA,-1,data engineer,na,['sql'],[],['excel'],[],[],[],bachelor,0-2 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),TX,10,data engineer,na,['sql'],"['azure', 'aws', 'snowflake', 'redshift']",[],"['snowflake', 'oracle']",[],[],,5-10 years
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['sql', 'python']",[],[],[],[],[],,0-2 years
Kaizen Dynamics,4.0,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],"['tableau', 'excel']",[],[],[],master,
Teamware Solutions (quantum leap consulting).,4.6,"South San Francisco, CA",Data Engineer - Onsite,"Hi,
Data Engineer
Bay Area, CA – Onsite(Hybrid)
Client: Decision Minds/PANW
Duration: Contract
Exp Level: 10+ Years
Must have skill: Google cloud exp
Job Responsibilities:
Expert in data engineering and GCP data technologies.
Work with client teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
Work with Agile and DevOps techniques and implementation approaches in the delivery
Key responsibilities: Architecture, Design and Development
Required Skills:
10+ Year experience in BI and Analytics
Hands on and deep experience ( at least 2 years) working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, ]etc.).
Experience in Spark (Scala/Python/Java) and Kafka, Airflow
Data Engineering and Lifecycle (including non-functional requirements and operations) management.
E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.
Experience with SQL and NoSQL modern data stores.
Thanks & Regards
Jagadeesh
Teamware Solutions Inc |2838 E. Long Lake Road,Suite# 210, TROY, MI 48085
Job Type: Full-time
Salary: $60.00 - $65.00 per hour
Experience level:
10 years
11+ years
9 years
Schedule:
On call
Ability to commute/relocate:
South San Francisco, CA 94080: Reliably commute or planning to relocate before starting work (Required)
Experience:
Google Cloud Platform: 4 years (Preferred)
Data Engineer: 9 years (Preferred)
Spark: 4 years (Preferred)
Work Location: One location",112500,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,Unknown / Non-Applicable,CA,20,data engineer,na,"['sql', 'scala', 'java', 'nosql', 'python']",['google cloud'],[],[],"['kafka', 'spark']",[],,2-5 years
Arthur Grand Technologies Inc,4.8,"Mount Laurel, NJ",Lead Informatica / Data Engineer,"Role: Lead Informatica / Data Engineer – On Prem –ETL (Onsite role) / Senior Informatica / Mid-Level Informatica
Location: Mount Laurel, NJ / Charlotte, NC
Duration : FTE
Client :: Hexaware / TD Bank
Key Skills: Informatica Power Centre, Autosys, Unix
Must Have
More than 12+ years of IT experience in Datawarehouse and ETL
Hands-on Experience on ETL Informatica Power Centre
Experience on Autosys, Unix and scripting knowledge on Python, Shell Scripts
Experience on Oracle Database
Ability to understand ETL Design, Source to target mapping (STTM) and create ETL specifications documents
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have
Any cloud experience on Azure or AWS or Informatica cloud connector
Any relevant certifications
Job Type: Full-time
Salary: $69,919.38 - $166,922.18 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 3 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road",118421,51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable,NJ,11,data engineer,senior,"['sql', 'shell', 'python']","['azure', 'aws']",[],['oracle'],[],[],,0-2 years
Predict Health,4.0,"Arlington, VA",Data Engineer,"Overview of the Role
We are looking for an exceptional Data Engineer who is interested in joining a small and rapidly growing Product and Services team at our company. We view our data-driven products and services as strategic assets, and you will be a key part of our team and company. The successful candidate will have 5+ years of experience designing and implementing data solutions and services. This role will report to our VP, Product Engineering and will be an instrumental member of our product development team.
The Data Engineer will participate in the design of new information intensive product and service offerings. They will be responsible for the development and implementation of the data pipelines and services underlying these new products. They will deepen their understanding of the political, policy, financial and business aspects of the health plan industry so as to be able to provide insight into the best manner to collect and manage information related to these businesses.
The Data Engineer will work closely with the Product Management function to understand customer needs and help identify capabilities for new offerings. They will also partner with the Data Science team to understand the analytical needs of the business and assist in the design and implementation of data solutions to support these efforts.
About Predict Health, Inc.
Predict Health is transforming how health plans, providers and their partners increase retention, quality, and lifetime value of their customer or patient relationships. The core competency of the organization is the curation, analysis, and presentation of information to support targeted operational improvements in healthcare enterprises. The company is an early-stage enterprise based in Washington, D.C. which focuses on three solution categories: Member Acquisition, Quality Management, and Retention Management.
Our company, Predict Health was founded in 2019 and is led by an experienced team of healthcare experts, data scientists and healthcare entrepreneurs. We are focused on building services and solutions designed to improve the healthcare experience of our nation’s seniors. We founded our company after seeing first-hand how challenging it is for the seniors in our lives - including our parents, grandparents and friends - to deal with the complexities of Medicare with little or no real help.
Detailed Job Description
The Data Engineer will initially support the deployment of an Azure solution to help collect, aggregate, transform, and distribute information related to Predict Health’s current service offerings. The Data Engineer is responsible for designing the solution, building the data integration and associated pipelines, design and deploying data structures required to support current products and analytical efforts, and automating the operational aspects of the data platform. The Data Engineer responsibilities will include:
· Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as, but not limited to, Azure Synapse, Azure Data Factory and Azure pipelines.
· Translate business requirements from product owners and analysts into technical code.
· Identify, fix and document bugs, bottlenecks in workflows and pipelines.
· Enforce security compliance, fine tune performance and promote code quality standards.
· Identify and curate new data sources required to support business requirements
· Explore, learn the latest Azure data management technologies to add to current competencies.
· Support development of data intensive solutions in a fast-paced, dynamic environment
· Support Business Intelligence Applications (e.g. Power BI) as required.
Qualifications
Requires minimum of five years relevant experience in design, development, implementation, and testing of data services
· Experience in data structures, data modeling, data warehousing and data pipeline development on Azure
· 3+ years of hands-on experience in the design, implementation and performance tuning of Azure Data Factory, Synapse, or other Azure data services
· Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure
· Experience developing software code in one or more programming languages (Java, Python, etc.)
· Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform
· Experience with data migration
· Experience defining system architectures and exploring technical feasibility trade-offs.
· Ability to prototype and evaluate applications and interaction methodologies.
Other Knowledge, Skills, and Abilities Required
· Demonstrated capability for problem solving, decision making, sound judgement and assertiveness.
· Ability to change direction to meet demand in a high-paced work environment
· Ability to focus on details and technicalities while simultaneously retaining a comprehensive vision of organizational goals
· Excellent collaborator and able to influence and direct work contributions from others to meet critical program deliverables and timelines
· Advanced interpersonal, written, and oral communication skills.
· Proficiency in Microsoft Office including Excel and PowerPoint
What we Offer
This is a full-time position and compensation, and benefits will be highly competitive.
Predict Health is committed to bringing together people from different backgrounds and perspectives to deliver real results for our clients and end-users. We recruit, employ, compensate, and promote regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.
This position will be based in our Arlington, VA office. We offer a flexible work schedule with a minimum of three days in the office each week.
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Employee discount
Flexible schedule
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Arlington, VA: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",115000,,,,,-1,,VA,-1,data engineer,na,"['java', 'python']",['azure'],"['power bi', 'excel']",[],[],['terraform'],,+10 years
The Sunwater Institute,4.0,"Rockville, MD",Data Engineer,"The Sunwater Institute is a nonprofit, nonpartisan public policy organization focused on improving the foundations of our social systems. The Sunwater Institute’s current programs focus on improving congressional performance, preserving and advancing constitutional rights and liberties, strengthening and enlarging property rights, and exploring the foundations of group decision-making.
The Sunwater Institute’s interdisciplinary, long-term orientation and focus on the building blocks of social systems sets it apart as a haven for bold, fresh ideas and free, constructive dialogue. The Sunwater Institute is located in North Bethesda and maintains a presence on Capitol Hill.
POSITION
The Sunwater Institute is seeking a Data Engineer who has a background in engineering or computer science and is highly skilled in SQL and database services technologies. The ideal candidate will possess strong problem-solving skills, excellent communication, and a great technical aptitude. The individual in this position will provide project support and help drive team initiatives. The candidate will be creative, and comfortable working in small teams and coordinating with US and non-US-based developers.
Responsibilities
Develop data models to store and manage large amounts of structured and unstructured data
Design efficient ETL processes to move and integrate data from different sources into the enterprise warehouse
Write complex queries using SQL Server etc. to analyze large datasets
Identify trends, patterns, and correlations within datasets by performing exploratory analysis
Ensure high quality of data by implementing best practices on storage, maintenance, indexing techniques, etc
Coordinate project status updates between teams and communicate deliverables and timelines
REQUIREMENTS
Bachelor’s degree in Computer Science or Engineering related field
4+ years of relevant experience in a similar role
Strong knowledge of relational databases including SQL Server
Proficient skills in writing complex queries using SQL language; developing stored procs, views, etc.
Experience with end-to-end ETL (Extract Transform Load) Processes
Familiarity with Big Data platforms such as AWS etc.
Ability to develop effective reports that help drive business decisions
Must be able to work effectively with teams but be able to work independently on tasks to meet tight deadlines
Exceptional communication skills verbal, written, and active listening
Extreme attention to detail and accuracy
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Vision insurance
Compensation package:
Yearly bonus
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Rockville, MD 20852: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",112889,,,,,-1,,MD,-1,data engineer,na,['sql'],['aws'],[],['sql server'],[],[],bachelor,2-5 years
Plaxonic Technologies,4.6,"Dallas, TX",Data Engineer,"Job Description – Sr. Data Engineer:
Minimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark
Ability to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark
Hands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies
Strong skill in understanding database architecture, data models and writing complex SQL queries/code
Hands on experience with integration of different data sources (Files, DBs, APIs)
Ability to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation
Knowledge of various ETL techniques
Experience with data pipeline and workflow management tools [Azure DevOps]
Strong analytic skill to work with unstructured data
Experience working with Agile teams scrums.
Strong customer handling skills and communication skills
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75287: Reliably commute or planning to relocate before starting work (Required)
Experience:
Microsoft Excel: 1 year (Preferred)
Microsoft Office: 1 year (Preferred)
Compliance management: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: One location
Speak with the employer
+91 7272167989",98641,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),TX,10,data engineer,na,['sql'],"['databricks', 'azure']",['excel'],[],[],[],,0-2 years
Ascent Technologies,4.0,Remote,Data Engineer,"Job Description: Involved in resource / volume ramp-up plan/ backfilling resources planned for rotation and resignation cases Tracking Performance of individual Projects within the delivery/SIPs. Participate / presenting SD performance during client visit Publish Daily, Weekly and Monthly reports on team productivity Reviewing the weekly / monthly dashboards Publishing dashboards to customer People Management Drive the team to reach the defined targets Ensure training and development of the team members Mentor team members on technical and communication aspects. onsite anni data enginneer-pheonix,az python,charlottee,nc ba-houston tx
Job Types: Full-time, Contract, Permanent
Salary: $39.87 - $86.24 per hour
Benefits:
Dental insurance
Compensation package:
1099 contract
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91 7842516676",113490,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'python']",[],[],[],[],[],,0-2 years
Cloudbc Labs,3.9,Remote,Anaplan Data Engineer,"Job Title :: Anaplan Data Engineer
Location :: Remote
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Remote",117000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,Unknown / Non-Applicable,Remote,8,data engineer,na,[],[],[],[],[],[],,
Decision Point Healthcare,5.0,"Boston, MA",Junior Data Engineer,"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a data engineer, you will be responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, management, and operation of our client data hubs, including data intake, data quality assessment/evaluation, data curation and enrichment processes. Our client data hubs consist of various health related data sources supporting Decision Point services including our AI/ML platform, analytics platform, and OPUS application suite. If this interests you, read on.
The position:
Develop and apply scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Support various components of the data pipelines, including ingestion, validation, cleansing and curation
Manage and ensure the success of ongoing data pipeline routines
Collaborate with our implementation team to assist with the identification and reconciliation of data anomalies
Create and maintain documentation on data pipelines
Engage with our software engineering team to ensure precise data points per application specifications
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering, or applicable experience
Expertise with SQL, database design and data manipulation methodologies
Expertise with ETL/ELT and the development of automated validation and data pipelines
Strong data profiling and analytic skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW, master data management and other database design principles
Comfortable working with high volume data in a variety of formats
Experience with Pandas/Numpy in a production environment
Experience with CI/CD and version control tools: Git preferred
Experience working within hybrid cloud environment; AWS experience is a plus
Excellent verbal and written communication
Excellent listener and collaborator with senior leaders, peers, and staff
Familiarity with data engineering and workflow management frameworks such as Airflow and dbt
Familiarity with healthcare data is a plus
A little bit about Decision Point:
We are a rapidly growing healthcare technology company changing the fundamentals of patient and provider engagement. For years, health plans have relied on descriptive data and reactive engagement. We empower our clients to understand and predict the whole member journey, enabling sustained improvements in member health outcomes and plan performance. We combine the latest, most practical technologies and a deep understanding of healthcare, bringing innovative, pragmatic solutions to an industry that touches us all.",92443,1 to 50 Employees,Company - Public,,,2013,Less than $1 million (USD),MA,10,data engineer,na,['sql'],['aws'],[],['dbt'],[],[],master,
Schneider Electric,4.2,"Columbia, SC",Manufacturing Data Engineer,"Job Description:
Schneider Electric has an opportunity for a Manufacturing Data Engineer in our Columbia, South Carolina location. The Manufacturing Data Engineer is a key contributor to the data strategies for the Columbia plant, specifically, in the areas of system design and performance.

What will you do?
Specializes in several areas of knowledge regarding production / manufacturing processes: process design, ergonomics, capacity, simulation tools, investment, and cost analysis
Supports, maintains, upgrades and troubleshoots applicable system infrastructure
Develops a variety of applications and reports, leveraging large data sets to facilitate business operations and boost organizational efficiency.
Analyzes business processes and develops reports, tools, scripts and procedures to bridge the gap between legacy systems
Provides end-user support, including researching user complaints/issues, answering technical questions, and/or assisting with application revisions.
Manages Enterprise Resource Planning (ERP) by updating material, Bill of Materials (BOMs), and routings when modifications are required by Engineering
What qualifications will make you successful?
Bachelor’s degree in a field of Engineering ME or EE
Good communication skills
Relevant experience preferred 3 plus years
Experience in Excel, Macros, SQL, and Tableau

Qualifications:
What's in it for me?
Schneider Electric offers a robust benefits package to support our employees such as flexible work arrangements, paid family leave, 401(k)+ match, and more.
Who will you report to?
Manufacturing Engineering Manager

Let us learn about you! Apply today.

About Our Company:
Why us?
Schneider Electric is leading the digital transformation of energy management and automation. Our technologies enable the world to use energy in a safe, efficient and sustainable manner. We strive to promote a global economy that is both ecologically viable and highly productive.

€25.7bn global revenue
137 000+ employees in 100+ countries
45% of revenue from IoT
5% of revenue devoted for R&D

You must submit an online application to be considered for any position with us. This position will be posted until filled

It is the policy of Schneider Electric to provide equal employment and advancement opportunities in the areas of recruiting, hiring, training, transferring, and promoting all qualified individuals regardless of race, religion, color, gender, disability, national origin, ancestry, age, military status, sexual orientation, marital status, or any other legally protected characteristic or conduct. Concerning agencies: Schneider Electric does not accept unsolicited resumes and will not be responsible for fees related to such.",90065,10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1836,$10+ billion (USD),SC,187,data engineer,na,"['sql', 'r']",[],"['tableau', 'excel']",[],[],[],bachelor,
Archon Resources,3.4,"Tulsa, OK",Data Engineer,"EDI/Data Engineer:
6-month contract to hire. Must be in Tulsa/OKC and able to be on site weekly.
Needed: Azure Data Factory; Microsoft SQL; Oracle; Experience with big data
Nice to Haves: Experience monitoring data in and out; data warehouse provisioning to feed analytical requirements
________________________________________________________________
JOB SUMMARY:
The Data Engineer will be responsible for expanding, optimizing and monitoring our data and data pipeline architecture, as well as optimizing data flow and collection across organizational teams. The Data Engineer will support our software engineers, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
KEY RESPONSIBILITIES:
Create and maintain optimal data pipeline architecture to support our next generation of products and data initiatives.
Assemble large, complex data sets that meet functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.
Experience in the development of SSIS, ETL and other standardized data management tools.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Performs other duties as required.
QUALIFICATIONS:
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong project management and organizational skills.
Ability to work independently, handle multiple tasks and projects simultaneously.
EDUCATION/EXPERIENCE:
Bachelors degree or equivalent experience required.
Project management skills preferred.
Willingness to work in a high-tech, continually evolving, innovative environment.
Job Types: Full-time, Contract
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Health insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Tulsa, OK 74105: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Tulsa, OK 74105",108000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,OK,-1,data engineer,na,['sql'],['azure'],['ssis'],['oracle'],[],[],,0-2 years
Chevron,4.1,"Denver, CO",Wells Data Engineer,"Chevron is accepting online applications for the position Wells Data Engineer through 03/08/2023 at 11:59 p.m. (CST).
Chevron’s strategy is straight-forward: be a leader in efficient and lower carbon production of traditional energy, in high demand today and for decades to come, while growing lower carbon businesses that will be a bigger part of the future. To achieve these goals, we’ll build on the assets, experience, capabilities, and relationships we’ve developed over 140 years to incubate and grow new business.

Technology will play a crucial role in unlocking ever cleaner and more affordable sources of energy. Chevron is seeking innovative, technology professionals with a desire to thrive in the global digital environment and help us lead the global energy transition. An IT career at Chevron offers you the opportunity to work in a technical environment with a global reach. You’ll find that we make a business of investing in our people and encouraging your professional development through a learning culture and challenging on-the-job opportunities. We differentiate ourselves through the application of cutting-edge technology, and by taking a collaborative approach that includes in-house expertise, proprietary solutions, and strategic partnerships. We also offer flexible work schedules and very competitive benefits.

Join Chevron IT. Lend us your skills and enjoy a great career with Chevron.
Wells Data Engineer responsible for multiple aspects of RBU Engineering Team data management and quality, automation, system connectivity, visualizations. This role is expanded through supporting performance related processes, benchmarking, and cost/metrics tracking in our Competitive Performance framework. The Data Engineer is also responsible for providing technical and analytical support in the area of data entry and standard templates for drilling, workover, intervention, and abandonments project management and execution.
The role

Wells Data Engineer responsible for multiple aspects of RBU Engineering Team data management and quality, automation, system connectivity, visualizations. This role is expanded through supporting performance related processes, benchmarking, and cost/metrics tracking in our Competitive Performance framework. The Data Engineer is also responsible for providing technical and analytical support in the area of data entry and standard templates for drilling, workover, intervention, and abandonments project management and execution.
Responsibilities for this position may include but are not limited to:

Understanding data systems architecture and management
Data sources, data quality, and QA/QC mechanisms needed for systems of record
Administrator level ownership of multiple data systems such as WellView, SharePoint, WellSafe systems, reporting systems, and data systems
Working with multiple data systems to provide clear and impactful data visualizations and tracking for the Wells leadership team
Working with the Wells Performance Engineer to support cost, metrics, and performance tracking processes
Support WellView well/job creation, data quality review and support, field training, and regulatory data
Utilizing PowerBI, Spotfire, or Excel to collate cost tracking information in Wellview and Siteview
Required to have strong written communication & excellent organizational skills.
Required Qualifications:

Data science or engineering related training and related systems
Advanced skills in Excel and other Microsoft Office Suite applications
Good communicator and able to collaborate effectively with multiple stakeholders (i.e. engineers, regulatory, operations, etc…)
Preferred Qualifications:

Experience working with WellView, PowerBI and ArcGIS applications
Experience in supporting oil and gas well drilling, completion, workover, and asset retirement operations.
Flexible Working
Chevron offers a complete package and provides career development opportunities to all employees. We do this through on-boarding, training and development, mentoring, volunteering opportunities and employee networking groups. We advocate work-life balance and offer employees access to various health and wellness programs.
What type of flex work does the position offer?
We offer alternative work schedules including 9/80 (work 9-hour days, with every other Friday off)
We offer a hybrid work model - work remotely from home 2-3 days a week

Relocation & International Considerations
Relocation [ may / will not be] considered.
Expatriate assignments [ may / will not be ] considered.

Chevron regrets that it is unable to sponsor employment Visas or consider individuals on time-limited Visa status for this position.
Working with us
Chevron is one of the world’s leading integrated energy companies. We believe affordable, reliable and ever-cleaner energy is essential to achieving a more prosperous and sustainable world. Chevron produces crude oil and natural gas; manufactures transportation fuels, lubricants, petrochemicals and additives; and develops technologies that enhance our business and the industry. We are focused on lowering the carbon intensity in our operations and seeking to grow lower carbon businesses along with our traditional business lines. More information about Chevron is available at
www.chevron.com
.
Benefits
Chevron offers competitive compensation and benefits programs which includes, but is not limited to, variable pay, healthcare coverage, retirement plan, insurance, time off programs, training and development opportunities and a range of allowances connected to specific work situations. Details of such benefits and allowances are available at
https://hr2.chevron.com/
.

The compensation and reference to benefits for this role is listed on this posting in compliance with Colorado law. The selected candidate’s salary will be determined based on his or her skills, experience, and qualifications.

Regulatory Disclosure for US Positions

The compensation and reference to benefits for this role is listed on this posting in compliance with applicable law. Please note that the compensation and benefits listed below are only applicable for U.S. payroll offers.
The anticipated salary range for this position is $ 112,200 – $ 221,900 The selected candidate’s compensation will be determined based on their skills, experience, and qualifications.
Chevron offers competitive compensation and benefits programs which includes, but is not limited to, variable pay, health care coverage, retirement plan, protection coverage, time off and leave programs, training and development opportunities and a range of allowances connected to specific work situations. Details are available at
http://hr2.chevron.com/
.
Regulatory Disclosure for US Positions:
Chevron is an Equal Opportunity / Affirmative Action employer. Qualified applicants will receive consideration for employment without regard to race, color, religious creed, sex (including pregnancy, childbirth, breast-feeding and related medical conditions), sexual orientation, gender identity, gender expression, national origin or ancestry, age, mental or physical disability (including medical condition), military or veteran status, political preference, marital status, citizenship, genetic information or other status protected by law or regulation.
We are committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or an accommodation, please email us at
emplymnt@chevron.com
.
Chevron participates in E-Verify in certain locations as required by law.",167050,10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1879,$10+ billion (USD),CO,144,data engineer,na,[],[],['excel'],[],[],[],,0-2 years
Crisp Inc,3.6,Remote,Data Engineer,"Here at Crisp, we value the strength in teamwork, and strongly believe that it’s the key to Crisp’s success. By bringing together bright, motivated creators wherever they live and work, we leverage diverse experiences and backgrounds to understand the challenges facing our food system and solve them together. Come join us, and help build the type of business you’d like to be a part of.
Crisp is a socially conscious, distributed team. We give you the opportunity to solve challenges in the global food industry while living where you’re most comfortable and working in areas where you can help foster and grow the community that you are a part of. We believe in transparency, diversity, and merit, and foster a culture of empowerment, personal impact and career growth.
As a Data Engineer at Crisp, you will help unlock the potential of our customers’ data by highlighting and elevating the semantic context. Your responsibilities will include data cleansing, semantic labeling, normalization, and using modern BI Technology to efficiently convey insights to our customers. Being part of the engineering team, you will not only help clients leverage our data platform, you will also help evolve the platform itself by being a subject matter expert involved in product development.
This is an evolving role with ample opportunity for growth. Whether you are coming from a startup or corporate background, you appreciate the significant impact to be had in smaller organizations and you relish the ability to shape your own role and the future of the company.
Signs of a great candidate for Crisp:
Collaborative: You know that your colleagues’ perspectives will make our customers successful. Similarly, you use your strengths to help us grow together. You propose ways for us to be more valuable to our customers.
Customer focused: Our customers are at the forefront of your day. You prioritize our customers’ voices to ensure their needs are met.
Ambitious, curious, and resourceful: You are innately curious, and you aren’t afraid to work hard. You are self-driven and able to find creative results on your own, but you also take direction well. You are driven to succeed because your hard work and results make you proud.
Disciplined and reliable: You enjoy the benefits of working on a distributed team while consistently delivering what you have committed to. When you hit a snag, you communicate and reset expectations early.
Appreciative of honest feedback: You know that the best way to learn and grow is through constructive feedback delivered kindly. You view feedback given to you as an opportunity to get better and strive to do the same for others.
Work smarter and harder: You often identify a problem, create a solution, and bring it to a state of completion - with others, or even on your own. You find ways of eliminating or automating stuff that is uninteresting or wasteful.
Signs of a great candidate for a Data Engineer:
Data oriented: You think about data in a rigorously structured manner. You live by the mantra “garbage in, garbage out” and are deeply experienced in the art of data cleansing.
Focus on the business problem: You are passionate about using visualizations to tell stories and glean actionable insights. In order to tell the story the right way, you need to understand how the business works and how to communicate with different stakeholders.
Brings business context to data engineering: You are the bridge between the business problem and the data pipeline. You are experienced in codifying business context via a semantic definition layer. You enable automation of labor intensive workflows.
Strong sense of aesthetics and user experience: You feel strongly about not just making Business Intelligence visually appealing, but also ensuring that it’s easy to learn and a pleasure to use.
Deep tooling expertise: Many BI tools have a point and click design layer, but you have a deep understanding of the modeling layer and how it interacts with the underlying data store. You are familiar with one or more tools that facilitate data exploration for the purposes of data cleansing or normalization. You hold strong opinions, born from experience, on the features that make a great semantic definition metadata capture tool. You are adept at transforming data in analytical databases using SQL.
We are building a team of people with a breadth of combined experiences so that we can collaboratively enable our customers to be successful. There are no hard requirements on specific background, experience or geographical location. Instead we’re looking for individuals that are capable, reliable, and hoping to grow along with us. Do you have strengths you can share? If so, we’d love to hear from you!",112889,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2018,Unknown / Non-Applicable,Remote,5,data engineer,na,['sql'],[],[],[],[],[],,
Horizon Health Alliance,4.0,"Getzville, NY",Data Engineer,"Are you seeking a rewarding and fulfilling career in the Mental Health and Addictions field?
Apply to be a Data Engineer today!
What will your day look like?
At Horizon, you will enjoy a supportive, team-based work environment. Have a question? There is always someone there to help! We offer a seamless onboarding experience that will ensure your success in your new role.
As a Data Engineer at Horizon you will...
Acquire and assemble datasets that align with business needs
Identifying, designing, and implementing internal process improvements including optimizing data delivery, and automating manual processes
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency
Working with stakeholders including executive teams and assisting them with data-related technical issues
Working with stakeholders to support their data infrastructure needs and understand company objectives
Build, test, and maintain database pipeline architectures
Create new data validation methods and data analysis tools
Assist and support development of data governance policies
Create and distribute corporate reports using industry standard reporting tools and software such as SQL, Power BI, and Python.
Develop and maintain the workflows and security controls related to data extract, transform, and load (ETL) of corporate data
Providing technical expertise in data storage structures, data mining, and data cleansing.
Develop and maintain databases, data marts, models, data sets, and other key technical solutions
Why choose Horizon to build your career?
Besides the fact that we’ve been named a Best Place to Work for 14 (yes, 14!) years in a row? At Horizon, you can be assured that you will make difference in the lives of others. Even better, your teammates will be just as motivated to make a difference!
What we offer that you’ll love…
Company Culture: At Horizon, we pride ourselves on cultivating an atmosphere of teamwork where all employees feel heard and valued.
Diversity & Inclusion: We are committed to equity, racial justice, and equal opportunity for all, and strive toward this goal through the work of our Diversity, Equity, Inclusion and Belonging Council, frequent trainings, ongoing conversations, affinity groups, and more.
Trainings, Trainings, and More Trainings: We have an entire team dedicated to your personal development and professional growth.
Team Building, Connection, and Relationships: At Horizon, we’re more than co-workers, we’re a community. We support each other, celebrate our achievements and milestones together, and have fun together!
Retirement: We know you want to retire comfortably and we’re here to help! Horizon offers 401(k) AND profit-sharing programs to make sure you’re set for the future.
Student Loan Assistance: We help pay off our team members' student loans every month. One year after joining, you’ll have been able to pay off an extra $600!
PTO & Holidays: We believe self-care is essential. Combined with holidays, you’ll earn up to 22.5 paid days in your first year. By your 3rd anniversary, that's almost doubled with up to 37.5 paid days off!
What makes you a great candidate?
We can’t wait to learn more about you! Here are a few specifics of what you’ll need for the job:
Bachelor's degree in Computer Science, Information Systems, Engineering or equivalent
1+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL and reporting/analytic tools
Strong experience in coding languages like SQL and Python
Fluent in relational based systems and writing complex SQL
Strong analytical and problem-solving skills
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Thorough knowledge of Microsoft Office
Ability to work with multidisciplinary teams
Ability to multitask and manage competing deadlines
Ability to communicate appropriately with all levels of management
Excellent understanding of Microsoft Office suite
Ability to build and optimize data sets
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytic skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Location:
55 Dodge Road, Getzville, NY 14068
Hours:
Full-time position, Monday - Friday; 8:00am-5:00pm
Disclaimer:
Horizon endorses public health measures including vaccinations. We encourage all applicants to be mindful of the fact that Horizon is a healthcare agency proving in person services throughout our community.
This information is intended to provide a general overview of the position; it is not a full job description.

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)",84681,1 to 50 Employees,Nonprofit Organization,,,-1,Unknown / Non-Applicable,NY,-1,data engineer,na,"['sql', 'python']",[],['power bi'],[],[],[],bachelor,+10 years
Epsilon,3.9,"Irving, TX",AWS Data Engineer,"Job Description

As a data engineer, you will design and maintain data platform road maps and data structures that support business and technology objectives. Naturally inquisitive and open to the deep exploration of underlying data, finding actionable insights, and working with functional competencies to drive identified actions. You also enjoy working both freely and as part of a team and have the confidence to influence and communicate with stakeholders at all levels, and to work in a fast-paced complex environment with conflicting priorities.
About the role:
Reporting into the delivery leader, you will deliver consumable, contemporary, and immediate data content to support and drive business decisions. The key focus of the role is to deliver a custom solution to support various business critical requirements. You will be involved in all aspects of data engineering from delivery planning, estimating and analysis, all the way through to data architecture and pipeline design, delivery, and production implementation. From the beginning you will be involved in the design and implementation of complex data solutions ranging from batch to streaming and event-driven architectures, across cloud, on-premise, and hybrid client technology landscapes.
Brief Description of Role:
We are looking for 3+ years of experience in data engineering in a customer or business facing capacity and experience in the following:
Ability to understand and articulate requirements to technical and non-technical audiences
Stakeholder management and communication skills, including prioritizing, problem solving and interpersonal relationship building
Strong experience in SDLC delivery, including waterfall, hybrid and Agile methodologies
Experience of implementing and delivering data solutions and pipelines on AWS Cloud Platform.
A strong understanding of data modelling, data structures, databases, and ETL processes
An in-depth understanding of large-scale data sets, including both structured and unstructured data
Knowledge and experience of delivering CI/CD and DevOps capabilities in a data environment
Develop new inbound data source ingestions required within the multi-tiered data platform to support analytics and marketing automation solutions
Supports data pipelines – Builds the required dimensions, rules, segments, and aggregates
Support all database operations: performance monitoring, pipeline ingestion, maintenance, etc.
Monitor platform health - data loads, extracts, failures, performance tuning
Create/modify data structures/pipelines
Leveraging capabilities of Databricks Lakehouse functionality as needed to build Common/Conformed layers within the data lake
Develop, document, and test software and environment setup to ensure that the outcome meets the needs of end-users and achieves business goals
Qualifications
The following skills are required:
Tech Stack: AWS pipeline, Glue, Databricks, Python, SQL, Spark, etc.
Building the Data Lake using AWS technologies like S3, EKS, ECS, AWS Glue, AWS KMS, EMR
Extensive experience in ETL and audience segmentation
Developing sustainable, scalable, and adaptable data pipelines
Attention to detail in design, documentation, and test coverage of delivered tasks
Strong written and verbal communication skills, team player
In addition, the candidate should have strong business acumen, interpersonal skills, and communication skills, yet also be able to work independently.
At least 3 years of experience with designing and developing Data Pipelines for Data Ingestion or Transformation using AWS technologies
At least 2 years of experience in the following Big Data frameworks: File Format (Parquet, etc.), Resource Management, Distributed Processing
At least 3 years of experience developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 1-2 years of experience with Spark programming (PySpark)
At least 2 years of experience with Databricks implementations
Familiarity with the concepts of “delta lake” and “lakehouse” technologies
The following skills are nice to have, and expertise is not required:
Adobe (Campaign, Audience Manager, Analytics)
MLFlow
Microsoft Power BI
SAP Business Objects

Additional Information

When you’re one of us, you get to run with the best. For decades, we’ve been helping marketers from the world’s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon’s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC:
Culture: https://www.epsilon.com/us/about-us/our-culture-epsilon
DE&I: https://www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR: https://www.epsilon.com/us/about-us/corporate-social-responsibility
Life at Epsilon: https://www.epsilon.com/us/about-us/epic-blog
Great People Deserve Great Benefits
We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.
#LI-SJ1
REF186919L",88775,5001 to 10000 Employees,Subsidiary or Business Segment,Media & Communication,Advertising & Public Relations,1969,$1 to $5 billion (USD),TX,54,data engineer,na,"['sql', 'python']","['databricks', 'aws']","['sap', 'power bi']",[],['spark'],[],,2-5 years
CINQCARE,4.0,"Washington, DC",Data Engineer,"Overview
The Data Engineer is a critical member of our growing data science team. In this role, you will have the chance to define and develop a core data asset which provides a representative and culturally aware view of the individuals and the communities that CINQCARE serves. You will work to evolve this asset over time using a product roadmap that includes identification and closure of gaps in existing data, introduction of new data sources, generation of proprietary data while quantifying and eliminating areas of structural biases. CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations and to so, we must also seek to fix gaps in data that ignore and marginalizes the Black and Brown communities that we serve.

An idea candidate for this role will embody CINQCARE’s core values, including, Trusted, Empathetic, Committed, Humble, Creative and Community-Minded. At CINQCARE, we don’t have patients or customers – we have Family Members.
Job Responsibilities
The Data Engineer will have the following responsibilities:
The Data Engineer will be responsible for the design, development and delivery of data pipelines and value-added data assets, leveraging a variety of data warehousing methodologies and disciplines to ingest the data from heterogeneous sources into cloud-based Data-Lake Environment in AWS.
Manage initiatives & projects of significant complexity and risk. Excellent business and communication skills to be able to work with business owners providing input to prioritized roadmaps, develop work estimates, and ensure successful delivery to support strategic planning and initiatives, improve organizational performance, and advance progress towards CINQCARE’s goals.
Assist in the overall architecture of the ETL Design and proactively provide inputs in designing, implementing, and automating the data pipelines.
Investigate and mine data to identify potential issues within the data pipelines, notify end-users and propose adequate solutions.
Ensure data quality and integrity within the data lake in the AWS environment with a focus on compliance to HIPAA and state level compliance requirements.
Oversee user permissions and configurations for adherence to documented access management standards and policies.
Independently (with minimal oversight) develop and maintain trusted advisor relationships with business, clinical, and operations leaders at the senior leadership level and with external partners, that include guidance for optimizing use of analytic capabilities and deliverables, and prioritization based on strategic vision.
Use coding/scripting pipelines and APIs to uncover and turn data into assets that are analysis-friendly using AWS services like Athena Data Catalogue, Quick Sight or any other big data tool on AWS.
Create a high-quality catalog of all pertinent data with the primary goal of establishing a single source of truth and significantly increasing productivity by reducing the time required for data search and discovery.
Crossing team boundaries, educate/advise on data projects, on how to combine and aggregate client data across platforms or technologies, and how to make the greatest use of data.
Lead consistent adherence to the Software Development Life Cycle framework and governance processes including, but not limited to leading planning sessions, collecting, and documenting requirements, identifying design patterns, create and define custom transformations, aggregations, and other data manipulations, developing data pipelines, creating documentation, developing test plans, performing unit testing, conducting peer review sessions.
Use knowledge of healthcare industry, market environment, and clinical and business workflows and activities, to inform solution design and development to execute high-quality or differentiated solutions in an established problem space.
Perform other job-related duties as assigned.

General Duties
The Data Engineer should have the following duties:
Leadership: The Data Engineer will lead the continued build out of the data asset to create business value, including collaborating with their team to design, develop, and execute those strategies and solutions to deliver desired outcomes.
Strategy: The Data Engineer will contribute to the business strategy and roadmap: (1) improve outcomes for CINQCARE Family Members; (2) enhance the efficacy of other CINQCARE. business divisions; and (3) develop and deliver external market opportunities for CINQCARE products and services. In establishing the business strategy, the Data Engineer will define and innovate sustainable revenue models to drive profitability of the Company.
Collaboration: The Data Engineer will ensure that AI capabilities form a cohesive offering, including by working closely with other business divisions to learn their needs, internalize their knowledge, and define solutions to achieve the business objectives of CINQCARE.
Knowledge: The Data Engineer will provide subject matter expertise in the AI solutions, including determining and recommended approaches for designing and building elegant data structures in support of existing reporting tools and custom visualization platforms.
Culture: The Data Engineer is accountable for creating a productive, collaborative, safe and inclusive work environment for their team and as part of the larger Company.
Qualifications
The Data Engineer should have the following qualifications:
Education: Bachelor’s degree in Computer Science, Engineering, Software Engineering, or related field; Master’s degree preferred.
Experience: The ideal candidate should have at least 3+ years of experience in healthcare data engineering. Experience with a variety of data projects and environments, whether on-prem or in-cloud (5+ years in SQL Server, ETL Tools, Business Intelligence & Analysis, Architecture). Familiarity with the Microsoft Stack; experience with other platforms is a plus. Mastery of Python and SQL. Strong foundational knowledge of data lakes and AWS products such as AWS Glue. Experience with healthcare eligibility and claims, implementing APIs, HL7/FHIR standards, ETL scheduling solutions, SQL, and healthcare data security.
Entrepreneurial: CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations. This position is accountable for ensuring CINQCARE is positioned to innovatively deliver on its promise.
Communication: Strong analytical and collaboration skills is required. Excellent verbal, written communication and presentation skills; ability to clearly articulate and present concepts and models in an accessible manner to CINQCARE’s team, investors, partners, and other stakeholders.
Relationships: Ability to build and effectively manage relationships with business leaders and external constituents.
Culture. Good judgement, impeccable ethics, and a strong team player; desire to succeed and grow in a fast-paced, demanding, and entrepreneurial Company.

Location: New York, NY
Compensation: $100,000-$120,000
My3ehHtP4z",110000,201 to 500 Employees,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'python']",['aws'],[],['sql server'],[],[],bachelor,+10 years
Liberty Source,3.0,"Hampton, VA",Data Engineer,"Company Background:

Liberty Source PBC combines state-of-the-art technology with a human overlay, enabling our clients to realize greater returns from their investments in artificial intelligence, machine learning, business intelligence and deep analytics platforms. We work with our clients’ Data Science teams, Data Operations staff, Data Quality functions, and other key stakeholders to refine and enhance the data that is vital to the success of their advanced technology initiatives. We are the Data Fitness experts.

Our specialized recruiting mission focuses on the talents of veterans and families of active-duty military personnel to fulfill our brand promise of 100% U.S.-based operations and staff.

Founded in 2014, Liberty Source PBC is based in Hampton, Virginia and is a Certified B Corporation.

Position Summary:

Are you ready to put your data wrangling skills to the test and take your career to the next level? Liberty Source is seeking a Data Engineer to join our rapidly growing team. The right individual will expand and optimize our data and data pipeline architecture and build the systems that collect, manage, and convert raw – and often unstructured - data into usable information for our clients.

The ideal candidate can demonstrate experience in optimizing data flow, has collected and distributed data across teams and across companies, has built data systems (both large and small) from the ground up, and isn’t afraid to wade into the lake for some data cleansing and transformation. We’re looking for an individual who is self-directed and comfortable supporting the data needs of multiple teams, systems, and products, and excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. Beyond technical prowess, our new Data Engineer will have the soft skills for clearly communicating highly complex data ideas and issues to the leadership team.

Position Responsibilities:

Create and maintain optimal data pipeline architecture
Build the infrastructure required for efficient ETL (extraction, transformation, and loading) of data from a wide variety of data sources using SQL, Hadoop and AWS ‘big data’ technologies.
Build analytic views that utilize the data pipeline to provide actionable insights into operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Leverage data to solve business problems, building and maintaining the infrastructure to answer questions and improve processes
Help streamline our data science customer workflows, adding value to our product offerings and building out the customer lifecycle and retention models
Work closely with customer data science and business intelligence teams to develop data models and pipelines for research, reporting, and machine learning
Be an advocate for best practices and continued learning

Requirements:

Bachelor’s degree in computer science, information technology, engineering, or related discipline.
Five or more years of experience in a Data Engineer role utilizing Python and data visualization/exploration tools.
Advanced SQL capabilities and experience working with relational databases, query authoring (SQL) as well as hands-on experience with a variety of data constructs.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Demonstrated ability to work with unstructured data.
Proven ability to build processes supporting data transformation, data structures, metadata, dependency.
Professional certifications such as Cloudera Certified Professional (CCP), IBM Certified Data Engineer or Google’s Certified Professional is a plus.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong project management and organizational skills.
Experience supporting and working with cross-functional, innovation-oriented teams in a dynamic environment.
Great communication skills, especially for explaining technical concepts to nontechnical business leaders.

Benefits:

Paid Time Off (PTO) and 10 paid holidays
Medical, dental, vision, life insurance, and other ancillary benefits
401k Plan

A pre-employment background check is required.

To learn more about our business, please visit our website at https://liberty-source.com/",91094,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2014,$5 to $25 million (USD),VA,9,data engineer,na,"['sql', 'python']",['aws'],[],[],['hadoop'],[],bachelor,
Lenovo,4.0,"Morrisville, NC",Data Engineer,"General Information
Req #
WD00037891
Career area:
Hardware Engineering
Country/Region:
United States of America
State:
North Carolina
City:
Morrisville
Date:
Wednesday, January 18, 2023
Working time:
Full-time
Additional Locations:
Morrisville - North Carolina - United States of America
Why Work at Lenovo
Here at Lenovo, we believe in smarter technology that builds a brighter, more sustainable and inclusive future for our customers, colleagues, communities, and the planet.

And we go big. No, not big—huge.

We’re not just a US$70 billion revenue Fortune Global 500 company, we’re one of Fortune’s Most Admired. We’re transforming the world through intelligent transformation, offering the world’s most complete portfolio of smart devices, infrastructure, and solutions. With more than 71,500 employees doing business in 180 markets, we help millions—not just the select few—experience our version of a smarter future.

The one thing that’s missing? Well… you...
Description and Requirements
Lenovo’s Infrastructure Solutions Group (ISG) is seeking a qualified candidate to join our global tools, data, and automation development team. This team designs, develops, deploys, and maintains software applications for productivity and automation solutions.
What You'll Do
The position will develop automation to drive efficiency and effectiveness, automate manual tasks, and extend existing automation platforms for others to build on. Additionally, this role will provide Technical Leadership to a worldwide team and utilize problem solving skills to provide solutions based on data for worldwide product assurance issues. Responsibilities Include:
Work with team to develop data strategies: data model, tools, storage, parsing, etc.
Define the physical components in data.
Define the configuration of a collection of components over time in data.
Define characteristics of a specific configuration over time in data.
Integrate with existing business data
Coordinate with functional teams within Lenovo to develop and automate data pipelines to continuously supply our analysts with clean and reliable data.
Work with to existing team to integrate, adapt, or identify new tools to efficiently collect, clean, prepare, and store data for analysis


Basic Qualifications:
Bachelor’s degree in Computer Science, Mathematics, Engineering, or in a related field
4+ years’ experience with object-oriented/object function scripting languages: Python, Scala, etc
4+ years’ experience building processes supporting data transformation, data structures, metadata, dependency and workload management
4+ years’ experience in data schema, data pipeline design and database management
4+ years’ experience in optimizing data pipelines, architectures and data sets Fluency in structured and unstructured data and management through modern data transformation methodologies


Preferred Qualifications:
4+ years’ experience with designing and managing data in modern ETL architect like Spark, Kafka, Hadoop, Snowflakes
Experience with cloud service environment
Working experience with NoSQL
Experience with Power BI ETL pipeline
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Strong analytical, problem solving, verbal and written communication skills



This position must sit in Morrisville, NC.
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any federal, state, or local protected class.
Lenovo adopted a COVID-19 Vaccination Policy for US-based employees. As a condition of employment, employees must adhere to Lenovo’s US Vaccination Policy and be fully vaccinated against COVID-19, subject to any applicable accommodations. To be fully vaccinated means individuals must receive the full series of a vaccine either approved by the FDA or WHO and listed by the CDC (e.g. two dose of the Moderna, AstraZeneca or Pfizer-BioNTech vaccines; or one dose of the Johnson & Johnson vaccine). This applies to all US-based employees, contractors and interns, regardless of work location. As a condition of employment, you must provide proof that you are fully vaccinated or follow Lenovo’s accommodation process.
TO BE DELETED - Multiple Cities (OLD)
Morrisville - North Carolina - United States of America
Multiple Countries (Posting Locations)
United States of America
Multiple States (Posting Locations)
North Carolina
Multiple Cities (Posting Locations)
Morrisville - North Carolina - United States of America",102019,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1984,$10+ billion (USD),NC,39,data engineer,na,"['nosql', 'go', 'python', 'scala']",[],['power bi'],[],"['kafka', 'hadoop', 'spark']",[],bachelor,+10 years
"i6 Group, Ltd.",4.2,Remote,Junior Data Engineer,"Junior Data Engineer
Job Location: Fully Remote – UK/EU
Job Type: Full-time, Permanent
Relocation: No
We’re pioneering the use of data to improve efficiency, transparency, and sustainability. Today, our aviation fuel management technology connects business functions, drives operational efficiencies, and enables environmental accountability across the globe. Our ambition is big, we’re transforming the aviation industry, and we need great people to make that happen.
Bring Your Skills and Passion to Life at i6:
As our company grows and matures from a start-up into a scale-up with an increase in customers and multi-tenancy applications, the opportunity for a curious and inspired Junior Data Engineer has opened up in our Data Team. This is your chance to join a team of innovative and like-minded engineers, analysts, and innovators pushing the boundaries of their products and themselves, working with our high-profile global customers to craft truly outstanding SaaS solutions. You will be responsible for creating, owning and driving the data solutions, including transforming data into critical assets for the business while ensuring security, reliability, privacy and performance by design.

Your Career at i6:
In your first month you’ll…
Ask lots of questions and begin to explore and really get to know our products, our clients and gain deeper insight into the Aviation industry
Start to build an idea of the challenges and requirements of our clients, think about what it really means to connect our biosphere to highly varied client architectures managing multiple interactions a second and dealing in near real time
Begin to translate business issues and requirements into data solutions
Maintain monitoring and alerting across the data processing systems
Start to learn and develop your skills in implementing data processes, ensuring consistency & re-usability
Start to work closely with data analysts ensuring data and its required structure & availability is accessible for reporting

Within 6 mths you’ll…
Start to develop the knowledge, expertise and confidence to identify pain points , challenge the status quo, then pioneer new ideas, develop designs and implement solutions helping us scale faster and drive the business forward
Participate fully in, and contribute to, sprint planning, retrospectives, standups and other ceremonies in a constructive and can-do manner
Begin to gather data requirements, working closely with business stakeholders and analysts and design & build the required integrations into the data pipeline following industry best practices on ETL, ELT processes & data warehousing to support project delivery
Working closely with the infrastructure team ensuring business data applications are well served and prioritise when required
Working closely with Solutions Architects, Developers, Product Owners & Managers to ensure adherence to data models when developing new products or applications
Start implementing data analysts reporting requirements into the data warehouse or pipeline

Within 1 year, you’ll…
Be able to proactively identify gaps & solutions in the data functionality requirements of the business
Effectively manage challenges associated with handling large volumes of data while working towards tight deadlines
Be proposing, researching & supporting the implementation of technical solutions that help the business achieve its commercial objectives in a cost-efficient and scalable manner
Feeling proud of what you have achieved, the influence you have had and how you have made positive changes for yourself, your team, i6, your clients and the planet
What you’ll bring to the role:
Bright, ambitious, humble and most of all inquisitive, you love telling stories with data, building reports and propositions that give real insight and impact, ensuring our products remain the best in the market. You love data and are a true evangelist, able to use your outstanding analytical and architecture skills to power internal insight and supercharge our data focused products. Data is the secret sauce to your superpower, and you are ready to share your expertise to help launch us to the next level.
What you will be working on: (You don't need all of this, but outstanding SQL is an essential)
Enterprise Systems
Near Real-Time Applications
Good understanding and experience using the following technologies: SQL, Python, Airflow, DBT, BigQuery, GitHub, GitHub Actions
Nice to have understanding of the following technologies: BI Tools (e.g. Tableau), Docker, Kubernetes, Google Cloud Platform, AWS, Azure Cloud
Bonus knowledge: Node.js, Typescript, MongoDB
SaaS
A dedicated, creative, and highly collaborative team of developers, engineers and analysts
A massive variety of systems and products for our major clients across the globe

Our Interview Process:

Stage 1: Initial screening interview with Talent Manager
Stage 2: In depth technical interview our Senior Data Engineer
Stage 3: Cultural interview with our CTO
Stage 4: Offer
Stage 5: Hand in notice and join our awesome team at i6

A Career With i6 Group

Our Values, The Six I’s That Make i6:
Improve - we’re open to new ideas and deliver an amazing customer experience.
Influence - we build strong relationships and make great decisions.
Impress - we work hard and smart to deliver great work on time.
Innovation - we share ideas, experiment and find new ways to solve challenges.
Intelligence - we continue to develop our skills, knowledge and behaviours.
Integrity - we respect and embrace differences in people.

Our Benefits

Subsidised subscriptions to Headspace, gym membership and health insurance (UK Only)
Various discounts and perks at high street shops, supermarkets and online vendors
Fully remote – work from anywhere within the UK/ Europe (with occasional travel to Farnborough Airport)
Clear goals, targets, and progression plan so you can maximise your career
Cross training opportunities
Sustainable workplace - Carbon footprint offset

Diversity & Inclusion: Equal opportunities for everyone.
We’re embracing diversity in all its forms and fostering an inclusive environment for all people to do their best work. This is integral to our mission of driving operational efficiency and environmental accountability across the world.

QTPX6EGOKQ",112889,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,Unknown / Non-Applicable,Remote,10,data engineer,na,"['sql', 'python']","['azure', 'aws', 'google cloud']",['tableau'],"['mongodb', 'dbt']",[],['docker'],,
Kaizen Dynamics,4.0,Remote,Data Engineer,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and system administrator for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Support the System Administration of associated tools and software of data and analytics landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, and analytic products. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
ETL processes development: 10 years (Preferred)
Work Location: Remote",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,[],[],[],[],[],[],master,0-2 years
Sanametrix,3.8,Remote,AWS Data Engineer,"Candidates Must be able to obtain/maintain a Public Trust (US Citizenship is required). This position is full-time w/benefits and 100% remote!
Sanametrix, Inc. is a fast-growing small business headquartered in Arlington, VA. We are dedicated to providing federal agencies with legendary customer service and focused solutions for their business and technology needs. This role is responsible for building data pipelines for transferring data from source systems (virtual machines, Microsoft SQL Server) into AWS Cloud using AWS Native Tools. This resource has strong data modeling and scripting experience and has a strong knowledge of AWS Data Services.
Responsibilities:
Perform data processing, algorithm / structures, pipeline orchestration, data quality, governance, discovery.
Work with structured and unstructured data, blob data
Develop and work with APIs
Collect and organize data using data warehousing technique and file storage technologies
Perform ELT and ETL processes
Gather data requirements
Develop and maintain scalable data pipelines and build out new API integrations to support continuing increases in data volume and complexity.
Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the organization.
Implement processes and systems to monitor data quality, to ensure production data accuracy, and ensure key stakeholder and business process access.
Write unit/integration tests, contribute to engineering wiki, and documents.
Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Work closely with a team of front-end and back-end engineers, product managers, and analysts.
Design data integrations and dataquality framework based on established requirements.
Must be able to obtain a Public Trust (US Citizenship required to obtain Public Trust)
Qualifications & Skills:
Scripting
SQL & Scripting
Python
Spark
Linux / shell scripting
Services / Tools (six or more)
S3 Lambda
Redshift
Lake Formation
Glue ETL
Kinesis
DMS
Glue catalog/Crawlers
Git
Jira
Airflow /Orchestration
Education, Experience, and Licensing Requirements:
BS or MS degree in Computer Science or a related technical field
4+ years of Python or Java development experience
4+ years of SQL or NoSQL experience
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining data processing systems
AWS Certified is preferred
Job Type: Full-time
Pay: $110,000.00 - $120,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Work Location: Remote",115000,Unknown,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'shell', 'java', 'nosql', 'python']","['aws', 'redshift']",[],['sql server'],['spark'],[],,5-10 years
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'python']","['aws', 'redshift']",[],"['dynamodb', 'oracle']","['kafka', 'spark']",[],bachelor,0-2 years
Epsilon,3.9,"Irving, TX",AWS Data Engineer,"Job Description

As a data engineer, you will design and maintain data platform road maps and data structures that support business and technology objectives. Naturally inquisitive and open to the deep exploration of underlying data, finding actionable insights, and working with functional competencies to drive identified actions. You also enjoy working both freely and as part of a team and have the confidence to influence and communicate with stakeholders at all levels, and to work in a fast-paced complex environment with conflicting priorities.

About the role:

Reporting into the delivery leader, you will deliver consumable, contemporary, and immediate data content to support and drive business decisions. The key focus of the role is to deliver a custom solution to support various business critical requirements. You will be involved in all aspects of data engineering from delivery planning, estimating and analysis, all the way through to data architecture and pipeline design, delivery, and production implementation. From the beginning you will be involved in the design and implementation of complex data solutions ranging from batch to streaming and event-driven architectures, across cloud, on-premise, and hybrid client technology landscapes.

Brief Description of Role:

We are looking for 3+ years of experience in data engineering in a customer or business facing capacity and experience in the following:
Ability to understand and articulate requirements to technical and non-technical audiences
Stakeholder management and communication skills, including prioritizing, problem solving and interpersonal relationship building
Strong experience in SDLC delivery, including waterfall, hybrid and Agile methodologies
Experience of implementing and delivering data solutions and pipelines on AWS Cloud Platform.
A strong understanding of data modelling, data structures, databases, and ETL processes
An in-depth understanding of large-scale data sets, including both structured and unstructured data
Knowledge and experience of delivering CI/CD and DevOps capabilities in a data environment
Develop new inbound data source ingestions required within the multi-tiered data platform to support analytics and marketing automation solutions
Supports data pipelines - Builds the required dimensions, rules, segments, and aggregates
Support all database operations: performance monitoring, pipeline ingestion, maintenance, etc.
Monitor platform health - data loads, extracts, failures, performance tuning
Create/modify data structures/pipelines
Leveraging capabilities of Databricks Lakehouse functionality as needed to build Common/Conformed layers within the data lake
Develop, document, and test software and environment setup to ensure that the outcome meets the needs of end-users and achieves business goals

Qualifications

The following skills are required:
Tech Stack: AWS pipeline, Glue, Databricks, Python, SQL, Spark, etc.
Building the Data Lake using AWS technologies like S3, EKS, ECS, AWS Glue, AWS KMS, EMR
Extensive experience in ETL and audience segmentation
Developing sustainable, scalable, and adaptable data pipelines
Attention to detail in design, documentation, and test coverage of delivered tasks
Strong written and verbal communication skills, team player
In addition, the candidate should have strong business acumen, interpersonal skills, and communication skills, yet also be able to work independently.
At least 3 years of experience with designing and developing Data Pipelines for Data Ingestion or Transformation using AWS technologies
At least 2 years of experience in the following Big Data frameworks: File Format (Parquet, etc.), Resource Management, Distributed Processing
At least 3 years of experience developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 1-2 years of experience with Spark programming (PySpark)
At least 2 years of experience with Databricks implementations
Familiarity with the concepts of ""delta lake"" and ""lakehouse"" technologies

The following skills are nice to have, and expertise is not required:
Adobe (Campaign, Audience Manager, Analytics)
MLFlow
Microsoft Power BI
SAP Business Objects

Additional Information

When you're one of us, you get to run with the best. For decades, we've been helping marketers from the world's top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon's best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC:
Culture: https://www.epsilon.com/us/about-us/our-culture-epsilon
DE&I: https://www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR: https://www.epsilon.com/us/about-us/corporate-social-responsibility
Life at Epsilon: https://www.epsilon.com/us/about-us/epic-blog

Great People Deserve Great Benefits

We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.

#LI-SJ1

REF186919L",90830,10000+ Employees,Company - Public,Media & Communication,Advertising & Public Relations,1926,$5 to $10 billion (USD),TX,97,data engineer,na,"['sql', 'python']","['databricks', 'aws']","['sap', 'power bi']",[],['spark'],[],,2-5 years
"AGM Tech Solutions, LLC",4.8,"Alpharetta, GA","Senior Data Engineer (Must be local to Atlanta, GA)","Sr. Data Engineer
Location: Must be local to Atlanta Metro area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $75.00 - $85.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Application Question(s):
How many years of experience do you have as a Data Engineer?
How many years of experience do you have building data pipelines with Spark or Databricks?
Can you work on W2 without sponsorship? No C2C engagements for this position.
Are you currently located in Atlanta Metropolitan area?
Experience:
Python: 4 years (Preferred)
Spark: 3 years (Preferred)
AWS: 3 years (Preferred)
Work Location: One location",144000,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['sql', 'java', 'python']","['databricks', 'azure', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",[],,2-5 years
OPENMIND TECHNOLOGIES INC,4.2,"King of Prussia, PA","Data Engineer - Hybrid Onsite - King of Prussia, PA","Details:
JOB DESCRIPTION:
We are working with a client in need of a talented Data Engineer to support our client located in King of Prussia, PA. This contractor will be working on a high-level corporate initiative to build an enhanced customer service model for a leading utilities organization. This contractor will be working with our client on Data Engineering topics, including creating relevant data models, developing powerful data pipelines, exposing them through various mechanisms including APIs, and using data visualization tools to efficiently present data.
Responsibilities:
· Partner with our client’s leadership teams, engineers, program managers and data analysts to understand data needs.
· Design, build and launch efficient and reliable data pipelines transforming data into useful report ready datasets.
· Use your data and analytics experience to ‘see what’s missing,’ identifying and addressing data gaps, build monitors to detect data quality issues and partner to establish a self-serve environment.
· Broad range of partners equates to a broad range of projects and deliverables, including ML Models, datasets, measurements, services, tools and process.
· Leverage data and business principles to automate data flow, detect business exceptions, build diagnostic capabilities, and improve both business and data knowledge base.
· Build data expertise and own data quality for your areas.
QUALIFICATIONS:
· At least 4+ years' of advanced SQL experience (including at least one SQL DBMS and one no SQL).
· 4+ years' of Python development experience.
· 3+ years' experience with Data Modeling.
· Experience analyzing data to discover opportunities and address gaps.
· Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. SnowFlake, Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
· BSc/BA in Data Science, Computer Science, Engineering.
· Familiarity with SAP order generation and invoicing modules
Job Types: Full-time, Contract
Salary: $75.00 - $85.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
advanced SQL (including at least one SQL DBMS, one no SQL: 4 years (Preferred)
Python development: 4 years (Preferred)
Data Modeling: 3 years (Preferred)
analyzing data to discover opportunities and address gaps: 1 year (Preferred)
cloud or on-prem Big Data/MPP analytics platform: 1 year (Preferred)
SnowFlake, Netezza, Teradata, AWS Redshift, Google BigQuery,: 1 year (Preferred)
Azure Data Warehouse: 1 year (Preferred)
SAP order generation and invoicing modules: 1 year (Preferred)
Work Location: One location",144000,Unknown,Private Practice / Firm,Information Technology,Enterprise Software & Network Solutions,-1,Unknown / Non-Applicable,PA,-1,data engineer,na,"['sql', 'python']","['snowflake', 'aws', 'azure', 'redshift']",['sap'],['snowflake'],[],[],,0-2 years
FlexIT Inc,4.0,"Beaverton, OR",Full Stack Data Engineer,"APLA is building capabilities around the company's data foundation to build data sources that are needed for reporting and analytics
The type of engineer were looking for is a Full Stack Data Engineer
Knowledge of data visualization engineering as well as consumption and view build engineering",106106,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,[],[],[],[],[],[],,
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,[],[],[],[],[],[],bachelor,
ITEOM,4.0,Remote,Data Engineer - Remote,"Our client is seeking a Data Engineer who would be responsible for design of data warehouse schemas as well as end-to-end design and implementation of fault tolerant and scalable data processing pipelines using a variety of technologies for orchestrating data movement, primarily Snowflake , DBT and Airflow . You'll work with a team of data engineers to design and implement solutions that provide business-critical insights.
Salary : Mid-Level - 130K to160K+ bonuses and equity options
Location: REMOTE WORK 100% is an option: (locations limited to CO, AZ, TX, VA, CA, GA, HI, MN, MI, NJ, NY, NC, OR, PA, WA)
How You’ll Do It
Working collaboratively with a team of other data engineers and developers of varying experience levels and areas of expertise, you'll be responsible for:
Responsibilities
Design and implement data warehouse schemas to store large data sets that can be easily and efficiently queried and joined together for reporting and analytical purposes.
Design and develop ETL/ELT pipelines to efficiently move and aggregate data, while optimizing for maximum code and data reuse.
Collaborate with other team members on improvements to existing systems
Investigate data anomalies and provide quick resolutions.
Provide technical support to business users and analysts.
What We’re Looking For
TOP SKILLS: ETL, Data Warehousing, SQL, Jinga Scripting, Python and Snowflake.
3+ years of working experience with large scale data warehouse systems.
Very strong knowledge of SQL and data manipulation best practices
Experience in building efficient and fault tolerant ELT/ETL data pipelines
Very strong knowledge of working with large scale datasets and data modeling and data warehouse design
Experience with Snowflake is preferable
Knowledge of DBT, Jinja scripting and Airflow is a big plus
Nice to Have
Experience working cross functionally with product and engineering teams
Desire to wear many hats and work your tail off for a bit - great earning potential and equity available.
Benefits: A diverse benefit package, sign on bonus, options, etc. is offered through our client.
About ITEOM: ITEOM Digital Technology Talent Partners collaborates with top technologists and top employers to align purpose and passion and re-incorporate belonging to make the job search and work experience more human. We are an equal opportunity employer; however, this role is not eligible for sponsorship at this time. Employees eligible for full time W2 employment are encouraged to apply.",145000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['snowflake'],[],"['snowflake', 'dbt']",[],[],,+10 years
Globaleur,4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes.
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders.
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information.
Deploy sophisticated analytics programs, machine learning, and statistical methods.
Ensure compliance with data governance and security policies.
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition.
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences.
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields.
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing.
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java.
Have working experiences in e-commence, travel, marketing domain is a plus.
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies.
Excellent problem solving and troubleshooting skills.
Process oriented with great documentation skills.
Excellent oral and written communication skills with a keen sense of customer service.


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.",119183,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017,Unknown / Non-Applicable,CA,6,data engineer,senior,"['java', 'nosql', 'sql', 'go', 'python']","['azure', 'aws']",['tableau'],['mongodb'],[],[],bachelor,
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['sql', 'python']","['databricks', 'azure', 'aws']",[],[],[],[],,0-2 years
plaxonic,4.6,"Hunt Valley, MD",AWS python data engineer,"AWS Python Data Engineer
Must Have: AWS Databricks Python, Spark, PySpark
Location Hartford, CT or St. Paul, MN or Hunt Valley, MD
Roles & responsibilities:
Acts as a single point of contact for data migration to AWS projects for customer
Provides innovative and cost-effective solution using AWS, Spark, python & customer suggested toolset
Optimizes the use of all available resources
Develops solutions to meet business needs that reflect a clear understanding of the objectives, practices and procedures of the corporation, department and business unit
As a leader in the Cloud Engineering you will be responsible for the overseeing development
Learn/adapt quickly to new Technologies as per the business need
Develop a team of Operations Excellence, building tools and capabilities that the Development teams leverage to maintain high levels of performance, scalability, security and availability
Skills:
The Candidate must have 3-5 yrs of experience in PySpark & Python
Hands on experience on AWS Cloud platform especially S3, lamda, EC2, EMR
Experience on spark scripting
Has working knowledge on migrating relational and dimensional databases on AWS Cloud platform
Relevant experience with ETL methods and with retrieving data from dimensional data models and data warehouses.
Strong experience with relational databases and data access methods, especially SQL.
Knowledge of Amazon AWS architecture and design
Job Type: Full-time
Schedule:
8 hour shift
Experience:
Python: 3 years (Preferred)
Work Location: On the road",81908,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),MD,10,data engineer,na,"['sql', 'python']","['databricks', 'aws']",[],[],['spark'],[],,2-5 years
"WeVideo, Inc.",4.4,"Mountain View, CA","Senior Data Engineer, Analytics","Job type: Full-time position. Location: Mountain View, CA

What inspires you? This is the question that drives most career decisions.

Is it working with a fantastic team that is dedicated to a common goal? Is it the ability to make a significant impact on the success of a product and company? Perhaps it is to contribute to a product that ignites the creativity of content creators, small businesses, educators, and schoolchildren worldwide?

We have an immediate opening for a Senior Data Engineer. You will be working within the Analytics team at WeVideo to help build out and grow our data infrastructure. The team plays a central role in shaping the data ecosystem in our company, and enables business performance by providing users across the company with the insights, tools, infrastructure, and consulting to make data-driven decisions. Our org is hungry for data and needs you to help us get to the next level.

What you will do in this role:
Be the lead engineer that builds and automates data pipelines for our Product data, as well as enable Data Integration projects.
Develop and improve the technical architecture of current data warehouse (BigQuery)
Design data models optimized for aggregation, visualization and advanced analytics (machine learning)
Design, implement and maintain data pipelines from beginning to end. Our Product data pipeline is high velocity and we need to re-architect it using Apache Kafka. Our Business data pipes are standard fare, and we use ‘modern data stack’ tooling (ELT, reverse ETL, DBT, self-serve BI)
Facilitate data integrations and write optimized data transformations in SQL/Python
Implement automated QA and data quality checking systems for pipelines and warehouse
Leverage automation using Apache Airflow where possible to help the team scale
Skills and knowledge you possess:
2+ years in data engineering with a Bachelor's degree in CS, Data Science, or similar technical field, or equivalent professional experience
Demonstrated success building and automating batch and streaming data pipelines, as well as end-to-end Monitoring and Alerting solutions
High fluency with advanced SQL in BigQuery (or Redshift) environment
Proficient in creating high quality, fast services and projects in Python
Experience building data pipes using Apache Kafka or Pub/Sub, Apache Airflow, GCP/AWS
Strong communication skills and interest in working with Marketing & Sales teams
Benefits & perks :
Potential remote opportunity
Medical, dental, vision and 401(k)
Generous PTO policy
Free lunch and snacks
Enough free caffeine to keep you up for 2 weeks straight
Employee development resources
Why you might like working here:
We’re a small, close-knit team that enjoys working and learning from each other.
People stick around. Some of your future colleagues have been for over 8+ years.
Our users love us; just take a look at the tweets shared by teachers.
About WeVideo:
WeVideo is a powerful, easy to use, cloud-based video creation platform that is the digital editing and storytelling choice of more than 22 million consumers, students, businesses, and third-party media solutions. WeVideo is available from virtually any computer or device at home, school, work, or on-the-go to capture, edit, view, and share videos. Built for the future in HTML5, WeVideo brings maximum speed, responsiveness, security, and expandability to browser-based video editing. WeVideo is a Google Play Editors' Choice selection with more than 9 million downloads to date. WeVideo is also the exclusive digital storytelling solution of Google’s Education Creative Bundle for Chromebooks and a Microsoft Education Partner. More than 6,500 schools use WeVideo to enhance classroom learning.",150960,Unknown,Company - Private,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['sql', 'go', 'python']","['aws', 'redshift']",[],['dbt'],['kafka'],[],bachelor,+10 years
Quirch Foods,3.0,"Coral Gables, FL",Data Engineer,"About Quirch Foods
In business for over 50 years, Quirch Foods is a global distributor and exporter of fresh and frozen food products with a unique focus on ethnic cultures. We service our customers from 5 distribution centers strategically located in the United States. We have consistently been ranked among the top 50 exporters in the US and are one of the largest importers of seafood. Our customers include independent grocers, chain supermarkets, foodservice distributors, cruise lines, restaurants and food processors/manufacturers. We have an extensive product list with over 8,000 SKU's and carry niche brands such as Panamei, Mambo and Chiquita along with national brands like Tyson, IBP, Excel, JBS, Iowa Premium, Smithfield, Dietz & Watson, along with many others. We are headquartered in Coral Gables, FL.
Quirch is a portfolio company of Palladium Equity Partners, a leading mid-market private equity fund based in New York City, which partnered with the Executive Team to capitalize on Quirch Foods’ record of excellence and to grow it decisively beyond its current business.
Position Summary
At Quirch Foods we believe in leveraging technology as a source of competitive advantage. This includes customized ERP Solutions, Business Intelligence, EDI, Web and Mobile Solutions. We are currently looking for a Data Engineer to join our development efforts.
Essential Duties and Responsibilities:
Develop, test and maintain custom business solutions by leveraging state of the art software development practices using an agile methodology
Evaluate business requirement and help define problems and develop solutions
Participate in design and architecture discussions with business leaders, end users and IT team members
Document business requirements and solution, document code and provide support for creation of end user documentation
Update business knowledge, technical skills and soft skills, leverage educational opportunities, participate in professional organizations
Experience:
MS SQL Programming
Visual Studio\C#\WPS Experience
Microsoft SSAS, SSIS, SSRS desired but not required
Biztalk experience desired but not required
Skills and Requirements:
Bachelor’s degree in Computer Science, Engineering, Math or equivalent and\or related experience and training preferred
Strong analytical and problem-solving skills
Strong collaborator and team player
Great organizational skills, attention to detail and follow thru
Effective oral and written communication skills
Extensive familiarity with data management principles
Benefits:
Professional growth and developmental opportunities to grow your skills using state of the art technology
Comprehensive benefits package that includes: Medical, Dental, Prescription Drug Plan, Disability Plan, Life insurance Plan
401K savings Plan
Paid Holidays
Personal Time off
Employee Discounts
Quirch Foods is an Equal Opportunity Employer (EOE). Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. All applicants must be eligible to work in the United States.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Coral Gables, FL 33134: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What is your desired compensation?
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Data Engineering: 1 year (Preferred)
Work Location: In person",89340,201 to 500 Employees,Company - Private,Retail & Wholesale,Wholesale,1967,$500 million to $1 billion (USD),FL,56,data engineer,na,['sql'],[],"['ssis', 'excel']",[],[],[],bachelor,0-2 years
TEKletics,4.0,"Scottsdale, AZ",Azure Data Engineer,"Azure Data Engineer
Tekletics is an information technology company focused on providing quality resources to support our customers needs. Successful candidates will work with world class organizations to deliver projects. Each candidate is required to have a strong work ethic and the ability to handle high pressure situations.
A little about this role:
As the Azure Data Engineer, you are primarily responsible for the collection and transformation of data across a multitude of data sources. This individual is also responsible for the optimization of the environment, structure, and processes associated with said data.
A day in the life:
Data Warehouse - As the Azure Data Engineer, you are responsible for the data warehouse design, development, testing, support, and configuration. You will review business requests for data warehouse data and data warehouse usage. You will also research data sources for new and better data feeds ensuring consistency and integration with existing warehouse structure.
Data Collection – You will be responsible for developing automated data pipelines and/or data integrations within the Azure Synapse environment. You will use SQL, Python scripts and Azure Functions to automate data collection from a wide variety of sources.
o API utilization – ability to leverage REST APIs as needed.
Data Transformation – You will create BI (Business Intelligence) and Data Warehousing cube design. You will create and manage ETL/ELT processes to transform and load data into data warehouse for reporting and analytics.
Data Optimization – As the Azure Data Engineer, you will create and maintain standards and policies. You will identify, design, and implement internal process improvements, including automation of manual processes and optimization of data delivery. You will continuously improve data reliability, efficiency, and quality.
Training – Identify and demonstrate techniques to optimize reporting for Data Visualization Analyst, provide and participate in internal and external training sessions, and produce documentation to support understanding and learnings around the Presence data/reporting environment.
This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Projects and responsibilities may change at any time with or without notice due to our business, industry, and/or market changes.
What we are looking for:
Previous experience using SQL, Python scripts and Azure Functions
Experienced in Azure Synapse environment
Experience designing, building, and maintaining data processing system
Dependable, extroverted, diplomatic person, able to problem-solve successfully with a wide variety of people and issues
Attention to detail and strong organizational skills, self-motivated
Ability to work independently while being a strong team player
Ability to mentor junior level developers
Passion for innovation and “can do” attitude to thrive in a fast-paced environment
Proficient in time management and adhering to deadlines
Knowledge and interest of the natural products/brands and retail landscape is a plus
Proficient computer (MS Office applications) and data-mining skills
Flexibility to successfully multi-task in a fast-paced environment with a positive attitude
Regular and predictable attendance is required
Ability to manage time and deadlines
Job Type: Full-time
Pay: $79,947.00 - $142,509.31 per year
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Scottsdale, AZ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 3 years (Required)
SQL: 3 years (Required)
Data warehouse: 3 years (Required)
Work Location: In person",111228,,,,,-1,,AZ,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],[],[],,2-5 years
Solect,3.4,"San Francisco, CA",Senior Data Engineer,"Job Locations US-CA-San Francisco | US-TX-Houston
Posted Date 1 month ago(4/7/2023 5:15 PM)
# of Openings
1
Category
Enterprise Technology
Job ID
2023-2250
Overview
Company Overview
Pattern Energy is a leading renewable energy company that develops, constructs, owns, and operates high-quality wind and solar generation, transmission, and energy storage facilities. Our mission is to transition the world to renewable energy through the sustainable development and responsible operation of facilities with respect for the environment, communities, and cultures where we have a presence.

Our approach begins and ends with establishing trust, accountability, and transparency. Our company values of creative spirit, pride of ownership, follow-through, and a team-first attitude drive us to pursue our mission every day. Our culture supports our values by fostering innovative and critical thinking and a deep belief in living up to our promises.

Headquartered in the United States, Pattern has a global portfolio of more than 35 power facilities and transmission assets, serving various customers that provide low-cost clean energy to millions of consumers.
Responsibilities
Job Purpose
Pattern Energy is embarking on a major business transformation to scale processes and systems to enable the significant growth of our business. One critical aspect of this transformation is building an enterprise data architecture capable of supporting strong enterprise analytical capabilities and interconnected data across functions for improved and real-time decision making. This new role of Senior Data Engineer will be instrumental in building this future.

The Senior Data Engineer will help guide Pattern’s journey by building novel and modern tools, pipelines, and data systems. Through lived experience this role will act in a key strategic resource in our transition from siloed, disparate data assets to a grand, unified, data estate. The senior data engineer will lead development of data systems, namely data lakehouse pipeline, and analytics environments to support data lifecycles across the business. These tools will help drive our efforts toward democratization of data, reducing duplication of effort, and adding value to the data stream.

Key Accountabilities
Work with the Data team to understand and interpret use cases around the business and develop tools/systems to support data lifecycle from the point of production to the point of consumption.
Implement modern technology and concepts to enable Pattern teams’ design of enterprise-grade systems capable of achieving data goals.
Develop pipelines and analytics tools, including automations wherever possible
Templatize these solutions to help our adoption of infrastructure-as-code
Qualifications
Experience/Qualifications/Education Required
Previous experience in building enterprise data lifecycle and cloud solutions.
5-7+ years experience, specific to data management, analytics, or data reporting.
B.S. in a technical field with M.S. preferred.
Significant experience delivering Data solutions using the Azure cloud stack
Experience with database, data lake, lakehouse design concepts and the use of SQL.
Experience with transformations and reporting solutions, namely Power BI
Experience with environment management, release management, code versioning, deployment methodologies, and CI/CD tools
Additional Requirements

Demonstrated excellence and ability to learn new programming skills and languages.
Strong understanding of security and governance principles, including access policies
Proven record of excellent communication and cooperation with varied stakeholders.
Renewable energy industry experience is strongly preferred.
Demonstrated experience building scalable data models & ingestion pipelines from a variety of systems including Enterprise Applications (e.g. ERP, CRM), Operational Assets (e.g. SCADA), Big Data (e.g. Weather or product configuration simulators).
Technical Skills
Experience throughout the Microsoft Integrated Data Platform (IDP) and related stacks including, but not limited to:
Azure, AAD, Synapse, Purview, Data Lake, Delta Lake, and Lakehouse design
Azure Data Factory, Data Explorer, Logic Apps, Power Automate
Databricks; PySpark, and optimized resource consumption, therein
The expected starting pay range for this role is $95,000 - $129,000 USD. This range is an estimate and base pay may be above or below the ranges based on several factors including but not limited to location, work experience, certifications, and education. In addition to base pay, Pattern’s compensation program includes a bonus structure for full-time employees of all levels. We also provide a comprehensive benefits package which includes medical, dental, vision, short and long-term disability, life insurance, voluntary benefits, family care benefits, employee assistance program, paid time off and bonding leave, paid holidays, 401(k)/RRSP retirement savings plan with employer contribution, and employee referral bonuses.

Pattern Energy Group is an Equal Opportunity Employer.",112000,51 to 200 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,2009,$25 to $100 million (USD),CA,14,data engineer,senior,['sql'],"['databricks', 'azure']",['power bi'],[],[],[],,+10 years
Nisum,4.0,Remote,Data Engineer GB4790,"Location: Remote, USA
Team: Data Science & Analytics
Work Type: Full Time
Nisum is a leading global digital commerce firm headquartered in California, with services spanning digital strategy and transformation, insights and analytics, blockchain, business agility, and custom software development. Founded in 2000 with the customer-centric motto “Building Success Together®,” Nisum has grown to over 1,800 professionals across the United States, Chile,Colombia, India, Pakistan and Canada. A preferred advisor to leading Fortune 500 brands, Nisum enables clients to achieve direct business growth by building the advanced technology they need to reach end customers in today’s world, with immersive and seamless experiences across digital and physical channels.

What You'll Do
Defines, designs, develops, and test software components/applications using Spark, Sql, and PySpark.
Building solutions using a variety of open-source tools Microsoft Azure services and a proven track record in delivering high-quality work to tight deadlines.
Ability to work with the customer as part of the Agile model of delivery.
Design and Build Modern Data Pipelines and maintain data warehouse schematics, layouts, architectures, and relational/non-relational databases for data access and Advanced Analytics
What You Know
Hands-on distributed computing development experience using PySpark, Spark SQL, and Databricks delta tables.
Should have experience in structured streaming, stateless, and state full.
Familiarity with Azure resources (AKV, Managed identity, SPN, ADLS, etc..).
Must have good craftsmanship skills including unit testing, and code quality, and be a creative thinker/problem solver.
Experience in CICD and operation tooling integrations is a plus.
#Li-Remote
Education
Bachelor’s degree in Computer Science, Information Systems, Engineering, Computer Applications, or related field
Benefits
In addition to competitive salaries and benefits packages, Nisum US offers its employees some unique and fun extras:
Professional Development - We offer in-house technical training and professional learning programs aimed at developing skills across a broad spectrum of topics such as technology, leadership, role-based training, and process expertise. We also offer an annual stipend for employees to attend external courses in order to maintain professional certifications
Health & Wellness Benefits - We believe that your health and welfare are important, and we strive to ensure that you have affordable options available to you, including some plans that are subsidized for employees and their families by up to 90%. We also have dental and vision plans in the US where Nisum pays 100% of premiums for employees
Volunteerism Pay - We believe in giving back and in the US, our employees are eligible for up to 40 hours of paid time off each year to volunteer towards the causes that they are most passionate about. This is in addition to personal PTO and paid holidays
Additional Benefits - We offer all the other important benefits to keep employees and their families healthy and financially secure, such as 401(k) retirement savings with a company match, pre-tax parking and transit programs, disability insurance, and Basic Life/AD&D, alongside exclusive employee discounts on a wide variety of products and services
Compensation Band
$120-125k per year
Nisum is an Equal Opportunity Employer and we are proud of our ongoing efforts to foster diversity and inclusion in the workplace.",122500,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,Unknown / Non-Applicable,Remote,23,data engineer,na,['sql'],"['databricks', 'azure']",[],[],['spark'],[],bachelor,
shaped.ai Inc.,4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!",160000,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NY,-1,data engineer,na,"['sql', 'python']",['aws'],[],[],"['flink', 'spark']",[],bachelor,+10 years
WellTrust Medical Group,4.0,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",112889,,,,,-1,,NC,-1,data engineer,na,"['sql', 'java', 'r', 'python']",[],"['looker', 'tableau', 'excel']","['sql server', 'oracle']",[],[],bachelor,2-5 years
Oak Ridge National Laboratory,3.6,"Oak Ridge, TN",Geospatial Data Engineer,"Requisition Id 7635
Overview:
The Geospatial Science and Human Security Division (GSHSD) is seeking a geospatial data engineer within the Location Intelligence group. This position is within the Division’s Human Dynamics section and part of the National Security Science Directorate. The Location Intelligence Group performs ground-breaking research into place-based knowledge extraction, narrative analysis, and geosocial data discovery. Leveraging primarily “non-traditional” geospatial data sources such as volunteered geographic information (VGI), internet of things (IoT) data, and telemetered sensor sources, and harmonizing with commercial and open data sources, the Location Intelligence group delivers novel approaches and technologies for describing places, points of interest, events, activities, and populations, as well as the patterns of interaction among the three. The group’s dynamic content portfolio addresses the timely need to broaden research in spatial data curation, multi-scale land-use modeling, generating higher-level features from ground photo imagery, disinformation detection, network spatialization, narrative analysis, and transformation to describe the landscape and patterns of activity within it, using novel and non-traditional data and theoretical techniques.

This position involves developing and deploying scalable geospatial applications supporting the group’s R&D portfolio. The position’s focus includes architecting geospatial services, working with the CI/CD deployment teams, performing stress testing of deployed services and applications, cloud-to-premise interfacing, and distributed computing to deploy real-time geospatial intelligence applications. The individual will collaborate with other scientists in developing transformative technologies that address these challenges through teaming and collaborative efforts. The work will involve harvesting and processing several hundred disparate data sources from all around the world. These sources contain text, sensors, ground-level imagery, social media, trajectory, and travel data, among others. Enabling this data for purposeful use requires designing and architecting RESTful services and distributed failover backend services. To confidently run them, individuals are required to have a good background in the following initiatives: Architect and design Spring boot and Java Application, open-source exploitation; geospatial OGC services architecture and development, Java and object-oriented concepts, application server failover and load balancing, and geospatial formats such as GeoJson and GeoPackage; and a working knowledge of GIS methodologies.

Our commitment to diversity:
As we strive to become the world’s premier research institution in the sciences and technologies that underpin critical national security missions, we are committed to creating an inclusive environment that highly values a diverse workforce. We recognize that breadth of perspectives, insights, and experiences is necessary to drive innovation and discovery mission-critical to national security sciences. Our commitment extends beyond our workforce to the next generation of researchers with STEM education outreach that seeks to engage a diverse range of students.

Major Duties and Responsibilities:
Architect services on NoSQL data (ElasticSearch), OGC GeoServer configuration, Java and Object-oriented application architecture fluency, Maven built Spring Boot, Docker, Kubernetes, CI/CD pipeline design and deployment.
Design and develop scalable and distributed data and backend services for GEOINT purposes.
Deployment and CI/CD and perform benchmark analysis and examination of results and logs from failover and robust service deployment.
Performing post-deployment logistics.
Examine new and developing technologies and tools to see if they can be used on the job.
Manage and update existing data format and perform regular improvements.
Augment existing research and development activities in location intelligence and association analysis.
Develop and support the plan for project data analysis and management.
As needed, provide technical assistance to other employees and customers.

Basic Requirements
Bachelor’s degree or higher in computer science, mathematics, or a related field and 2+ years of related experience.
Prior experience with J2EE application development, microservices, distributed Java Caching, regular expressions, web-based technologies.
Prior expertise with REST data services, APIs, and microservices as part of a service-oriented architecture (SOA).
Proficiency in open-source or commercial geospatial tools such as ArcGIS, QGIS, and PostGIS.
Prior expertise with relational (MS SQL Server, MySQL, PostgreSQL, and so on), document (JSON, ElasticSearch, and so on), spatial, graph, and other unstructured databases is required.
Experience with Python, R, and other similar languages for data manipulation, exploration, graph analysis, and statistical analysis.
Hands-on experience using cloud-enabled technologies and platforms that includes Google Cloud and AWS.
Excellent written and verbal communication and demonstrated ability to work in interdisciplinary teams.

Preferred Qualifications:
Agile software development approaches are a plus.
Working knowledge of both structured and unstructured data.
Working knowledge of version control systems such as Git.
Working knowledge of automated deployment and testing environments is a plus.
UML, Object-oriented programming, Linux, shell scripting, and other related skills are required.
Containerization technologies such as Docker and Conda have been used to design and manage computing environments.
Experience with geospatial ETL process design and implementation
Experience with database technologies such as MySQL and ElasticSearch to store, analyze, and manipulate data.
Experience in statistics, computational sciences, activity and event data, and working with social media data.

This position will remain open for a minimum of 5 days after which it will close when a qualified candidate is identified and/or hired.
We accept Word (.doc, .docx), Adobe (unsecured .pdf), Rich Text Format (.rtf), and HTML (.htm, .html) up to 5MB in size. Resumes from third party vendors will not be accepted; these resumes will be deleted and the candidates submitted will not be considered for employment.

If you have trouble applying for a position, please email ORNLRecruiting@ornl.gov.

ORNL is an equal opportunity employer. All qualified applicants, including individuals with disabilities and protected veterans, are encouraged to apply. UT-Battelle is an E-Verify employer.",77071,5001 to 10000 Employees,Government,Government & Public Administration,National Agencies,1943,Unknown / Non-Applicable,TN,80,data engineer,na,"['python', 'shell', 'java', 'sql', 'r', 'nosql']","['aws', 'google cloud']",[],"['sql server', 'postgresql', 'elasticsearch', 'mysql']",[],['docker'],bachelor,+10 years
Macy’s,3.4,"Johns Creek, GA",Lead Data Engineer,"Macy's, Inc is building an Enterprise Data & Analytics team to further grow our capabilities in support of our mission to be a data - led, customer centric company. This team will focus on accelerating impact from analytics, coordinating an enterprise-wide roadmap, and ensuring proper data governance and management. As a member of this team, the engineer will help lead the charge to execute on our vision to build profitable lifetime customer relationships by embedding data & analytics at the heart of everything we do.
Position Overview:
The Lead Data Engineer is responsible for development and support of data products on a modern cloud-based data lake, leveraging expertise and knowledge of multiple technologies & data domains to help build a robust, scalable, and reliable data engineering platform.
The Lead Data Engineer is responsible for providing data services for enterprise-grade analytical environments, utilizing automated data pipelines at scale, and streamlining efficient data transformations for priority use cases, be involved hands on in development of the codebase and partner closely with business units and peer technology groups to support analytics execution.
Essential Functions
Solution Design & Implementation:

Work closely with business stakeholders, implement scalable solutions to meet requirements
Follow and improve existing processes and procedures
Lead a pod of data engineers, providing both technical oversight and supporting their growth
Build, maintain and simplify enterprise data pipelines with emphasis on reusability & data quality
Work with Legal and Privacy teams to adhere to data privacy and security requirements

Culture:
Train and mentor fellow engineers on both technical stack and data domain specifics
Establish a pro-active approach to data management, ensuring business stakeholders & platforms can access required data within the SLA window
Drive change management to increase user adoption of enterprise data repositories and leverage standardized data pipelines across use cases
Increase agility in identifying data issues and taking action to remediate

Qualifications
Education/ Experience:

Data engineering experience with:
3+ years of experience in designing and implementing cloud-based data solutions
3+ years of experience integrating with analytics reporting solutions (e.g. Tablaeu, PowerBI)
3+ experience with big data processing technologies such as Hadoop, Spark, etc.
5+ years of experience building & automating ETL data pipelines using enterprise grade tools
5+ years of experience building enterprise-grade data warehouses (either on-prem or on cloud)
8+ years of overall programming experience, including recent experience with Python & SQL
Ability to effectively share technical information, communicate technical issues and solutions to all levels of business stakeholders
Customer-centric and experienced with cross-functional collaboration
Excellent written and verbal communication skills

What we can offer you:

Exciting, challenging problems to solve - you'll never have a boring day at work
A refreshingly fun work environment where you will collaborate with a smart and talented team
Unique freedom to build and lead a team in next gen thinking
A chance to learn and participate in the growth of NYC’s largest retailer

This job description is not all inclusive. Macy’s Inc. reserves the right to amend this job description at any time. Macy's Inc. is an Equal Opportunity Employer, committed to a diverse and inclusive work environment.",161600,10000+ Employees,Company - Public,Retail & Wholesale,"Department, Clothing & Shoe Stores",1858,$10+ billion (USD),GA,165,data engineer,senior,"['sql', 'python']",[],[],[],"['spark', 'hadoop']",[],,+10 years
Fathom Management LLC,2.0,Remote,Sr. Data Engineer Remote Opportunity,"Sr. Data Engineer

seeking a Senior Data Engineer who possesses expert level knowledge of appropriate data sources to address the specific requirements of projects for data modeling. Understand business requirements and translating into technical work. Design and implement features in collaboration with team engineers, product owners, data analysts, business partners using Agile/SCRUM Methodology.

This is a full- time position / 100% Remote.
The salary range of $140,000 - $160,000 will be based on technical experience and technical interview.

Responsibilities:

Ability to build programs or systems that can take data and turn it into meaningful information that can be studied.
Build ETL/ELT jobs and workflows to combine data from disparate sources.
Install continuous pipelines of huge pools of filtered information so that data analyst/scientists can pull relevant data sets for their analyses.
Build data workflows using SQL Server Integration Services (SSIS)
Build data workflows using Microsoft Azure (Azure Data Factory, Storage Accounts, Synapse)
Build data workflows using Databricks
Collaborate with business stakeholders, business operations, and product engineering teams in analyzing business problems, in building and testing solutions and data models.
Experience implementing and operating analytic models and services.
Document the current-state and target-state software architecture and create roadmap plans for success on various software components.
Assist in the design, implementation, and maintenance of complex solutions.
Build systems that collect, manage, and convert raw data into usable information for business analysts to interpret.
Make data accessible for evaluation and optimization
Collaborate with business stakeholders, business operations, and product engineering teams.
Coordinate activities with other technical personnel as appropriate.
Works with back-end data and develops tables using SQL scripts, SSIS, and SSMS.
Experience with Azure cloud platforms and Data bricks

Required Experience and Education:
Master's degree in computer science, systems engineering, or related technical discipline is preferred with 7-10 years of experience as a Data Engineer/Administrator or similar role. OR , B.S. in Computer Science with 15 years of relevant experience.

Benefits Overview: Full-time employees are offered comprehensive and competitive benefits package including paid vacation, sick leave, holidays, health insurance, life insurance, military leave, training, tuition reimbursement, a wellness program, short- and long-term disability, 401(k) retirement plan with company matches/immediate vesting, commuter benefits, and more.
COVID Policy: In accordance with the Federal Executive Order on Ensuring Adequate COVID Safety Protocols for Federal Contractors, this position requires that you are fully vaccinated at least 2 weeks before your start date. You will be required to provide proof of vaccination before you begin employment.
EEO Policy: It is our policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits, and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability, or any other characteristic protected by applicable federal, state or local law.",150000,1 to 50 Employees,Self-employed,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,['sql'],"['databricks', 'azure']",['ssis'],['sql server'],[],[],master,5-10 years
Farm Credit East,3.9,"Enfield, CT",Data Engineer,"Be part of a team focused on the success of our customers, the success of our communities, and the success of each other. Farm Credit East is the leading provider of loans and farm advisory services to farm, forest product, fishing, and other agricultural business owners across the northeast. We are One Team Working Together with a focus on our five pillars: Outstanding Customer and Employee Experience, Quality Growth, Operational Excellence, Commitment to our Communities, and Protecting Customer Information.

Position Summary
The Data Engineer is responsible for cleaning, managing, and sharing data that guides business decisions. Using ETL tools you will gather data from a variety of sources, checking for anomalies, automating processes, and generally making it easier for business stakeholders to generate valuable insights. This position will collaborate with internal and external organization to capture requirements, design, create, document, manage, and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.

Duties and Responsibilities
Work with product stakeholders to implement, maintain, and enhance data models and solutions used to define and measure quality of data domains.
Design data models to meet requirements
Perform ETL (Extract, Transform, and Load) on data to meet stakeholder specifications.
Design and develop data access methods, datasets, views etc.
Develops data modeling and is responsible for data acquisition, access analysis, archive, recovery, load design and implementation.
Coordinates new data developments to ensure consistency with existing warehouse structure.
Collaborates with internal customers to capture requirements, design, create, document, manage and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.
Assists with the development, implementation, and maintenance of front-end presentation (dashboards), automated report solutions and other BI solutions to support tactical and strategic reporting needs of the organization.
Assists in identification of data integrity problems and recommends solutions.
Work collaboratively with key stakeholders both internally and externally, including but not limited to Senior Management, Business Unit Leaders, Knowledge Exchange, and Farm Credit Financial Partners (FPI).

Job Qualifications/Requirements
Bachelor’s Degree in Computer Science, Business, Finance, or other related field from an accredited University.
Experience with MSFT SQL Server
Microsoft Azure (Data Bricks, Data Factory, Logic Apps, Functions, etc.)
2 plus years of experience in Finance related informatics, performance measurement, or analysis with strong relational database SQL skills.
1 + years of experience using Microsoft Azure product to perform ETL
Familiar with Databricks Unity catalog

Farm Credit East is an Equal Opportunity Employer. As an Equal Opportunity Employer, we do not discriminate on the basis of race, color, religion, national origin, sex, sexual orientation, gender identity or expression, age, marital status, parental status, political affiliation, disability status, protected veteran status, genetic information or any other status protected by federal, state or local law. It is our goal to make employment decisions that further the principle of equal employment opportunity by utilizing objective standards based upon an individual's qualifications for a specific job opening. In compliance with the Americans with Disabilities Act (“ADA”), if you have a disability and would like a reasonable accommodation in order to apply for a position with Farm Credit East, please call 1-800-562-2235 or e-mail FarmCreditCareers@farmcrediteast.com .",86685,201 to 500 Employees,Company - Private,Financial Services,Banking & Lending,1916,$100 to $500 million (USD),CT,107,data engineer,na,['sql'],"['databricks', 'azure']",[],['sql server'],[],[],bachelor,+10 years
United Digestive,4.0,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time",112889,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,GA,-1,data engineer,na,"['sql', 'java', 'python']","['databricks', 'azure', 'aws']","['tableau', 'power bi']","['sql server', 'postgresql', 'oracle']","['kafka', 'spark', 'hadoop']",[],bachelor,+10 years
YSI,3.7,Remote,Senior Data Engineer,"Position Title: Senior Data Engineer/ Oracle Apex Developer
Job Id: 202301002
Location: Herndon, VA (Remote)
Yakshna Solutions, Inc., (YSI) is a CMMI Level 3 assessed, ISO 9001, 20000:1, 27001 certified, woman-owned small business enterprises, headquartered in Herndon, Virginia, USA. YSI provides professional IT solutions and services to business corporations and government organizations. YSI is committed to serve its business communities as a leading IT vendor providing innovative, quality, and cost-effective IT business solutions and services.
Our benefits are very competitive that include 401(k), health, dental, and vision insurance, Life insurance, short-term and long-term disability insurance, paid time off, training, and professional development assistance.
YSI is seeking a highly qualified Senior Data Engineer. The selected candidate will be able to communicate effectively (written/verbal), possess strong interpersonal skills, be self-motivated, and be innovative in a fast-paced environment.
Responsibilities:
The Data Engineer will be the senior technical expert on work associated with data management, data quality and data structures, coordinating with the ADA as needed to ensure development and data structures are synchronized.
The Data Engineer will design the approach for data tasks and will contribute to completion of data tasks and oversee execution, providing advice and guidance as necessary to junior staff.
Required Qualifications and Skills:
Bachelors or master’s in relevant filed.
Good Data Engineering/Management experience
· Should be familiar with the the Civil Works missions for Hydropower, Recreation, Environmental Stewardship, and Water Supply.
· Extensive technical knowledge of programming in a software stack that includes an Oracle Relational database, SQL, PL/SQL, Oracle Spatial, JavaScript, Oracle REST Data Services (ORDS), and Oracle APEX to make the necessary revisions to the relevant systems. In addition to these general skills and experience, also possess the following:
· Knowledge and experience in developing and maintaining a relational database operated in Amazon Web Services (AWS cloud).
· Knowledge and experience of data entry and options to increase automation and efficiency.
· Knowledge and experience with documenting data systems and processes and creating reports for increased efficiencies.
· Knowledge and experience in optimizing the performance of existing Oracle based applications, including procedures, functions, etc.
· Knowledge and experience with creating Representational State Transfer (RESTful) services utilizing common data dissemination formats.
· Knowledge and experience with making websites 508 compliant.
· Knowledge and experience with authentication and authorization procedures
· Knowledge and experience with maintaining geospatial data in oracle relational database.
· Knowledge and experience with geospatial web services (OGC & ESRI REST)
· Knowledge and experience with cloud native development methodologies
Job Types: Full-time, Contract
Pay: $110,000.00 - $120,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Herndon, VA 20170: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
Oracle Apex: 5 years (Preferred)
AWS CLOUD: 5 years (Preferred)
Erwin: 8 years (Preferred)
Data modeling: 8 years (Preferred)
Metadata: 5 years (Preferred)
Work Location: Remote",115000,10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,2011,$1 to $5 billion (USD),Remote,12,data engineer,senior,['sql'],['aws'],[],['oracle'],[],[],master,5-10 years
Shutterfly,3.3,"Eden Prairie, MN",Senior Data Engineer,"Description
At Shutterfly, we make life’s experiences unforgettable. We believe there is extraordinary power in the self-expression. That’s why our family of brands helps customers create products and capture moments that reflect who they uniquely are.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.
What You'll Do as a Sr. Data Engineer II
Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines am improve the operation and performance of the Data Warehouse
The Skills You'll Bring
Expert knowledge Python, Spark, SQL scripting and Java
7+ years of hands-on experience with building data ingress frameworks and experience with dealing with a wide variety of source data formats
Hands-on experience in building data applications, including design, implementation, debugging, and support.
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it’s the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.",132365,10000+ Employees,Company - Private,Information Technology,Internet & Web Services,1999,$1 to $5 billion (USD),MN,24,data engineer,senior,"['sql', 'java', 'python']",['aws'],[],[],['spark'],[],,+10 years
Apple,4.2,"Cupertino, CA",Biomedical Data Engineer - Health Technologies,"Summary
Posted: Aug 8, 2022
Weekly Hours: 40
Role Number:200402289
The Health Technologies Team conceives and proves out innovative technology for Apple’s future products and features in health. We are seeking a highly capable Biomedical Data Engineer to join a multi-disciplinary team. Successful candidates will be able to integrate with our research study leads, data scientists and engineers to develop and support effective data analysis and machine learning workflows.
Key Qualifications
Experience with software engineering frameworks
Excellent coding skills in Python (e.g.,Pandas, Spark, Jupyter)
Workflow orchestrations (e.g., Airflow, Luigi)
Designing and maintaining (non-)relational databases (e.g. Postgres, Cassandra, MongoDB) and file systems (e.g. Parquet, CSV, JSON)
Great understanding of infrastructure designs
Linux, MacOS based development frameworks
iOS/ watchOS development (e.g., Swift, Objective-C)
Web Service APIs (e.g., AWS, REDCap, XNAT)
Version control frameworks (Git, virtualenv)
Familiarity with best practices for information security, including safe harbor privacy principles for sensitive data
Experience with biomedical sensors/platforms for measuring physiological signals in the health, wellness and/or fitness realms
Description
- Work closely with team members and study staff to design, build, launch and maintain systems for storing, aggregating and analyzing large amounts of data - Process, troubleshoot, and clean incoming data from human studies - Automate and monitor data ingestion and transformation pipelines, with hooks for QA, auditing, redaction and compliance checks per data management specifications - Create and maintain databases with existing and incoming clinical data - Architect data models and create tools to harmonize disparate data sources - Incorporate and comply with regulations as they pertain to electronic and clinical data and databases
Education & Experience
BS/MS in Computer Science, Engineering, Informatics, or equivalent with relevant 4+ years industry experience with biomedical, health or sensitive data.
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $104,000 and $190,000 annualized, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",112889,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1976,$10+ billion (USD),CA,47,data engineer,na,['python'],['aws'],[],['mongodb'],['spark'],[],,+10 years
Koch Ag & Energy Solutions,3.8,"Wichita, KS",Project Data Engineer / Analyst,"Description
Koch Ag & Energy Solutions (KAES) is looking for a Project Data Engineer/Analyst who will be a primary point of contact for project cost forecasting, change management, and risk management on small to large capital construction projects and turnarounds. This opportunity utilizes data from multiple sources to extract insights and perform analysis.
Our Team

We support cross site, cross functional teams throughout the project lifecycle on projects and turnarounds across (up to) 6 plants throughout the US and a site in Canada.
This position works a Monday – Friday, 8-hour day, and is based out of Wichita, KS with expected travel about 25% of the time.
What You Will Do
Use critical thinking, analysis, curiosity, and collaboration throughout the following to enable cost competitive project delivery (but not limited to) functions:
Analyze per project actual cost, schedule performance, and estimate details to forecast final cost per project at the cost breakdown structure level.
Track project progress against the baselines (scope, cost, and schedule).
Partner with project teams to identify issues and risks early and develop proactive resolutions and mitigations.
Monitor purchase order commitments, manage change orders, and adjust forecasts to align with reality.
Prepare various charts, tables and reports with insights to understand and communicate the forecast including what has changed and why.
Works with team members in assessing data store solutions that supports dashboards, insights, and other analytical solutions.
Demonstrate initiative to improve project outcomes, data integrations, processes, and forecasting quality.
Share knowledge and solve problems across teams to improve project results across KAES.
Work around the field construction sites without assistance.


Who You Are (Basic Qualifications)
Minimum of 2 years of experience in Projects within construction/oil & gas/manufacturing industry OR Bachelor’s Degree in a Business Administration or Engineering field
Demonstrated ability to analyze trends/variances and determine root causes
Experience creating and maintaining automated reporting processes
What Will Put You Ahead
Experience on projects and/or turnarounds in an industrial or construction environment.
Experience with reporting and analytics software such as Alteryx, Tableau, PowerBI, etc.
Experience with project cost management (including forecasting, change management, WBS/CBS buildup, risk management, reporting, etc.)
Familiarity with Project Management and Project Controls tools (e.g. EcoSys, Maximo, P6 or similar project portfolio or scheduling tools).
Experience performing risk based contingency assessments and identifying key drivers of risks.
Experience influencing change across an organization.


Relocation may apply based on candidate.

At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate’s knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.
Hiring Philosophy
All Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.
Who We Are

As a Koch company, Koch Ag & Energy Solutions (KAES) is a global provider of value-added solutions for the agriculture, turf and ornamental, energy and chemical markets. From agriculture to energy, KAES makes things grow better with plant nutrient and biological technologies. Our team of innovators unleash their potential while developing the technologies that feed and power the world.
At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.
Our Benefits
Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.
Equal Opportunities
Equal Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please visit the following website for additional information: http://www.kochcareers.com/doc/Everify.pdf",81180,10000+ Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,1940,$10+ billion (USD),KS,83,data engineer,na,[],[],['tableau'],[],[],[],bachelor,
Merkle,3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.",120000,5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD),PA,52,data engineer,na,"['sql', 'python']","['snowflake', 'aws']",[],['snowflake'],[],[],,
Redzara.us,4.0,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",90450,,,,,-1,,GA,-1,data engineer,na,['sql'],['azure'],['ssis'],['sql server'],[],[],,0-2 years
"Contact Government Services, LLC",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
39U0b7kJu8",103213,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['postgresql'],[],[],,+10 years
"FreightWaves, Inc.",3.5,Remote,Senior Data Engineer,"Are you smart, driven, curious, resourceful, and not afraid to fail? Then we want to meet you! Our team of bold, innovative, and creative teammates is what makes us a top startup to work for. FreightWaves delivers news and commentary as well as data and analytics which empower risk management and actionable market insights in the logistics and supply chain industry. If you are ready to join our team, it is time for YOU to apply!
FreightWaves is on the hunt for a curious, tenacious, and team-oriented Senior Data Engineer to join our fast paced engineering team. The ideal candidate is inquisitive, versatile, team oriented, thrives on change, and has a positive attitude. If you are ready to be challenged, learn new and exciting technologies, and have the unique opportunity to work with some of the most talented developers in the country, we want you to apply!
**This position is fully remote.**
**Must RESIDE in the United States and be eligible to work.**
What you will be doing:
Implementing ingestion pipelines, using Airflow as the orchestration platform, for consuming data from a wide variety of sources (API, SFTP, Cloud Storage Bucket, etc.).
Implementing transformation pipelines using software engineering best practices and tools (DBT)
Working closely with Software Engineering and DevOps to maintain reproducible infrastructure and data that serves both API-only customers and in-house SaaS products
Defining and implementing data ingestion/transformation quality control processes using established frameworks (Pytest, DBT)
Building pipelines that use multiple technologies and cloud environments (for example, an Airflow pipeline pulling a file from an S3 bucket and loading the data into BigQuery)
Create and ensure data automation stability with associated monitoring tools.
Review existing and proposed infrastructure for architectural enhancements that follow both software engineering and data analytics best practices.
Working closely with Data Science and facilitating advanced data analysis (like Machine Learning)
What you bring to the table:
Strong working knowledge of Apache Airflow
Experience supporting a SaaS or DaaS product, bonus points if you were creating new data products/features
Strong in Linux environments and experience in scripting languages
Python Expert
Strong understanding of software best practices and associated tools.
Experience in any major RDBMS (MySQL, Postgres, SQL Server, etc.).
Strong SQL Skills, bonus points for having used both T-SQL and Standard SQL
Experience with NoSQL (Elasticsearch, MongoDB, etc.)
Multi-cloud and/or hybrid-cloud experience
Strong interpersonal skills
Comfortable working directly with data providers, including non-technical individuals
Experience with the following (or transitioning from equivalent platform services):
Cloud Storage
Cloud Pubsub
BigQuery
Apache Airflow
dbt
DataFlow
Bonus knowledge/experience:
Experience implementing cloud architecture changes
Working knowledge of how to build and maintain APIs using Python/FastAPI
Transforming similar data from disparate sources to create canonical data structures
Surfacing data to BI platforms such as Looker Studio
Data Migration experience, especially from one cloud platform to another
Certification: Professional Google Cloud Certified Data Engineer
Our Benefits:
An excellent work environment, flat hierarchies, and short decision paths.
Work from home
A generous benefits package including 100% employer-paid health, dental, vision and Life insurance, STD, LTD
Stock options
Appealing 401k matching plan
Career Mentorship Opportunities
Personal Development Credit (Can be used toward Student loans or relevant PD Courses)
Annual life achievement bonus of $2000 for having a baby, buying a house, or getting married (max one per year)
No set days off Vacation policy (our team takes time off as needed with supervisor approval)
Up to $50 for Gym or Virtual Gym membership.
Audible or Kindle Unlimited subscription
Discount on Ford vehicles
oYXkhYiWkU",112889,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2017,Unknown / Non-Applicable,Remote,6,data engineer,senior,"['sql', 'nosql', 'python']",['google cloud'],['looker'],"['elasticsearch', 'mysql', 'sql server', 'mongodb', 'dbt']",[],[],,
"NexTek, LLC",4.0,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",112889,,,,,-1,,MN,-1,data engineer,senior,[],[],[],['mysql'],[],[],,+10 years
Argo Data,2.7,"Richardson, TX",Senior Data Engineer,"ARGO is a leading provider of software and analytics solutions for both the financial services and healthcare industries. ARGO transforms business processes for financial service providers and healthcare organizations using proven business models and software innovation informed by real customer challenges, breakthrough technology, and rich analytics.
ARGO- Data Engineer II (Markham, ON; Memphis, TN; ...) (theapplicantmanager.com)
JOB SUMMARY
The Data Engineer II is responsible for designing and implementing data solutions to support business needs. The successful candidate will have a strong background in software engineering and database design, as well as experience with working with cloud-based tools and technologies.
EXPECTED WORK AND PERFORMANCE
This section is intended to provide a more detailed overview of what the expectations of the job are to include: work to be performed, expected performance results and how they will be measured, as well as connection to corporate strategy, values, and mission. Job duties and responsibilities include the following. Other duties may be assigned as needed.
Design, build, and maintain data pipelines for data extraction, transformation, and loading (ETL) from various sources.
Ensure data security and privacy protocols are implemented and enforced.
Monitor and optimize data storage and processing performance to ensure high performance and cost-effectiveness.
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and translate them into technical solutions.
Maintain documentation of data systems, processes, and policies.
Stay current with the latest data tools, technologies, and trends and recommend improvements.
Mentor and train junior data engineers as needed.
QUALIFICATIONS
Bachelor's degree in Computer Science, Software Engineering, or related field.
3-5 years of experience in software engineering and SQL database design.
Proficiency in programming languages such as Python.
Strong understanding of data warehousing, ETL processes, and data modeling.
Experience with cloud infrastructure technologies such as Microsoft Azure.
Excellent problem-solving and analytical skills.
Ability to work well in a team-oriented environment and communicate effectively with both technical and non-technical stakeholders.
PHYSICAL DEMANDS
The candidate will work at our corporate headquarters in Richardson, TX. While performing the duties of this job, the employee is regularly required to stand; walk; sit; use hands to finger, handle, or feel; reach with hands and arms; talk or hear. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.
ADDITIONAL REQUIREMENTS:
Applicants for U.S. or Canadian based positions with ARGO must be legally authorized to work in the United States or Canada. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available at this time for ARGO positions.
Pre-employment background screening will be conducted.
Job Type: Full-time
Pay: $70,745.42 - $141,624.04 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richardson, TX 75081: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Experience:
Informatica: 3 years (Required)
SQL: 5 years (Required)
Data warehouse: 3 years (Required)
Python: 3 years (Required)
Azure: 3 years (Required)
Work Location: In person",106185,201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,1980,$25 to $100 million (USD),TX,43,data engineer,senior,"['sql', 'python']",['azure'],[],[],[],[],bachelor,2-5 years
Leadstack Inc,4.3,Remote,Lead Data Engineer with Sql/Python,"LeadStack Inc. is an award winning, one of the nation's fastest growing, certified minority owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Title: Lead Data Engineer with Sql/Python
Location: Remote
Duration: 6+months
Direct Client
Immediate interview
Top Skills: Azure, SQL/Python Developer.
Job Description:
Design and develop data system integrations and data models for reporting and alerting.
Essential Job Functions:
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state
Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions
Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Interpreting data, analyzing results using statistical techniques
Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality
Acquiring data from primary or secondary data sources and maintaining databases
Promote the reuse of data assets, including the management of the data catalog for reference
Key Responsibilities:
Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy
Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Mentor team members in data principles, patterns, processes and practices
Promote the reuse of data assets, including the management of the data catalog for referenceDraft and review architectural diagrams, interface specifications and other design documents
To know more about current opportunities at LeadStack , please visit us on https://leadstackinc.com/careers/
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Azure: 5 years (Preferred)
Data warehouse: 1 year (Preferred)
SQL developer: 6 years (Preferred)
Work Location: Remote",135000,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,Unknown / Non-Applicable,Remote,7,data engineer,senior,"['sql', 'nosql', 'python']",['azure'],[],[],[],[],,5-10 years
"Ikigai Labs, Inc.",4.9,"Cambridge, MA","Software Engineer, Data Engineering","Ikigai Labs is a fast growing startup founded out of MIT to empower data operators. We are building an easy to use AI augmented data processing and analytics platform on the cloud. Our users depend on us to automate, maintain, and enhance day-to-day mission critical operations. We are a team of talented, hardworking and fun-loving engineers, data scientists, and data analysts working towards the goal of building the next generation of data tools.
Job Description
JOB TITLE: Software Engineer, Data Engineer [Full-time]
LOCATION: Cambridge, MA
SUMMARY:
Ikigai Labs is seeking a dynamic and passionate engineer with strong software fundamentals to join a high-performing data platform development team. We are looking for a team player who is a quick learner, performs in a rapid development cycle, has a drive to surpass expectations, and an eagerness to share their work and knowledge.
We encourage applicants from all backgrounds and communities. We are committed to having a team that is made up of diverse skills, experiences, and abilities.
Technologies
Languages: Python3, SQL
Databases: Postgres, Elasticsearch, DynamoDB, RDS
Cloud: Kubernetes, Helm, EKS, Terraform, AWS
Data Engineering: Apache Arrow, Dremio, Ray
Misc.: Apache Superset, Plotly Dash, Metabase, Jupyterhub, Stripe, Fivetran
The Position
Design and develop scalable data integration (ETL/ELT) processes
Design and develop an on-demand predictive modeling platform with gRPC
Utilize Kubernetes to orchestrate the deployment, scaling and management of Docker containers
Utilize and learn various AWS services to solve cloud-native problems
Implement a testing platform which performs sanity check, load test, scale test, heartbeat test, and performance test
Provide periodic support to our customer success team
Qualifications
0-3 years of experience with a bachelor's degree in Computer Science, Math, or Engineering; or a master's degree
Experience with Python, AWS services, and/or ETL/ELT pipeline experiences
Experience with Kubernetes and/or EKS (optional)
Understanding of the fundamentals of design patterns and testing best practices
The ability to learn quickly in a fast-paced environment
Excellent organizational, time management, and communication skills
The desire to work in an AGILE environment with a focus on pair programming
Willingness to discuss obstacles, find creative solutions, and take initiative
The ability to receive and give both constructive and encouragement feedback",102419,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MA,-1,data engineer,na,"['sql', 'python']",['aws'],[],"['dynamodb', 'elasticsearch']",[],"['terraform', 'docker']",bachelor,2-5 years
Clinical Ink,4.4,Remote,Data Engineer,"Company Information
Clinical ink is the global life science company that brings data, technology, and patient science together to unlock clinical discovery. Our deep therapeutic-area expertise, coupled with Direct Data Capture, eCOA, eConsent, telehealth, neurocognitive testing, and digital biomarkers advancement, drive the industry standard for data precision and usher in a new generation of clinical trials. With offices in Philadelphia, PA, Winston Salem, NC, and Iowa City, IA, Clinical ink is rewriting the clinical development experience.

Job Description
Clinical ink is seeking a Data Engineer to join our Data Team based remotely across the United States! The Data Engineer will work to develop solutions used in applications for clinical trials. The ideal candidate will be a minimum of two years of experience as a software engineer and prior experience working with a variety of tools and frameworks. The Data Engineer's responsibilities include:
Develop data engineering solutions used in applications for clinical trial data collection that both make data available for further use and generate value out of data
Contribute to the methodology by which advanced analytics projects are delivered to clients and codify the tooling needed to support them
Build and support tools that allow data analysts and data scientists to work in complex projects
Implement quality, availability, and integrity of code, solutions, and respective systems and follow best practices related to data integrity, security, scalability, etc.
Participate in code inspections, reviews, and other activities to ensure quality
Qualifications
Bachelors in Mathematics, Statistics, Computer Engineering, Computer Science, or related field of study
2-5 years of experience in software engineering, working on multi-discipline teams
Experience with a variety of tools and frameworks such as Snowflake, Airflow, Spark, Kafka, RedShift, Sage Maker, Kubernetes, etc., AWS ecosystem (Lambda, Glue, S3, E2C, etc.), programming tools and querying languages (i.e., Python, C++, SQL, Scala, Java, etc.)
At least 2+ years of experience with Python
Data modelling and database development experience required
Data visualization experience preferred in Tableau and/or AWS QuickSight
Nice to have experience with issue tracking tools such as JIRA and Confluence
Ability to think creatively and take initiative; ability to learn new technical topics and develop new technical skills quickly
Willingness to learn and explore bleeding-edge/cutting-edge technologies
Additional Information
Clinical ink is an equal opportunity employer and does not discriminate against otherwise qualified applicants on the basis of race, color, creed, religion, ancestry, age, sex, marital status, national origin, disability or handicap, or veteran status.
www.clinicalink.com",112889,201 to 500 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2007,Unknown / Non-Applicable,Remote,16,data engineer,na,"['sql', 'java', 'scala', 'python']","['snowflake', 'aws', 'redshift']",['tableau'],['snowflake'],"['kafka', 'spark']",[],,+10 years
CliftonLarsonAllen,3.9,"Minneapolis, MN",Data Engineer,"The Data Engineer collaborates with internal team members to develop solutions that enable data, insights, analytics and actionable triggers and solutions. S/he utilizes a full range of data technologies including data modeling techniques, data architecture, engineering, data analysis, and reporting for a rapidly growing data analytics platform on Azure.
Essential Job Functions
Pipeline Building: Builds data pipelines utilizing Azure Databricks and Azure Data Factory to build a scalable solutions, following an existing framework and agreed upon design. Operationally monitors and maintains the systems to ensure pipelines continue to run successfully.
Data Solutions: Collaborates with other members of the Data Engineering team to analyze existing software programs and technology processes to ensure effectiveness and efficiency. Troubleshoots data-related problems as needed. Drives strategic initiatives and provides technology solutions for complex business problems. Assists in gathering and analyzing data for business projects, as well as mapping underlying processes and data flows.
New Technology Implementation: Assists with implementing new technology for firm use, as well as creating roadmaps and tools to guide and monitor usage.
Reporting: Routinely provides reports and data extraction as requested by other teams for use within the firm and on behalf of clients.

Requirements
Bachelor’s degree in computer science, mathematics, or related field required. (3 or more years’ experience in data engineering may be considered in lieu of Bachelor’s degree.)
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities.

Wellness at CLA
To support our CLA family members, we focus on their physical, financial, social, and emotional well-being and offer comprehensive benefit options that include health, dental, vision, 401k and much more.
To view a complete list of benefits click
here
.",92921,5001 to 10000 Employees,Company - Private,Financial Services,Accounting & Tax,1953,$500 million to $1 billion (USD),MN,70,data engineer,na,[],"['databricks', 'azure']",[],[],[],[],bachelor,
Syngenta Group,4.0,"Downers Grove, IL",Senior Master Data Engineer,"Company Description

Syngenta Group is one of the world’s leading sustainable agriculture innovation companies, with roots going back more than 250 years. Our 53,000 people across more than 100 countries strive every day to transform agriculture through tailor-made solutions for the benefit of farmers, society and our planet –making us the world’s most local agricultural technology and innovation partner.
Syngenta Group is committed to operating at the highest standards of ethics and integrity. This is a commitment that we are making to investors, customers, society and employees. Syngenta Group is also committed to maintaining a workplace environment free from discrimination and harassment.

Job Description

The Senior Master Data Engineer will be responsible for ensuring Syngenta has the right identity data capabilities to support the current and future Syngenta production and commercial needs.
We have the responsibility to think beyond our past needs and help unlock future opportunities with one of our most valuable data assets. Robust, accurate and trusted identity data will unlock opportunities to improve customer experience, simplify vendor interactions and allow us to explore new ways of marketing our products and services.
The investment in the MDG platform has been the first phase or our journey to support our current and future business needs. The Senior Master Data Engineer will be responsible to build on this first phase and help to define our vision, strategy, operating model, and roadmap for the future of Party data capabilities. This may include supplementing the MDG platform with additional technology and services
Responsibilities
Contributes to creating a breakthrough transformation that shapes the Party data (including Identity and Reference Data) capability in line with Syngenta's strategic vision
Help define & deliver the strategy and roadmap for Identity data and reference data (including operating model, data products, technology, data quality & process analytics/health) ensuring solutions and technologies are maintainable and scalable
Enroll and align with stakeholders to experiment and leverage the Identity data capabilities within relevant domains (Employee, Sales/Customers, Legal Entities, Intercompany, Vendors). Including simplification, automation, rationalization, and harmonization.
Create and advocate Identity data offers (data as a product) that add value and solve business problems that improve integration and adoption.
Provide technical leadership in leveraging and experimenting with appropriate technologies including existing platforms as well as new opportunities (e.g. Microservices, API’s, AI, ML, etc.) to create Identity data products and services to meet business needs.
Seamlessly embed themselves in a cross-functional teams as a subject matter expert and participate in Identity data design authority.
Contribute to the creation and implementation of a reference data capability the compliments Identity master data.
The preferred candidate will be near an established Syngenta location The right candidate will be considered for a remote setting IN THE UNITED STATES.
We are unable to provide Visa Sponsorship for this position at this time.

Qualifications

Required Skills / Experience
Bachelor’s degree with 8 or more years of relevant experience
MUST HAVE hands on experience with a popular MDM tool
Extensive experience in customer master data management
Must have thought leadership and the ability to influence the business with best practices
Experience in reference data principles and practices.
Understand relevant technologies (Master Data Management tools, Microservices, etc)
Stakeholder management / influencing: able to engage with different functions, leadership levels and cultures

Additional Information

What we Offer
A culture that celebrates diversity & inclusion, promotes professional development, and strives for a work-life balance that supports the team members
We offer flexible work options to support your work and personal needs
Full Benefit Package (Medical, Dental & Vision) that starts your first day
401k plan with company match, Profit Sharing & Retirement Savings Contribution
Paid Vacation, 9 Paid Holidays, Maternity and Paternity Leave, Education Assistance, Wellness Programs, Corporate Discounts, among other benefits
Syngenta is an Equal Opportunity Employer and does not discriminate in recruitment, hiring, training, promotion or any other employment practices for reasons of race, color, religion, gender, national origin, age, sexual orientation, marital or veteran status, disability, or any other legally protected status
Family and Medical Leave Act (FMLA)
(http://www.dol.gov/whd/regs/compliance/posters/fmla.htm)
Equal Employment Opportunity Commission's (EEOC)
(http://webapps.dol.gov/elaws/firststep/poster_direct.htm)
Employee Polygraph Protection Act (EPPA)
(http://www.dol.gov/whd/regs/compliance/posters/eppa.htm)

#LI-SB2",100820,10000+ Employees,Company - Private,Agriculture,Crop Production,2000,$100 to $500 million (USD),IL,23,data engineer,senior,[],[],[],[],[],[],master,
Axos Bank,3.6,"San Diego, CA",Junior Business & Technology Analyst - Data Engineer,"Job Summary and Opportunity:
This is an exciting opportunity to join a unique and immersive rotational program as a first step in your career in technology. This full-time rotational program is geared toward providing multi-software platform exposure that focuses on the expansion of knowledge and real-life application within each. We are seeking innovative and energetic individuals who are excited about expanding their skillsets and accelerating their career path with immediate exposure to software applications.
For this position, you will be in the Data Engineer Rotational Program where you will be joining the Axos' Center of ExcellenceTeam. You will get to be a part of a team responsible for the implementation of cutting-edge software driven solutions. As you progress through the program, you will rotate into different complimentary areas within the Data program where roles and responsibilities will change. The final goal of the program is permanent placement within your area of focus. For those looking to make an impact this is where it begins.
In this role you will be focused on SQL related software or software built on direct interactions with SQL. Through the different rotations completed, you will gain the knowledge and skills database development, data quality, and business intelligence reporting to provide enterprise level solutions.
This position is on-site and will be located at our HQ in San Diego, California.
Responsibilities:
Define, prepare, execute and implement data validation and unit and integration testing methods to ensure data quality
Create SSIS packages for data transformation, cleansing, caching, aggregation, staging, and transfer
Analyze and define data flow requirements and prepare applicable system documentation and operation manuals as needed
Code, test and maintain new and existing SQL jobs, stored procedures and functions
Performance tune existing stored procedures, tables and indexes
Troubleshoot problems that may come up with database environments: performance issues, replication issues, or operational issues
Review SQL code written by other developers to ensure compliance to coding standards and best practices as well as maximum performance
Perform data analysis and data profiling tasks to provide support and recommendations for development and design decisions
Develop standardized reporting dashboards to meet the needs of the multiple business units across the Bank
Apply advanced modeling, data mining, machine learning and/or statistical techniques to data and dashboards to generate actionable insights enabling informed decision-making for optimized business and operational performance
Create mock-ups of reporting products, scorecards, dashboards, etc. to provide visualization to the end user
Work with teams within the organization to gather and document reporting requirements
Join client meetings to communicate status, give demos, provide timelines and offer insights
Participate in daily meetings that go over testing, and code reviews
Work with IT, Enterprise Data Management, Project Managers, Business Analysts, stakeholders across multiple business units to systematically plan the launch of new or enhanced dashboards, prepare launch collateral/documentation and work closely with users during through the different phases of a project
Develop deep understanding of the Bank's databases, identify appropriate data sources, relationships and logic needed to produce consistently reliable reports
Contribute to the overall strategy and quality of dashboarding
Document process steps of repetitive tasks performed
Partner with IT and other Infrastructure teams to tackle software upgrades, and coordinate testing
Perform any additional duties as assigned
Requirements:
Bachelor's degree in Information Technology, Computer Science, Business Administration, Mathematics or a related discipline
Customer Obsession: ""Good enough"" isn't good enough for you. You're obsessed with perfecting the customer experience
Leadership: A confident person with the ability to connect and inspire others to achieve success, whether or not they directly report to you
Results Oriented: A driver who possess the ability to take actions and implement effective solutions in a timely manner. Excuses aren’t in your vocabulary because you always find alternative solutions when issues arise
Ethics: Highest level of professional integrity and honesty as well as personal credibility. Your reputation for precedes you in this regard
Innovation: Dedication to maintaining cutting edge talent with the courage to implement new ideas, technology, and aggressively challenge the status quo. You don’t accept responses to new ideas like “That’s the way it’s always been done” because you use facts, data, and people skills to implement meaningful change
Immersion: A propensity to rapidly master the understanding and application of new technology
Excellent verbal and written communication skills, including ability to simplify complex concepts for technical and non-technical audience
Preferred:
Basic to intermediate knowledge in SQL server database development and testing
Working knowledge of Tableau
1+ year's working in an office environment or recent college graduate
APPLY DIRECTLY FOR CONSIDERATION:
Born digital, Axos Bank has reinvented the banking model and grown to over $18.4 billion in assets since our founding in 2000. With a broad and ever-growing range of financial products, Axos Bank is rated among the top 5 digital banks in the country! Axos Financial is our holding company and publicly traded on the New York Stock Exchange under the symbol ""AX"" (NYSE: AX).

We bring together human insight and digital expertise to anticipate the needs of our customers. Our team members are innovative, technologically sophisticated, and motivated to achieve.

Learn more about working here!

A targeted annual base salary range of USD $24/HR - $30/HR, based on the experience, skills, and education/certification required for this position. Eligibility for a discretionary semi-annual incentive compensation plan, based upon performance, payable in cash and/or share grants (RSU’s) that may vest over time. The annual discretionary target bonus percentage is up to 20%.

Axos benefits and perks include:
3 weeks’ Vacation, Sick leave, and Holidays (about 11 a year); Medical, Dental, Vision, Life insurance and more
HSA or FSA account and other voluntary benefits
401(k) Retirement Saving Plan with Employer Match Program and 529 Savings Plan
Employee Mortgage Loan Program and free access to Self-Directed Trading

Pre-Employment Drug Test:

All offers are contingent upon the candidate successfully passing a credit check, criminal background check, and pre-employment drug screening, which includes screening for marijuana. Axos Bank is a federally regulated banking institution. At the federal level, marijuana is an illegal schedule 1 drug; therefore, we will not employ any person who tests positive for marijuana, regardless of state legalization.

Equal Employment Opportunity:

Axos Bank is an Equal Opportunity employer. We are committed to providing equal employment opportunities to all employees and applicants without regard to race, religious creed, color, sex (including pregnancy, breast feeding and related medical conditions), gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship status, military and veteran status, marital status, age, protected medical condition, genetic information, physical disability, mental disability, or any other protected status in accordance with all applicable federal, state and local laws.

Job Functions and Work Environment:

While performing the duties of this position, the employee is required to sit for extended periods of time. Manual dexterity and coordination are required while operating standard office equipment such as computer keyboard and mouse, calculator, telephone, copiers, etc.

The work environment characteristics described here are representative of those an employee may encounter while performing the essential functions of this position. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this position.

#LI-Onsite",48600,1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,2000,$500 million to $1 billion (USD),CA,23,data engineer,na,"['sql', 'go']",[],"['tableau', 'ssis']",['sql server'],[],[],bachelor,+10 years
Opensignal,4.5,"Boston, MA",Senior Data Engineer,"Department: Product/Technology
Location: Boston / US East Coast or Canada

Purpose of Role

We’re looking for a Senior Data Engineer to join our Marketing Performance Group in
transforming our real-world raw data into valuable and credible industry-leading metrics that
provide insights to our analysts and our customers.

What you will be doing
The creation and implementation of a framework to assist in building complex statistical
models. Working closely with our data scientists and our data engineers to create and
evolve products that measure market dynamics in the Telecommunication space that drive
our customers short-term marketing campaign tactics and their longer-term customer
acquisition and retention strategies. This role reports to our Engineering Manager.

We expect our Lead Data Engineer to do:
Own and improve our data pipeline. Assemble large, complex data sets that
meet business requirements, with engineering best practices in mind.
Champion building scalable and resilient data infrastructure, as well as tools to
extract and transform data used by stakeholders and customers.
Be security conscious and sensitive to privacy concerns and legislation related
to the data within the platform.
Have a continuous improvement mindset when it comes to both the platform
and the process.
Work efficiently, automate manual processes where possible, and take a test-
driven approach to engineering.
Take a keen interest in improving the platform’s scalability while understanding
the cost.
Be a good team player, with an agile approach and a can-do attitude.
Keep yourself current and make sure we follow best practices and engineering
standards.
Be an advocate for the platform and its health. Take ownership of your work
from conception through to support.
Be detail orientated and understand the importance of the credibility of our
metrics. Document and communicate with stakeholders in a language
understood by all.
Can work in a fast-paced environment with an ability to shift priorities and focus
on changing requirements and market demands.
Able to coach and mentor Data Engineers in best practices.
Cross-collaborate with the wider team to drive and maintain high standards in
our data pipeline builds.
Comfortable and effective at working in a remote capacity, collaborating with
team members across different locations through digital channels.

What we need from you:
As a Senior Data Engineer, we would expect you to have previous experience in
manipulating, processing, storing, and extracting value from big data.
Advanced with hands-on experience in architecting, crafting, documenting, and
developing highly scalable distributed data processing systems.
Advanced with big data tools, specifically Apache Spark.
Advanced with relational SQL databases. Prior experience with MSSQL,
Postgres, AWS Athena (Presto).
Advanced with SQL query authoring including DBT.
Experience with data pipeline / workflow tools i.e. Apache Airflow.
Experience with AWS cloud services like EC2, S3, managed Kubernetes, AWS
ECS, and Aurora.
Experience with object-oriented/object function scripting languages: Python and
Scala.
Experience in implementing complex clustering and classification models on
large datasets to support new product development.
Experience in writing tests, especially in BDD style and working with Git.
Strong analytic skills in working with unstructured datasets.
Experience in root cause analysis of data when asked to answer specific
business questions.
Experience building and optimizing & ""big data"" data pipelines.
Experience supporting and working with cross-functional teams.
Strong self-organizational skills
Experience being part of a cross-functional team, using agile methodologies.
Bachelor’s degree

For US applicants only

At this time, the company will not sponsor a new applicant for employment sponsorship for this position.
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

About Us
Opensignal is the leading global provider of independent insight and data into network
experience and market performance. Our user-centric approach allows communication
providers to constantly improve their network and maximize commercial performance.
Leading analysts, investors and financial institutions place a high value on our independent
analysis and we are regular contributors to their reports.
Real network experience is our focus and ultimately that’s what influences customer choice.
Our mission is to advance connectivity for all and here at Opensignal, the team is leading
the industry in enabling operators to link their network experience and market performance
in a way that has never before been possible.
With offices in London, Boston and Victoria, British Columbia, we are truly global, with
employees working across four continents and representing over 25 nationalities. We are
an equal opportunity employer dedicated to building an inclusive and diverse workforce.

Benefits:
We believe we are stronger when we not only celebrate our many differences, values, and
voices but include them in everyday practice. Having a diverse and inclusive culture is
essential, which is why we offer a flexible approach to work-life balance, operating in a
remote-hybrid way. We’ll help you get set up with the essentials you need to work from
home or the office. We also offer an attractive range of additional benefits, including:
Competitive compensation packages including a long-term equity program.
Comprehensive group benefits package and company-sponsored retirement
savings plan (details depend on your country of work).
Professional development opportunities: education reimbursement, learning
allowance, company-sponsored workshops, and more!
Generous holiday allowance, sick leave, parental leave, flexibility including Flex
Fridays, and the opportunity to work from abroad.
Charity matching and time off for community volunteering and DE&I
program/committees.
Regular virtual and in-person events and socials.",148405,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2010,Unknown / Non-Applicable,MA,13,data engineer,senior,"['sql', 'scala', 'python']",['aws'],[],['dbt'],['spark'],[],bachelor,
Johns Hopkins University,4.0,"Baltimore, MD",PARADIM Data Engineer,"The Platform for the Accelerated Realization, Analysis, and Discovery of Interface Materials (PARADIM), a National Science Foundation Materials Innovation Platform developing the next generation of electronic and quantum materials, is looking for a full-time PARADIM Data Engineer to join the platforms materials data science team.

The successful candidate must be capable of designing and implementing software solutions related to data acquisition, streaming, processing, and storage, and should have a keen desire to partner with scientists developing a versatile, real-time, streaming data infrastructure to enable discovery and development of novel interface materials that will drive the quantum computing revolution.

The ideal candidate will also have technical skills that complement a vision and creativity to find solutions to connecting Big Data from diverse experimental and computational laboratories as well as the rigor and experience to develop high-quality, production software and data pipelines to address these challenges. The ability to travel by car to the campuses of both Cornell University in Ithaca, NY, and the Johns Hopkins University in Baltimore, MD, is strongly preferred.

Specific Duties & Responsibilities

With the chief data officer and PARADIM leadership team, design and implement software solutions related to data acquisition, streaming, processing, and storage, both centrally and at end stations at both PARADIM Institutions (JHU and Cornell).
Work with PARADIM scientists to design and implement a versatile, real-time, streaming data infrastructure to enable discovery and development of novel interface materials that will drive the quantum computing revolution.
Implement software tools for data analysis and interpretation as needed by the PARADIM platform.
Participate in the design of back-end and front-end systems to implement PARADIMs data vision.
Research and implement new technologies that could be beneficial to PARADIM.
Evaluate, test and vet new technology in support of PARADIM efforts.
Work with vendors to procure prototypes and demo units.
Attend department and University-sponsored training to increase knowledge, improve skills, and learn new skills.
May substitute University training for supervisor approved commercial job-related course offerings.

This position may be primarily remote (90%). Occasional in-person trips to Cornell University and/or Johns Hopkins University will be needed for work directly involving software on new hardware. This requirement may best suit candidates located between Baltimore, MD and Ithaca, NY.
Minimum Qualifications

Bachelor's Degree
Five years related experience
Additional education may substitute for required experience and additional related experience may substitute for required education, to the extent permitted by the JHU equivalency formula

Preferred Qualifications

The ideal candidate will also have technical skills that complement a vision and creativity to find solutions to connecting Big Data from diverse experimental and computational laboratories as well as the rigor and experience to develop high-quality, production software and data pipelines to address these challenges. The ability to travel by car to the campuses of both Cornell University in Ithaca, NY, and the Johns Hopkins University in Baltimore, MD, is strongly preferred.

Proficiency in at least one major object-oriented language such as Java, C++, or C#
Proficiency in at least one major software versioning and tracking platform (e.g. git, github, gitlab, svn)
Experience with python and pydata packages such as jupyterlab, cython, numpy, tensorflow
Experience working with instrumental data
Contributions towards open-source software and commitment to open-source development
Experience with Apache Kafka or Confluent Platform
Level of Independent Decision Making
Works independently

Classified Title: Systems Engineer
Role/Level/Range: ATP/04/PE
Starting Salary Range: $71,230 - $97,880 - $124,510 annually (Commensurate with experience)
Employee group: Full Time
Schedule: Monday-Friday, 8:30 am - 5:00pm
Exempt Status: Exempt
Location: Homewood Campus (Hybrid)
Department name: Chemistry
Personnel area: School of Arts & Sciences

Total Rewards
The referenced salary range is based on Johns Hopkins University's good faith belief at the time of posting. Actual compensation may vary based on factors such as geographic location, work experience, market conditions, education/training and skill level. Johns Hopkins offers a total rewards package that supports our employees' health, life, career and retirement. More information can be found here: https://hr.jhu.edu/benefits-worklife/

Please refer to the job description above to see which forms of equivalency are permitted for this position. If permitted, equivalencies will follow these guidelines:

JHU Equivalency Formula: 30 undergraduate degree credits (semester hours) or 18 graduate degree credits may substitute for one year of experience. Additional related experience may substitute for required education on the same basis. For jobs where equivalency is permitted, up to two years of non-related college course work may be applied towards the total minimum education/experience required for the respective job.

**Applicants who do not meet the posted requirements but are completing their final academic semester/quarter will be considered eligible for employment and may be asked to provide additional information confirming their academic completion date.

The successful candidate(s) for this position will be subject to a pre-employment background check. Johns Hopkins is committed to hiring individuals with a justice-involved background, consistent with applicable policies and current practice. A prior criminal history does not automatically preclude candidates from employment at Johns Hopkins University. In accordance with applicable law, the university will review, on an individual basis, the date of a candidate's conviction, the nature of the conviction and how the conviction relates to an essential job-related qualification or function.

The Johns Hopkins University values diversity, equity and inclusion and advances these through our key strategic framework, the JHU Roadmap on Diversity and Inclusion .

Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

EEO is the Law

Learn more:
https://www.eeoc.gov/sites/default/files/migrated_files/employers/poster_screen_reader_optimized.pdf

Accommodation Information

If you are interested in applying for employment with The Johns Hopkins University and require special assistance or accommodation during any part of the pre-employment process, please contact the Talent Acquisition Office at jhurecruitment@jhu.edu . For TTY users, call via Maryland Relay or dial 711. For more information about workplace accommodations or accessibility at Johns Hopkins University, please visit accessibility.jhu.edu .

Johns Hopkins has mandated COVID-19 and influenza vaccines, as applicable. Exceptions to the COVID and flu vaccine requirements may be provided to individuals for religious beliefs or medical reasons. Requests for an exception must be submitted to the JHU vaccination registry. For additional information, applicants for SOM positions should visit https://www.hopkinsmedicine.org/coronavirus/covid-19-vaccine/ and all other JHU applicants should visit https://covidinfo.jhu.edu/health-safety/covid-vaccination-information/ .

The following additional provisions may apply, depending on campus. Your recruiter will advise accordingly.

The pre-employment physical for positions in clinical areas, laboratories, working with research subjects, or involving community contact requires documentation of immune status against Rubella (German measles), Rubeola (Measles), Mumps, Varicella (chickenpox), Hepatitis B and documentation of having received the Tdap (Tetanus, diphtheria, pertussis) vaccination. This may include documentation of having two (2) MMR vaccines; two (2) Varicella vaccines; or antibody status to these diseases from laboratory testing. Blood tests for immunities to these diseases are ordinarily included in the pre-employment physical exam except for those employees who provide results of blood tests or immunization documentation from their own health care providers. Any vaccinations required for these diseases will be given at no cost in our Occupational Health office.

Note: Job Postings are updated daily and remain online until filled.

To apply, visit https://jobs.jhu.edu/job/Baltimore-PARADIM-Data-Engineer-MD-21218/1027795600/

jeid-8befc9e383ba7d4394f407df6f0f70f2",124510,10000+ Employees,College / University,Education,Colleges & Universities,1876,$1 to $5 billion (USD),MD,147,data engineer,na,"['java', 'python']",[],[],[],['kafka'],['gitlab'],bachelor,
Boston Dynamics AI Institute,4.7,"Cambridge, MA",Data Engineer,"Our Mission
Our mission is to solve the most important and fundamental challenges in AI and Robotics to enable future generations of intelligent machines that will help us all live better lives.

Data Engineers will work cross-functionally, creating new technology to improve software development for robots. If you have a passion for developing technology for robots and use it to advance their capabilities and usefulness, you will want to join us! We are onsite in our new Cambridge, MA office where we are building a collaborative and exciting new organization.
Responsibilities
Work collaboratively with research scientists and software engineers on software development for a range of different robotic platforms
Develop and maintain our data warehouses and data pipelines in cloud and on-premise infrastructureBuild event and batch driven ingestion systems for machine learning and R&D as needed
Develop and administer databases, knowledge bases, and distributed data stores
Create and use systems to clean, integrate, or fuse datasets to produce data products
Establish and monitor data integrity and value through visualization, profiling, and statistical tools
Perform updates, migrations, and administration tasks for data systems
Develop and implement a data governance, data retention strategyUse Python and SQL to develop, maintain and scale our data stores
Requirements
BS/MS in computer science, robotics, or a related field
5+ years of experience in a data engineering or similar role
Demonstrated experience with a variety of relational database and data warehousing technology such as AWS Redshift, Athena, RDS, BigQuery
Demonstrated experience with big data processing systems and distributed computing technology such as Databricks, Spark, Sagemaker, Kafka, etc
Strong experience with ETL design and implementations in the context of large, multimodal, and distributed datasets
Bonus (Not Required)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)2+ years of experience with Airflow
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",112287,201 to 500 Employees,Subsidiary or Business Segment,Information Technology,Computer Hardware Development,1992,$5 to $25 million (USD),MA,31,data engineer,na,"['sql', 'shell', 'nosql', 'r', 'python']","['databricks', 'snowflake', 'aws', 'redshift']",[],"['snowflake', 'hive', 'mysql']","['kafka', 'hadoop', 'spark']",[],,+10 years
TECKpert,4.9,"Miami, FL",Data Engineer,"We are looking for a Data Engineer to support our client based in Miami, Florida.
US BASED CANDIDATES ONLY.
This is an hybrid position. Candidates must be located in or near Miami, Florida.
Who we are
Founded in 2009 and headquartered in beautiful Miami, FL, TECKpert is a tech consulting and staff augmentation firm. At TECKpert, we offer a contingent workforce built for any size digital transformation project. Experts in design, development, IT, analytics and marketing, provide innovative digital solutions to achieve success in our new economy. Our leaders identify the technical talent best suited to bolster our client’s capabilities, across all industries, including, healthcare, government, finance, legal, real estate, and startups.
The project
TECKpert seeks to hire a Data Engineer based in Miami, FL to support our client, a large healthcare system based in Miami, FL with centers throughout the United States.
The Data Engineer leads data integration and analytics projects that support data collection, automation, transformation, storage, delivery, and reporting processes. Optimizes data retrieval and processing, including performance tuning, delivery design for down-stream analytics, machine learning modeling (including feature engineering), and reporting.
Responsibilities
Lead data engineering projects and collaborate with stakeholders to develop end-to-end solutions, including designing data structures for downstream analytics, machine learning modeling, feature engineering, prototype development, and reporting.
Assist in all stages of data orchestration, including working with diverse data sources, data cleaning, data transformation, ETL/ELT processes, and data visualization.
Create analytics solutions using Azure Cloud tools, leveraging the capabilities of the platform.
Design and engineer efficient data pipelines for data collection, processing, and distribution using appropriate data platform infrastructure.
Build visualizations to extract meaningful business insights and construct cloud data warehouses/data marts utilizing Azure Data Lake.
Extract data from relational and structured/unstructured sources, perform data analysis, identify correlations, patterns, and other relevant insights.
Identify gaps in master data and transactional data through data analysis.
Assist in defining and maintaining reporting and dashboard standards, guidelines, and processes to ensure high-quality data.
Compensation and Term
This opportunity is for a full-time, ongoing need and pay commensurate with experience up to $96,000 to $122,000 per year. Medical, dental & vision insurance, employee mental health program, paid time off, paid holidays, 401(k) with employer match, employee stock purchase program, tuition reimbursement and much more.
Qualifications
Bachelor's degree in Computer Science, Math, Statistics, Economics, Accounting, Business, or a related field.
Minimum of 2 years of hands-on development experience in building analytics solutions using Microsoft Azure Cloud, with knowledge of Cloud Security, DevOps, Governance, and Data privacy.
Proficiency in programming languages such as Python, Java, Scala, and SQL. Familiarity with database systems, distributed computing systems, and big data technologies (e.g., Hadoop, Spark, Kafka).
Experience in developing and supporting database systems for medium to large organizations, including database structure systems, data management resources, data mining, and data modeling.
Implement data pipelines for Azure Analysis Services reporting data models.
Conduct complex analyses of business data and processes.
Provide strategic and analytic models to address key business questions.
Collect, organize, manipulate, and analyze diverse datasets.
Track and report on the performance of deployed models.
Assist in developing dashboards to facilitate strategic decision-making by executives.
Perform data studies and product experiments related to new data sources or novel applications of existing data sources, interpreting the results effectively.

crqHwoeq0Q",109000,1 to 50 Employees,Company - Private,Information Technology,Internet & Web Services,2009,$1 to $5 million (USD),FL,14,data engineer,na,"['sql', 'java', 'scala', 'python']",['azure'],[],[],"['kafka', 'spark', 'hadoop']",[],master,2-5 years
Financial Information Technologies LLC,3.7,"Tampa, FL",Senior Data Engineer,"Join Fintech as a Senior Data Engineer!
Fintech is the leading business solutions provider for the beverage alcohol industry, empowering alcohol suppliers, distributors, and retailers with smart solutions that simplify beverage alcohol management. Our unique, thriving company culture promotes collaboration and growth at every level, and our comprehensive employee benefits have earned Fintech the title of a Tampa Bay Times Top 100 Workplaces for 2020 and 2021.
Fintech’s Senior Data Engineer brings a depth of relational database modeling and an understanding of transactional processing across a myriad of database types. They can analyze and assess new data sets to understand nuances of content in the context of purpose with an ability to conceptualize cleansing, harmonization, and modeling efforts. Working under the direction of the principal process architect the senior data engineer will lead a small team of experienced data wranglers to tackle a myriad of ad-hoc custom projects as well as service the development needs within our warehouse and app abstraction layers.
Essential Functions:
Collaborates with ELT/process automation, data insights, and data science teams
Builds data models in accordance with prescribed methodologies
Serves as knowledgeable backstop for level III ticket resolution
Guides and instructs junior developers and engineers on how to implement directives in accordance with project needs within adopted framework
Gains a familiarity with and contributes to the core meta-data driven data processing engine
Advising on data model consumption in analytics layers
Contributes to knowledge base
Qualifications:
8 + years of experience with SQL in multiple database flavors (SQL Server, Oracle, Snowflake, Postgres, Greenplum)
5 + years of experience with data ingest transformations and harmonization
5 + years of experience with database object creation and modeling
Analytical thinker that can adapt and problem solve in a fast-paced environment
Team oriented
Must be able to consume, understand, and implement a complicated but flexible processing back-end in a short time frame
Our Benefits:
Employer Matched 401K (Up to 10% of Employee Salary)
Company Paid Medical Insurance Option (Employee and Dependent Children)
Company Paid Dental Insurance Option (Employee only)
Company Paid Vision Insurance Option (Employee only)
Company Paid Long and Short-Term Disability
Company Paid Life and AD&D Insurance
Employee Recognition Program
18 PTO Days a Year
Six Paid Holidays
Business Casual Dress Code
Check out www.fintech.com for more information!
We E-Verify.
Fintech is an Equal Opportunity Employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth and pregnancy-related conditions), gender identity or expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, genetic information, or any other characteristic protected by applicable federal, state, or local laws and ordinances. Fintech’s management team is dedicated to this policy with respect to recruitment, hiring, placement, promotion, transfer, training, compensation, benefits, employee activities, access to facilities and programs and general treatment during employment.
Fintech is a Drug-Free Workplace.",109213,51 to 200 Employees,Company - Private,Financial Services,Financial Transaction Processing,1991,$25 to $100 million (USD),FL,32,data engineer,senior,['sql'],['snowflake'],[],"['sql server', 'snowflake', 'oracle']",[],[],,+10 years
Chmura Economics and Analytics,3.6,"Cleveland, OH",Data Engineer,"Description

Founded in 1998, Chmura Economics & Analytics is headquartered in Richmond, Virginia’s historic Shockoe Slip with a regional office in Cleveland, Ohio. We provide labor market software, consulting, and data so our clients can make informed decisions that grow their communities and organizations.

For example, our technology:
Helps economic developers understand their local industries and labor market
Allows workforce development practitioners to guide workers to high-wage, high-demand jobs
Assists educators in training their students for well-paying, in-demand careers
Helps site selectors choose the best site for their clients’ expansion or relocation by understanding the talent availability in competing locations
Allows staffing and recruitment firms to determine competitive wages
We’re more than a technology company – we help our clients win.
Our employees are encouraged to think differently, ask challenging questions, and pursue what’s best for our clients.

We want our clients to make confident decisions. If you want to help communities and organizations thrive, you’ve come to the right place.

Responsibilities
Implement, maintain, and continuously optimize data processing solutions for a wide variety of complex big data sets.
Architect, design, develop, and maintain data integration solutions as they relate to all stages of extract, transform, and load (ETL) pipelines.
Build internal tooling that serves to improve upon the ability to create, test, build, serve, compare, and optimize complex data models and their related data sets.
Troubleshoot data issues and effectively triage timely solutions.
Contribute to and maintain documentation as it relates to new and existing data models, processes, storage, and optimization techniques.
Requirements
Experience in a relevant programming language: C#, Python, Java, etc.
Experience with Elasticsearch
Experience with containerization platforms (Docker, K8S, etc)
Experience with schema design and writing queries for SQL Server, Postgres or similar
Azure experience
Kanban/Agile experience
Familiarity with machine learning and NLP is nice to have but not required
At least 2 years. This is not an “junior” position.

We offer a comprehensive compensation and benefits package.
Salary is commensurate with experience.
Chmura is not able to provide sponsorship for this role.

Chmura is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, physical and mental disability, genetic information, marital status, sexual orientation, gender identity/assignment, citizenship, pregnancy or maternity, protected veteran status, or any other status prohibited by applicable national, federal, state or local law. Chmura promotes a drug-free workplace. Chmura will consider for employment, qualified applicants with a criminal history in a manner consistent with the requirements of applicable federal, state, and local laws and regulations regarding criminal background inquiries, including, to the extent applicable, following applicable federal, state, and local laws and regulations regarding criminal background inquiries.",90299,1 to 50 Employees,Company - Public,Management & Consulting,Research & Development,1998,Unknown / Non-Applicable,OH,25,data engineer,na,"['sql', 'java', 'python']",['azure'],[],"['sql server', 'elasticsearch']",[],['docker'],,2-5 years
"VISUAL SOFT, INC",4.1,"Washington, DC",Data Engineer - Active TOP SECRET - REMOTE-ONSITE,"Visual Soft, Inc is seeking qualified candidates to work on our efforts with a Prime for their end customer, a federal agency.

Position: Data Engineer - (50% REMOTE and 50% ONSITE)
Location: Washington, DC or Crystal City, Arlington, VA
Shift time: 8 am to 5 pm

JOB DESCRIPTION:
As a Data Engineer, you’ll implement data engineering activities on some of the most mission-driven projects in the industry. You’ll deploy and develop pipelines and platforms that organize and make disparate data meaningful. You will collaborate and work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You’ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients.
**Desirable skills include, Spark, Databricks, Data Lakes, Bigdata Tools and Technologies and AWS
Years of Experience:: 5+ years of experience
Education Requirement: BS degree preferred
Clearance requirement: Top SECRET is a MUST

Standard Benefits:
Our standard benefits include: Our standard benefits include 3 weeks of Paid time off (PTO that includes sick leave). Any unused PTO will be issued as a check at the end of an employee's anniversary with us. we also provide 2 floating and 8 public holidays. Floating and holidays expire at the end of every year of service of an employee. In addition, company will cover 50% of health and dental insurances only for all full time employees, however, dependents can be added at extra cost. Employee's health and dental coverage becomes effective after 30 days or first of the month after an employee completes initial 30 working days, we cover 50% for the employee's health and dental insurances. Dependents coverage for health and dental insurances is available as an out of pocket expense for employees. An employee has to finish all of your paper work for health and dental in the first 30 days of your employment with us. We provide STD, LTD and one time salary equivalent of life insurance at NO cost to all full time employees. All full time employees or w-2 employees with no benefits will be eligible to participate in company's 401k program after 90 days of employment with a company match of 4%, immediate vesting. In addition, all w-2 employees are eligible to be part of company's profit sharing, no employee contributions required. No commuting and/or parking expenses provided.",98574,1 to 50 Employees,Company - Public,,,-1,$1 to $5 million (USD),DC,-1,data engineer,na,[],"['databricks', 'aws']",[],[],['spark'],[],,+10 years
Wingsoft Consulting LLC,4.0,Remote,AWS Data Engineer – W2 Role,"Greetings,
This is Deepak Sharma Technical recruiter from Wingsoft consulting LLC. I have urgent opening for AWS Data Engineer for Charlotte, NC Locationwith one of our direct client. Please let me know if you or anyone interested and available for this role!!
Title: AWS Data Engineer – W2 Role
Location: Charlotte, NC (onsite would be preferred) will consider remote
Duration:1 year contract +
Job Description
AWS Data Engineer Lead, Core Technical Skills
1) 5+ years of AWS experience
2) Experience in a regulated environment
3) AWS services - S3, EMR, Glue Jobs, Lambda, Athena, CloudTrail, SNS, SQS, CloudWatch, Step Functions
4) Experience with Kafka/Messaging preferably Confluent Kafka
5) Experience with databases such as Document DB, MySQL, Postgres, Glue Catalog, Lake Formation, Redshift, DynamoDB and Aurora and SQL
6) Tools and Languages – Python, Spark, PySpark
7) Experience with Secrets Management Platform like Vault and AWS Secrets manager
8) Experience with Event Driven Architecture
9) Experience with Rest APIs and API gateway
10) Experience with AWS workflow orchestration tool like Airflow or Step Functions
AWS Data Engineer Lead Additional Technical Skills (nice to have, but not required for the role)
11) Experience with native AWS technologies for data and analytics such as Kinesis, OpenSearch
12) Databases - Document DB, MongoDB Atlas
13) Data Lake platform (Hive, Druid, Apachi Hudi/Apache Iceberg/Databricks Delta)
14) Java, Scala, Node JS, Pandas
15) Workflow Automation
16) Experience transitioning on premise big data platforms into cloud-based platforms such as AWS
17) Strong Background in Kubernetes, Distributed Systems, Microservice architecture and containers
18) Day to Day Responsibilities/project specifics:
a. Provides technical direction, guides the team on key technical aspects and responsible for product tech delivery
b. Lead the Design, Build, Test and Deployment of components
i. Where applicable in collaboration with Lead Developers (Data Engineer, Software Engineer, Data Scientist, Technical Test Lead)
c. Understand requirements / use case to outline technical scope and lead delivery of technical solution
d. Confirm required developers and skillsets specific to product
e. Provides leadership, direction, peer review and accountability to developers on the product (key responsibility)
f. Works closely with the Product Owner to align on delivery goals and timing
g. Assists Product Owner with prioritizing and managing team backlog
h. Collaborates with Data and Solution architects on key technical decisions
i. The architecture and design to deliver the requirements and functionality
i. Mentor other developers in development of components and related processes
Job Type: Contract
Pay: Up to $90.00 per hour
Schedule:
8 hour shift
On call
Experience:
AWS: 7 years (Preferred)
Security clearance:
Confidential (Preferred)
Work Location: Remote",162000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'java', 'scala', 'python']","['databricks', 'aws', 'redshift']",[],"['dynamodb', 'mongodb', 'hive', 'mysql']","['kafka', 'spark']",[],,5-10 years
Mass General Brigham,3.8,"Somerville, MA",Sr. Data Engineer (Data Lakes),"Sr. Data Engineer (Data Lakes)
- (3244480)

About Us:
At Mass General Brigham Digital, we pride ourselves on our ability to create maximum strategic, clinical, and operational value from established and emergent technologies for our patients, care teams, researchers, and employees. Digital health will not only enhance the equity and efficiency of healthcare delivery, but it will also help make medicine more personalized and precise.
We recognize that increasing value and continually improving quality while maintaining an inclusive focus are essential to organizational excellence, and we invite you to join us on this journey. The work we do in Digital is a strategic imperative, and there is a strong and growing understanding of how together we will transform Mass General Brigham in innovative and impactful ways.
General Summary/ Overview:
At Mass General Brigham Digital, we pride ourselves on our ability to create maximum strategic, clinical, and operational value from established and emergent technologies for our patients, care teams, researchers, and employees. Digital health will not only enhance the equity and efficiency of healthcare delivery, but it will also help make medicine more personalized and precise.
We recognize that increasing value and continually improving quality while maintaining an inclusive focus are essential to organizational excellence, and we invite you to join us on this journey. The work we do in Digital is a strategic imperative, and there is a strong and growing understanding of how together we will transform Mass General Brigham in innovative and impactful ways.
Summary:
Reporting to the Engineering Manager, Data Lake, the Senior Data Engineer (Azure Data Lake) will work towards analyzing, designing, developing, and building ADF data pipelines, ELT/ETL frameworks, and Azure data lake platforms, primarily focusing on Epic (EHR) data and other healthcare data; and will thrive as a member of an experienced, high performing and highly motivated team. Role will be responsible for participating in building out our existing EDW and our new Data Lake, expanding our data ecosystem and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Requires advanced experience with data engineering and building Azure Cloud Data Lake, Azure Big Data Analytics technologies and architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures, and data sets. Expert level of experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Azure Data Bricks, Azure ML, SQL Data Warehouse. Advanced Experience with Hadoop based technologies (e.g., hdfs, Spark) and Programming experience in Python, SQL, Spark.
Principal Duties and Responsibilities:
Design, Develop, construct, test and maintain Data Lake architectures and large-scale data processing systems.
Support big data ecosystem related Tool selection and POC analysis.
Gather and process raw data at scale that meet functional / non-functional business requirements (including writing scripts, REST API calls, SQL Queries, etc.).
Develop data set processes for data modeling, mining and production.
Integrate new data management technologies ( Informatica DQ..) and software engineering tools into existing structures.
The candidate will be responsible for participating in building out our Data Lake platform, expanding and optimizing our data ecosystem and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.
The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up.
The Data Engineer will support our Software Developers, Database Architects, Data Analysts and Data Scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements on cloud based data platforms (e.g. Azure) and relational data systems (SQL Server, SSIS).
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.
Build the data infrastructure required for optimal extraction, transformation, and loading of data from traditional/legacy data sources.
Work with stakeholders including the Management team, Product owners, and Architecture teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Use/s the Mass General Brigham values to govern decisions, actions, and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability & Service Commitment, Decisiveness, Innovation & Thoughtful Risk; and how we treat each other: Diversity & Inclusion, Integrity & Respect, Learning, Continuous Improvement & Personal Growth, Teamwork & Collaboration.
Working Conditions:
This is a remote position.
Diversity Statement
As a not-for-profit organization, Mass General Brigham is committed to supporting patient care, research, teaching, and service to the community. We place great value on being a diverse, equitable and inclusive organization as we aim to reflect the diversity of the patients we serve. At Mass General Brigham, we believe in equal access to quality care, employment and advancement opportunities encompassing the full spectrum of human diversity: race, gender, sexual orientation, ability, religion, ethnicity, national origin and all the other forms of human presence and expression that make us better able to provide innovative and cutting-edge healthcare and research.

5+ Years of experience data engineering and building Azure Cloud Data Lake technologies and architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures and data sets.
5-7 Years of Experience with Hadoop based technologies (e.g. hdfs, Spark). Spark Experience desirable
5+ years of Programming experience in Python, SQL, PySpark.
Healthcare experience, most notably in Clinical data, Epic, Clarity, Caboodle, Payer data and reference data is a plus but not mandatory.
Experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Snowflake, Azure Data Bricks, Powershell.
Experience with Design and Architecture of relational SQL and NoSQL databases, including MS SQL Server, Cosmos DB.
Experience with Design and Architecture of data security and Azure security, VM, Vnet.
Experience with building processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience leading and working with cross-functional teams in a dynamic environment.
Experience building Big data pipeline with Spark and/or Data Bricks is a plus.
Leading development of Data Lake Architectures from scratch.
Experience with Azure DevOps/CI-CD, Continuous integration and deployment.
Experience with Real time analytics on Spark, Kafka, Event Hub is a plus.
Experience in petabyte scale data environments and integration of data from multiple diverse sources.
Skills/Abilities/Competencies:
Advanced hands-on SQL, Spark, Python, pySpark (2+ of these) knowledge and experience working with relational databases for data querying and retrieval.
Strong SQL skills on multiple platform (preferred MPP systems).
Data Modeling tools (e.g. Erwin, Visio).
Strong interpersonal and communication skills, both written and verbal.
Strong Scrum/Agile development experience.
Excellent organizational skills and attention to detail, manage multiple tasks and projects, meet deadlines, follow through, and manage to schedule.
Strong innovation capabilities and the ability to think creatively.
Strong collaboration and team building skills within, across and outside of an organization.
Maintain and promote a positive team environment.
Maintains stable performance under pressure, demonstrating sensitivity to diverse organizational culture.
Ability to effectively cope with change, remain flexible and adaptable within a fast-paced environment with rapidly changing requirements, and ability to negotiate situations when the big picture is not clearly defined.

EEO Statement

Mass General Brigham is an Equal Opportunity Employer. By embracing diverse skills, perspectives, and ideas, we choose to lead. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under the law. We will ensure that all individuals with a disability are provided a reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment.

Primary Location MA-Somerville-MGB Assembly Row
Work Locations MGB Assembly Row 399 Revolution Drive Somerville 02145
Job Business and Systems Analyst
Organization Mass General Brigham
Schedule Full-time
Standard Hours 40
Shift Day Job
Employee Status Regular
Recruiting Department MGB Digital
Job Posting May 12, 2023",118726,1001 to 5000 Employees,Nonprofit Organization,Healthcare,Health Care Services & Hospitals,1994,$10+ billion (USD),MA,29,data engineer,senior,"['sql', 'nosql', 'python']","['azure', 'snowflake']",['ssis'],"['sql server', 'snowflake']","['kafka', 'spark', 'hadoop']",[],,+10 years
Millennium Physician Group,3.4,"Fort Myers, FL",IT Data Engineer,"Millennium Physician Group is seeking an experienced Data Engineer to join our Technology and Innovation Services Team in Fort Myers, FL. t
The Data Engineer role is responsible for building the foundation for a Datawarehouse using best practice ETL/ELT methodologies. This position will develop and maintain data pipelines, interfaces, and process automation. The Data Engineer will be required to investigate and understand datasets from dozens of discrete sources that may or may not always have available documentation. This position must communicate highly complex data trends to organizational leaders in a way that's easy to understand. The position requires collaborating with the existing BI/Analytics teams, Software and Database developers to create a centralized repository and platform to be used by all data consumers in the organization. The Data Engineer must demonstrate advanced knowledge of SQL, OOP development concepts, and Business Intelligence/Analytics. The position requires the ability to develop code for use in the automation of data pipelines. This position reports to the Data engineering Manager.
Essential Duties and Responsibilities include the following. Other duties may be assigned.
Build scalable data pipelines that clean, transform, and aggregate data from disparate sources
Assemble large, complex data sets that meet functional / non-functional business requirements
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Snowflake technologies
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Implements processes and systems to monitor data quality to ensure production data is always accurate and available
Writes unit/integration tests, contributes to engineering wiki, and documents work
Defines company data assets (data models)
Designs data integrations and data quality framework
Build analytics tools that utilize the data pipeline to provide actionable insights into key business performance metrics
Works closely with a team of frontend and backend engineers, developers, and analysts

Education and Qualifications:
Master's Degree preferred, Bachelor's required in Computer Science, Information Technology, Informatics, Engineering, Statistics or equivalent.
4+ Years Prior experience as a Data Engineer, Data Scientist, or Database Administrator required
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience with object-oriented/object function scripting languages: Python, Java, C#, Scala, etc.
Experience with Cloud Datawarehouse technologies (Snowflake/Redshift) and design
Ability to create and maintain optimal data pipeline architecture
Hands-on experience building and maintaining ETL/ELT processes
Experience interacting with and extracting data from RESTful APIs
Experience with commercial ETL/ELT toolsets: Matillion, Talend, Fivetran, etc.
Experience building data visualizations using BI tools
Experience with AWS Cloud services
ABOUT MILLENNIUM PHYSICIAN GROUP
Formed in 2008, Millennium Physician Group has grown into one of the largest comprehensive primary care practices with more than 400 health care providers located throughout Florida. With corporate headquarters in Fort Myers, Florida, Millennium Physician Group consists of primary care offices, Imaging Centers, Lab Services, Physical Therapy, and Wellness Programs. We also have several administrative departments supporting our medical offices, such as Quality Assurance, ACO, Business Services, Coding ACO, IT, Human Resources, and more.

If you are interested in joining an organization that emphasizes teamwork and family, Millennium Physician Group is the right choice.
Millennium's core values summarize how we treat others, patients, and fellow community members. Millennium CARES for every patient every time.

ARE YOU READY TO JOIN OUR TEAM? If you are the right candidate for this position, please click the link to apply today. We look forward to meeting you!",94511,1001 to 5000 Employees,Private Practice / Firm,Healthcare,Health Care Services & Hospitals,-1,$25 to $100 million (USD),FL,-1,data engineer,na,"['sql', 'java', 'scala', 'python']","['snowflake', 'aws', 'redshift']",[],['snowflake'],[],[],master,+10 years
Synergy technologies,4.7,Remote,Data Engineer (W2),"Data Engineer
** Remote Opportunity **
JOB DESCRIPTION:
Data Engineer -
Within this role, you would be a hands-on leader in data engineering functions including schema design, data movement, data transformation, encryption, and monitoring: all the activities needed to build, sustain and govern big data pipelines.
Mandatory Skills:
SCALA
SPARK
Java, SQL
AWS
Glue
S3
Responsibilities
Own development of large-scale data platform including operational data store, real time metrics store and attribution platform, data warehouses and data marts for advertising planning, operation, reporting and optimization
Wider team collaboration and system documentation
Maintain next-gen cloud based big data infrastructure for batch and streaming data applications, and continuously improve performance, scalability and availability
Advocate the best engineering practices, including the use of design patterns, CI/CD, code review and automated integration testing.
Required Education, Experience, Skills and Training
Bachelor or above in computer science or EE
5+ years of professional programming in Scala, Java and SQL
5+ years of experience developing in Amazon Cloud technologies including S3, Glue, EC2, and Kinesis
5+ years of big data design experience with technical stacks like Spark, Flink, Druid, Clickhouse, Single Store, Snowflake, Kafka, Nifi and AWS big data technologies
Proven track record with cloud infrastructure technologies, at least two of Terraform, K8S, Spinnaker, IAM, ALB, and etc.
Experience building highly available and scalable services for public consumption
Experience with processing large amount of data at petabyte level
Strong knowledge of system design, application design and architecture
Proficiency in both written and oral English
Job Type: Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Application Question(s):
What is your Work Authorization/ Visa status?
Experience:
Data Engineering: 5 years (Preferred)
Programming in Scala, Java and SQL: 5 years (Preferred)
Big data design: 5 years (Preferred)
Cloud infrastructure: 5 years (Preferred)
Work Location: Remote
Speak with the employer
+91 9549530709",76500,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'java', 'scala']","['snowflake', 'aws']",[],['snowflake'],"['kafka', 'flink', 'spark']",['terraform'],bachelor,5-10 years
Delta,4.3,"Atlanta, GA","Data Engineer, Operations Analysis, and Performance","United States, Georgia, Atlanta
Operations Anlys & Performance
12-May-2023
Ref #: 19868
How you'll help us Keep Climbing (overview & key responsibilities)
Deltas brand is based on best-in-class operational performance, the foundation of which is providing safe and reliable operations for our customers travel experience. The role of Operations Analytics (OA) is to support this mission by providing strategic insights by first understanding business processes and then leveraging data and analytics to drive continuous improvement efforts.

Data has transformed the way Delta operates.The role of a data engineer is to further harness the power of data by making it accessible, available, and curated for reporting and analysis. Data engineering teams within Deltas Operations Analysis and Performance (OAP) division are typically responsible for: curating Single Source of Truth data tables to be used by analytics teams, producing and distributing automated reporting in a modular and scalable manner, leading efforts to transition to modern data and reporting tools, and acting as a center of expertise for efficient data processing and management.

Responsibilities:
Locate and extract data from a variety of sources for use in analysis, models and project development.
Clean and curate datasetsby researching new data sources and collaborating with other business units to determine the best source for the data, aggregating views into meaningful hierarchies
Be familiar with data environments and collaborate to improve automated reports and analyses in support of divisional leaders and business units
Leverage emerging technologies and proactively identify efficient and meaningful ways to communicate data and analysis in order to satisfy divisional needs.
Support process improvement and project management engagements for both individual business units and cross-divisional initiatives
Train and mentor other team members in various skillsets and subjects
Practice safety-conscious behaviors in all operational processes and procedures
Have a team first attitude with the success of our team and business partners as the top priority
Be intellectually curious, ask questions, and speak up when they have an idea
Enjoy working in a high-profile environment with fluid priorities, ambiguity and aggressive deadlines.

Benefits and Perks to Help You Keep Climbing
A career at Delta not only gives you a chance to see the world, but we also provide excellent benefits to help you keep climbing along the way!
Competitive salary, industry leading profit sharing and 401(k) with generous direct contribution and company match
Comprehensive health benefits including medical, dental, vision, short/long term disability and life benefits
A detailed wellness plan that recognizes the importance physical, emotional, financial, and social wellbeing
Domestic and International flight privileges


What you need to succeed (minimum qualifications)
3+ years of related experience
Proficiency in SQL and ETL/ELT patterns
Proficiency with Python, SAS or other similar tools and programming languages
Ability to troubleshoot a reporting database environment
Strong attention to detail and ability to work autonomously and manage multiple requests with varying timelines
Self-starter with a resilient, solution-minded approach to complex problems working individually or in a group
Demonstrates that privacy is a priority when handling personal data.
Embraces a diverse set of people, thinking and styles.
Consistently makes safety and security, of self and others, the priority.
What will give you a competitive edge (preferred qualifications)
Bachelor's degree or certificate in Computer Science, Engineering, Information Science, or other relevant quantitative field
Previous airline experience
Working knowledge of and/or experience with cloud-based solutioning (i.e. Azure, AWS, GPC, etc.)

< Go back",102659,10000+ Employees,Company - Public,Transportation & Logistics,"Airlines, Airports & Air Transportation",1928,$10+ billion (USD),GA,95,data engineer,na,"['sql', 'go', 'python']","['azure', 'aws']",[],[],[],[],bachelor,+10 years
"Yes Energy, LLC",4.6,"Boulder, CO","Data Engineer (PL/SQL) Hybrid - Boulder, CO / Chicago, IL / Dedham, MA / Houston, TX","Data Engineer (PL/SQL)

Join the Market Leader in Electric Power Trading Solutions
The electrical grid is the largest and most complicated machine ever built. Yes Energy’s industry leading electric power trading analytics software provides real time visibility into the massive amount of data that is generated by the North American electrical grid every day. Our unique and innovative view of the data informs real time trading decisions that keep utility prices low and the grid up and running. It’s both challenging work and work with a purpose.
Be a part of our successful, growing business.
We are currently working in a hybrid environment and are seeking to fill one full time Data Engineer (PL/SQL) position immediately in Boulder - CO, Chicago - IL, Dedham - MA or Houston - TX.
About the team
At Yes Energy, our Market Data Operations (MDO) team plays a crucial role in our business. We are the control room for our customers, responsible for ensuring access to reliable and timely data every minute of every day.
We take pride in our responsibility to maintain the accuracy and reliability of our data. We understand that our clients rely on us to provide them with the information they need to make informed decisions, and we take that responsibility very seriously. Like a control room operator who is constantly monitoring the grid and making adjustments to ensure stability, our team is constantly monitoring our data pipelines and making adjustments to ensure data accuracy and reliability.
Our team is passionate about what we do, and we are dedicated to helping clients navigate the complex and dynamic North American Energy Markets. We work together in a collaborative environment, and we look to continuously improve our processes to ensure that we are providing the highest quality data possible to our clients.
About you
You have a passion for working with inherently messy data
You believe that a deep understanding of the data leads to better solutions
You have a competitive attitude, taking ownership and accountability of the work you produce
You have strong problem-solving skills and a curious mindset
You have experience maintaining and designing data pipelines
Like to design, develop, analyze and troubleshoot PL/SQL code

What you will do
Maintain our real-time data pipelines and ensure so that we can provide reliable and accurate information to our clients
Support clients by answering complex data questions, providing timely and effective solutions, so that we can empower our clients to make informed decisions
Ensure highest possible quality and integrity of Yes Energy data; recommend and implement ways to improve data reliability, efficiency, and quality
Participate in weekly on-call rotations to help resolve critical data pipeline failures for our clients

Requirements
4+ years of SQL or equivalent experience
4+ years of Oracle PL/SQL or equivalent experience
Bonus points for
Experience with ETL and complex data pipelines
Energy industry experience or experience in equities/commodities trading
Experience with web scraping, including HTML parsing, HTTP protocols and network logs
Experience with Python and Bash Scripting
Familiarity with Agile development methodologies
Position Details
Full time
Reports to Data Operation team Lead
Minimal travel may be required (up to 10 days per year)
Keywords:
Oracle, SQL, PL/SQL, REST API, Time Series Data, ETL, CLI tools.
About Yes Energy
Overview
Yes Energy delivers real-time market data and electric power trading decision solutions. Over 1,000 market participants use Yes Energy solutions daily. The business is a leader in all aspects of information content collection and management, as well as in developing and delivering data and market analytics solutions. Since its inception in 2008 Yes Energy has become a trusted and respected supplier of innovative and reliable solutions focused on the needs of power market analysts, traders and trade managers. Yes Energy has a team of amazing professionals located in Boulder, CO (HQ), Dedham, MA and Chicago, IL.
Culture
At Yes Energy we care about saying “Yes” to customers. We like to listen and learn, and develop our solutions in line with our customers’ needs. We think about customers as business partners and when we help them to be more successful … We are more successful too.
Around the office our culture is driven by some pretty fundamental values that we’re proud of:
We love innovation and solving tough challenges.
We are “high standards people” who combine passion and pride with hard work and rewards of all kinds- in an ethic that is consistent across the company.
We’re team-focused with a flat hierarchy- we work in small teams on well-defined projects that directly impact the success of the business.
We play to the strengths and experience of each person, while each of us also works along a continuum of roles adjacent to our focus area. This presents a challenge of maintaining a broad set of skills as well as an opportunity to learn and contribute in many ways.
We are constantly growing. Professional development happens every day and every year.
Compensation and Benefits
Salary Range: $90,000 - $120,000, plus bonus.
We offer strongly competitive salaries and real bonuses that are achievable and that you can impact. Our benefits package is also very competitive, and it includes medical insurance, 401K Plan with matching, flexible vacation and flexible work schedules. Investment in both formal and informal professional development is encouraged and funded by Yes Energy.
At Yes Energy we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
Yes Energy provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Yes Energy complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.",105000,51 to 200 Employees,Company - Private,Information Technology,Software Development,2008,Unknown / Non-Applicable,CO,15,data engineer,na,"['sql', 'python']",[],[],['oracle'],[],['bash'],,+10 years
Chewy,3.5,"Richardson, TX",Data Engineer II,"Our Opportunity:
Chewy’s Data Analytics team has an exciting opportunity for a Data Engineer III to join the pack. Leveraging your strong expertise and background in data engineering and data analysis, you will be a part of a team responsible for operational and tactical reporting generating insights to grow Customer Service operations and planning. This includes building high quality data pipelines that drives analytic solutions and creating data products for analytics and data scientist team members to improve their productivity. Our organization is a fast-paced environment with new challenges and new opportunities each day. You will be responsible for building and implementing data products and technologies which will handle the growing business needs and play a key role in redefining what it means to be a world-class customer service organization.
What You'll Do:
Design, develop, optimize, and maintain data architecture and pipelines using design and programming patterns that follow best-in-class practices and principles.
Manage, maintain, and improve our SSOT tables and data marts, which drive critical business decisions every day.
Work closely with analytics teams and business partners, serving as a trusted partner who can advise, consult, and communicate data solutions.
Mentor and coach other data practitioners on data standards and practices.
Lead the evaluation, implementation and deployment of emerging tools and process for data engineering to improve overall productivity for the organization.
Partner with leaders, vendors, and other data practitioners across Chewy to develop technical architectures for strategic enterprise projects and initiatives.
Document technical details of work and follow agile sprint methodology, using tools like Jira, Confluence etc.

What You'll Need:
Bachelor of Science or Master’s degree in Computer Science, Engineering, Information Systems, Mathematics or related field
3+ years of enterprise experience as a data engineer and/or software engineer
3+ years applying and implementing database and data modeling techniques
3+ years working with enterprise data warehouse (ex. Snowflake, Vertica) and cloud environments (ex. AWS)
3+ years of experience building data integrations and pipelines from data lake, APIs, relational databases, and third-party systems
Strong software development skills in SQL
Self-motivated with strong problem-solving and self-learning skills.
Bonus:
Strong working knowledge of Python programming
Excellent communication and collaboration skills with ability to influence and guide stakeholders
Experience building dimensional models in data warehouses
Experience with data streaming tools and technologies like Kafka, Kinesis, or similar technologies
AWS Developer Certifications
E-commerce, Retail or startup experience
Experience in BI tools such as Tableau, Plotly, Power BI, etc.
Chewy is committed to equal opportunity. We value and embrace diversity and inclusion of all Team Members. If you have a disability under the Americans with Disabilities Act or similar law, and you need an accommodation during the application process or to perform these job requirements, or if you need a religious accommodation, please contact CAAR@chewy.com.

If you have a question regarding your application, please contact HR@chewy.com.

To access Chewy's Customer Privacy Policy, please click here. To access Chewy's California CPRA Job Applicant Privacy Policy, please click here.",112889,10000+ Employees,Company - Public,Retail & Wholesale,Pet & Pet Supplies Stores,2011,$5 to $25 million (USD),TX,12,data engineer,na,"['sql', 'python']","['snowflake', 'aws']","['tableau', 'power bi']",['snowflake'],['kafka'],[],bachelor,+10 years
Charles Schwab,4.1,"Lone Tree, CO",Senior Big Data Engineer,"Your Opportunity

At Schwab, the Data and Rep Technology (DaRT) organization governs the strategy and implementation of the enterprise data warehouse, Data Lake, and emerging data platforms. Our mission is to drive activation of data solutions, rep engagement technology (Sales, Marketing and Service) and client intelligence to achieve targeted business outcomes, address data risk and safeguard competitive edge. We help Marketing, Finance, Risk and executive leadership make fact-based decisions by integrating and analyzing data.

As part of the Business Data Delivery team, you will partner with our Business stakeholders and Data Engineering team to design and develop data solutions for data science, analytics and reporting. We are a team of passionate data engineers and SMEs who bring a lot of energy, focus and fresh ideas that support our mission to contribute by seeing the world “Through Clients' Eyes”. ETL Developers work with large teams, including onshore and offshore developers, using best-in-class technologies including BigQuery, Teradata, Informatica, and Hadoop. You will design, development and implement enterprise data integration solutions with opportunities to grow in responsibility, work on exciting and meaningful projects, train on new technologies and lead other Developers to set the future of the Data Warehouse.
What you are good at

Designing, Developing and implementing new data ingestion workflows by practical application of existing and new data engineering techniques
Leading large complex projects for successful delivery
Developing data ingestion workflows across wide variety of data sources and data ingestion patterns such as batch, near real-time and real time
Working with business analysts to understand business/new data requirements and use cases
Crafting and updating ETL specifications and supporting documentation
Developing solution design by working with technical directors, Data Modelers and cross-functional teams to ensure an accurate and efficient implementation of requirements and following standards Defining and executing quality assurance and test scripts
Guiding the ETL delivery team with technical expertise
Reviewing ETL delivery from 3rd party vendor teams
Advocating for agile practices to increase delivery efficiency
Ensuring consistency with published development, coding and testing standards
Applying data integration best practices for data quality and automation
Working with product vendors to identify and manage open product issues.
What you have

Demonstrated ability as an ETL lead with a track record of delivering projects with minimal defects
7+ years of hands-on experience with data integration tools such as Informatica Power Center and Talend
7+ years in Data Warehouse platforms such as Teradata and BigData/Hadoop
At least 5 years of experience in data modeling (logical and/or physical)
At least 5 years of hands-on experience working with near realtime and/or real-time data ingestion techniques
Expertise in schema design and demonstrable ability to work with complex data is required
Strong SQL experience with the ability to develop, tune and debug complex SQL applications is required
Experience with Google Cloud Platform, BigQuery and Informatica Intelligent Cloud Services (IICS) helpful. Experience with scheduling tools (eg. Control M, ESP)
Demonstrable experience in working in large environments such as RDBMS, EDW, NoSQL, BigData etc. is preferred
Prior experience collaborating with various partners, including vendors, offshore development teams and internal groups
Ability to quickly learn & become proficient with new technologies
Strong analytical, problem-solving, influencing, prioritization, decision making and conflict resolution skills
Outstanding interpersonal skills, including collaboration, communication, and negotiation
Ability to help drive processes, run projects and solve highly complex problems using innovative solutions
Ability to work independently with little instruction on day-to-day work and lead multiple projects requiring cross-team and external collaboration
Ability to coach and mentor individuals on technical matters by sharing knowledge Ability to train and handle delivery with a team of developers.",140000,10000+ Employees,Company - Public,Financial Services,Investment & Asset Management,1973,$10+ billion (USD),CO,50,data engineer,senior,"['sql', 'nosql']",['google cloud'],[],[],['hadoop'],[],,5-10 years
Mars,4.3,"Newark, NJ",Data Engineer,"Job Purpose/Overview
One Demand Data & Analytics (ODDA) is a Mars Wrigley program that harnesses the power of data and insights to solve some of the critical business-wide problems we face – unlocking quality growth and operational excellence.
Through ODDA, we deliver connected insights across the entire demand ecosystem. We empower our Associates with the right data, tools and capabilities so they can take decisive action, maximizing value and making a meaningful impact on our consumers, our customers and our business.
The Portfolio & Innovation Analytics vertical within ODDA seeks to equip Mars Associates with the capabilities needed to address portfolio health and innovation from a holistic and analytics-driven viewpoint.

Key Responsibilities
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals
Solve complex data problems to deliver insights that helps business to achieve goals
Create data products for engineer, analyst, and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data and analytic professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Define and execute the Analytics Data Engineering roadmap (and work with enterprise BI to enhance the data lake and a real-time reporting environment for operations)
Lead complex process improvement and project management engagements for both individual business units and cross-divisional initiatives
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering in order to improve productivity as a team
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with machine learning engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Mentor junior members in technical proficiency and business acumen
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Interface with business unit leaders to develop and maintain internal customer relationships
Job Specifications/Qualifications
Master’s degree in computer science, application programming, software development, information systems, database administration, mathematics, engineering, or other related field
6+ years in a rapid development environment, preferably within an analytics environment
Demonstrated ability to be work with internal (Operations) and external (IT) stakeholders
Must be able to interact and collaborate at all levels within Operations Analysis & Performance, OCC, cross-divisional working groups, and outside entities
Experience with increasing code quality and implementing best practices across teams
Advanced technical skills in the following areas:
Proficiency in SQL (CTE, window functions, temporal data), SAP HANA experience is a large plus
Proficiency in a scripting language (Python preferred)
Proficiency of API Consumption
Proficiency in ETL tooling (such as Informatica)
Proven expertise in SAP ECC and SAP APO is a big plus
Excellent communication skills and ability to present concepts to non-technical audience
Must be able to interact and collaborate at all levels within Operations Analysis & Performance, OCC, cross-divisional working groups, and outside entities
Strong project management, organizational, and prioritizations skills
2 to 4 years' experience in applied data science role or equivalent; ideally in a CPG, Retail
Knowledge and experience in modelling techniques and advanced applied skills (e.g. significance testing, GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.) using tools like Spark, Scala, SAS, R, Python, Bayesia, H2O, Storm, Yarn, and Kafka
Experience querying databases (SQL, Hive)
Experience working with big data platforms such as Hadoop ecosystem (Azure), including in-memory solutions (SAP HANA and Apache Spark)
Working knowledge of data visualization tools such as Tableau, Power BI, D3, ggplot, to deliver output to the broader business community to improve decision making and productivity
Strong communication and presentation skills
What can you expect from Mars?
Work with over 130,000 like-minded and talented Associates, all guided by The Five Principles.
Join a purpose driven company, where we’re striving to build the world we want tomorrow, today.
Best-in-class learning and development support from day one, including access to our in-house Mars University.
An industry competitive salary and benefits package, including company bonus.
#LI-Hybrid",105850,10000+ Employees,Company - Private,Manufacturing,Food & Beverage Manufacturing,1911,$10+ billion (USD),NJ,112,data engineer,na,"['r', 'sql', 'scala', 'python']",['azure'],"['sap', 'tableau', 'power bi']",['hive'],"['kafka', 'hadoop', 'spark']",[],master,2-5 years
Anywhere Real Estate,3.5,"Madison, NJ",Data Engineer,"Anywhere Real Estate Inc is seeking a remote data engineer to join the Database Service Team, DNA (Data and Analytics Division)! Reporting to Database Team Manager, the Data Engineer will be involved with all phases of IT development projects, database and data mart administration and 24x7 production support.
We expect the joining Data Engineer should have 3-5 years of significant experiences with excellent SQL server and AWS skills.
Job Responsibilities
Development, solving, and performance tuning of complicated SQL server stored procedures and SQL Server Integration Services (SSIS) packages.
Development of ETL pipelines in and out of data warehouse using combination of AWS tools/Python (or Scala) and Snowflakes stored procedures and views.
Conducting data investigations and assisting business partners with sophisticated data analysis and ad-hoc queries.
Must be available 24x7 for Production Support with a rotating on-call schedule.
Required Skills
Bachelor’s degree in Computer Science, Engineering, Information Systems, or related field or equivalent experience
Confirmed experience with SQL Server in developing, implementing, and supporting SQL server databases for web-based applications.
Must have strong Transact-SQL skill and be able to work on complicated stored procedures.
3 years working experience with AWS Cloud Services including Apache Airflow and AWS Glue, Athena, EMR, EC2, S3, Lambda, etc.
Strong Technical hands-on experience in programming languages – T-SQL, Python, Lambda, JavaScript.
In addition to Microsoft SQL Server database, Previous experience in MongoDB, DynamoDB and Snowflake databases will be a huge plus.
Good understanding of SDLC.
Good to know Agile methodology including using Jira boards and Confluence pages.
Understanding of ETL design and development.
Excellent problem solving and root cause analysis skills.
Excellent written and verbal communication skills.

#LI-JC1
#LI-Remote


Exciting News: We are excited to announce that Realogy is now Anywhere Real Estate Inc. It will take a few months for us to transition to our new brand. For more information about this change, please click here .

EEO Statement: EOE AA M/F/Vet/Disability

Compensation Range:
$85,500 - $182,200 ; At Anywhere, actual compensation within that range will be dependent upon the individual’s skills, experience, and qualifications.",133850,Unknown,Company - Public,Real Estate,Real Estate,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'scala', 'python']","['snowflake', 'aws']",['ssis'],"['sql server', 'dynamodb', 'mongodb', 'snowflake']",[],[],bachelor,2-5 years
"Lithia Motors, Inc.",3.3,Oregon,Senior Data Engineer,"Dealership:
L0105 Lithia Home Office
Senior Data Engineer
The Senior Data Engineer is responsible for developing and supporting the cutting-edge data solutions by using the Azure stake (Data Lake, Data Warehouse, Data Factory, Functions) SQL script design/dev, and stored procedures.
The Senior Data Engineer reports to a Lead Data Engineer. This role will be Remote.
Responsibilities
Design and implement data load processes from disparate data sources into Azure Data Lake and subsequent Azure SQL & SQL Data Warehouse
Migrate existing processes and data from our On Premises SQL Server and other environments to Azure Data Lake
Explore and learn the latest Azure technologies to provide new capabilities and increase efficiency
Ensure all existing data is created in the right way, and that new data is created according to appropriate standards and with proper documentation
Read, write, and configure code for end-to-end service telemetry, alerting and self-healing capabilities
Strive for continuous improvement of code quality and development practices
Work closely with the Lead Data Engineer and other Data Engineers to develop and document solutions for providing data to the enterprise
Mentor and teach more junior developers
Skills and Qualifications
3+ years of experience in working as an analytics or data engineering member working with cross functional teams
3+ years of SQL Server development or equivalent
Azure SQL DB, SQL Data Warehouse, Azure Data Factory a plus
Version control using Git or TFS
Bachelor’s Degree in computer sciences, Analytics, Systems Eng., Statistics or related field
Strong attention to detail and sense of urgency
Competencies
Does the right thing, takes action and adapts to change
Self-motivates, believes in accountability, focuses on results, makes plans and follows through
Believes in humility, shares best practices, desires to keep learning, measures performance and adapts to improve results
Thrives on a team, stays positive, lives our values
Physical Demands
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job.*
Up to 1/3 of time: standing, walking, lifting up to 25 pounds
Up to 2/3 of time: sitting, kneeling, reaching, talking, hearing
*Reasonable accommodations may be made to enable individuals to perform the essential functions.
NOTE: This is not necessarily an exhaustive list of responsibilities, skills, or working conditions associated with the job. While this list is intended to be an accurate reflection of the current job, the company reserves the right to revise the functions and duties of the job or to require that additional or different tasks be performed.
We offer best in class industry benefits:
Competitive pay
Medical, Dental and Vision Plans
Paid Holidays & PTO
Short and Long-Term Disability
Paid Life Insurance
401(k) Retirement Plan
Employee Stock Purchase Plan
Lithia Learning Center
Vehicle Purchase Discounts
Wellness Programs
High School graduate or equivalent, 18 years or older required. Acceptable driving record and a valid driver's license in your state of residence necessary for select roles. We are a drug free workplace. We are committed to equal employment opportunity (regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status). We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.",112889,10000+ Employees,Company - Public,Retail & Wholesale,Vehicle Dealers,1946,$10+ billion (USD),Oregon,77,data engineer,senior,['sql'],['azure'],[],['sql server'],[],[],bachelor,5-10 years
Chicago Transit Authority,3.3,"Chicago, IL",Senior Data Engineer,"Under general supervision, takes a broader-scale focus to building, developing and maintaining data architectures, including back-end infrastructures, complex data integrations and associated processes. Participates in the design, evaluation, selection, implementation and support of new data engineering techniques related to processing data in both structured and unstructured formats. Makes use of a range of industry standard programming languages and data frameworks. Recommends ways to improve the reliability, quality and usefulness of agency data as it relates to the needs of enterprise Data Scientists, Analysts, related stakeholders and departmental direction.


Qualifications
PRIMARY RESPONSIBILITIES

% time

1

45%

Develops, constructs, tests and maintains data architecture, including databases, data processing systems and related applications. Develops custom applications and infrastructure as necessary. Researches and deploys data infrastructure best practices. Builds and optimizes data pipelines to support cross-functional analytics, automation and programming needs.

2

25%

Develops and maintains best practices for marrying disparate CTA data systems. Liaises with Data Scientists, Programmers, Developers and Analysts to support integration of new data processing and warehousing technologies.

3

15%

Maintains existing data analytics infrastructure and architectures. Leads new efforts to acquire and centralize data from disparate systems. Works as directed to assist department Director in data related project oversight and execution.

4

5%

Researches best practices, recommends changes to improve efficiencies, assists in revising processes and procedures and establishes enterprise data architectures. Assesses existing business needs and anticipates future business needs in order to support the growth of department and agency infrastructure and applications.

5

5%

Hires, trains, develops, monitors and evaluates staff. Reviews and recommends personnel actions for approval.

6

5%

Performs other duties as assigned.

MANAGEMENT RESPONSIBILITIES
Reporting to this position are the following jobs:
Job Title

Senior Analyst, Data Analytics
Data Scientist
Programmer Analyst, Data Analytics
Business Intelligence Developer

CHALLENGES
Working with and managing massive amounts of data sourced from several disparate data systems and applications.
Implementing effective time and project management processes to deliver data services on time and on budget.
Balancing multiple high-priority requests simultaneously while adapting to rapidly-changing demands and meeting project deadlines.
Providing consistency in data processing efforts to help shape the utilization and actionability of information.

EDUCATION/EXPERIENCE REQUIREMENTS
Bachelor’s degree in Statistics, Computer Science, Mathematics, Operations Research, Industrial Engineering, Quantitative Analysis, Economics or a related field, plus four (4) years of experience in data engineering, development, and/or a combination of education and experience.

PHYSICAL REQUIREMENTS
Requires sitting for extended periods of time, standing, visual acumen, manual dexterity and fingering for working with computer keyboards.
Chicago Transit Authority requires all employees to be COVID-19 vaccinated. If you are offered employment, you must provide proof of full COVID-19 vaccination or proof that you are in the process of becoming fully vaccinated as part of the hiring process and as a condition of employment. Visit www.transitchicago.com/careers
Service Area Requirement: Exempt (Non-Union) employees must live within the boundaries of the CTA Statutory Service Area either at the time of employment or within 6 months of beginning employment at CTA.


KNOWLEDGE, SKILLS, AND ABILITIES
Detailed knowledge of various data programming languages and frameworks (i.e. Python, SQL, Java, JavaScript, PHP, C#/.NET, AngularJS, etc.).
Detailed experience in custom and/or industry-standard data migration, integration, and ETL tools and techniques.
Detailed knowledge of big data frameworks and tools (Hadoop, Spark, MongoDB, Cassandra, etc.)
Detailed knowledge of programming development requirements and use of source/revision control systems (i.e. Git, Mercurial etc.)
Strong knowledge of cloud-based data infrastructure (AWS, Azure, Google, Vertica)
Strong knowledge of a range of RDBMS platforms (Postgres, Oracle, SQL Server, MySQL, DB2, etc.)
Strong knowledge of microservice architectures.
Strong knowledge of continuous delivery and deployment pipelines.
Strong analytical, problem-solving, and decision-making skills.
Strong report preparation and presenting skills.
Strong oral and written communication skills.
Strong interpersonal and team skills across a variety of fields and management levels.
Strong project management skills.
Strong organization and time management skills.
Ability to effectively analyze and translate data engineering challenges to the business.
Ability to manage large amounts of data and attention to detail.
Ability to multitask competing projects and deadlines for completion.


WORKING CONDITIONS
General office environment.
Required to occasionally travel to locations throughout the CTA system and Chicago area as needed.

EQUIPMENT, TOOLS, AND MATERIALS UTILIZED
Standard office equipment.
Modern data engineering tools, platforms and processes.


Additional Details
Please note, employees and/or union members will be given priority consideration in the hiring process, per the applicable labor contracts.

Final salary will be determined in part by the qualifications of the selected candidate and may be higher or lower than target.

Applicants, if hired,must comply with CTA's residency ordinance.

CTA IS AN EQUAL OPPORTUNITY EMPLOYER

No employee or applicant for employment will be discriminated against because of race, color, creed, religion, sex, marital status, national origin, sexual orientation, ancestry, age, unfavorable military discharge, disability or any other status protected by federal, state, or local laws; except where a bona fide occupational qualification exists We are committed to providing an inclusive environment for our workforce and supporting the communities we serve. CTA will make reasonable accommodations for the known disabilities of otherwise qualified applicants for employment as well as its employees, unless undue hardship would result. If you require an accommodation in the application or hiring process, please contact arc@transitchicago.com prior to the submission of your application or upon notification of your actual test date. CTA will work with you to determine if an accommodation can be provided.

Primary Location: USA-Illinois-Chicago
Job: Data Analytics
Job Posting: May 12, 2023, 12:19:19 PM
Position Type: Full-time Permanent (FTP)",121471,10000+ Employees,Government,Transportation & Logistics,Taxi & Car Services,1947,$500 million to $1 billion (USD),IL,76,data engineer,senior,"['sql', 'java', 'python']","['azure', 'aws']",[],"['sql server', 'mongodb', 'oracle', 'mysql']","['spark', 'hadoop']",[],bachelor,
Analytica,3.4,Remote,Senior Data Engineer (Remote),"Analytica is seeking a Senior Data Engineer with Microsoft SQL experience to support a long term data science project for a federal government client. The ideal candidate will be comfortable being a lead data engineer working with data scientists to understand data requirements, develop SQL queries and stored procedures for feeding the database and data models.

Analytica has been recognized by Inc. Magazine as the fastest-growing private US small business. We work with U.S. government customers in health, civilian, and national security missions. As a core member you’ll work with a diverse team of professionals to solution matters, architect nuisances, and come up with alternatives. We offer competitive compensation with opportunities for bonuses, employer paid health care, training and development funds, and 401k match.

Responsibilities include (But Are Not Necessarily Limited To):
Conduct database development on Microsoft SQL Server
Develop business solution logic using SQL and T-SQL or similar query/scripting languages to execute on the reporting and data validation needs related to the data platform.
Create and maintain SQL scripts, stored procedures, and other program logic to create, update, and delete data.
Design and implement schema changes, manage indexes, and alter data objects to optimize performance.
Design efficient data models from a logical design based on business requirements and available use patterns.
Identify entity relationships, referential integrity constraints, and primary key structures. Perform analysis of tradeoffs between alternative schemas.
Extend data templates to include relevant fields, publish data standards, document and publish data taxonomy and hierarchies.
Support data science team to compile, analyze and extract data for use in advanced machine learning and modeling.
Create and maintain documentation that includes file specifications, schema, core record layouts, programs, business requirements, test plans, and other artifacts used in the administration, creation, and execution of database operations.
Use GitHub for code deployment and version control in a collaborative development environment.



Minimum Qualifications:
Bachelor’s Degree in Computer Science, IT, Computer Engineering, or related field
5+ years Structured Query Language (SQL) programming
5+ years’ experience in Microsoft SQL database programming / development / administration
Experience working in an Agile environment or Scrum teams
Expertise in designing and building enterprise-grade applications
Strong communication, leadership, and problem-solving skills
Experience developing data streams in AWS a plus
Familiarity with tax or financial related data a plus

About ANALYTICA: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD, the company is an established 8(a) small business that has been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider.

As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation.",112889,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2009,$5 to $25 million (USD),Remote,14,data engineer,senior,['sql'],['aws'],[],['sql server'],[],[],bachelor,+10 years
"JPMorgan Chase Bank, N.A.",3.8,"Wilmington, DE",Lead Data Engineer,"As a Lead Data Engineer within Consumer and Community Banking, in Home Lending, you are an integral part of an agile team that works to enhance, build, and deliver data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. As a core technical contributor, you are responsible for maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job Responsibilities
Design and implement end-to-end data pipelines supporting analytical and operational needs accounting for data management practices focused on data quality, metadata management etc.
Architect, design, and implement cloud native solutions on AWS.
Define and implement event driven architecture patterns leveraging messaging / streaming solutions like Kafka, Kinesis, Flink, and Spark
Ability to decompose large initiatives / designs into manageable smaller bodies of work to demonstrate continuous progress
Collaborate with business stakeholders, product owners, architects, data domain owners to understand current landscape and develop solutions in alignment with business & technology strategy. Assist in refining /evolving data strategy highlighting clear outcomes.
Deep understanding or desire to continue to learn new database technologies, cloud computing & storage services
Understanding of the pros / cons associated with various technology choices and ability to pick the right technology based on the use case

Required qualifications, capabilities, and skills
Formal training, or certification on data engineering concepts, and 5+ years of experience. In addition, demonstrated coaching and mentoring experience
Programming experience in Java, Python, Scala etc.
Experience in using distributed frameworks like Spark, Hadoop etc.
Experience with AWS services like Lambda, EC2, EMR, Redshift, Glue, S3, IAM, RDS, Aurora, DynamoDB etc.
Knowledge of cloud networking, security, storage, and compute services
Infrastructure provisioning experience using Cloud Formation, Terraform etc.
Experience implementing solutions leveraging CI / CD etc.

Preferred qualifications, capabilities, and skills
AWS Solutions Architect / Developer or any advanced level certification preferred
Experience and proficiency across the data lifecycle
Experience with database back-up, recovery, and archiving strategy
Proficient knowledge of linear algebra, statistics, and geometrical algorithms
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans",116853,10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD),DE,224,data engineer,senior,"['java', 'scala', 'python']","['aws', 'redshift']",[],['dynamodb'],"['kafka', 'hadoop', 'flink', 'spark']",['terraform'],,0-2 years
ABBVIE,3.9,"Crystal Lake, IL",Senior Data Engineer,"AbbVie Information Research is seeking a Senior Data Engineer who would contribute to the architecture, design, and development of the Data & Analytics Platform supporting world-class research and development at AbbVie. As a Senior Data Engineer, you will be a core member of a high-performance team of data engineers and architects focusing on driving technology innovation and continuous improvement. This role collaborates with solution architects, product owners, program managers, business analysts, infrastructure teams, and service providers to deliver the solutions.
Responsibilities:
Demonstrate mastery across a wide variety of data engineering activities, including data warehousing, master data management, data cataloging, system integration, data streaming, data visualization, data analysis, and data ops.
Demonstrate knowledge of pharmaceutical R&D/Life Science centric datasets and utilize this knowledge to advance agile, impactful, and cost-effective solutions rapidly.
Collaborate & contribute to the architecture, design, development, and maintenance of large-scale data & analytics platforms, system integrations, data pipelines, data models & API integrations to support evolving business strategy.
Contribute and maintain the team's methodology to conform and curate data, benchmarking against industry standards. Ensure that data are optimally standardized and analysis-ready.
Prototype emerging business use cases to validate technology approaches and propose potential solutions.
Research and recommend opportunities to adopt new technologies for continuous improvement.
Ensure compliance with applicable AbbVie software development lifecycle policies and procedures.

Bachelor's degree with 7 years of IT experience
Must have experience with software development life cycle; Experience with DevOps is preferred.
Must have experience with data analysis programming languages (e.g., SQL, Python & Apache Spark, SAS & R)
Must have experience with database technologies (e.g., Oracle, Postgres, Hive, and HBase)
Must have experience with ETL/Orchestration tools (e.g., Informatica, Autosys, and Airflow, etc.)
Experience with AWS and Cloudera Public Cloud architecture is preferred.
Experience working with Pharmaceutical R&D industry-centric datasets is preferred.


AbbVie is an equal opportunity employer including disability/vets. It is AbbVie’s policy to employ qualified persons of the greatest ability without discrimination against any employee or applicant for employment because of race, color, religion, national origin, age, sex (including pregnancy), physical or mental disability, medical condition, genetic information, gender identity or expression, sexual orientation, marital status, status as a disabled veteran, recently separated veteran, Armed Forces service medal veteran or active duty wartime or campaign badge veteran or a person’s relationship or association with a protected veteran, including spouses and other family members, or any other protected group status. We will take affirmative action to employ and advance in employment qualified minorities, women, individuals with a disability, disabled veterans, recently separated veterans, Armed Forces service medal veterans or active-duty wartime or campaign badge veterans. The Affirmative Action Plan is available for viewing in the Human Resources office during regular business hours.",119356,10000+ Employees,Company - Public,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2013,$10+ billion (USD),IL,10,data engineer,senior,"['sql', 'r', 'python']",['aws'],[],"['hive', 'oracle']",['spark'],[],master,
GE Healthcare,4.2,Illinois,Sr Data Engineer,"Job Description Summary
Responsible for managing business critical data engineering processes and data architecture solutions in order to enable analytical and reporting solutions. Responsible for analyzing and preparing the data needed for data science based outcomes. Also responsible for managing and maintaining metadata data structures besides providing necessary support for post-deployment related activities. Accountable to deliver results in a timely manner using agile methodologies.
Job Description
Roles and Responsibilities
In this role, you will:
Design & build technical data dictionaries and support business glossaries to analyze the datasets
Perform data profiling and data analysis for source systems, manually maintained data, healthcare industry standard messages
Design & build both logical and physical data models for both Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP) solutions
Develop and maintain data mapping specifications based on the results of data analysis and functional requirements
Build a variety of data loading & data transformation methods using multiple tools and technologies.
Design & build automated Extract, Transform & Load (ETL) jobs based on data mapping specifications
Manage metadata structures needed for building reusable Extract, Transform & Load (ETL) components.
Analyze the impact of changes to downstream systems/products and recommend alternatives to minimize the impact.
Design and build data warehouses and data marts
Minimum Qualifications
Bachelor's Degree in Computer Science or “STEM” Majors (Science, Technology, Engineering and Math) with minimum 6 years of relevant experience
Exposure to industry standard data modeling tools
Exposure to Extract, Transform & Load (ETL) tools like SSIS or Azure Data Factory
Exposure to industry standard BI tools like Power BI and Tableau
Hands-on experience in writing SQL scripts for SQL Server, MySQL, PostgreSQL or HiveQL
Exposure to unstructured datasets and ability to handle Avro, Parqueet, JSON file formats
Conduct exploratory data analysis and generate visual summaries of data. Identify data quality issues proactively.

Desired Qualifications:
Knowledge of for industrial applications in healthcare settings.
A good team player with self-driven execution capabilities.
Ability to communicate ideas clearly with cross teams.
Ability to showcase teamwork skills to achieve common goals, provide resolutions and share ideas.
Demonstrate the presentation and influencing skills
Eligibility Requirements
GE HealthCare may choose to sponsor visas as business needs dictate.
GE HealthCare will only employ those who are legally authorized to work in the United States for this opening.
Work/Life Balance
Our team puts a significant value on work-life balance. Having a healthy balance between your personal and professional life is crucial to your happiness and success here. We don’t focus on how many hours you spend at work or online. Instead, we’re happy to offer a flexible schedule so you can have a more productive and well-balanced life—both in and outside of work.
Mentorship & Career Growth
We maintain diverse engineering, and leadership perspectives and backgrounds across technology and beyond. Our employees are excited to share their experiences and mentor more junior engineers. Team members are highly encouraged to set up mentorship relationships with seasoned engineers, not only in our team, but also across the broader GE Healthcare population.

Inclusive Team Culture
Here at GE HealthCare, we embrace our differences. We are committed to furthering our culture of inclusion. We have many employee-led affinity groups, innovative benefit offerings, and encourage ongoing learning experiences.
While GE HealthCare does not currently require U.S. employees to be vaccinated against COVID-19, some GE HealthCare customers have vaccination mandates that may apply to certain GE HealthCare employees.

#LI-RW1
Additional Information
GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.
GE will only employ those who are legally authorized to work in the United States for this opening. Any offer of employment is conditioned upon the successful completion of a drug screen (as applicable).
Relocation Assistance Provided: No",112889,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1892,$10+ billion (USD),Illinois,131,data engineer,senior,['sql'],['azure'],"['tableau', 'power bi', 'ssis']","['sql server', 'postgresql', 'mysql']",[],[],bachelor,
Capgemini,3.8,"Malvern, PA",data engineer,"Duration: 8+ months

Job Description:

Collaborate with product owners, UX designers, data analysts, scrum masters, and developers to develop a world-class user experience.
Recommend and effectively explain improvements in functionality, and UX elements that enable users to achieve their goals.
Be an expert in building working relationships across divisions.
Research, advocate and implement industry standard coding methodologies to improve workflows through the selection of evolving technologies and tools that your team will use to build expert-level user experiences.

Qualifications
Excellent analytical skills are essential
Successful candidates will have exhibited the ability to begin working independently and have demonstrated ability to apply theory and concept
Must have ability to communicate effectively with team members and others in the work group, as well as with customers
Must be comfortable working in MS Office, and industry standard statistics and data visualization Packages Power BI/Tableau
Experience working with SQL or statistical languages R or python
An accredited 4-year degree, preferably in accounting, finance, statistics, economics, mathematics, or a similar field with quantitative coursework and 1-2 years of experience OR Masters degree in an associated field.

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.",92252,10000+ Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,1967,$10+ billion (USD),PA,56,data engineer,na,"['sql', 'r', 'python']",[],"['tableau', 'power bi']",[],[],[],,0-2 years
"R1 RCM, Inc.",3.4,Remote,Data Engineer III,"The Software Engineer III serves as a Data Engineer focused on building next generation of healthcare products and services. As part of a collaborative, motivated Agile-based team, the Software Engineer III is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
Responsibilities
Create and maintain optimal data pipeline architecture to assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders to optimize the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies.
Work with stakeholders including the Product, Data and Engineering teams to assist with data-related technical issues and support their data infrastructure needs.
Adhere to and ensure engineering best practices using modern SDLC that enables CI/CD and favors automation, auditability, automated testing, infrastructure, and policy as code.
Ability to collaborate effectively with other architects and engineers delivering end to end, high-performing, highly reliable, scalable, and operable solutions.
Required Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical field; or equivalent experience.
3+ years of experience delivering data engineering applications and services.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets .
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience in any one of the big data technologies (Hadoop, MapReduce , Spark, Data Lake etc. )
Experience in any one of the stream-processing systems ( Kafka, Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages : Python, Java, Scala, etc.
Understanding of modern software development including version control, unit testing, and continuous integration and deployment.
Strong communication skills, with proven ability to present complex ideas and to document concisely.
Preferred Qualifications
Working knowledge of Cloud providers and services such as Amazon AWS or Microsoft Azure.
Experience with relational SQL and NoSQL databases, including MongoDB and ElasticSearch.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12.
Knowledge of the healthcare revenue cycle, EMRs, practice management systems.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",112889,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2003,Unknown / Non-Applicable,Remote,20,data engineer,na,"['sql', 'scala', 'java', 'nosql', 'python']","['azure', 'aws']",[],"['mongodb', 'elasticsearch']","['kafka', 'spark', 'hadoop']",[],bachelor,+10 years
Riverside Research,4.2,"Wright Patterson AFB, OH",Data Engineer (TS/SCI clearance),"Riverside Research is an independent National Security Nonprofit dedicated to research and development in the national interest. We provide high-end technical services, research and development, and prototype solutions to some of the country's most challenging technical problems.

Job Number: 1246
Riverside Research is seeking full-time Data Engineer to support Director, Air Force Chief Data Office (SAF/CO) sponsored activities across the Air Force Enterprise to ensure the visibility, accessibility, understanding, sharing, and trustworthiness of data across air, space, and cyberspace domains. Candidates will provide subject matter expertise in and perform on multidisciplinary teams that support data preparation and architecture, development of agile algorithmic solutions, evaluate and/or execute data governance and data maturity models; and conduct data analytics using state of the art mathematical and machine learning/artificial intelligence techniques and other data analytic lines of research/effort. Positions will be at various CONUS Air Force installations and the National Capital Region.
All Riverside Research opportunities require U.S. Citizenship
Job Duties
Provide expertise on all data concepts for the broader advanced analytics group, and inspire the adoption of advanced analytics, data engineering and data science across the organization.
This will include Installing continuous pipelines of large pools of filtered information so that data analyst/scientists can pull relevant data sets for their analyses.
Required Qualifications:
Top Secret clearance with SCI adjudication
Bachelor's degree in the requisite relevant field. A Master's degree in a relevant field may be substituted for 3 years of general experience.
7 years or more of experience in the data engineering field, at least three of which must have been in a data analytics environment preferably in DoD or the intelligence community.
Familiarity with the manipulation of unstructured data in a data analytics environment, and the use of open-source tools, cloud computing, machine learning and data visualization.
Familiar with specialized languages relevant to the technologies employed such as Apache, Hadoop, etc.

Riverside Research strives to be one of America's premier providers of independent, trusted technical and scientific expertise. We continue to add experienced and technically astute staff who are highly motivated to help our DoD and Intelligence Community (IC) customers deliver world class programs. As a not-for-profit, technology-oriented defense company, we believe service to customers and support of our staff is our mission. Our goal is to serve as a destination company by providing an industry-leading, positive, and rewarding employee experience for all who join us. We aspire to be a valued partner to our customers and to earn their trust through our unwavering commitment to achieve timely, innovative, cost-effective and mission-focused solutions.
All positions at Riverside Research are subject to background investigations. Employment is contingent upon successful completion of a background investigation including criminal history and identity check.
Riverside Research does not mandate COVID vaccination as a condition of employment. However, proof of vaccination or negative test may be required to enter certain government facilities and sites. Vaccination requirements will depend on the status of the federal contractor mandate and customer site-specific requirements. To protect the health and safety of its employees, their families, and to comply with customer requirements, the company requires all employees to disclose vaccination status (upon hire).
Our EEO Policy
Riverside Research is an equal opportunity employer. We recruit, employ, train, compensate and promote without regard to race, religion, sex, color, national origin, age, gender identity, sexual orientation, marital status, disability/veteran, status as a protected veteran, or any other basis protected by applicable federal, state and local law.
If you need assistance at any time in our application or interview process, please contact Recruiting at email Recruiting@RiversideResearch.org. A member of the Recruiting team will be available to assist.
This contractor and subcontractor shall abide by the requirements of 41 CFR 60-741.5(a). This regulation prohibits discrimination against qualified individuals on the basis of disability and requires affirmative action by covered prime contractors and subcontractors to employ and advance in employment qualified individuals with disabilities.
This contractor and subcontractor shall abide by the requirements of 41 CFR 60-300.5(a). This regulation prohibits discrimination against qualified protected veterans and requires affirmative action by covered contractors and subcontractors to employ and advance in employment qualified protected veterans.
For more information on ""EEO is the Law,"" please visit:
http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf
https://www.dol.gov/sites/dolgov/files/ofccp/regs/compliance/posters/pdf/eeopost.pdf",64624,501 to 1000 Employees,Nonprofit Organization,Government & Public Administration,National Agencies,1967,$25 to $100 million (USD),OH,56,data engineer,na,[],[],[],[],['hadoop'],[],bachelor,5-10 years
"JPMorgan Chase Bank, N.A.",3.8,"Wilmington, DE","Software Engineer III, Cloud Data Engineer","As a Software Engineer III, Cloud Data Engineer, within Corporate Enterprise Technology, in Finance, Risk, Data, & Controls, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Executes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems
Creates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems
Produces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets both on-prem and on AWS cloud in service of continuous improvement of software applications and systems
Proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture
Contributes to software engineering communities of practice and events that explore new and emerging technologies
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Formal training or certification on software engineering concepts and 3+ years applied experience
3+ years of working on Big Data Platforms and building frameworks for data pipelines
2+ years of building cloud solutions - AWS preferred.
1+ year of working on cloud data lake solution. Experience working with Terraform, Glue DB, Collibra, Athena, Snowflake, Redshift, EMR would be big plus.
Proficient in coding in one or more languages - Java, Scala and Python are mostly used.
Experience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages
Solid understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security

Preferred qualifications, capabilities, and skills
Advanced knowledge of Apache Spark, Scala, Java, Python and Spring
Understanding of integration technologies such as Apache Kafka
Working knowledge of API-Apigee Edge, Swagger
Containers-Docker advanced development, Kubernetes
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans",104623,10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD),DE,224,data engineer,na,"['java', 'scala', 'python']","['snowflake', 'aws', 'redshift']",[],['snowflake'],"['kafka', 'spark']","['terraform', 'docker']",,0-2 years
Peraton,3.6,"Huntsville, AL",Sr Data Engineer - ETL,"Responsibilities:
Technology is constantly changing and our adversaries are digitally exceeding law enforcement’s ability to keep pace. Those charged with protecting the United States are not always able to access the evidence needed to prosecute crime and prevent terrorism. The Government has trusted in Peraton to provide the technical ability, tools, and resources to bring criminals to justice. In response to this challenge, Peraton is seeking a Senior Data Engineer with ETL expertise to provide proven, industry leading capabilities to our customer.

Experience with data exploration techniques and development of quantitative and qualitative data analysis process, design robust ETL pipelines.
Manage team of data engineers and should have experience working with Data Scientists and Data architects. Databricks or equivalent platform using Apache Spark with Scala, Python, Java.
Work in a team environment to design, develop, and support a software system which is undergoing a modernization.
Participate in developing new functionality and migrating the application into the cloud and introducing new technologies into the tech stack.
Participate in Agile Scrum SDLC activities.
Support developing Agile SDLC phase documentation.
Perform unit and integration testing of software/systems prior to release to the users for user acceptance testing.
Qualifications:
Required Qualifications
BS degree and twelve (12) years of experience with programming and software development including analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
Five (5) years of experience architecting software solutions based on customer requirement.
Led a technical team for at least five (5) years.
Three (3) years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
A current Top Secret security clearance with SCI eligibility and the ability to obtain a polygraph.

Desired Qualifications
Knowledge of Data Dictionary
Knowledge of Normalizations
Experience with AWS cloud services including Glue, Kinesis and Container services.
Knowledge of data acquisition and ingestion of structured and unstructured data sources ensuring quality and data integrity
Experience with open source technologies like Docker, ElasticSearch, and NoSQL Databases,
Experience with an Agile environment and have developed User Stories.
Peraton Overview:
Peraton drives missions of consequence spanning the globe and extending to the farthest reaches of the galaxy. As the world’s leading mission capability integrator and transformative enterprise IT provider, we deliver trusted and highly differentiated national security solutions and technologies that keep people safe and secure. Peraton serves as a valued partner to essential government agencies across the intelligence, space, cyber, defense, civilian, health, and state and local markets. Every day, our employees do the can’t be done, solving the most daunting challenges facing our customers.
Target Salary Range: $112,000 - $179,000. This represents the typical salary range for this position based on experience and other factors. EEO Tagline (Text Only): An Equal Opportunity Employer including Disability/Veteran.",145500,10000+ Employees,Company - Private,Information Technology,Information Technology Support Services,2017,$5 to $10 billion (USD),AL,6,data engineer,senior,"['nosql', 'java', 'scala', 'python']","['databricks', 'aws']",[],['elasticsearch'],['spark'],['docker'],,
CVS Health,3.1,Pennsylvania,Lead Data Engineer,"Designs and develops complex and large-scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs

Writes complex ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing

Develop frameworks, standards & reference material for architecture and associated products

Designs data marts and data models to support Data Science and other internal customers. Behaves as mentor to junior team members to provide technical advice

Applies knowledge of Aetna systems and products to consult and advise on additional efforts across multiple domains spanning broader enterprise

Collaborates with data science team to transform data and integrate algorithms and models into highly available, production systems

Uses in-depth knowledge on Hadoop architecture, HDFS commands and experience designing & optimizing queries to build scalable, modular, and efficient data pipelines

Uses advanced programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems

Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards

Experiments with available tools and advice on new tools in order to determine optimal solution given the requirements dictated by the model/use case

Pay Range
The typical pay range for this role is:
Minimum: 115,000
Maximum: 230,000

This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.

In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company's 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (PTO) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.

For more detailed information on available benefits, please visit
jobs.CVSHealth.com/benefits

Required Qualifications
5+ years of progressively complex related experience

5+ years experience with bash shell scripts, UNIX utilities & UNIX Commands

5+ years experience building and implementing data transformation and processing solutions

Advanced knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar

Advanced knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment

Preferred Qualifications
Has in-depth knowledge of large-scale search applications and building high volume data pipelines

Ability to leverage multiple tools and programming languages to analyze and manipulate large data sets from disparate data sources

Ability to understand and build complex systems and solve challenging analytical problems

Education
Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.

Master’s degree or PhD preferred

Business Overview
Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.",172500,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1963,$10+ billion (USD),Pennsylvania,60,data engineer,senior,"['nosql', 'java', 'shell', 'python']",[],[],"['hive', 'mysql']",['hadoop'],['bash'],bachelor,+10 years
GE Renewable Energy,3.8,"Charleroi, PA",Customer Order and Data Engineer,"Job Description Summary
Execute the design, analysis, or evaluation of assigned projects using sound engineering principles and adhering to business standards, practices, procedures, and product / program requirements. This work would include mechanical and/or electrical engineering for all products
Job Description
Required Qualifications
Developing in-depth knowledge of low voltage schematics as they pertain to high voltage circuit breakers.
In-depth understanding of key business drivers; uses this understanding to accomplish own work. In-depth understanding of how work of own team integrates with other teams and contributes to the area.
Uses some level of judgment and has ability to propose different solutions outside of set parameters but with guidance. Uses prior experience and on-the-job training to solve straightforward tasks. Has access to technical skills and analytic thinking required to solve problems. May use multiple internal sources outside of own team to arrive at decisions
Utilize business systems to receive a customer specification, translate the specification to schematics and bills of material, and provide this information to production in a timely manner free from errors.
Maintain a digital tool that involves database maintenance as well as programming logic in order to generate a Bill of Materials given user input
Participate in Phase-In Phase-Out reviews and manage product phasing
Investigate and evaluate current state of the product configurator, understand cross-functional needs of product within the factory, define opportunities for improvement, and create a project plan. The goal of the project is to refine, improve, simplify, and evolve the product configurator for Dead Tank Circuit Breakers to meet factory growth demands in products, options, and function of the tool
Develop/incorporate routing in the product configurator for SAP, BOM, and MES systems as required.
Writes rules, guidelines, and operating procedures necessary for product configurator use.
Train, coach, and develop additional users for product configurator in the future.
Performs other duties as assigned
Position Requirements:
Bachelor's degree in Mechanical or Electrical Engineering (or Associate's degree in Electrical or mechanical with a minimum of 6 years of relevant experience)
Minimum of 4 years of relevant experience related to data analysis, bills of materials, manufacturing, project management, and/or product design
Desired Characteristics
Strong SAP experience
Good understanding of product configurations, R&D Engineering, Planning, Sales, Contract engineering, and Manufacturing.
High voltage and Dead tank circuit breaker knowledge
Minimum of 1 year with ProEngineer design and AutoCAD
Strong Microsoft Excel experience
Experience in project management, planning, activity follow-up.
Experience in MS Office software including MS Excel and MS PowerPoint.
Experience working cooperatively and effectively in a team environment.
GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.
GE will only employ those who are legally authorized to work in the United States for this opening. Any offer of employment is conditioned upon the successful completion of a drug screen (as applicable).
Relocation Assistance Provided: Yes",62861,10000+ Employees,Subsidiary or Business Segment,"Energy, Mining & Utilities",Energy & Utilities,2015,$10+ billion (USD),PA,8,data engineer,na,['r'],[],"['sap', 'excel']",[],[],[],bachelor,0-2 years
"School Specialty, LLC",3.0,"Lombard, IL","Oracle Data Engineer (Mansfield OH, Greenville WI or Lombard IL)","People Passion Purpose
Everything School Specialty offers is designed for one purpose – to help students succeed. We believe every student can flourish in an environment where they feel safe and inspired to explore and grow.

We’re determined to positively impact the future, one child at a time. We need to talk if you share our passion:

Transforming more than classrooms.®

Benefits

School Specialty offers Medical, Dental, & Vision plans (Effective Day 1), Wellness programs, Health Savings Accounts, Flexible Spending Accounts, 401 (k), PTO, Promise Hours dedicated to volunteering, Education Reimbursement, Paid Holidays, Fall & Winter Flexible Hours, Employee Discounts and much more!

Data Engineer
(Hybrid role- in Greenville WI, Lombard IL, or Mansfield OH)

Candidate will be responsible for designing, building, and maintaining scalable and efficient data systems supporting our organization's data-driven decision-making. You will work closely with our data scientists and business analysts to develop ETL processes and pipelines that transform raw data into valuable insights. The Data Engineer will serve as a backup to the DBA role in the event of their absence or unavailability.

The base salary range for this role is $77-100K Annually

Summary of Primary Responsibilities
Design, build, and maintain scalable and efficient data systems and pipelines
Develop and maintain ETL processes that transform raw data into valuable insights, including data cleansing, data mapping, and data transformation
Work closely with data scientists and business analysts to understand their data requirements and develop solutions to meet their needs
Design and implement data storage solutions that are secure, reliable, and accessible
Develop data quality checks and monitoring to ensure data accuracy and completeness
Develop and implement data processing and validation procedures
Develop and maintain documentation on data pipelines, data dictionaries, and data lineage
Perform data profiling, data mapping, and data modeling to support data analysis and reporting
Collaborate with cross-functional teams to integrate data from different sources
Continuously optimize and improve data systems and pipelines for performance, scalability, and reliability
Creating and executing backups, performing database tuning and optimization, monitoring database activity and usage, and providing support to end-users
Stay up-to-date with emerging trends and technologies in data engineering
Minimum Experience Requirements
Proven experience as a Data Engineer or similar role
Strong understanding of data modeling, database design, and data architecture principles
Experience building ETL processes and pipelines, including data cleansing, data mapping, and data transformation
Proficiency in SQL and experience working with Oracle and MS-SQL database technologies
Experience in database administration and be able to troubleshoot issues related to database connectivity, security, and performance
Ability to work independently and collaboratively in a fast-paced environment
Excellent problem-solving and communication skills
Willingness to learn and adapt to new environments and technologies
Self-starter and confident
Preferred Knowledge and Skills
Experience with big data technologies such as Hadoop, Spark, or NoSQL databases
Experience with distributed data processing frameworks like Apache Hadoop, Apache Spark, or Apache Flink
Knowledge of data warehousing concepts and tools such as Redshift, Snowflake, or BigQuery
Familiarity with data visualization and reporting tools such as Tableau or Power BI
Experience working with data streaming and real-time data processing frameworks like Apache Kafka or Apache Storm
Disclaimers
The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. All personnel may be required to perform duties outside of their normal responsibilities from time to time, as needed.
School Specialty, LLC. is a Drug Free Workplace. All applicants are subject to a drug screen and background check as a condition of employment.
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.
If you need a reasonable accommodation for any part of the employment process, please contact us by email at Opportunities@SchoolSpecialty.com and let us know the nature of your request and your contact information.
#LI-Hybrid",88500,1001 to 5000 Employees,Company - Private,Education,Education & Training Services,1959,$500 million to $1 billion (USD),IL,64,data engineer,na,"['sql', 'nosql']","['snowflake', 'redshift']","['tableau', 'power bi']","['snowflake', 'oracle']","['kafka', 'flink', 'spark', 'hadoop']",[],,
Titan America,3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019",80000,1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable,VA,121,data engineer,na,"['r', 'python']","['snowflake', 'aws', 'google cloud', 'azure']",[],['snowflake'],[],[],bachelor,5-10 years
MyCare Medical Group,4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",137500,501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable,FL,5,data engineer,na,"['sql', 'python']","['snowflake', 'azure', 'redshift']","['looker', 'tableau', 'power bi']",['snowflake'],"['spark', 'hadoop']",[],bachelor,0-2 years
OpenSecrets.org,4.0,Remote,Snowflake Data Engineer,"About us
Our goal is to is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy..
Our work environment includes:
Modern office setting
Work-from-home days
Wellness programs
Data Engineer
OpenSecrets is seeking to add a new member to our data team. The Data Engineer will be responsible for helping build and maintain our data systems.
Responsibilities include:
Implementing methods to improve data reliability and quality
Analyzing raw data and implementing that data into our system using data pipelines
Troubleshooting data errors and pipeline issues
REQUIRED QUALIFICATIONS:
Experience working with and analyzing large datasets, equivalent to 2-3 years in a similar position
Proven ability to build complex data pipelines using Snowpipe and SSIS
The ability to work with diverse populations
Experience with MSSQL Server and Snowflake
Advanced knowledge of ANSI SQL
PREFERRED QUALIFICATIONS:
Excellent written and verbal communication skills.
The ability to manage multiple priorities and projects both independently and as a member of a team
Snowflake SnowPro Data Engineer certification
Experience with Apache Spark
Experience with machine learning matching models
Knowledge of and interest in American government and money in politics.
SALARY AND BENEFITS:
We are a fully remote organization and this person can be based anywhere in the U.S.
OpenSecrets offers an excellent benefits package, including full medical, dental, 403(b) match, generous vacation and sick leave, and all federal holidays paid.
TO APPLY:
Please visit this link: https://airtable.com/shrGe2bugeapKVr8a
People of diverse backgrounds are strongly encouraged to apply. OpenSecrets is an equal opportunity employer committed to creating a diverse and inclusive organizational culture. We do not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under the law.
ABOUT US: OpenSecrets is the nation's premier research group tracking money in U.S. politics and its effect on elections and public policy. Our mission is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy.
Job Type: Full-time
Pay: $83,874.82 - $100,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Experience:
SQL: 3 years (Required)
Snowflake: 1 year (Preferred)
ETL: 2 years (Required)
Work Location: Remote",91937,,,,,-1,,Remote,-1,data engineer,na,['sql'],['snowflake'],['ssis'],['snowflake'],['spark'],[],,2-5 years
Trellis,4.4,Remote,Data Engineer,"Overview
As a Data Engineer for Trellis, you will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure to support the deployment of machine learning models for educational applications. You will be working with large volumes of text data, and will be responsible for ensuring data quality, performing data cleaning, and implementing data transformation processes.
Responsibilities
Build and maintain data pipelines for processing large volumes of text data
Perform data cleaning and preprocessing to ensure data quality and consistency
Implement data transformation processes to convert raw data into formats suitable for machine learning models
Collaborate with data scientists and other stakeholders to understand project requirements and provide data engineering support as needed
Develop and maintain data infrastructure to support machine learning workflows
Monitor and troubleshoot data pipelines to ensure high availability and reliability
Qualifications
Bachelor's or Master's degree in Computer Science, Data Science, or a related field
Strong programming skills in Python and experience with data processing frameworks such as Spark and Hadoop
Experience with text data processing and natural language processing (NLP) techniques
Familiarity with machine learning workflows and frameworks such as TensorFlow and PyTorch
Experience with cloud-based data storage and processing technologies such as AWS, Google Cloud, or Azure
Strong problem-solving skills and attention to detail
Excellent communication and collaboration skills
Benefits
SF office, but remote-friendly. Come into the office 60% of the time — you’ll want to! It’ll be built as a library in a way that’s anti-fatigue. We will also have offices in NYC and Montreal.
Health insurance with 100% premium covered
Generous PTO / sick leave
401(k) plan with employer match
Free lunch and snacks
Annual company retreat in Montreal
Bring your dog to work",112889,51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2012,$5 to $25 million (USD),Remote,11,data engineer,na,['python'],"['azure', 'aws', 'google cloud']",[],[],"['hadoop', 'spark']",[],bachelor,
Purpose Financial,4.2,Remote,Data Engineer,"Address : 322 Rhett St, Greenville, South Carolina, United States - 29601
Purpose Financial, Inc. is an innovative consumer financial services company that offers a diverse suite of credit products, promoting financial inclusion and meeting consumers wherever they are. Through its brands, the company is committed to helping customers achieve their version of financial stability in the moment and in the future. Since 1997, Purpose Financial has been a pioneer in the consumer credit and financial services market offering money solutions in over 1,000 storefronts locations and online lending. Providing services in over 20 states, Purpose Financial employs over 2,700 team members.
At Purpose Financial, our people are our greatest asset. We are inclusive and encourage different points of view. We all belong here. We respect our colleagues. We value our customers, and we support our communities.
What do we do? Our sole purpose is to help each customer achieve their version of financial stability in the moment and in the future.
We have a history! Consumers know that when they choose Purpose, they’re backed by a trusted, reliable financial services company with 25 years in the business, 2,700+ employees, 1,000+ store locations, and operating in 24 states.
Benefits and Perks. We believe our employee benefits should make life better. Purpose Financial offers a comprehensive benefits package to everyone on our team.
Health & Welfare Benefits. When you take care of yourself, you’re a better you – which is better for everyone!
Health Savings & Flexible Spending Accounts. Pre-tax money for your family’s out-of-pocket expenses.
401k with Matching Contributions. Think bigger! You can build a better future.
Wellness & Employee Assistance Programs. Take care of yourself in every way that matters.
Generous Vacation Policy & Paid Time Off. Time off to take care of yourself, your family – and your bucket list!
Volunteer Days. We want you to care. Always. Take 2 days off each year to volunteer and make a difference in your community.
Tuition Reimbursement. Pursue your goals without fretting over all the costs.
Employee Discounts. Save on home and auto insurance, cell phone service, entertainment, shopping, pet insurance and even travel.
Performance Incentives & Awards. We want you to feel valued for all your contributions.
Hybrid Workforce Support. We believe in getting it done – in whatever way works best for you and your team.
Want to meet our executive team? Visit our home page at www.havepurpose.com.
Position Summary
The job function is responsible for building a scalable data infrastructure. The role will be part of the effort to enable Purpose Financial to democratize data by developing self-serving and curated data models for our single source of truth. The job function includes understanding of business requirements, defining data requirements, identifying enterprise data sources, planning, designing, and building data acquisition from sources, defining data model, designing data ingestion pipelines, data normalization, data transformation, and consumption using platforms such as Snowflake and AWS.
Job Responsibility
Take ownership of building/optimizing data pipelines to increase efficiency and performance
Develop and maintain data ingestion and integration processes that focus on data quality and monitoring
Use AWS and Snowflake tools such as Glue, Lambda, Snowpark, and Snowpipe to build data ingestion pipelines from various sources
Incorporate standards and best practices into data pipeline solutions
Participate in troubleshooting and problem-solving design discussions
Provide logical and physical data design, and database modeling
Solve complex data issues around data integration, unusable data elements, unstructured data sets, and other data processing incidents
Continuously design the future state of our data & reporting process by collaborating with IT and business partners.
Partner with enterprise teams, data scientist, architects to define development standards and solutions.
Manage cloud infrastructure as code (IaC), using Terraform.
Work closely with other members of the data team to better design and build metrics that enhance our analytical capabilities
Manage workflows and data build processes that help the BI team build key business metrics and design dashboard visualizations
Assess the effectiveness and accuracy of new data sources, data points, and data gathering techniques
Troubleshoot problems with data (missing, corrupted, invalid) and build data recovery strategy
Apply best practices for AWS & Snowflake architecture, ELT and data models
Be a leader for our data journey, act as SME for data and database related topics, and assist with development of data lakes and warehouses
Education Required
Bachelor’s degree (or foreign equivalent degree) in any Computer, Engineering, or other technical field.
Experience Required
5+ years experience in data warehousing
3+ years of experience in data architecture and design, with a good understanding of various data systems and structures (i.e. SQL, NoSQL, Key-Value, Streams)
3+ years of experience with building data pipelines and using data orchestration tools and implementing end to end Monitoring & Alerting solutions
3+ years of experience working with AWS or public cloud provider.
Strong understanding of SQL, data structures, data integrity, and schema design best practices
Programming experience required, preferably Python.
A strong advocate of data governance and data quality
Experience with database internals, database design, SQL and database programming
Experience in financial services (Banking and consumer lending preferred)
Experience of authoring CI/CD pipelines, automation elements related to infrastructure composition, deployment orchestration, and monitoring.
Familiarity and experience with Agile/SCRUM based development methodology.
Knowledge Required
Excellent written and verbal communications skills as would be needed to communicate in person, by phone, and through email; adaptability and flexibility to changing environment; and comfortable working in a dynamic, high volume, fast-paced environment. Ability to read, write, evaluate, and apply information. Ability to interact professionally and exhibit appropriate social skills. Ability to understand and ensure compliance with policies, procedures, and laws governing our industry/business and products. Ability to develop and maintain business relationships.
Physical Requirements
Sitting for long periods of time; standing occasionally; walking; bending; squatting; kneeling; pushing/pulling; reaching; twisting; frequent lifting of less than 10 lbs., occasional lifting of up to 20 lbs.; driving and having access during the workday to an insured and reliable transportation; typing; data entry; grasping; transferring items between hands and/or to another person or receptacle; use of office equipment to include computers; ability to travel to, be physically present at, and complete the physical requirements of the position at any assigned location.
Competencies
Integrity
Customer Advocacy
Interpersonal Skills
Results/Accountability
Inclusiveness
Technical Proficiency/Leadership
Big Data Trends
Data Management
Adaptability
Travel - None
Attire - Business Casual
Other - Must be eligible to work in the USA and able to pass a background check.
Job Type: Full-time
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Day shift
Work Location: Remote",112889,Unknown,Company - Private,Financial Services,Financial Transaction Processing,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python', 'nosql']","['snowflake', 'aws']",[],['snowflake'],[],['terraform'],bachelor,5-10 years
Integration Developer Network LLC,4.9,Remote,Data Engineer with PL/SQL Background,"Requirements:
Recent PL/SQL development work experience with Oracle 19.
Must have working experience with tools like Airflow, Git.
Must have python coding experience and REST API development experience.
Plus would be experience with microservices, docker, Kubernetes.
Plus would be experience with accounting and financial data
Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.
Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL.
Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations. Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts.
Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
2+ years’ experience in the design, implementation and support of full life-cycle data engineering projects.
Bachelor's degree in Computer Science, Engineering or other equivalent degree, with 5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree with 2+ years data engineering preferred.
High school diploma or GED is required.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
9 years
Schedule:
8 hour shift
Experience:
Python: 8 years (Required)
PL/SQL: 5 years (Required)
ETL: 5 years (Required)
Work Location: Remote",112500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2011,$1 to $5 million (USD),Remote,12,data engineer,na,"['sql', 'python']",[],[],"['mongodb', 'elasticsearch', 'oracle']",[],['docker'],bachelor,5-10 years
Trella Health,4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.",113846,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable,PA,8,data engineer,senior,"['r', 'go', 'python']","['snowflake', 'aws']",[],"['snowflake', 'mysql']",[],[],bachelor,
iManage,4.5,Remote,Senior Data Engineer (Azure),"We offer a flexible working policy that supports the health and well-being of our iManage employees. As an organization, we value collaborating and learning from our peers in person, while providing the necessary flexibility for our employees to have a meaningful work-life balance. Please reach out to learn more.
Being a Senior Data Engineer at iManage Means…
You are excited about data and believe in the democratization of data to support data driven decision-making. You will partner with our Information Technology team to implement, support, and extend our Enterprise Data Lake hosted on Azure and built using Azure Synapse. You will gather requirements from iManage business units and craft solutions which provide access to critical business data. You will develop data models and data pipelines for our Enterprise Data Lake, and provide integration with BI platforms and tools such as Totango and Power BI. You are passionate about lakehouse architecture and have experience using Delta Lake and bronze, silver, and gold data lake design.
Here is what one of our leaders, Cloud Services Director (Jacqueline Toepfer), has to say about the role: “As a Senior Data Engineer on our team, you will get the opportunity to showcase your expertise and make a real difference across the organization. You will be part of a truly collaborative team that is passionate about delivering quality solutions. You will be the in-house expert in the data models of multiple, disparate enterprise SaaS systems and utilize your wealth of knowledge to provide recommendations and solutions for consolidation, transformation, and integration of the disparate data sources.”
iM Responsible For…
Modeling, managing, and reporting of data stored in Azure Data Lake.
Gathering data requirements from various business units and translating these requirements into data models.
Using Python, PySpark, and system specific APIs to extract, transform, store and analyze data from a variety of systems.
Data modeling, defining data pipelines, and integrations necessary to present data in BI platforms such as Totango, or BI tools like Power BI.
Identifying and modeling all current disparate data sources and the data flows between these data sources.
Analyzing current repositories and proposing changes to data repositories and data flows to better support company objectives for the measurement of user experience and customer success.
Understanding the business needs of data integration and governance from disparate systems to drive the enhancement of the enterprise data lake.
Applying best practices to ensure the security and privacy of the data repositories.
Ensuring data repositories meet company standards for storage of PII.
Developing proficiency with the iManage product APIs for all iManage Cloud services.
iM Qualified Because I Have...
A Bachelor’s degree or higher in Computer Science or equivalent field.
3-5 years of experience working with data in a business setting.
Proficiency in data extraction, manipulation, and subsequent reporting with Spark and Python.
Experience designing data pipelines with a cloud-native mindset using Azure or AWS.
Knowledge and experience with architecting a data lake with Azure Synapse or adjacent technologies like Databricks.
Experience ingesting data from SaaS solutions and other services via API or other related technologies.
A passion to be a thought leader and work collaboratively within a team.
Commitment to understanding data requirements and delivering scalable, robust solutions that meet those requirements.
A creative mindset with a desire to explore new technologies and create innovative solutions.
Bonus Points If I Have…
Familiarity with Delta Lake.
A background with relational databases and data warehouse design using star schemas.
Experience with cloud-based data models for business solutions like Salesforce, Zendesk, and NetSuite.
Don't meet every qualification listed above? Studies show that women and people of color are less likely to apply to jobs unless they meet all qualifications. At iManage, we are committed to building a diverse and inclusive environment and encourage everyone to show up as their full authentic selves. We welcome those that come with a growth mindset and a hunger for learning; so, if you are excited about this role but your past experience doesn't align perfectly with every qualification, we encourage you to apply anyways!
iM Getting To…
Join a supportive, experienced team with an inclusive, encouraging, and vibrant culture.
Have flexible work hours that allow me to balance my ‘me time’ with my work commitments.
Collaborate in a modern open plan workspace, with a gaming area, free snacks, drinks and regular social events.
Focus on impactful work, solving complex, real challenges utilizing the latest technologies and protocols.
Own my career path with our internal development framework. Ask us more about this!
Learn new skills and earn certifications with access to unlimited courses in LinkedIn Learning.
Join an innovative, industry leading SaaS company that is continuing to grow & scale!
iManage Is Supporting Me By...
Creating an inclusive environment where I can help shape the culture not just by fitting in, but by adding to it.
Providing a market competitive salary that is applied through a consistent process, equitable for all our employees, and regularly reviewed based on industry data.
Rewarding me with an annual performance-based bonus.
Offering comprehensive Health/Vision/Dental/Life Insurance, and a 401k Retirement Savings Plan with a company match up to 4%.
Giving access to HealthJoy, a healthcare concierge service, to help me maximize my health benefits.
Granting enhanced leave for expecting parents; 20 weeks 100% paid for primary leave, and 10 weeks 100% paid for secondary leave.
Providing me with a flexible time off policy to take the time off that I need. Be it for vacation, volunteering, celebrating holidays, spending time with family, or simply taking time to recharge and reset.
Caring for my mental health and well-being with multiple company wellness days and free access to the Healthy Minds app for mindfulness, meditation and more.
About iManage…
iManage is dedicated to Making Knowledge WorkTM. Over one million professionals across 65+ countries rely on our intelligent, cloud-enabled, secure knowledge work platform to uncover and activate the knowledge that exists inside their business content and communications.
We are continuously innovating to solve the most complex professional challenges and enable better business outcomes; Our work is not always easy but it is ambitious and rewarding.
So we’re looking for people who love a challenge. People who are happiest when they’re solving problems and collaborating with the industry’s best and brightest. That’s the iManage way. It’s how we do things that might appear impossible. How we develop our employees’ strengths and unlock their potential. How we find meaning in everything we do.
Whoever you are, whatever you do, however you work. Make it mean something at iManage.
iManage provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Learn more at: www.imanage.com
Please see our privacy statement for more information on how we handle your personal data: https://imanage.com/privacy-policy/
#LI-LM1
#LI-Remote
V436F7WSwa",112889,501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,$100 to $500 million (USD),Remote,8,data engineer,senior,['python'],"['databricks', 'azure', 'aws']",['power bi'],[],['spark'],[],bachelor,5-10 years
LatentView Analytics,4.0,"Dallas, TX",Data Engineer,"Role : Data Engineer
Experience : 6 - 8+ Years
Location : Dallas,Tx (Onsite)
Position: FullTime Only
Skills :Python,SQL Server ,Scala, Hadoop, HPCC, Storm, Cloudera, Cassandra,Excel, R,Docker,Kubernetes,Snowflake,Azure,Kafka,Redshift,Hadoop,AWS.
Job Type: Full-time
Pay: $80,000.00 - $1,100,000.00 per year
Compensation package:
Yearly pay
Experience level:
6 years
Schedule:
8 hour shift
Experience:
data engineer: 6 years (Preferred)
Work Location: In person
Speak with the employer
+91 9876543210",109012,1001 to 5000 Employees,Company - Public,Management & Consulting,Business Consulting,2006,$25 to $100 million (USD),TX,17,data engineer,na,"['r', 'sql', 'scala', 'python']","['snowflake', 'aws', 'azure', 'redshift']",['excel'],"['sql server', 'snowflake']","['kafka', 'hadoop']",['docker'],,5-10 years
Octo,4.2,"Chantilly, VA",Data Engineer,"You…
As a Data Engineer, you will be joining the team that is deploying and delivering a cloud-based, multi-domain Common Data Fabric (CDF), which provides data sharing services to the entire DoD Intelligence Community (IC). The CDF connects all IC data providers and consumers. It uses fully automated policy-based access controls to create a machine-to-machine data brokerage service, which is enabling the transition away from legacy point-to-point solutions across the IC enterprise.
Us…
We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client’s missions.
Program Mission…
The CDF program is an evolution for the way DoD programs, services, and combat support agencies access data by providing data consumers (e.g., systems, app developers, etc.) with a “one-stop shop” for obtaining ISR data. The CDF significantly increases the DI2E’s ability to meet the ISR needs of joint and combined task force commanders by providing enterprise data at scale. The CDF serves as the scalable, modular, open architecture that enables interoperability for the collection, processing, exploitation, dissemination, and archiving of all forms and formats of intelligence data. Through the CDF, programs can easily share data and access new sources using their existing architecture. The CDF is a network and end-user agnostic capability that enables enterprise intelligence data sharing from sensor tasking to product dissemination.
Responsibilities...
Primary responsibility is to work with data providers within the IC and DoD Enterprise to identify and ingest data sets into the CDF data broker. In this role you will:
Develop, optimize, and maintain data ingest flows using Apache Nifi and Python.
Develop within the components in the cloud platform, such as Apache Kafka, NiFi, and HBase.
Communicate with data owners to set up and ensure CDF streaming and batching components are working (including configuration parameters).
Document SOP related to streaming configuration, batch configuration or API management depending on role requirement.
Document details of each data ingest activity to ensure they can be understood by the rest of the team
What we’d like to see…
A minimum of 3 years of experience with programming and software development including analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems
DoD 8570 IAT Level II Certification (e.g. Security+) or the ability to obtain the certification within 90 days
Demonstrable CentOS command line knowledge
Working knowledge of web services environments, languages, and formats such as RESTful APIs, SOAP, FTP/SFTP, HTML, JavaScript, XML, and JSON
Understanding of foundational ETL concepts
Experience implementing data ignorations with in the IC DoD Enterprise.
Desired Skills:
Experience or expertise using, managing, and/or testing API Gateway tools and Rest APIs (desired)
2+ Experience in Python Development
Experience or expertise configuring an LDAP client to connect to IPA (desired)
Advanced organizational skills with the ability to handle multiple assignments
Strong written and oral communication skills
Years of Experience: Junior Level (0-4 years),Mid Level (5-8 years), Senior Level (9+)
Education: Bachelor's degree in systems engineering, computer engineering, or a related technical field (preferred)
Location: Chantilly, VA
Clearance: Active TS/SCI w/ ability to obtain CI Poly (preferred)",99041,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,$100 to $500 million (USD),VA,17,data engineer,na,['python'],[],[],[],['kafka'],[],bachelor,5-10 years
TIAA,4.0,"Iselin, NJ",Senior Cloud Data Engineer,"The Senior Data Platform Engineer, Cloud role designs datastore systems that are appropriate for applications, customer needs and consistent with the overall design of the organization's information systems architecture. Under limited supervision, this job is responsible for the solution engineering and design, provisioning, delivery, service management, continuous automations of the organization's datastore systems.

Key Responsibilities and Duties
Design, develop and deliver cloud datastore solutions and develop automation pipelines to migrate data sets from On-prem to Cloud platforms. Practice Infrastructure as code to develop automation routines and integration flows to manage state of the datastore platform systems

Provision secures from start datastores and enable them with required security controls including encryption, masking, certificate/keys rotation etc.

Collaborates with developers, analysts, various system administrators to identify business requirements in designing efficient datastore solutions and interfaces.

Identifies and documents all system constraints, implications, and consequences of various proposed system changes.

Reviews technical documentation to guide system users and to assist with the ongoing operation, maintenance, and development of the system. Evaluates the efficiency and effectiveness of application operations and troubleshooting problems.

Provide expert level IT technical lead services, including the direction, evaluation, selection, configuration, implementation, and integration of new and existing technologies and tools in a cloud platform.

Responsible for development of cloud integrations and data migrations to support operations of Cloud infrastructure, provisioning, monitoring with (IaaS) and (PaaS) models

Deploy, automate, maintain, and manage AWS cloud-based production system, to ensure the availability, performance, scalability, and security of productions systems.

Manage the governance framework for DB-Services specific to private, hybrid and public cloud platform adhering to standards and integration with existing tools.

Ability to anticipate technology changes within a rapidly evolving environment.
Educational Requirements
Bachelor's Degree Preferred
Work Experience
3+ Years Required; 5+ Years Preferred
Physical Requirements
Physical Requirements: Sedentary Work

Career Level
7IC

Required Skills:
3 or more years of experience in SQL, ETL and ELT Tools.
Experience working with Data Virtualization Platforms like Starburst, Presto, Denodo, Dremio.
Preferred Skills:
Experience with AWS or GCP, PySpark, CI/CD Pipelines using ElectricFlow.
Base Pay Range: $88,600/yr. - $147,600/yr.
Actual base salary may vary based upon, but not limited to, relevant experience, time in role, base salary of internal peers, prior performance, business sector, and geographic location. In addition to base salary, the competitive compensation package may include, depending on the role, participation in an incentive program linked to performance (for example, annual discretionary incentive programs, non-annual sales incentive plans, or other non-annual incentive plans).
_____________________________________________________________________________________________________
Company Overview
TIAA is the leading provider of financial services in the academic, research, medical, cultural and government fields. We offer a wide range of financial solutions, including investing, banking, advice and education, and retirement services.
Benefits and Total Rewards
The organization is committed to making financial well-being possible for its clients, and is equally committed to the well-being of our associates. That’s why we offer a comprehensive Total Rewards package designed to make a positive difference in the lives of our associates and their loved ones. Our benefits include a superior retirement program and highly competitive health, wellness and work life offerings that can help you achieve and maintain your best possible physical, emotional and financial well-being. To learn more about your benefits, please review our
Benefits Summary
.
Equal Opportunity
We are an Equal Opportunity/Affirmative Action Employer. We consider all qualified applicants for employment regardless of age, race, color, national origin, sex, religion, veteran status, disability, sexual orientation, gender identity, or any other protected status.
Read more about the Equal Opportunity Law
here
.
Accessibility Support
TIAA offers support for those who need assistance with our online application process to provide an equal employment opportunity to all job seekers, including individuals with disabilities.
If you are a U.S. applicant and desire a reasonable accommodation to complete a job application please use one of the below options to contact our accessibility support team:
Phone: (800) 842-2755
Email:
accessibility.support@tiaa.org
Privacy Notices
For Applicants of TIAA, Nuveen and Affiliates residing in US (other than California), click
here
.
For Applicants of TIAA, Nuveen and Affiliates residing in California, please click
here
.
For Applicants of Nuveen residing in Europe and APAC, please click
here
.
For Applicants of Greenwood residing in Brazil (English), click
here
.
For Applicants of Greenwood residing in Brazil (Portuguese), click
here
.
For Applicants of Westchester residing in Brazil (English), click
here
.
For Applicants of Westchester residing in Brazil (Portuguese), click
here
.",118100,10000+ Employees,Company - Private,Financial Services,Investment & Asset Management,1918,$100 to $500 million (USD),NJ,105,data engineer,senior,['sql'],['aws'],[],[],[],[],bachelor,+10 years
Delta,4.3,"Atlanta, GA",Senior Data Engineer Modeler,"United States, Georgia, Atlanta
Information Technology
11-May-2023
Ref #: 20782
LinkedIn Tag: #LI-JM2
How you'll help us Keep Climbing (overview & key responsibilities)
Location: Atlanta GA - NOT Remote

Delta IT is on a journey of transformation. We are changing the way we do business from top to bottom. As leaders with vision within Delta, we strive to build important and innovative solutions and are looking for team members to help us realize our vision.

Delta employees are problem solvers, doers, innovators.

We are proactive.

We are collaborative.

We deliver impact to our customers.

Join us on our transformation journey in becoming a best-in-class IT organization at the world's best airline!

The Senior Data Engineer, Modeler to join our Enterprise Data team. This position is responsible for building, modifying, modernizing the conceptual, logical, and physical data models used in Enterprise Data databases. The position also requires establishing and maintaining effective partnerships with the Business and IT stakeholders.

We are looking for someone with strong analytical and organizational skills to transform data into insights, distill requirements, and develop processes. The candidate should have excellent communication skills, business maturity, and feel comfortable in a fast-paced environment.

Responsibilities
Understand the data needs of the company for ingestion, migration, storage, and access
Work with business teams to gather requirements for the database design and model
Collaborate with the Enterprise Data team and business teams to define requirements, then design and build database models
Design and build conceptual, logical, and physical data models in accordance with companys data standards
Apply relational and dimensional models for raw ingestion and curated/semantic layers
Create Physical Data Structures (DDLs) and corresponding metadata
Creating and maintaining data reference architecture architectures and integration patterns
Updating knowledge by tracking and understanding emerging large data and modeling practices and standards
Benefits and Perks to Help You Keep Climbing
Our culture is rooted in a shared dedication to living our values Care, Integrity, Resilience, Servant Leadership, and Teamwork every day, in everything we do. At Delta, our people are our success. At the heart of what we offer is our focus on Sharing Success with Delta employees. Exploring a career at Delta gives you a chance to see the world while earning great compensation and benefits to help you keep climbing along the way:
Competitive salary, industry-leading prot sharing program, and performance incentives.
401(k) with generous company contributions up to 9%
Paid time off including vacation, holidays, paid personal time, maternity and parental leave.
Comprehensive health Benefits including medical, dental, vision, short/long term disability and life Benefits.
Family care assistance through fertility support, surrogacy and adoption assistance, lactation support, subsidized back-up care, and programs that help with loved ones in all stages.
Holistic Wellbeing programs to support physical, emotional, social, and financial health, including access to an employee assistance program offering support for you and anyone in your household, free financial coaching, and extensive resources supporting mental health.
Domestic and International space-available flight privileges for employees and eligible family members
Career development programs to achieve your long-term career goals.
World-wide partnerships to engage in community service and innovative goals created to focus on sustainability and reducing our carbon footprint
Business Resource Groups created to connect employees with common interests to promote inclusion, provide perspective and help implement strategies
Recognition rewards and awards through the platform Unstoppable Together
Access to over 500 discounts, specialty savings and voluntary benefits through Deltaperks such as car and hotel rentals and auto, home, and pet insurance, legal services, and childcare
What you need to succeed (minimum qualifications)
7 or more years of experience in Information Technology or related technical capacity
Expert in concepts and principles of data modeling
Knowledge of entity relationship, dimensional modeling, big data, enterprise data, and physical data models
Knowledge of relational databases and data architecture computer systems, including SQL Familiarity
Ability to design, build, and develop a new product, technology, or service from feasibility through to production
Familiarity with data modeling software such as SAP PowerDesigner, Microsoft Visio, E/R Studio or Erwin Data Modeler
Must have hands-on experience with cloud platforms; AWS preferred
Knowledge of big data platforms such as Teradata, Oracle DB, DB2, AWS Aurora, AWS Athena, etc.
Experience using Python and/or PowerShell scripting for data processing
Strong attention to detail
Excellent communicationwith both technical and business stakeholders
Ability to work in a fast-paced environment
Ability to work both independently and as part of a team
Understanding of the business and the ability to assess and address risk without negatively impacting the business
Consistently prioritizes safety and security of self, others, and personal data.
Embraces diverse people, thinking, and styles.
Possesses a high school diploma, GED, or high school equivalency.
Is at least 18 years of age and has authorization to work in the United States.
What will give you a competitive edge (preferred qualifications)
Bachelors or masters degree in Information Technology, Computer Science, Mathematics, Engineering, Information Systems, or equivalent
In-depth understanding of ETL and data ingestion processes used in large scale data warehouses
Knowledge of data architecture principles for on-prem and cloud solutions
Flexibility to adapt and plan for changing business objectives
Solid Understanding of Agile/Scrum development methodologies
Experience translating business outcome requirements into data model requirements
Ability to adapt communication style for technical and business audiences
< Go back",124024,10000+ Employees,Company - Public,Transportation & Logistics,"Airlines, Airports & Air Transportation",1928,$10+ billion (USD),GA,95,data engineer,senior,"['sql', 'r', 'python', 'go']",['aws'],['sap'],['oracle'],[],[],,5-10 years
Great Dane,2.9,"Chicago, IL",Data Engineer,"Data Engineer - (230005R)
Description
With thousands of employees worldwide, teamwork and collaboration are valued here.

We look for employees who are driven, determined and ready to accelerate their future. By joining our team, you will earn competitive pay, benefits, insurance, 401k, pension and more while working in an environment with the highest safety standards in the industry.
The Position:
The Data Engineer is responsible for expanding and optimizing our current data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is experienced in both data pipeline creation and data transformation. The Data Engineer will support our software developers, system architects, data analysts and Business Analysts on all data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s current data architecture to support our next generation of products and data initiatives.
Key Responsibilities:
Create and maintain optimal data pipeline architecture.
Assemble complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep data separated and secure across all platforms used.
Work with data and business experts to strive for greater functionality in our data systems.
Other duties as assigned.
Qualifications
Requirements:
Education: Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Experience: 5+ years of experience in Data Engineer role
Skills: Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing data pipelines architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from disconnected datasets.
Project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with relational SQL databases, including MS SQL server, Oracle, DB2, and Maria.
Experience with Data Cloud platforms like SnowFlake and or Data Bricks.
Experience with data pipeline and workflow management tools: SQDR, Airflow, Fivetran, Airbyte, etc.
Experience with object-oriented/object function scripting languages: Python, Java
Travel: 20% at most
Physical Demands/Work Environment:
The physical demands and work environment characteristics described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Physical demands: While performing duties of job, employee is occasionally required to stand; walk; sit; use hands to finger, handle or feel objects; reach with hands and arms; talk and hear. Specific vision abilities required by the job include close and distance vision.
Work environment: The noise level in the work environment is usually minimal to moderate.
Must be willing to work occasional nights and/or weekends as business commitments dictate.
Great Dane is an Equal Opportunity Employer
Primary Location
: US-IL-Chicago
Work Locations
: Chicago N. LaSalle St. 222 N. LaSalle St. Suite 920 Chicago 60601
Job
: Information Systems
Schedule
: Full-time
Shift
: 1st Shift",104652,5001 to 10000 Employees,Company - Private,Manufacturing,Transportation Equipment Manufacturing,1900,$500 million to $1 billion (USD),IL,123,data engineer,na,"['sql', 'java', 'python']",['snowflake'],[],"['sql server', 'snowflake', 'oracle']",[],[],bachelor,+10 years
Texas Capital Bank,3.0,"Richardson, TX",Senior Data Engineer,"Overview:
A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses on development and delivery of analytical solutions using various tools including AWS Glue, Lambda, Snowflake and AWS RDS. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.
Responsibilities:
Partner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (onshore and offshore)
Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storage
Advanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plans
Skilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumption
Expert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging, and error handling
Support continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review board
Develop and ensure adherence to published system architectural decisions and development standards
Lead and foster junior data engineers in their careers to produce higher quality solutions at a faster velocity through optimization training and code review
Multi-task across several ongoing projects and daily duties of varying priorities as required
Interact with global technical teams to communicate business requirements and collaboratively build data solutions
Qualifications:
Education
Bachelor’s degree in computer science or MIS related area required or equivalent experience (industry experience substitutable)
Job experience
4-8 years of total experience in data engineering/Cloud development activity.
1+ years of experience in Banking and financial domain

Technical Requirement
Must be extremely proficient in Data Warehouse ETL Design/Architecture, dimensional/relational data modelling.
Experience in atleast one ETL development project, writing/analyzing complex stored procedures.
Should have entry level/intermediate experience in Python/PySpark – working knowledge on spark/pandas dataframe, spark multi-threading, exception handling, familiarity with different boto3 libraries, data transformation and ingestion methods, ability to write UDF.
Snowflake – Familiarity with stages and external tables, commands in snowflake like copy, unload data to/from S3, working knowledge of variant data type, flattening nested structure thru SQL, familiarity with marketplace integrations, role-based masking, pipes, data cloning, logs, user and role management is nice to have.
Familiarity with Coalesce is an added advantage for this job
Collibra integration experience for Data Quality and Governance in ETL pipeline development is nice to have.
AWS – Should have hands-on experience with S3, Glue (jobs, triggers, workflow, catalog, connectors, crawlers), CloudWatch, RDS and secrets manager.
AWS - VPC, IAM, Lambda, SNS, SQS, MWAA and Athena is nice to have.
Should have hands-on experience with version controlling tools like github, working knowledge on configuring, setting up CI/CD pipelines using yaml, pip files.
Streaming Services – Familiarity with Confluent Kafka or spark streaming or Kinesis (or equivalent) is nice to have.
Data Vault 2.0 (hubs satellite links) experience will be a
Highly proficient in Publisher, PowerPoint, SharePoint, Visio, Confluence and Azure DevOps
Working knowledge of best practices in value-driven development (requirements management, prototyping, hypothesis-driven development, usability testing)
Good communicator with problem solving mindset and focus on process improvement
Strong time management skills and a proven track record of meeting various deadlines
Strong executive presentation skills with expertise in PowerPoint and presentation best practices.
Consistently demonstrates clear and concise written and verbal communication skills
Good interpersonal skills, ability to interact with Senior Management
Highly self-motivated with a strong sense of initiative
Excellent multitasking skills and task management strategies
Ability to work well in a team environment, meet deadlines, demonstrate good time management, and multi-task in a fast-paced project environment.",106721,1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,1998,Unknown / Non-Applicable,TX,25,data engineer,senior,"['sql', 'python']","['snowflake', 'aws', 'azure']",[],['snowflake'],"['kafka', 'spark']",[],bachelor,+10 years
"Mastery Logistics Systems, Inc",3.9,"Omaha, NE",Senior Data Engineer (Kafka),"About the Role
In the world of transportation, data is constantly moving, and Kafka is the roadway that keeps that traffic running smoothly to its destination. As a technical expert, you must be comfortable working across teams on multiple, high impact projects. You will be a valued part of a team that is constantly maturing Kafka use and event-driven architecture. Members of this team are responsible for the overall use and implementation of Kafka components including the Confluent platform, observability, governance, best practices, and solution development. An understanding of Kafka principles and enterprise integration patterns is required.
In order to be successful:
You are a self-directed person who can identify priorities.
You are a detail-oriented person who takes pride in keeping data correct and always having a backup plan.
You are a problem-solver who might write a script or find a tool to get things done when there isn't an established solution.
You want to learn and grow in the event-driven world.
You love Kafka! When you hear terms like ""event-driven"" or ""real-time streaming"" you're ready ready to dive in!
Responsibilities
Lead a team of Kafka engineers in an operational capacity
Develop and implement solutions using Kafka.
Administer and improve use of Kafka across the organization including Kafka Connect, ksqlDB, Streams, and custom implementations.
Work with multiple teams to ensure best use of Kafka and data-safe event streaming.
Understand and apply event-driven architecture patterns and Kafka best practices. Enable development teams to do the same.
Assist developers in choosing correct patterns, event modeling, and ensuring data integrity.
Continuous learning to be a Confluent/Kafka subject matter expert.
Work with Kafka and Confluent API's (e.g. metadata, metrics, admin) to provide pro-active insights and automation.
Work with SRE's to ensure Kafka-related metrics are exported to New Relic.
Perform regular reviews of performance data to ensure efficiency and resiliency.
Contribute regularly to event-driven patterns, best practices, and guidance.
Review feature release and change logs for Kafka, Confluent, and other related components to ensure best use of these systems across the organization.
Work with lead to ensure all teams are aware of technology changes and impact.
Acquire a deep understanding of source and sink connector technical details for a variety of platforms including PostgreSQL, MS SQL Server, Snowflake, and others as required.
Requirements
Be able to describe the primary components of Kafka and their function (brokers, zookeeper, topics).
At least two years of experience supporting applications in a production environment.
You will be expected to read and navigate code in multiple languages. Multi-language fluency and writing is not required.
Experience in a microservice architecture
Experience with event driven architecture
Proficiency in at least one programming language and one scripting language.
Proficiency with Docker containers.
Ability to participate in and contribute to code management in Github including actively collaborating in peer-reviews, feature branches, and resolving conflicts and commits.
Excellent written and verbal communication skills.
Strong sense of responsibility with a bias towards action.
Comfortable self-directing and prioritizing your own work.
Microservices experience is a plus.
Distributed tracing experience a plus.
An understanding of any cloud (Azure preferred) infrastructure and components is a plus, but is not required.
Create reference solutions.",96493,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NE,-1,data engineer,senior,['sql'],"['snowflake', 'azure']",[],"['sql server', 'postgresql', 'snowflake']",['kafka'],['docker'],,
University of Arizona,4.2,"Tucson, AZ",KMAP Data Engineer,"Posting Number
req16040

Department
Research Innovation & Impact

Department Website Link
https://kmap.arizona.edu/

Location
Main Campus

Address
Tucson, AZ USA

Position Highlights
The Institutional Knowledge Map (KMap) program at the University of Arizona is in search of a KMap Data Engineer (Applications Developer Programmer Analyst I). KMap is a data science system that helps organizations to understand their knowledge landscape. It uses advanced technologies to analyze research information like research papers, patents, grants, biography, CV, and web presence to make an artificial map of the organization's knowledge. This visual map helps to find experts and collaborators. This position is responsible for collecting, cleaning, and aggregating data for research activities of the university. This position will take care data collections and data processing pipelines of the KMap project.
The ideal candidate must possess proficiency in Python and MongoDB for efficient handling, processing, and analysis of research information.
Outstanding UA benefits include health, dental, and vision insurance plans; life insurance and disability programs; paid vacation, sick leave, and holidays; UA/ASU/NAU tuition reduction for the employee and qualified family members; state and optional retirement plans; access to UA recreation and cultural activities; and more!
The University of Arizona has been recognized for our innovative work-life programs.


Duties & Responsibilities
Duties & responsibilities:
Develop and maintain the data collection and processing pipelines.
Collaborate with data partners and other related groups
Technical documentation of the project
Collecting, cleaning, and transforming new data from internal and external sources
Write and execute automatic test cases to test data and application
Additional duties may be assigned
Knowledge, skills & abilities:

Knowledge of data processing techniques and tools
Knowledge of using API, and different data collection methodologies
Strong analytic skills related to working with unstructured and RDMBS datasets
Proficiency in Python and MongoDB for efficient handling, processing, and analysis of research information

Minimum Qualifications
Bachelor's degree in Information Technology or equivalent advanced learning attained through experience required and a minimum of 1 year of relevant work experience required.

Preferred Qualifications
Experience in data science tasks with Python is a plus
Experience working with complex data in a NoSQL-based environment
Experience with Python packages such as pandas, NumPy
Experience working with Linux server environments including shell scripting
Experience with aggregation tasks in MongoDB
Experience working with Neo4j graph database
Experience with graph data analysis and visualize
Experience with Python programming and MongoDB database
Experience with using source-controlling systems

FLSA
Exempt

Full Time/Part Time
Full Time

Number of Hours Worked per Week
40

Job FTE
1.0

Work Calendar
Fiscal

Job Category
Information Technology

Benefits Eligible
Yes - Full Benefits

Rate of Pay
DOE

Compensation Type
salary at 1.0 full-time equivalency (FTE)

Grade
8

Career Stream and Level
PC1

Job Family
Applications Development

Job Function
Information Technology

Type of criminal background check required:
Name-based criminal background check (non-security sensitive)

Number of Vacancies
1

Target Hire Date

Expected End Date

Contact Information for Candidates
Iqbal Hossain
hossain@arizona.edu

Open Date
5/11/2023

Open Until Filled
Yes

Documents Needed to Apply
Resume and Cover Letter

Special Instructions to Applicant

Diversity Statement
At the University of Arizona, we value our inclusive climate because we know that diversity in experiences and perspectives is vital to advancing innovation, critical thinking, solving complex problems, and creating an inclusive academic community. As a Hispanic-serving institution, we translate these values into action by seeking individuals who have experience and expertise working with diverse students, colleagues, and constituencies. Because we seek a workforce with a wide range of perspectives and experiences, we provide equal employment opportunities to applicants and employees without regard to race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, gender identity, or genetic information. As an Employer of National Service, we also welcome alumni of AmeriCorps, Peace Corps, and other national service programs and others who will help us advance our Inclusive Excellence initiative aimed at creating a university that values student, staff and faculty engagement in addressing issues of diversity and inclusiveness.",79143,10000+ Employees,College / University,Education,Colleges & Universities,1885,$1 to $5 billion (USD),AZ,138,data engineer,na,"['nosql', 'shell', 'python']",[],[],"['mongodb', 'neo4j']",[],[],bachelor,0-2 years
"Vertex, Inc.",3.9,Remote,Sr. Data Engineer- Cloud (Remote),"Job Description:
This position is responsible for performing analysis, design, implementation, testing, maintenance, and support tasks for data-intensive software applications programming. Improves system quality by identifying issues and common patterns and developing standard operating procedures. Enhances applications by identifying opportunities for improvement, making recommendations, and designing and implementing systems. ESSENTIAL JOB FUNCTIONS AND RESPONSIBILITIES: · Prepare technical design specifications based on functional requirements and analysis documents. · Implement, test, maintain and support software, based on technical design specifications. · Improve system quality by identifying issues and common patterns, and developing standard operating procedures · Enhance applications by identifying opportunities for improvement, making recommendations, and designing and implementing systems · Maintain and improve existing codebases and peer review code changes · Liaise with colleagues to implement technical designs · Investigating and using new technologies where relevant · Provide written knowledge transfer material · Review functional requirements, analysis, and design documents and provide feedback. · Assist customer support with technical problems and questions. · Assist and mentor other development staff. · Perform special assignments. · Participate in architecture and code reviews. · Lead or participate in other projects or duties. · Occasional travel required. (Up to 5%) · Participate in other projects or duties. SUPERVISORY RESPONSIBILITIES: · N/A KNOWLEDGE, SKILLS, AND ABILITIES: * * * * * * * · Ability to network with key contacts outside own area of expertise. * * · Must possess strong interpersonal, organizational, presentation and facilitation skills. · Must be results oriented and customer focused. · Must possess good organizational skills. * * EDUCATION AND TRAINING: · Bachelor’s degree in computer science, Information Systems, or related field; or equivalent combination of education/experience. Master’s degree is a plus. · 7 years or more of extensive experience developing mission critical and low latency solutions. · At least 4 years of experience with developing and debugging distributed systems and data pipelines in the cloud. AWS is a must. · Extensive experience with SQL cloud databases like Snowflake (a must-have experience), and MS SQLServer. Experience with NoSQL databases like AWS DynamoDB and Azure Cosmos is a plus. · Good understanding of data modeling, ETL, data curation, and big data performance tuning. · Experience with Business Intelligence tools is a plus. · Experience working with AWS and/or Azure DevOps and extensive debugging experience. · Ability to code in one or more languages like Python, Java, Scala. · An understanding of unit testing, test driven development, functional testing, and performance · Knowledge of at least one shell scripting language. Other Qualifications The Winning Way behaviors that all Vertex employees need to meet the expectations of each other, our customers, and our partners. • Communicate with Clarity - Be clear, concise, and actionable. Be relentlessly constructive. Seek and provide meaningful feedback. • Act with Urgency - Adopt an agile mentality - frequent iterations, improved speed, resilience. 80/20 rule - better is the enemy of done. Don’t spend hours when minutes are enough. • Work with Purpose - Exhibit a ""We Can"" mindset. Results outweigh effort. Everyone understands how their role contributes. Set aside personal objectives for team results. • Drive to Decision - Cut the swirl with defined deadlines and decision points. Be clear on individual accountability and decision authority. Guided by a commitment to and accountability for customer outcomes. • Own the Outcome - Defined milestones, commitments and intended results. Assess your work in context, if you’re unsure, ask. Demonstrate unwavering support for decisions. COMMENTS: The above statements are intended to describe the general nature and level of work being performed by individuals in this position. Other functions may be assigned, and management retains the right to add or change the duties at any time.",112889,1001 to 5000 Employees,Company - Public,Information Technology,Software Development,1978,$100 to $500 million (USD),Remote,45,data engineer,senior,"['scala', 'shell', 'java', 'nosql', 'sql', 'python']","['snowflake', 'aws', 'azure']",[],"['dynamodb', 'snowflake']",[],[],bachelor,2-5 years
Delaware North,3.6,"Buffalo, NY",Data Engineer,"The Opportunity
Delaware North Global Headquarters is hiring a Data Engineer to join our Information Technology team in Buffalo, New York. As a Data Engineer, you will work with programming languages, frameworks, databases, front-end tools, back-end tools, and applications connected via APIs to collect raw data and transform the data into canonical models. The Data Engineering team is tasked with harmonizing and enhancing the data to provide trusted datasets to our consumers. The work our data manager team does forms the foundation of company initiatives to help automate business processes and gather insights, to help the company make more informed decisions.
Minimum - Anticipated Maximum Salary: $70700 - $93700 / year
The advertised pay range represents what we believe at the time of this job posting, that we would be willing to pay for this position. Only in special circumstances, where a candidate has education, training, or experience that far exceeds the requirements for the position, would we consider paying higher than the stated range. Information on our comprehensive benefits package can be found at https://careers.delawarenorth.com/whatweoffer.

At Delaware North, we care about our team member’s personal and professional journeys. These are just some of the benefits we offer:
Health, dental, and vision insurance
401(k) with company match
Performance bonuses
Paid vacation days and holidays
Paid parental bonding leave
Tuition and/or professional certification reimbursement
Generous friends-and-family discounts at many of our hotels and resorts
Responsibilities
Utilize technology stacks such as Apigee, Python, Django, Apache Airflow, MongoDB, PostgreSQL, and Amazon Web Services such as Lambda, EBS, S3, SQS, ECS, RDS, EC2 and Redshift to build datasets.
Design and implement project-based solutions.
Implement data platform improvements and new features.
Assist the support team with the resolution of data platform bug fixes.
Interface with clients, vendors, and internal users of the data platform on understanding the data.
Author documentation for standard operating procedures and knowledge base articles.
Develop integration tests to validate solutions.
Qualifications
Bachelor’s degree or equivalent from an accredited college or university in Computer Science, Information Systems, or similar STEM field preferred.
Minimum of 2 years of experience developing data pipeline ETL processes.
Extensive experience following End to End Agile Development Lifecycle and writing in SQL and Query.
Data persistence methods such as NoSQL and RDBMS, data structures and formats such as JSON, XML, and parquet.
Cloud computing experience as it relates to event-based serverless architecture, AWS preferred.
Extensive experience with handling large data sets, high performance computing, building high performance solutions and data integration projects.
Technical specification and use case documentation, such as UML, Domain and Entity-Relationship Modeling, Business Process Notation.
Must be legally authorized to work in the US without sponsorship.
Who We Are
At Delaware North, you’ll love where you work, who you work with, and how your day unfolds. Whether it’s in sporting venues, casinos, airports, national parks, iconic hotels, or premier restaurants, there’s no telling where your career can ultimately take you. We empower you to do great work in a company with 100 years of success, stability and growth. If you have drive and enjoy the thrill of making things happen - share our vision and grow with us.
Delaware North Companies, Incorporated and its subsidiaries consider applicants for all positions without regard to race, color, religion, creed, gender, national origin, age, disability, marital or veteran status, sexual orientation, or any other legally protected status. Delaware North is an equal opportunity employer.",82200,10000+ Employees,Company - Private,Restaurants & Food Service,Catering & Food Service Contractors,1915,$1 to $5 billion (USD),NY,108,data engineer,na,"['sql', 'nosql', 'python']","['aws', 'redshift']",[],"['mongodb', 'postgresql']",[],[],bachelor,0-2 years
Veolia,3.8,"Paramus, NJ",Senior Data Engineer,"Company Description Veolia Group aims to be the benchmark company for ecological transformation. With nearly 220,000 employees worldwide, the Group designs and provides game-changing solutions that are both useful and practical for water, waste and energy management. Through its three complementary business activities, Veolia helps to develop access to resources, preserve available resources and replenish them. In 2021, the Veolia group provided 79 million inhabitants with drinking water and 61 million with sanitation, produced nearly 48 million megawatt hours and recovered 48 million tonnes of waste. Veolia Environnement (Paris Euronext: VIE) achieved consolidated revenue of 28,508 billion euros in 2021. www.veolia.com
Job Description
Develop and operate data management tools, monitoring data flows, data quality, data cleansing and data processing
Create and document logical data integration strategies for data flows between disparate systems and the enterprise data warehouse/data lakes
Collaborate with different stakeholders (engineers, data stewards) to collect required data from internal and external systems
Work in an Agile environment that focuses on collaboration and teamwor
Improve and extend existing data infrastructure services
Monitor production job schedule and correct job failures in a timely manner

Qualifications
MS degree in Computer Science or computer related field from an accredited institution.
5+ years hands proven experience as a Data Engineer or similar role.
5+ years of strong experience building, running and maintaining datalake(s) and warehouse(s) in a cloud environment.
More than 4 years of experience developing with Python.
4+ years performing with production environments in a DevOps culture managing code composed of multi-developer teams, following industry best practices.
4+ years SQL development experience.
Experience with data modeling
4+ years bash scripting experience.
Strong experience with Git, CI/CD (preferably GitLab) and Docker.
Experience deploying and running services in Cloud Big Data platforms such as BigQuery and Snowflake.
Strong experience with GCP services.
Experience designing and building data pipelines using tools like Apache Beam, CDAP (Data Fusion) or other ETLs.
Knowledge with CDC design patterns and their challenges.
Experience with DAG workflows orchestration such as Apache Airflow.
Experience with NoSQL databases is a plus (i.e Firestore, MongoDB).
Experience designing and developing APIs is a plus (i.e using FastAPI, Flask).

(Nice to have) Google Cloud Data Engineer
Abilities:
Being able to work in a large company with different stakeholders.
Embrace mentorship through design sessions, code reviews, and community building.
Take ownership and support solutions you develop.
Value collaboration with other members of the team.
Have a product mindset.
Good communication.

Additional Information

A subsidiary of Veolia group, Veolia North America (VNA) offers a full spectrum of water, waste and energy management services, including water and wastewater treatment, commercial and hazardous waste collection and disposal, energy consulting and resource recovery. VNA helps commercial, industrial, healthcare, higher education and municipality customers throughout North America. Headquartered in Boston, Mass., Veolia North America has more than 10,000 employees working at more than 350 locations across the continent. www.veolianorthamerica.com As an inclusive company, Veolia is committed to diversity and gives equal consideration to all applications, without discrimination. We are an Equal Opportunity Employer! All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. As an inclusive company, Veolia is committed to diversity and gives equal consideration to all applications, without discrimination. We are an Equal Opportunity Employer! All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.
Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. The Company reserves the right to modify this information at any time, subject to applicable law.",119169,10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1853,$1 to $5 billion (USD),NJ,170,data engineer,senior,"['sql', 'nosql', 'python']","['snowflake', 'google cloud']",[],"['mongodb', 'snowflake']",[],"['gitlab', 'bash', 'docker']",,+10 years
Crisis Prevention Institute,3.0,"Milwaukee, WI",Senior Data Engineer,"Our Story:
Crisis Prevention Institute Inc. is the worldwide leader in evidence-based de-escalation and crisis prevention training, and dementia care services. Since 1980, we’ve helped train more than 15 million people within service-oriented industries including education, healthcare, behavioral health, long-term care, human services, security, corrections, corporate, and retail.

At CPI, we are dedicated to changing behaviors and reducing conflict for the Care, Welfare, Safety, and SecuritySM of everyone. We believe in the power of empathy, compassion, and meaningful connections. We believe personal safety and security are the antidotes to fear and anxiety. It’s a philosophy that is central to everything we do, and traces back to our beginning. It is what defines and differentiates us, and informs our core beliefs.
As a member of the team, you can expect to:
Make a difference through your work – You’ll be proud to tell your family and friends about what you do.
Gain significant career experience only obtained within a fast-growing organization – Entry-level roles through executive leadership.
Feel fulfilled and have fun – We work hard but make the time to build meaningful relationships and celebrate the wins.

The Role:
The Senior Data Engineer will focus on quality engineering best practices to meet and exceed internal and external client expectations. In this position, you will analyze, design, develop, test and document solutions supporting data integration, performance tuning, and data modeling to drive organization growth objectives. The Senior Data Engineer will define the standards for data architecture, platform architecture, and data quality and governance. This role is responsible for ensuring that the function is aligned with the overall CPI organization and continuously works to meet critical service levels in access, delivery and security.
What You Get To Do Everyday:
Co-architect CPI’s next gen cloud data analytics platform.
Increase operating efficiency and adapt to new requirements.
Monitor and maintain the health of solutions generated.
Support and enhance our data-ops practices.
Provide task breakdowns, identify dependencies, and provide effort estimates.
Model data warehouse entities in Erwin.
Build data transformation pipelines with Data Build Tools (DBT).
Evaluate the latest technology trends and develop proof-of-concept prototypes that align with CPI opportunities.
Develop positive relationships with clients, stakeholders, and internal teams.
Understand business goals, drivers, context, and processes to suggest technology solutions that improve the organization.
Work collaboratively on creative solutions with engineers, product managers, and analysts in an agile like environment.
Perform, design, and code reviews.
Perform other position-related duties as assigned.
You Need to Have:
Bachelor’s degree in computer engineering, computer science, data science, or related field
Two years or more experience designing and implementing data warehouses in Snowflake
Eight years or more experience working with data modeling, architecture and engineering
Experience with all core software development activities, including requirements gathering, design, construction, and testing
Experience performing data transformation using DBT
Experience working with DQ products such as Monte Carlo, BigEye, or Great Expectations
Experience with Azure DevOps (Repos, Pipelines, Boards, Wiki, Test Plans)
Experience with formal software development methodologies including Software Development Life Cycle (SDLC), Agile or SCRUM
Experience building high-performance and highly reliable data pipelines
Experience Knowledge of data warehouse design patterns (star schema, data vault)
Experience building dashboards with business integrations tools
Knowledge of DataOps
with cloud-based compute, storage, integration and security patterns
Knowledge and understanding of RESTful APIs
Knowledge of current data engineering trends, best practices, and standards
Knowledge of SQL and Python
Ability to work in a collaborative environment
Ability to facilitate evaluation of technologies and achieve consensus on technical standards and solutions among a diverse group of information technology professionals
Ability to work in an organization driven by continuous improvement or with equivalent focus on process improvement
Ability to manage multiple, competing priorities and attain the best possible outcomes for the organization
Excellent verbal and written communication and effective listening skills
We'd Love to See:
Experience in delivering an end-to-end data analytics platform using modern data stack components
Experience with AI/ML
SnowPro Advanced Certification
DBT Analytical Engineer Certification
What We Offer:
Competitive salary
Comprehensive benefits package
401k
PTO
Health & Wellness Days
Paid Volunteer Time Off
Continuing education and training
Hybrid work schedule
???????Crisis Prevention Institute is an Equal Opportunity Employer that does not discriminate against any applicant or employee on the basis of age, race, color, ethnicity, national origin, citizenship, religion, creed, sex, sexual orientation, gender, gender identity, or expression (including against any individual that is transitioning, has transitioned, or is perceived to be transitioning), marital status or civil partnership/union status, physical or mental disability, medical condition, pregnancy, childbirth, genetic information, military and veteran status, or any other basis prohibited by applicable federal, state, or local law. The Company will consider for employment qualified applicants with criminal histories in a manner consistent with local and federal requirements. Our management team is dedicated to this policy with respect to recruitment, hiring, placement, promotion, transfer, training, compensation, benefits, employee activities, and general treatment during employment.",95913,201 to 500 Employees,Company - Private,Education,Education & Training Services,1980,$25 to $100 million (USD),WI,43,data engineer,senior,"['sql', 'python']","['snowflake', 'azure']",[],"['snowflake', 'dbt']",[],[],bachelor,
Skillable,4.7,Remote,Senior Data Engineer,"Job Type
Full-time
Description


Skillable is a 100% remote and virtual tech company that’s modernizing the world of training. Come share your professional magic with highly talented, drive and fun colleagues who believe in the power of “skilling.” Experience what a true team focused on doing the right thing feels like!

Our people and talent are what make us great and fun! We work together to create amazing solutions and experiences for our customers and their clients. We utilize our employees’ personal strengths to help our company grow and ensure our team is living their best, authentic life. We don’t just share our appreciation for our team members once a year with a branded mug—it’s shared on a daily basis. Our remote work environment blends the demands of work and life without the added pressure of commuting or feeling guilty about leaving early to visit the dentist.

Come work with us and learn what teamwork and integrity blended with an emphasis on well-being and balance can do for your career!

The Senior Data Engineer is a highly skilled data professional responsible for executing and guiding on the strategic development and maintenance of our data resources. Responsible for a broad range technical and detailed initiatives including being a thought partner to leadership in the architecture and creation of new data products to working with security and infrastructure resources to ensure the health of databases and data pipelines. This role will work closely with cross-functional teams, including data science, software engineering and product management to build, improve and maintain our data infrastructure, and contribute to the overall success of the company. Partner with leadership to give insights to the team of data engineers and database administrators, ensuring project organization around databases and focusing on continuous stability, security, and performance of databases to contribute to the overall success of the company.
Requirements
Provide technical data expertise and guidance to support core business data systems and data-driven processes.
Serve as a key contributor in the design, development, and maintenance of data pipelines for real-time and batch processing with the support and partnership of other technical team members.
Design, develop, execute on the implementation and management of databases.
Build and optimize data models for efficient querying and analysis.
Monitor and troubleshoot data pipeline and database issues.
Design and implement database structures and ensure their stability, reliability, and performance.
Troubleshoot database performance and optimize as necessary.
Assist in planning database backups and maintenance, disaster recovery, and replication.
Continuously improve data infrastructure and processes.
Provide technical knowledge and experience in serving as a lead in the process of creating, delivering, and scaling “Data as a Service” projects or products and data-related features.
Collaborate with data scientists, engineers, and analysts to understand data needs and requirements.
Parter with outside teams, stakeholders and executives to understand business needs and requirements.
Build and develop key relationships across the enterprise to help build internal enthusiasm and momentum for project initiatives.
Regularly assist leadership and key Company stakeholders in the development and implementation of data governance policies to ensure data quality.
Stay current with new database technologies and assist in upgrading systems as necessary.
Regularly review current and new data technologies and industry trends and implement as appropriate.
Assist in guiding, influencing and developing a company culture of data-driven decision making.
Assist leadership in mentoring a growing team of data professionals.
Represent Skillable at industry events and conferences as required.
Support and promote the company values through positive interactions with both internal and external partners and customers on a regular basis.
Other strategic business initiatives or cross-functional project involvement as required.
Qualifications
Bachelor’s degree in related field (product management, marketing, business, finance, product development, project management, etc.) or equivalent work experience.
10+ years of relevant professional experience working intimately in data engineering or database administration.
Experience as a functional leader on key projects or teams of technical talent or providing guidance on cross-functional work groups or project teams successfully preferred, but not required.
Experience mentoring and coaching others within an assigned function or cross-functionally.
Proven track record of strong problem-solving and business analysis skills using data creatively.
Naturally inquisitive with a desire to solve problems and dig into detailed analysis.
Experience working cross-functionally and promoting collaborative partnerships to drive results.
Proven ability to communicate effectively to various audiences/levels including leadership through various mediums.
Ability to take complex data and problems and deconstruct it into a concise, impactful message(s).
Ability to present and convey material both formally and informally to all levels of the organization.
Thorough understanding (or willingness to learn expeditiously) of business operations and processes.
Provide a high-level of confidence, integrity, enthusiasm and professional presence.
Experience with real-time data systems, data warehousing, ETL technologies, data modeling, and data governance.
Experience with cloud computing platforms such as AWS, GCP, or Azure.
Strong knowledge of database management systems, especially SQL Server.
Experience with cloud-based databases, especially Azure SQL.
Experience managing high-availability and -uptime databases.
Strong programming skills in SQL.
Demonstrated ability to prioritize and manage workload and meet project deadlines.
Interest and ability in mentoring other team members as applicable.
Strong MS Office, web conferencing and internal communication software experience.
Detail oriented and organized.
What’s in it for You? Rewards and Perks

We believe in providing a suite of benefits that ensure our employees know we appreciate them as people first. Skillable wants to be a company that promotes physical, emotional and all around well-being through our benefit offerings! Subject to eligibility requirements, the Company offers comprehensive benefits including:
Fully remote with a monthly stipend to pay for office services and supplies
Medical (2 plan options), dental (2 plan options), vision, health savings account with generous employer contributions, healthcare spending accounts, dependent care spending accounts, EAP, group paid life insurance, group paid STD and LTD and voluntary life/AD&D insurance, accident and critical illness options.
401(k) with Company match, tuition reimbursement, healthy lifestyle reimbursements.
Open PTO, Paid holidays, bereavement leave, parental leave, caregiver leave and paid FMLA leave.
Friends and Family Friday to end our standard workweek at 2pm local time; Full company closure during the 4th of July holiday week.
Access to pet insurance; Access for employees and dependents to Skillable learning opportunities through our product and more!
Working Conditions

The job conditions for this position are in a remote home office setting, requiring a space that supports privacy and focus to attend to regular and frequent video and voice calls. Employees in this position use PC and phone on an on-going basis throughout the day. Travel is anticipated up to 10% of the time for critical business meetings and industry events.",112889,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2004,Unknown / Non-Applicable,Remote,19,data engineer,senior,['sql'],"['azure', 'aws']",[],['sql server'],[],[],bachelor,+10 years
(ISC)2,3.3,Remote,Data Engineer,"Overview:
(ISC)² is an international nonprofit membership association focused on inspiring a safe and secure cyber world. (ISC)² offers a portfolio of credentials that are part of a holistic, pragmatic approach to security. Our association of candidates, associates, and members, nearly 330,000 strong, is made up of certified cyber, information, software, and infrastructure security professionals who are making a difference and helping to advance the industry. Our vision is supported by our commitment to educate and reach the general public through our charitable foundation – The Center for Cyber Safety and Education™. For more information on (ISC)², visit www.isc2.org, follow us on Twitter, or connect with us on Facebook and LinkedIn.

We are committed to an inclusive and equitable environment that values the unique perspectives and experiences of our entire workforce. We strive for a true sense of belonging for all our employees and to foster authenticity, trust, empowerment and connectedness that leads to everyone’s success. For more information, visit www.isc2.org/dei.
Position Summary:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities:
Implement Azure Data Services and tools to ingest , egress and transform data from multiple sources and create and maintain optimal data pipeline architecture.
Responsible for creating ETL pipeline with Azure Ecosysem like Azure Synapse ,Azure Data Factory.
Implement and support ETL related jobs to curate , transform and aggregate data to create models for end user consumption.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Azure from Sales Force ,Pearson Vue etc.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Maintain ownership of a given pipeline or domain and raise flags to leadership when appropriate regarding architectural concerns.
Demonstrates commitment to valuing diversity and contributing to an inclusive working and learning environment.
Miscellaneous duties as assigned.
Qualifications:
Bachelor’s degree in computer science or other equivalent degree
7+ years of experience in a Data Engineer role.
Experience with Cloud Data warehouses such as Azure , AWS and Google BigQuery
Experience with big data tools: Hadoop, Spark and Kafka.
Experience with Azure Dedicated and Azure Synapse
Experience with Analytics one or more of the following reporting tools; Tableau, PowerBI, Looker, Domo and Microstrategy
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++,etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Skills/Competencies:
Ability to demonstrate and support the 5 Company Core Values: Integrity, Excellence, Unity, Accountability, Agility
Ability to build an inclusive culture that encourages, supports and celebrates diversity; serve as a role model to promote DEI best practices.
Strong analytic skills related to working with unstructured datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Physical & Mental:
Up to 5% travel may be required.
Work normal business hours and extended hours when necessary.
Remain in a stationary position, often standing or sitting, for prolonged periods
Regular use of office equipment in a remote environment such as a computer/laptop and monitor computer screens
Dexterity of hands and fingers to operate a computer keyboard, mouse, and other computer components
#LI-remote
Equal Employment Opportunity Statement:
All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic as protected by applicable law.
Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.",112889,201 to 500 Employees,Nonprofit Organization,Management & Consulting,Membership Organizations,1989,$25 to $100 million (USD),Remote,34,data engineer,na,"['java', 'sql', 'python', 'nosql']","['azure', 'aws']","['looker', 'tableau']",[],"['kafka', 'spark', 'hadoop']",[],bachelor,+10 years
University of California San Francisco,4.0,California,Software and Data Engineer,"The software and data engineer role includes the design, build, configuration, and support of research projects within UCSF’s APeX Enabled Research (AER) team. Most projects will be in partnership with other UCSF technical teams and involve highly customized research solutions. Communication skills and inventive technical solutioning are crucial.

The AER team provides a large array of services to the UCSF Research community, including project consultation, grant support, budget estimations, and project implementation and support. Project examples include:
Development of EHR-based interventions via clinical trials embedded within healthcare delivery systems to generate scientific evidence while delivering healthcare.
Enabling UCSF researchers with algorithms, digital tools, and/or clinical interventions with strong evidence of feasibility and acceptability.
Develop technical approaches and budgets in order to implement these tools within the electronic medical record.
Supporting the development of scalable, low-cost infrastructure to enable ongoing research.

Specifically, the software and data engineer will develop, implement, and maintain infrastructure and applications that support research informatics priorities and research projects priorities The largest piece of infrastructure for which they will be responsible will be the Health Informatics Platform for Advanced Computing (HIPAC), cloud infrastructure which supports deployment and maintenance of artificial intelligence in the health system. The engineer will work closely with data scientists and health informatics experts in the ongoing design and optimization of this infrastructure, in addition to other complex software applications.

Competitive applicants for this position are software engineers who have experience writing and maintaining production-ready and scalable applications. Candidates are ideally proficient in Python and SQL, have working experience with common DevOps and CI/CD tools such as docker, and have experience developing cloud-based applications.
To see the salary range for this position (we recommend that you make a note of the job code and use that to look up): TCS Non-Academic Titles Search (ucop.edu)
Please note: The compensation ranges listed online for roles not covered by a bargaining unit agreement are very wide, however a job offer will typically fall in the range of 80% - 120% of the established mid-point. An offer will take into consideration the experience of the final candidate AND the current salary level of individuals working at UCSF in a similar role.
For roles covered by a bargaining unit agreement, there will be specific rules about where a new hire would be placed on the range.
To learn more about the benefits of working at UCSF, including total compensation, please visit: https://ucnet.universityofcalifornia.edu/compensation-and-benefits/index.html
Department Description
The University of California, San Francisco (UCSF) Department of Information Technology Academic Research Systems (ARS) group is chartered to provide data services and infrastructure that support the UCSF Research Community’s computing and analytic requirements through centralized informatics services in the areas of Data, Tools, Secure Compute Environments, and Consulting Services.
Required Qualifications
Bachelor's degree in Computer Science, Computer Engineering, or related area and/or equivalent experience/training.
Demonstrated advanced knowledge of full software development lifecycle
Advanced experience with Python; ability to write clean, efficient, and production-level Python code
Advanced experience with SQL (e.g., SQLServer, PostgreSQL)
Experience working with DevOps and CI/CD pipeline toolsets such as Docker, Jenkins, GitHub, etc.
Demonstrated experience in developing complex, automated testing
Advanced experience with cloud-based architecture in platforms such as AWS, GCP, Azure, etc.
Demonstrated effective communication and interpersonal skills
Demonstrated ability to communicate technical information to technical and non-technical personnel at various levels in the organization
Self-motivated and works independently and as part of a team. Able to learn effectively and meet deadlines
Demonstrated broad problem-solving skills
Demonstrated ability to interface with management on a regular basis
Strong interest in working with healthcare data and understanding the challenges that face complex healthcare delivery systems
Ability to work in a highly matrixed organization, reporting to multiple teams
Preferred Qualifications
Master’s degree or Ph.D. in Computer Science, Computer Engineering, or related area and/or equivalent experience/training.
Cloud development certifications such as AWS Developer – Associate
Epic Clarity or Clinical Data Model
Demonstrated experience with data modeling, data warehousing, and building ETL pipelines
Familiar with data analysis and machine learning tools such as Jupyter, Pandas, scikit-learn, Numpy/Scipy, TensorFlow, etc.
Familiar with data visualization tools (e.g., Tableau).
Experience with the Epic Clarity Data structures and data
About UCSF
The University of California, San Francisco (UCSF) is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It is the only campus in the 10-campus UC system dedicated exclusively to the health sciences. We bring together the world’s leading experts in nearly every area of health. We are home to five Nobel laureates who have advanced the understanding of cancer, neurodegenerative diseases, aging and stem cells.
Pride Values
UCSF is a diverse community made of people with many skills and talents. We seek candidates whose work experience or community service has prepared them to contribute to our commitment to professionalism, respect, integrity, diversity and excellence – also known as our PRIDE values.

In addition to our PRIDE values, UCSF is committed to equity – both in how we deliver care as well as our workforce. We are committed to building a broadly diverse community, nurturing a culture that is welcoming and supportive, and engaging diverse ideas for the provision of culturally competent education, discovery, and patient care. Additional information about UCSF is available at diversity.ucsf.edu

Join us to find a rewarding career contributing to improving healthcare worldwide.
Equal Employment Opportunity
The University of California San Francisco is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Organization
Campus
Job Code and Payroll Title
000652 APPLICATIONS PROGR 4
Job Category
Clinical Systems / IT Professionals
Bargaining Unit
99 - Policy-Covered (No Bargaining Unit)
Employee Class
Career
Percentage
100%
Location
Mission Center Building (SF)
Shift
Days
Shift Length
8 Hours
Additional Shift Details
Mon - Fri 8:00 to 5:00",112889,10000+ Employees,College / University,Education,Colleges & Universities,1864,$25 to $100 million (USD),California,159,data engineer,na,"['sql', 'python']","['azure', 'aws']",['tableau'],['postgresql'],[],['docker'],bachelor,
ArchWell Health,4.0,Remote,IT - Data Engineer,"ArchWell Health is a new, innovative healthcare provider devoted to improving the lives of our senior members. We deliver best-in-class care at comfortable, accessible neighborhood clinics where seniors can feel at home and become part of a vibrant, wellness-focused community. Our members experience greater continuity of care, as well as the comfort of knowing they will be treated with respect by people who genuinely care about them, their families, and their communities.

ArchWell Health requires all new hires to provide proof that they are fully vaccinated from COVID-19, or represent that they will be fully vaccinated within 30 days of their start date.

Duties/Responsibilities:
Build data integrations from internal and external sources to centralize data into a Data Warehouse environment.
Monitor data integration operations, data quality, troubleshoot, and resolve problems.
Profile data sources and map to target table formats.
Develop and monitor data quality processes and address problems.
Develop, unit test and system test integration components.
Create support documentation describing the functionality of the integrations.
Participating in technical design & requirements gathering meetings.
Participate in planning and implementing data integration and data migration activities.
Perform QA tests to ensure data integrity and quality.
Research data issues between source systems and the data warehouse.
Required Skills/Experience:
Bachelor’s degree required; Master's degree (in data science, computer science or MIS, mathematics, engineering, or related field) preferred.
5+ years of prior experience in Data Management / ETL / ELT / Data Warehousing
Experience in writing Data Quality routines for cleansing of data and capturing confidence score
Experience with master data management
Strong knowledge of Structured Query Language (SQL) and Transact-SQL (T-SQL)
Experience using scripting languages such as JavaScript or Python
Experience Healthcare data models, datasets, and source systems (e.g. EHR, claims, labs, etc.)
Experience with healthcare reference data (ICD, CPT etc.)
Experience with agile delivery methodologies
Data Modeling experience preferred.
Strong organizational, administrative, and analytical skills required.
Experience managing and working in cloud environments such as Amazon Web Services or Azure
Knowledge of HIPAA; ability to implement systems and processes in accordance with regulations
Excellent interpersonal communication skills, both written and verbal
ArchWell Health is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to their race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other protected classification.",112889,201 to 500 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],[],[],bachelor,+10 years
McDonald's Corporation,3.5,"Chicago, IL","Data Engineer, Global Data, CRM","Company Description

McDonald’s evolving Accelerating the Arches growth strategy puts our customers and people first, and leverages our competitive advantages to strengthen our brand. We are recognized on lists like Fortune’s Most Admired Companies and Fast Company’s Most Innovative Companies.
Doubling Down on the 4Ds (Delivery, Digital, Drive Thru, and Development)
Our growth pillars emphasize the important role technology plays as the leading, global omni-channel restaurant brand. Technology enables the organization through digital technology, and improving the customer, crew and employee experience each and every day.
Global Technology forging the way
Leading the digitization of our business is the Technology organization made up of intrapreneurs who build industry defining tech using the latest innovations and platforms, like AI and edge computing to deliver on the next set of cutting-edge opportunities for the business. At McDonald’s you get to solve technology innovation challenges at an incredible scale, and work across global teams who are always hungry for a challenge. This provides access to exciting career paths for technologists. It’s bonus points when you get to see your family and friends use the tech you build at their favorite McD restaurant.

Job Description

We are seeking a highly motivated individual for the Data Engineering Manager for the Marketer Customer Data Platform team on Global Data and Architecture. This role will manage, develop, and lead the execution and implementation cloud-based data delivery systems for the MCD Customer Data Platform. This role will specialize in leading and managing the engineering teams in both onshore and offshore locations, as well as working with the relevant product management teams on overall product direction and implementation of the product vision.
What you will do:
Lead and manage the day-to-day operations for the CRM Data Engineering team with both, onshore and offshore, resources to with a focus on CDP/ CRM customer data technologies
Independently drive efforts in marketing technology and automation, collaborating with key stakeholders and business units. Must be able to find marketing solutions alongside building a long-term strategic vision of a marketing ecosystem across a highly technical, complex data stack
Participate in activities of the data engineering team on the solution, design, architect, implementation, support, and monitoring of CRM & CDP related data products
Develop and implement monitoring and remediation structure and applications for critical data flows
Continuously develop and deploy solutions to streamline and enhance the processes, logic, and tactics around customer cohort identification, and targeting via corporate CRM, marketing, and advertising systems. Additionally, develop the overall best practice framework for use by
Gather business and functional requirements and translate into robust, scalable, and user-friendly solutions

Qualifications
Bachelor’s degree in Information Technology or related technical fields such as computer science, marketing tech, mathematics, engineering, etc.
Minimum 2-5 years of relevant work experience in data analytics, engineering, computer science, mathematics or similar logic based fields
Minimum 2 years hands on experience with complex marketing automation tech stacks, leveraging CDP tools such as mParticle, Segment, and Tealium
Strong analytics skills, root cause analysis experiences on internal and external data and processes to answer business questions and improve application performance
Comfortable partnering and working with a wide range of stakeholders at various levels of management and leadership
Ability to focus on details as well as big picture
Competence and comfort focusing on specific detail engineering questions, as well as “big picture” questions
Ability to leverage critical thinking, problem-solving, and analytical skills to improve user experience, adoption, and outcomes
Curious, task-oriented self-starter with a strong bias for action; Proven track record of “getting things done”
Self-motivated with ability to set priorities and mentor others in a performance driven environment.
Passion for technology and innovation & uncompromising attention to detail & passion for problem solving
Desired Skills

Extensive experience working with Agile methodologies and expertise in production deployment processes and continuous deployment best practices.
1-2 years of hands-on experience with Braze and/or similar market automation delivery platforms
1-2 years of hands on experience with marketing automation and technology experience, understanding key components of marketing technology stacks, including CDP tools such as mParticle, Tealium, or Segment
Experience with clickstream tracking architecture implementations, and knowledge of Adobe Analytics and/or Google Analytics
Demonstrated ability to problem solve in complex technology solutioning
Experience developing Confluence pages and working with Jira
Demonstrable knowledge of multiple programming languages leveraged in martech/adtech stacks – javascript, ruby (liquid logic), html/css, python etc.

Additional Information

McDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.com
McDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Nothing in this job posting or description should be construed as an offer or guarantee of employment.",104499,10000+ Employees,Company - Public,Restaurants & Food Service,Restaurants & Cafes,1955,$10+ billion (USD),IL,68,data engineer,na,['python'],[],[],[],[],[],bachelor,2-5 years
Certec Consulting,5.0,"Durham, NC",Oracle PL/SQL Data Engineer 1189,"Title: Oracle PL/SQL Data Engineer 1189
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B, Hire Friendly ) ONLY NO C2C
50-55/Hour W2



Location: hybrid 5 days onsite/month. Locations are Boston MA, Durham NC, Merrimack NH or Westlake TX
Duration: 9+ months, open ended

What does your team do?
This position is for equity team and will soon need another req for fixed income. Could place this candidate on either team. They take care of all things data within Asset Management, data transformation, data quality, building API.
Primarily responsible for the development of large scale data efforts tied to the cloud such as moving data to new cloud based solutions and building data lakes etc. To accomplish this the resource uses AWS, Python, Snowflake, and other data driven technologies
What are the top must have skills?
6-10 yrs
Oracle pl/sql sql is primary skill, AWS,
Go to our Website Job listing here: Job Listing 1189
Please download and complete this Matrix prior to submission.
PM 1189
Then
PLEASE USE EASY ""APPLY BUTTON"" (not just apply button) TO SUBMIT RESUME AND SKILLS MATRIX
Job Listing 1189
The send an email to the listers email address with just candidate name and job number. NO need to attached resume or anything else.
Thanks,
Jay Kernes
Certec Consulting, inc
Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",85804,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,NC,-1,data engineer,na,"['sql', 'go', 'python']","['snowflake', 'aws']",[],"['snowflake', 'oracle']",[],[],,
Republic National Distributing Company,3.8,"Atlanta, GA",Data Engineer - Senior,"Overview: The Senior Data Engineer is responsible for managing and organizing RNDC's enterprise data. They will translate requirements and designs into functional data pipelines while ensuring the continued quality and completeness of information. Senior Data Engineers will combine raw information from different sources to create consistent and machine-readable datasets that are easy to analyze and support company initiatives. They will support other Data Engineers and Data Analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They will also implement methods to improve data reliability and quality, improve data visibility and reduce effort through automation. Responsibilities:
Contribute on a team of data engineers through design, demand delivery, code reviews, release management, implementation, presentations, and meetings.
Mentor fellow data engineers and contribute to ongoing process improvements for the team
Evaluate business needs and objectives and align architecture/designs with business requirements
Build the data pipelines required for the optimal extraction, transformation, integration and loading of raw data from a wide variety of data sources
Assemble large, complex data sets and model our data in a way that meets functional / non-functional business requirements
Create data tools for analytics team members that assist them in generating innovative industry insights that provide our business a competitive advantage
Implement data tagging mechanisms and metadata management so data is accurately classified and visible to the organization
Build processes to help identify and improve data quality, consistency and effectiveness
Ensure our data is managed in a way that it conforms to all information privacy and protection policies
Use agile software development processes to iteratively make improvements to our data management systems
Identify opportunities for automation
Be an advocate for best practices and continued learning Qualifications: Bachelor's/Tech School degree in Computer Science, Information Systems, Engineering or equivalent and/or commensurate years of real-world experience in software engineering. 4+ years of relevant experience in data management3+ years in data engineering with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELTExperience with performance analysis and optimizationExperience in data acquisition, transformation and storage design using design principles, patterns and best practicesData engineering certification is a plus Informatica, Kafka, CDC, SQL, Irwin, Python, AWS (S3, Athena, Glue, Kinesis, Redshift), Spark, Scala, AI/MLWe are an Equal Opportunity employer.",97233,5001 to 10000 Employees,Company - Private,Retail & Wholesale,Wholesale,1898,$5 to $10 billion (USD),GA,125,data engineer,senior,"['sql', 'scala', 'python']","['aws', 'redshift']",[],[],"['kafka', 'spark']",[],bachelor,+10 years
LTIMindtree,3.8,"Ontario, CA",Senior Data Engineer,"SQL DB /Technical Skills
o Experience with SQL Server or an equivalent database product – Must have.
o Experience in troubleshooting and problem solving role – Must have.
o Experience with Relational Database management systems and concepts
o High Availability: Experience/Knowledge of Windows Server Cluster Services in On-Premise environments
o Experience in SQL Server Performance
o Experience in Database and Server Administration
o Should able to deploy and Troubleshoot SQL Server Failover Cluster Instances etc.
o Experience/Knowledge of Windows Server Cluster Services in Azure cloud
o Having Azure domain experiences is a big asset.
o Experience in a customer facing or customer support role
Time of operation: This track will be in 24X7 shift model. Therefore, candidates should be flexible to cover during holidays and weekends to accommodate MS roster

Disclaimer :L&T Infotech has an accommodation process in place and provides accommodations for applicants with disabilities. If you require a specific accommodation because of a disability or a medical need during our recruitment processes, please let us know so that arrangements can be made for the appropriate accommodations to be in place.

Job Segment: Database, System Administrator, SQL, Engineer, Technology, Customer Service, Engineering",108343,10000+ Employees,Company - Public,Information Technology,Information Technology Support Services,1997,Unknown / Non-Applicable,CA,26,data engineer,senior,['sql'],['azure'],[],['sql server'],[],[],,
OpenSea,4.6,Remote,"Senior Data Engineer, Infrastructure","OpenSea is the first and largest marketplace for non-fungible tokens, or NFTs. Applications for NFTs include collectibles, gaming items, domain names, digital art, and many other items backed by a blockchain. OpenSea is an open, inclusive web3 platform, where individuals can come to explore NFTs and connect with each other to purchase and sell NFTs. At OpenSea, we're excited about building a platform that supports a brand new economy based on true digital ownership and are proud to be recognized as Y Combinator's #3 ranked top private company.

When hiring candidates, we look for signals that a candidate will thrive in our culture, where we default to trust, embrace feedback, grow rapidly, and love our work. We also know how critical it is to celebrate and support our differences. Employing a team rich in diverse thoughts, experiences and opinions enables our employees, our product and our community to flourish. We are dedicated to equal employment opportunities regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. To help facilitate this, we support remote, hybrid or onsite work at either New York City, San Francisco or the Silicon Valley for the majority of our opportunities.

Our engineering team at OpenSea is in search of a strong and curious Data Engineer to take charge of our analytics and machine learning pipelines. As a member of our data engineering team, you will collaborate with other engineers, data analysts, data scientists, and product managers, contributing significantly to the growth of one of the most rapidly expanding NFT marketplaces in the Web3 ecosystem.
Responsibilities
Design, build, and maintain data pipelines from end-to-end, ensuring data accuracy, availability, and quality for the Analytics and Data Science teams
Collaborate closely with Data Scientists to understand data requirements, develop data models, and optimize data pipelines for advanced analytics and machine learning use cases
Develop and maintain scalable, efficient, and reliable ETL processes, using best practices for data ingestion, storage, and processing
Work with stakeholders to identify and prioritize analytics requirements, and build out necessary analytics tools and dashboards
Proactively monitor data pipelines, troubleshoot, and resolve data-related issues
Contribute to the continuous improvement of data engineering practices, including documentation, code reviews, and knowledge sharing
Desired Experience
5+ years of experience in data engineeringExperience with big data technologies such as Snowflake, Hadoop, Spark, Airflow, or Flink
Strong knowledge of AWS services, particularly those related to data storage, processing, and analytics (e.g., S3, Redshift, Glue, EMR, Kinesis, Lambda, and Athena)
Expert in SQL and proficiency in at least one programming language (Python, Go, Java)
Familiarity with data warehousing concepts and schema design principles (e.g., Star Schema, Snowflake Schema)
Strong problem-solving skills, a data-driven mindset, and a passion for working with large, complex datasets
Excellent communication and collaboration skills, with the ability to work effectively across teams and stakeholders

If you don't think you meet all of the criteria below but still are interested in the job, please apply. Nobody checks every box, and we're looking for someone excited to join the team.

The base salary for this full-time position, which spans across multiple internal levels depending on qualifications, ranges between $160,000 to $305,000 plus benefits & equity.

#LI-Remote",112889,201 to 500 Employees,Company - Private,Information Technology,Internet & Web Services,2017,Unknown / Non-Applicable,Remote,6,data engineer,senior,"['sql', 'java', 'go', 'python']","['snowflake', 'aws', 'redshift']",[],['snowflake'],"['flink', 'spark', 'hadoop']",[],,+10 years
Farm Credit Financial Partners,3.4,"Springfield, MA",Data Engineer III,"For over 25 years, Farm Credit Financial Partners, Inc. (FPI) has provided technology products and services to the Farm Credit System. We care deeply about the agricultural credit associations (ACAs) we serve through our mission of delivering trusted technology solutions to help American agriculture thrive. As a customer-owned service organization, we support six ACAs from Maine to California with over 62,000 customer-members and over $40 billion in loan volume . Everyone here contributes to the success of our customers, and to the vibrant culture that makes FPI a great place to work. Throughout the year, you will find us having fun and jamming out to FPI’s band, coming together to support local charities, and celebrating our wins together.
We offer a robust benefits package that includes competitive earnings, hybrid and remote work options, tuition reimbursement, generous 401(k) matching, and development opportunities through company-sponsored trainings and certifications.
Come grow with us: financialpartners.com .
Farm Credit Financial Partners, Inc. is an Equal Opportunity Employer, and all qualified applicants will receive consideration for employment without regard to age, race, color, national origin, sex or gender, religion, pregnancy, marital status, status as a veteran, sexual orientation, gender identity, disability, or any other characteristic protected by law. EEO / AA / Minorities / Female / Disabilities / Veterans

JOB SUMMARY: The Data Engineer III is responsible for transforming data that can be easily analyzed. The position will be responsible for expanding and optimizing our data and data pipeline for our Association partners. The Data Engineer III primarily works with project teams on developing new data platforms to support strategic initiatives in alignment with business and/or enterprise strategies.
ESSENTIAL FUNCTIONS:
Work with architects, modelers and other data engineers to implement design and assemble large, complex data sets that meet end user business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Participate in technical design and code reviews to ensure quality and best practices are maintained.
Perform as technical lead on smaller projects and be a collaborative member on larger projects.
Contribute to the effective data governance of the organization’s business data. This includes data quality, data management, data policies, business process management, and risk management surrounding the handling of organizational data.
ADDITIONAL FUNCTIONS:
Coach and mentor junior engineers
Communicate effectively with stakeholders regarding project status and delivery timeframe
Foster innovative team culture and process improvement during development phase
OTHER DUTIES: This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that
are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without
notice.

QUALIFICATIONS:
Bachelor’s degree in Computer Science or MIS, and 5+ years’ experience in data modeling and cloud technologies. Strong working experience with the Azure technology stack including:
Databricks
Data Factory
SQL
Python
Power BI
Experience with Agile and Waterfall methodologies
Strong organization, analytical, and communication skills
Proficiency in the following technologies: R, Microsoft Office Suite
Strong customer service focus (data consumers as customers)
Familiar with software development best practices
WORK ENVIRONMENT: Typical noise levels for an open, cubicle-styled environment.
PHYSICAL DEMANDS: This position requires periods of standing, walking, and the use of computer equipment. Additional physical demands include, but may not be limited to, talking or hearing, push/pull, stooping, kneeling, reaching w/hands and arms, and lifting at least 10 pounds.
Working off-hours nights and weekends may be required on occasion for mission-critical needs.
WORK AUTHORIZATION: Authorization to work in the United States is required.
REASONABLE ACCOMMODATION : Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.",119377,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1995,$25 to $100 million (USD),MA,28,data engineer,na,"['sql', 'r', 'python']","['databricks', 'azure']",['power bi'],[],[],[],bachelor,
Luttechub,4.0,"Columbus, OH",Lead Data Engineer / Data Architect,"Lead Data Engineer / Data Architect
Only candidate with years of experience: +7
Those authorized to work in the U.S. are encouraged to apply.
Remote
Job description
Responsibilities
Stitch and normalize sparse and noisy data across various data sources.
Undertake the preprocessing of structured and unstructured data
Design, develop, test, implement, and support technical solutions in full-stack development tools and technologies.
Work with a team of developers with deep experience in machine learning, distributed microservices, and full-stack systems
Utilize programming languages like Python, Java, and Open Source RDBMS and cloud-based data warehousing services such as Aurora or Big Query
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with engineering and product development teams
Deliver on timeline commitments where necessary
Build trust and strong relationships at all levels
Desirable Qualities:
Strike a balance between critical thinking and actual hands-on analyses.
Have a keen interest in learning data science while bringing engineering rigor to the team.
Research mindset- the ability to structure a project from idea to experimentation to prototype to implementation.
Be a driven and focused self-starter, great communicator, with exceptional follow-through. You aggressively tackle work and love the responsibility of being individually empowered.
Be resolute to overcome challenges that will inevitably arise.
Qualifications & Experience
Required
Must have designed, developed, and supported a complex software solution.
Proficient in SQL.
Proficient in dimensional/multidimensional data modeling.
Experience with graph, transactional, and operational data modeling is a plus.
Familiarity with all stages of the product development cycle. Experience maintaining engineering best practices, including defect tracking, design reviews, and appropriate testing.
Hold strong organizational and problem-solving skills.
Take a pragmatic, product-oriented approach.
Possess the ability to work cross-functionally with minimal supervision.
Preferred
B.S. in Computer Science, Electrical Engineering, Mathematics, Statistics, Physics, or similar quantitative fields/work experience.
Experience with cloud environments
7+ years of experience in application or data warehousing development
7+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)
4+ years of experience working on streaming data applications
5+ years of experience with Agile engineering practices
5+ years of experience developing Java-based software solutions, scripting and OOP languages
5+ years of experience with UNIX/Linux, including basic commands and shell scripting
4+ years of experience with GCP
4+ years of experience with Ansible / Terraform
3+ years in the healthcare industry and knowledge of their business practices.
Job Type: Contract
Pay: $65.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Columbus, OH 43081: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 7 years (Required)
data warehousing development: 7 years (Required)
scripting language (Python, Perl, JavaScript, Shell): 7 years (Required)
Work Location: Hybrid remote in Columbus, OH 43081",117000,201 to 500 Employees,Company - Public,,,-1,Unknown / Non-Applicable,OH,-1,data engineer,senior,"['sql', 'java', 'shell', 'python']",[],[],[],[],"['terraform', 'ansible']",,5-10 years
Intone Networks,4.5,Remote,Lead Data engineer,"MUST HAVE: - 10+ years of experience in: Technical solutioning and system architecture design Evaluation and Recommend of ETL tools Implementation of recommended ETL tools Strong in SQL and Python, with 3+ years hands-on coding experience with both Experience building automated big data pipelines Experience performing data analysis and data exploration Experience working in an agile delivery environment Strong critical thinking, communication, and problem solving skills Experience with big data frameworks (i.e. Hadoop and Spark) Experience with cloud-based platforms (i.e. Azure, GPC, AWS) Experience working in multi-developer environment, using version control (i.e. Git) Experience with real-time and streaming technology (i.e. Azure Event Hubs, Azure Functions Kafka, Spark Streaming) Experience with deployment/scaling of apps on containerized environment (i.e. Kubernetes, AKS) Experience partnering cross-functionally with other technical teams (i.e. data ingestion, data science, operational systems) to align priorities and achieve deliverable outcomes Experience with setting coding standards, performing code reviews, and mentoring junior developers Experience overseeing project delivery by mentoring junior technical developers PREFERRED TO HAVE: Previous healthcare experience and domain knowledge Exposure/understanding DevOps best practice and CICD (i.e. Jenkins) Exposure/understanding of containerization (i.e. Kubernetes, Docker) Experience with Snowflake and hands-on query tuning/optimization. Experience with orchestrating pipelines using tools (i.e. Airflow, Azure Data Factory) Experience with REST API/Microservice development using Python Full legal Name: Contact Number: Email: Current Location: Work Authorization: Years of Total Work Experience: DOB Linkedin: Pay rate Education Details and Years of Graduation:",112889,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$5 to $25 million (USD),Remote,-1,data engineer,senior,"['sql', 'python']","['azure', 'aws', 'snowflake']",[],['snowflake'],"['kafka', 'spark', 'hadoop']",['docker'],,+10 years
JM Family Enterprises,3.4,"Deerfield Beach, FL",Senior Data Engineer,"Sr. Data Engineer will work in the Actuary, Data & Analytics Department of Jim Moran and Associates Inc., a division of JM Family Enterprises, and will play a critical role in development and management of organizational data assets for data science, advanced analytics and self-service reporting to deliver business insights and drive actionable results.
You will interface with business stewards, data end-users and the data storage/architecture team, leveraging a common data platform, to fulfill data-related requests that meet business needs.
Key Responsibilities:
Acquire data from primary or secondary data sources, prepare, integrate, and maintain data structures in support of advanced analytics, self-service reporting, and strategic data initiatives
Work closely with management, data product team, and data end-users to prioritize business and information needs
Validate data for completeness and accuracy based on guidelines set by project stakeholders or downstream data and analytics systems
Perform necessary data profiling, data cleansing, aggregations, and normalization to deliver data products that are available for end-user consumption
Work with data providers to improve data management workflow
Develop and execute standard or custom queries and/or reports to retrieve data
Conduct root cause study of business problems; suggest areas for process improvement
Execute and promote data management and data governance strategies and standards
Qualifications:
Bachelor’s degree in computer science, Management Information Systems, or related field
5+ years of experience in data warehousing, data lifecycle management, database designing, data modelling and computer programming
Excellent understanding of Structured query language (SQL), Coding experience with a general-purpose, dynamic programming language (e.g., Python, PERL)
Strong learning orientation and curiosity
Comfortable learning new systems/software applications
Strong analytical thinking skills and problem-solving skills
Solid written and oral communication skills
Detail oriented
Good time management skills and multitasking ability
Knowledge in the following is preferred:
MS Azure: Azure Data Lake Store, Synapse
Data Factory/SSIS or related tool
Python or similar programming languages
Business visualization tools (e.g., Tableau, PowerBI)
Statistical software (e.g., SAS, SPSS)
#LI-AM1
#LI-Onsite
JM FAMILY IS PROUD TO BE AN EQUAL OPPORTUNITY EMPLOYER
JM Family Enterprises, Inc. is an Equal Employment Opportunity employer. We are committed to recruiting, hiring, retaining, and promoting qualified associates without regard to age, race, religion, color, gender, sex (including pregnancy, childbirth and related medical conditions), sexual orientation, gender identity, gender expression, mental or physical disability, national origin, marital status, citizenship, military status, genetic information, veteran status, or any other characteristic protected by federal, state, provincial, or local law.

DISABILITY ACCOMMODATIONS
If you have a disability and require a reasonable accommodation to complete the job application process, please contact JM Family’s Talent Acquisition department at
talentacquisition@jmfamily.com
for assistance. If you have an accommodation request for one of our recruiting events, please notify us at least 72 hours prior so that we may provide assistance.",116093,1001 to 5000 Employees,Company - Private,Retail & Wholesale,Vehicle Dealers,1968,$10+ billion (USD),FL,55,data engineer,senior,"['sql', 'python']",['azure'],"['tableau', 'ssis']",[],[],[],bachelor,
Phasorsoft LLC,4.0,"Seattle, WA",1348 - Cloud Data Engineer - Onsite - W2,"NOTE
NOT FOR C2C
We are looking for the candidates who can work on w2 ( Pay Roll )
Job description
Responsibilities:
Design and implement ETL data pipelines using Azure pipelines or other cloud technologies.
Extract data from diverse sources and transform it using Pyspark and Python languages within Azure Data Bricks.
Develop and optimize advanced SQL queries for data retrieval and manipulation.
Collaborate with cross-functional teams to understand data requirements and perform data mapping.
Utilize Snowflake for data storage and retrieval.
Follow Scrum/Agile methodologies to ensure timely delivery of projects.
Apply Kafka experience to enable real-time data streaming and processing.
Write Pyspark code to enhance data processing capabilities.
Use Powershell for automation and orchestration tasks.
Requirements:
Strong exposure to cloud platforms such as GCP, AWS, or Azure.
Proven experience in ETL development and cloud-based data pipelines.
Proficiency in using Azure Data Bricks and Pyspark for data transformations.
Advanced SQL skills for complex data querying and manipulation.
Familiarity with Snowflake for data warehousing.
Knowledge of data mapping and data modeling concepts.
Experience working in Agile/Scrum development environments.
Previous exposure to Kafka for real-time data streaming is preferred.
Proficiency in Pyspark coding.
Familiarity with Powershell scripting for automation.
Job Type: Contract
Pay: $76,479.45 - $158,309.78 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: In person",117395,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,WA,-1,data engineer,na,"['sql', 'python']","['azure', 'aws', 'snowflake']",[],['snowflake'],['kafka'],[],,0-2 years
SpartanNash,4.0,"Avondale Estates, GA",Senior Data Engineer,"At SpartanNash, we deliver the ingredients for a better life through customer-focused innovation. We do this for our supply chain customers and U.S. military commissaries, retail store guests and, most importantly, our Associates. In fact, we see a day when each will say, “I can’t live without them.”

Our SpartanNash family of Associates is 17,500 strong, ranging from bakery managers to order selectors; from IT developers to vice presidents of finance; from HR Business Partners to export specialists. Each of them plays an integral role in SpartanNash’s People First culture, Operational Excellence and Insights that Drive Solutions. Ready to contribute to the success of our food solutions company? Apply now!

Location: 850 76th Street S.W. - Byron Center, Michigan 49315

Job Description:
Position Summary:
This role is responsible to create, integrate, implement, and support large, highly complex systems and/or suites of applications. Provide expert technical leadership and guidance in projects. Oversee and review the programming/development work of others.

Here’s what you’ll do:
Technical Expert / Reference Resource acting independently under limited direction provides expert level technical design and development services and guidance for one or more systems or specialized applications
Provides technical leadership on complex solutions Scope and Function Definition develops, defines and documents project/system/function scope and objectives
Designs or enhances systems, applications, functions, and procedures to solve complex problems and meet business objectives
Prepares detailed functional specification Solution Development has full technical knowledge of all phases of analysis, integration, development, quality assurance, and implementation procedures
May be responsible for multiple phases of a project
May be responsible for the instruction, oversight, and review work of other associates
Application/System Troubleshooting interfaces with appropriate corporate area to provide detailed application/system information to assist with problem determination and resolution
Business Liaison/Mentor Acts as the primary reference resource interacting with business users to provide detailed application/system information
Mentors and provides guidance and training to other development associates
Maintain an efficient infrastructure by recognizing and reducing latencies and gaining efficiencies where recognized or needed
Participate in the on-call support rotation

Here’s what you’ll need:
Bachelor's Degree (Required) Computer Science, Computer Science Information Technology, or related field or equivalent combination of education and/or experience
Minimum 5+ years of work experience in Business Intelligence, Relational Databases, and ETL processes with modern ETL/ELT tools (Talend/Matillion/DataStage/Informatica) preferably in a cloud environment
Experience with Cloud Databases (Snowflake preferred) and Big data technologies
Qlik Replicate/Qlik Compose experience preferred
Advanced SQL skills, able to write and optimize complex queries in multiple database platforms (DB2, Oracle, Informix, SQL Server, MySQL, Netezza, Snowflake)
Experience with Unix scripting
Strong written and verbal communication skills, and effective interpersonal skills to work with internal and external customers and vendors
Good analytical/research and problem-solving skills
As part of our People First culture, SpartanNash is proud to offer a robust and competitive Total Rewards benefits package .

SpartanNash is an Equal Opportunity Employer, including disability and veteran, that celebrates diversity and believes employing a diverse workforce is key to our success. We are committed to providing equal employment opportunities to all individuals.

We are not able to sponsor work visas for this position.",114024,10000+ Employees,Company - Public,Retail & Wholesale,Grocery Stores,-1,$5 to $10 billion (USD),GA,-1,data engineer,senior,['sql'],['snowflake'],['qlik'],"['sql server', 'snowflake', 'oracle', 'mysql']",[],[],bachelor,+10 years
hims & hers,4.2,United States,Senior Data Engineer,"Hims & Hers Health, Inc. (better known as Hims & Hers) is a multi-specialty telehealth platform building a virtual front door to the healthcare system. We connect consumers to licensed healthcare professionals, enabling people to access high-quality medical care—from wherever is most convenient—for numerous conditions related to sexual health, hair care, mental health, skincare, primary care, and more.
With products and services available across all 50 states and Washington, D.C., Hims & Hers is on a mission to help the world feel great through the power of better health. We believe how you feel in your body and mind transforms how you show up in life. That's why we're building a future where nothing stands in the way of harnessing this power. We normalize health & wellness challenges—and innovate on their solutions—to make feeling happy and healthy easy to achieve. No two people are the same, so we provide access to personalized care designed for results. At our core, our mission is deeply personal—because we too are customers.
In January 2021, the company was listed on the NYSE and is traded under the ticker symbol ""HIMS"". To learn more about our brand and offerings, you can visit forhims.com and forhers.com.
About the Role:
As a Senior Data Engineer, you will work with analytics engineers, product managers, engineers, security, DevOps, designers and others to build a data platform that backs the self-service analytics, machine learning models, and products serving 900,000+ Hims & Hers users.
You Will:
Architect and develop data pipelines to optimize for performance, quality, and scalability
Collaborate with analytics engineering data analysts, and business partners to build tools and data marts that enable self-service analytics
Build, maintain & operate scalable, performant, and containerized infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Design, develop and own robust, scalable data processing and data integration pipelines using Python, dbt, Kafka, Airflow, Spark, and REST API endpoints to ingest data from a variety of external data sources to data lake
Develop testing frameworks and monitoring to improve data quality, observability, and data quality
Partner with the analytics engineers to ensure the performance and reliability of our data sources
Orchestrate sophisticated data flow patterns across a variety of disparate tooling
Partner with machine learning engineers to deploy predictive models
Partner with the security team to build frameworks and implement data compliance and security policies
Partner with DevOps to build IaC and CI/CD pipelines
Support code versioning and code deployments for data pipelines
You Have:
5+ years of professional experience designing, creating and maintaining scalable data pipelines using Python, API calls, and scripting languages
Demonstrated experience writing clean, efficient & well-documented Python code and are willing to become effective in other languages as needed
Demonstrated experience writing complex, highly-optimized SQL queries across large data sets
Experience with cloud technologies such as AWS or Google Cloud Platform
Experience with IaC technologies like Terraform
Experience with data warehouses like BigQuery, Databricks, Snowflake, and Postgres
Experience with event streaming technologies like Kafka / Confluent
Experience with modern data stack (Airflow, Databricks, dbt, Fivetran, Tableau / Looker)
Experience with containers and container orchestration tools such as Docker or Kubernetes
Project management skills and a demonstrated ability to work autonomously
Understanding of SDLC and Agile frameworks
Nice to Have:
Experience with Machine Learning & MLOps
Experience building data models using dbt
Experience with Javascript
Experience with CI/CD (Jenkins, GitHub Actions, Circle CI)
Experience designing and developing systems with desired SLAs and data quality metrics
Experience with microservice architecture
Our Benefits (there are more but here are some highlights):
Employee Stock Purchase Program
An inclusive culture where we are always seeking improvement and cherish your input
Great compensation package with equity compensation
Unlimited PTO (10 holidays off), Mental Health days (1 day off per quarter)
Generous Parental Leave
High-coverage medical, dental & vision
Mental health & wellness benefits
Offsite team retreats
Access to Amazon HIMS Store to order any additional equipment to ensure you have the gear you need
Employee discounts on hims & hers & Apostrophe online products, and Apple Store
$75 monthly connectivity stipend (phone/internet)
401k Match
We are focused on building a diverse and inclusive workforce. If you're excited about this role, but do not meet 100% of the qualifications listed above, we encourage you to apply.
Hims is an Equal Opportunity Employer and considers applicants for employment without regard to race, color, religion, sex, orientation, national origin, age, disability, genetics or any other basis forbidden under federal, state, or local law. Hims considers all qualified applicants in accordance with the San Francisco Fair Chance Ordinance.

#LI-Remote
Outlined below is a reasonable estimate of H&H’s compensation range for this role.

H&H also offers a comprehensive Total Rewards package that includes equity grants of restricted stock (RSU’s) so that H&H employees own a piece of our company.

The actual amount will take into account a range of factors that are considered in making compensation decisions including but not limited to, skill sets, experience and training, licensure and certifications, and location.

Consult with your Recruiter during any potential screening to determine a more targeted range based on the job-related factors. We don’t ever want the pay range to act as a deterrent from you applying!
An estimate of the current salary range is
$125,000—$175,000 USD",112889,201 to 500 Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2017,Unknown / Non-Applicable,TX,6,data engineer,senior,"['sql', 'python']","['databricks', 'snowflake', 'aws', 'google cloud']","['looker', 'tableau']","['snowflake', 'dbt']","['kafka', 'spark']","['terraform', 'docker']",,+10 years
Commonwealth Care Alliance,3.5,"Boston, MA",Lead Data Engineer,"Why This Role is Important to Us:


The Data Engineer role at Commonwealth Care Alliance (CCA) is a heavy technical and architect role that designs, contributes to, and maintains data pipelines and systems that support clinical point of care and business decisions. This role is highly collaborative with other data-focused departments at CCA including Analytics, Actuarial, and Finance. This role is a “forward-deployed” engineer that is expected to engage with and continually build a greater understanding of the clinical and operational context.
The Data Engineer must understand existing data, infrastructure, and transactional systems to design the appropriate usage of existing data assets or the development of new data assets, including large structured and unstructured data. The Data Engineer must automate robust workflows to efficiently perform extract, load, transform, modeling, and computational tasks in continuous integration and continuous delivery pattern. The Data Engineer will also support and contribute to rapid prototyping and deployment of analytics, model training, and model deployment. This role will report to the Director of Data Strategy, Mgmt. and Sol. at CCA.



What You'll Be Doing:


Engage with stakeholders on pre-defined projects
You will be responsible to understand the business requirements and architect robust data platform on cloud technologies
Build scalable and robust data model/infrastructure
Help managing enterprise data platform
Dealing with data governance, such as documentation, data integrity/quality and data security
Build data pipelines (ETL, validation, automation, monitoring and logging...) that enable analysts and other stakeholders across the organization for data-focused product and data-driven business decisions
Work closely with product managers and engineers to design, implement, test and continually improve scalable applications and services running on Azure
Make changes on the existing data system to optimize and improve accuracy of the data process
Give instruction or/and help stakeholder on the best practice to pull and use data
Create scalable and actionable solutions, in the form of analytics, reports and dashboards for stakeholders to solve business and technical problems through ad-hoc requests
Write andrevise technical documents and blogs, including design, development and application
Complete data engineering projects under supervision and guidance
You will be responsible to understand the business requirements and architect robust data platform on cloud technologies
You will be responsible for creating reusable and scalable data pipelines
You will be responsible for development and deployment of new data platforms
You will be responsible for deploying AI algorithms into the data platform to run predictive analytics at scale
Help the Data Engineering team produce high-quality code that allows us to put solutions into production
Collaborate with IT Architect and IT Security and Privacy teams to architect and deploy data pipeline solutions that are secure and performant
Day to day management and reliability of data pipelines deployed securely on the public cloud
Executes quality excellence through standards, best practices, and continuous improvements



What We're Looking For:


Education Required:
Master's degree + 5 year of equivalent work experience (STEM major or related field preferred)
OR bachelor's degree + 8 years of equivalent work experience (STEM major or related field preferred)
Experience Required:
Experience in design and develop code, scripts, and data pipelines that leverage structured and unstructured data
Strong foundation in data engineering principles and an architectural best practice
Experience in architecting solution on public cloud (preferably Azure)
Experience in instituting data architecture best practices (i.e. dimensional modeling, ETL pipeline, large scale distributed ETL pipelines)
Extensive experience of database query languages (i.e. SQL or equivalent), database design, optimizing queries, internals knowledge of query planning
Experience in authoring or reviewing system design documents for enterprise solutions
Knowledge, Skills & Abilities Required:
Technical Skills:
SQL, Oracle and Python
ETL tool (Informatica, Talend)
BI tool (Looker, Tableau, PowerBI ...)
Git-based version control systems
Strong knowledge of data modeling and mapping
Good understanding of different dimensional modeling techniques such as Star vs SnowFlake Schemas
Soft skills:
Be organized and flexible
Take initiative to own projects
Strong analytical skills
Strong verbal and written communicational skills
Be able to work with interdisciplinary teams
Be able to explain technical concepts/results to non-technical audiences
Passion for creating work that is well-documented and reproducible
Ability to work with short iteration times in agile mode as well as the ability to carry out projects in a self-directed manner
Healthcare: passion for working at a healthcare organization
Preferred Skills and Experience:
Experience with R
Experience in healthcare or health insurance organization
Experience with clinical claims and health record data
Experience building data pipelines in the cloud under the constraints of HIPAA
Publication or presentation of innovation in data science, machine learning or related area
Experience with Azure cloud and data services (cosmos, synapse, datafactory, databricks etc.)
Understanding of infrastructure (including hosting, container based deployments and storage architectures) would be advantageous",133540,1001 to 5000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2003,$1 to $5 billion (USD),MA,20,data engineer,senior,"['sql', 'r', 'python']","['databricks', 'azure', 'snowflake']","['looker', 'tableau']","['snowflake', 'oracle']",[],[],master,5-10 years
Fuge Technologies Inc,4.7,"Jersey City, NJ",Big Data Engineer,"Big Data Engineering Technical Lead
Mandatory Skills
Big Data – Spark, Kafka, Databricks, Hive, Nifi
AWS, Java, Scala
Client Interview Needed for Selection (yes / No)
Yes
Detailed JD (Pl share the Detailed Description, 1 liner JD will not work)
10+ yrs experience as Data Engineer
Extensive experience in spark on data processing
Kafka design experience like monitoring & parallelism.
Comfortable with Python/Java/Scala coding experience
Crisp / clear cut communication
Flexible with learning new technology
Flink experience is a plus
AWS experience is nice to have
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130500,1 to 50 Employees,Contract,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['sql', 'java', 'scala', 'python']","['databricks', 'aws']",[],['hive'],"['kafka', 'flink', 'spark']",[],,0-2 years
RelMap Consulting,4.8,"Addison, TX",Sr. Data Engineer (Hybrid),"The data engineer entails:
Advanced SQL Server Development
Ability to create and modify views
Ability to create and modify custom functions
Ability to write complex queries, including joins, temp table processing, and common table expressions (CTE’s)
Ability to create and modify stored procedures using TSQL
Ability to analyze and optimize query performance using indices
Ability to use Red Gate’s SQL Compare to perform a schema (AKA “do a manual roll”) between one server an another, and between revision control and a database
Ability to stand-up new database server VM’s in Linux and Windows using VMWare
Ability to backup and restore a database in a Linux and Windows environment
Ability to manage and script automated database jobs
Ability to use PowerShell / Bash to interact with databases from the command line
Addition skills that would be desired but not required
· Experience with Redis
· Experience with MongoDB / CosmosDB
· Experience with Azure and Azure SQL
· C#, JavaScript (NodeJS / ES6), or Python experience
Job Type: Contract
Pay: $70.00 - $80.00 per hour
Compensation package:
Hourly pay
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Addison, TX 75001: Reliably commute or planning to relocate before starting work (Required)
Work Location: Hybrid remote in Addison, TX 75001",135000,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2010,$1 to $5 million (USD),TX,13,data engineer,senior,"['sql', 'python']",['azure'],[],"['sql server', 'mongodb', 'redis']",[],['bash'],,5-10 years
Atika Technologies,4.0,"Pittsburgh, PA",Data Engineer,"Role: Data Engineer/PySpark Developer, Pittsburgh, PA - Day One Onsite
Fulltime and Contract(W2)
LinkedIn Must
USC/GC/EADs
Three round of interview: First two on Zoom Video Call, End Client Round will be Onsite on the Client's Location (Pittsburgh, PA)
Job Description:
Python(PySpark), Jupiter hub, airflow, Java
Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable
Job Types: Full-time, Contract
Salary: $120,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Compensation package:
Bonus pay
Performance bonus
Yearly pay
Experience level:
10 years
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA 15259: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Required)
Spark: 4 years (Required)
Regulatory reporting: 3 years (Required)
SQL: 4 years (Required)
IT: 10 years (Required)
Airflow: 4 years (Required)
Jupiter: 4 years (Required)
Work Location: One location",125000,,,,,-1,,PA,-1,data engineer,na,"['sql', 'java', 'python']",[],[],['oracle'],['spark'],[],,2-5 years
spar information systems,3.5,Remote,Azure Data Engineer,"Role: Sr Azure Data Engineer
Location: Remote
Duration: 3 Months Contract to hire Full Time (W2 Only)
Must have 11+ IT Experience
Required Skills:
3+ years of experience in data software development, programming languages and developing with big data technologies
2+ years of experience designing and building on existing and new data applications
2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework
1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express.
Thanks & Regards,
Arvind Kumar Bind
Cell: 732 716 7403 (Text)
Direct Number:- 469-750-0607
Email : Arvind.B@sparinfosys.com
Job Types: Full-time, Contract, Permanent
Pay: $120,555.79 - $150,000.00 per year
Benefits:
401(k)
Health insurance
Paid time off
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Big data: 3 years (Required)
SQL: 1 year (Required)
Data lake: 3 years (Preferred)
Work Location: Remote",135278,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$25 to $100 million (USD),Remote,11,data engineer,na,['sql'],"['azure', 'aws']",[],[],[],[],,2-5 years
Appsintegration INC,4.5,"California, KY",Data Engineer,"Role:- Data Engineer
Position:- Onsite (W2 position)
Experience:- 10+ Years
Responsibilities
Working closely with business teams to understand their requirements and develop reporting or analytic solutions.
Producing and automating the delivery of key metrics and KPIs to the business.
In some cases, this will focus on making data available and others will be developing full reports for end users.
Managing stakeholder demand and a backlog of reporting requirements and needs.
Actively working with users to understand data issues, tracing back data lineage, and helping the business put appropriate data cleansing and quality processes in place.
Managing secure access to data sets for the business.
Requirements
Bachelor's degree in Computer Science or equivalent.
Proficient in creating Data Factory pipelines for on-cloud ETL processing.
Experience implementing data pipelines using the latest technologies.
Experience developing using SQL, relational databases, and data warehousing technologies.
Extensive ETL experience.
Ability to scale systems in Cloud as per requirement.
Experience in handling structured and unstructured datasets.
Good to have experience in data modeling and Advanced SQL techniques.
Must-Have Skills: Azure Data Factory (ADF), SQL, Cloud-based ETL.
Good to Have Skills: Azure File Storage Services, Databricks, Azure Synapse.
you can reach me out on LinkedIn
here is my link :- linkedin.com/in/shravanthi-thakur-705a81231
Job Type: Contract
Salary: $42.00 - $45.00 per hour
Ability to commute/relocate:
California, KY 41007: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",78300,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,KY,-1,data engineer,na,['sql'],"['databricks', 'azure']",[],[],[],[],bachelor,0-2 years
Apolis,3.9,"Saint Louis, MO",Data Engineer (Python Focused),"Role: Data Engineer (Python Focused) (W2 ONLY)
Location : St. Louis, MO 63102
Duration: 18 Months
The primary responsibilities of this role will be to work as a part of the data and analytics team of the client. The ideal candidate would have experience using ETL and other data engineering tools in a cloud environment. Additionally, this candidate would have coding and scripting experience creating code to move and manipulate data using tools such in Python such as Spark or Pandas. Microsoft Azure experience is a plus but relevant experience with other cloud-based environments will be considered.
This team uses Microsoft Azure to build and productionize data sets and processes to the product owners and product users and this role will be expected to contribute to those objectives. As a member of this team, you will work in conjunction with other data engineers, platform engineers, solution developers and others to execute and deliver working solutions to the end users.
Technical Skills
High focus on Python
More Programmer/Coder experience, less Cloud experience needed.
Must have experience with data analytics/engineering and cloud technologies and an in-depth working knowledge and understanding of data engineering, design, and integration.
Intermediate or higher level of proficiency with Python Must have the proven ability to learn quickly and apply cloud data engineering principles and skills to new tools and use cases.
Ability to define and take technical ownership of detailed requirements and collaborate effectively with various team members such as Business Analysts and Project Managers and end users.
Excellent communication (verbal and written) and problem-solving skills are essential for this position.
Initiative, Proactive Cooperation, and curiosity are mandatory behavioral skills required for this position.
Experience with several of the following technologies is preferred:
Experience with Microsoft Azure Platform. In particular, the data processing toolsets including Azure Data Factory Pipelines, Azure Databricks, Azure Data Lake Storage.
Experience using Notebooks (Such as Jupyter, DataBricks) also desired.
Experience using code languages such as Python and C#
Experience using query languages such as SQL, MDX and DAX
Experience using scripting languages such as PowerShell, M-Query (Power Query), and Windows batch commands.
Understanding of Agile and DevOps principles for software development and ownership.
Familiar with compliance and controls for software development and deployment.
Education Required
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is not required but is preferred.
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Compensation package:
Overtime pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 7 years (Preferred)
ETL: 5 years (Preferred)
Cloud infrastructure: 4 years (Preferred)
Work Location: One location",117000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD),MO,27,data engineer,na,"['sql', 'python']","['databricks', 'azure']",[],[],['spark'],[],bachelor,2-5 years
Fracsys Inc,4.0,"Washington, DC",Data Engineer,"Fracsys Inc is hiring a Data Engineer position. The ideal candidate must have at least 12+ years of industry experience. He or she must be responsible for successful technical delivery of Data Warehousing and Business Intelligence projects using a formal project management methodology.
Responsibilities
Directs, motivates and develops key technical personnel
Provides technical direction across the full lifecycle system development process
Provides strategic planning and comprehensive senior level technical consulting to IT senior management and senior technical staff
Working independently within guidelines
Provides detailed inputs for plans and schedules, including goals, risks, and resource allocation
Conduct diagnostic, troubleshooting steps and present conclusions to a varied audience
comfortable slinging some code to solve a problem
Build Data Marts and Data warehouses to facilitate analytical capabilities across the varied sources of data
Design and implement data patterns on cloud using cloud Data warehouse and data virtualization tools
Evaluate new tools and technologies for target state on cloud
Learn and implement new ETL, BI tools and data warehousing technologies
Qualifications
Minimum twelve (12) years in a Technical Lead or Consulting role leading Information Management related initiatives (system integration, data warehouse build, data mart build, or similar)
MUST have proven leadership experience for team of at least 3 technical resources.
Experience creating Data Marts, Data warehouses on-premise and on cloud for real time and batch processing frameworks
Experience building and optimizing big-data data pipelines and data sets including Postgres, AWS Relational Data Service
extensive hands-on experience building and optimizing data structures for data analytics, data science and business intelligence
Experience building self-service data consumption patterns and knowledge of cloud-based data Lake platforms
Experience wrangling data (structured and unstructured), in-depth knowledge of database architecture
Experience utilizing or leading implementations leveraging ETL tools (Informatica / Talend), BI Reporting tools such as MicroStrategy, Microsoft Power BI, data modeling tools such as Erwin, Oracle, SQL server, NoSQL, JDBC, UNIX shell scripting, PERL and JAVA, XML/JSON files, SAS , Python, AWS cloud-native technologies, S3, Athena, Redshift
Familiarity with the following technologies: Hadoop, Kafka, Airflow, Hive, Presto, Athena, S3, Aurora, EMR, Spark
Ability to drive, contribute to, and communicate solutions to technical product challenges
Ability to roll-up your sleeves and work in process or resource gaps and fill it in yourself
Excellent written and oral communication skills in English
Education
Bachelor’s degree in computer science, Data Analytics, Information Systems, or a related degree or equivalent experience. Master’s degree is preferred.
Job Type: Full-time
Pay: From $96,336.72 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Experience level:
11+ years
Schedule:
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",96337,,,,,-1,,DC,-1,data engineer,na,"['python', 'shell', 'java', 'sql', 'nosql']","['aws', 'redshift']",['power bi'],"['sql server', 'hive', 'oracle']","['kafka', 'spark', 'hadoop']",[],bachelor,+10 years
Fulcrum Analytics,4.3,Remote,Data Engineer,"At Fulcrum, we provide cutting-edge data science consulting and software solutions to help companies solve challenging business problems. Through the utilization of our innovative products and dedicated team, clients are able to operate more efficiently, manage risk, and make informed business decisions every day. Fulcrum Analytics has stood at the forefront of data, analytics, and innovative software for over 25 years, offering sophisticated solutions, groundbreaking technologies, and winning strategies that help companies achieve their targeted results.
Role Description:
We are seeking a highly-skilled data engineer with experience in data analytics and data science. The ideal candidate will be responsible for building and maintaining data pipelines, designing and implementing databases and data models, and developing data-driven solutions to drive business growth for our clients.
Responsibilities:
Design, build, and maintain efficient, scalable, and reliable data pipelines using SQL, or programming languages such as Python or Java.
Develop and maintain databases and data models to meet clients’ analytical needs.
Identify data quality issues and develop processes to ensure data accuracy, completeness, and consistency.
Work with cross-functional teams to define and implement data integration strategies for new and existing systems.
Develop and maintain automated testing and deployment of analytics code and data pipelines.
Collaborate with data scientists to develop and implement machine learning models, statistical models, and other advanced analytics techniques.
Communicate analytical results and recommendations to technical and non-technical stakeholders through presentations and visualizations.
Stay up to date with emerging technologies and industry best practices, and recommend tools and processes that can improve data engineering and analytics processes.
Qualifications:
Bachelor's or master's degree in computer science, statistics, data science, or a related field.
2+ years of experience in data engineering, data analytics, and data science.
Strong programming skills in Python, with experience in building data pipelines using frameworks such as Airflow, Glue, Lambda, Athena, and BigQuery as well as data platforms such as Hadoop, Snowflake, and GCP.
Experience working with SOAP or RESTful web services.
Familiarity with version control, in particular Git.
Experience in designing, building, and optimizing relational and NoSQL databases.
Familiarity with data visualization tools, such as Tableau or Power BI.
Ability to understand and transform complex data into actionable insights.
Strong analytical and problem-solving skills, with a focus on delivering practical solutions to business challenges.
Ability to learn and solve problems independently.
Excellent communication and collaboration skills, with the ability to explain complex concepts to both technical and non-technical audiences.
If you are passionate about data analytics and want to work in a dynamic environment with talented colleagues, we encourage you to apply for this role. We offer competitive salary and benefits packages, as well as opportunities for professional growth and development.


Position Type: Full-time salaried
Reports to: Senior Data Engineer
Base Compensation: $110,000 - $140,000
Benefits: Medical, Dental, Vision, 401k with Match, Bonus, Unlimited PTO
Location: Remote within the United States of America; Eastern time zone preferred",125000,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,1993,$5 to $25 million (USD),Remote,30,data engineer,na,"['sql', 'java', 'nosql', 'python']",['snowflake'],"['tableau', 'power bi']",['snowflake'],['hadoop'],[],bachelor,+10 years
Take Command Health,4.9,"Dallas, TX",Data Engineer,"Company: Take Command
Position: Data Engineer
Salary: $120,000-$130,000 plus bonus
Location: Dallas or Austin, TX
About Take Command
Take Command is a start-up on a mission to improve the healthcare system, starting with health insurance. Pragmatically speaking, we help employers reimburse employees for individual insurance instead of offering a traditional one-size-fits-all group plan. We believe this model can empower employees (when they have the right support) to be savvy healthcare consumers and have a transformative impact on the entire healthcare system.
Let’s be honest—health insurance is usually a confusing, frustrating, and even emotional experience for people. We want to fix that with a new model, great technology, and a superior user experience. We have made a great start, but we need your help to fully realize our long-term vision.
As a part of the dev team, you'll build the framework for both our clients and our team to succeed. You will be a pillar in our company as we grow and make our product the best product on the market!
In this role, you will:
Take ownership for delivering multiple features, their quality and assist team members in delivering their features
Recommend improvements to data quality, processes and code to make it easier to use, manage and maintain
Experience designing and developing data models and data migrations
Experience managing and maintaining ETL or ELT processes (Hevo, BigQuery, Looker, etc.)
Hands-on development experience with SQL, relational database design (ERDs)
Must be able to write SQL & contribute code
Be product-minded; focus not only on what needs to be built but why something needs to be built
You may be a great fit for this role if you have:
3 or more years of professional experience as a data analyst or data engineer
Cloud-Based Data Tools e.g. Hevo, BigQuery, Looker, etc.
Database technologies: SQL, RDBMS / Relational databases (Postgres, Amazon RDS / Aurora, etc.)
Nice to have experience: Python, Java, Typescript or another modern OOP language
Cloud infrastructure: AWS
Source control: Git
Working at Take Command
We’re excited to build a team and culture that reflects our values! We offer competitive pay, health benefits, and equity options to share with this position. We want you to be an owner along with us in revolutionizing healthcare.
A generously funded ICHRA for medical, dental, and vision premiums and medical expenses. You get to use our own product and we think that’s so exciting and rare!
Unlimited personal vacation in addition to regular company holidays.
401(k): 90-day eligibility for 2% match that vests immediately!
Access to LinkedIn Learning.
We have two beautiful offices conveniently located near The Hill (Dallas) and Holly Commons (Austin). The kitchen is well-stocked and we've designed the space to have lots of different areas to work!
Paid parental leave for new parents.
Flexible on where you work – we hope to see you around the office on average 3 days a week especially when white-boarding or building relationships with your colleagues, but you also have the ability to work from home or wherever you'd like when you need to get focus work done.
More About Us
We secured our Series A funding in 2021 and are thrilled to be able to expand our team. Despite being a small startup in a land of health insurance giants, we’re the recognized industry leader for what we do (health insurance reimbursements) and passionate about bringing it to market because we know we can help fix a broken system and improve our clients’ wellbeing and health outcomes.
We’ve been featured in The New York Times, The Wall Street Journal, The Dallas Morning News, and other national healthcare publications and are excited about our growth opportunities.
Take Command knows diversity and inclusion among our teammates is integral to our company’s success and growth. Our vision is to recruit, develop, and retain the best team from a diverse candidate pool.
This has mostly been about us, but we’d love to hear from you-we can’t wait to hear your story!
Take Command is an equal opportunity employer! We celebrate diversity and are committed to creating an inclusive environment for all employees.
Xrygn4NXMr",125000,1 to 50 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2014,Unknown / Non-Applicable,TX,9,data engineer,na,"['sql', 'java', 'python']",['aws'],['looker'],[],[],[],,
Intersec,4.5,Remote,Senior Data Engineer,"Architect and implement highly scalable data pipeline to streamline migration of data from source data repositories to the centralized data platform
Implement agreed upon data models (e.g., data marts) to enable faster and efficient consumption of data by downstream consumers
Conduct performance tuning to optimize data migration techniques and ensure adherence to identified service-level agreements (SLAs)
Define and implement automated validation checks to ensure data integrity between source and target
Assist in onboarding consumers with existing technology to connect to the platform
Setup predictive alert mechanism for pipeline degradation for IMF pipelines.
Enhance data quality framework for data pipelines and derived datasets
Enhance Data Audit Balance and Control mechanism for data pipelines
Enhance pipelines to be certified copies of authoritative source
Populate metadata for the Enterprise with particular focus on the Data Sources being onboarded to EDP (Technical and Business Glossaries) related to IMF Modernization Programs
Validate correct lineage for datasets
Required:
*8+ years of Databricks, Data Modeling, and/or Metadata management experience
* Familiarity with architecting and developing real and near-real-time solutions
* Bachelor's Degree in Engineering, Mathematics, computer Science, Statistics or other quantitative discipline, or 4 years equivalent professional experience
* US Citizenship and ability to gain a public trust
Preferred:
* Familiarity with IRS systems and ETL pipelines
* Ability to work independently and manage multiple tasks
* IRS MBI Clearance with IRS Laptop and Informatica Installed
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Are you a US/GC card holder?
How many years of Databricks, Data Modeling, and/or Metadata management experience do you have?
Work Location: Remote",130500,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,Unknown / Non-Applicable,Remote,19,data engineer,senior,[],['databricks'],[],[],[],[],bachelor,2-5 years
Cureatr,4.1,Remote,Data Engineer,"About Us
We are a thriving, medication management solutions company, headquartered in Manhattan, but largely virtually distributed, committed to tackling the $528B suboptimal medication management problem. Patient medication information is often incomplete for healthcare providers and systems – it’s our mission to fill in the gaps. We provide real-time, universal access to accurate medication data for over 265 million patients and high-quality interventions by board certified telepharmacists. Through our solutions, healthcare providers can reduce preventable hospitalizations, avoid readmissions, and ultimately, improve patient outcomes. We’re already used by dozens of hospital systems, with tens of thousands of clinicians and supporting staff using us to care for millions of patients.
Here at Cureatr, we are driven by a core set of values and strive to incorporate them into everything we do. We care deeply about our users, patients, and employees, and we aim to live and operate compassionately. We take pride in creating a diverse, inclusive, and equitable company culture, where our open and honest approach to work fosters innovation and encourages employees to achieve professional and personal growth.

The Role
We’re looking for a Data Engineer to join our growing Engineering team. Use the power of healthcare data to find insights that will demonstrate the value Cureatr is delivering to our customers, help improve patient outcomes, and expand the scope of Cureatr’s mission. As a Data Engineer at Cureatr, you will be responsible for designing, implementing and maintaining our data infrastructure, pipelines, automation, and ETL processes. You will work closely with our Data Science and Analytics teams to ensure that our data is clean, accurate, and readily available for analysis. You will also be responsible for ensuring that our data meets industry standards for security, privacy, and compliance.
This is a remote, work from home position, but candidates must be located within the U.S. Ideal candidates must be able to travel to our New York City office, or TBD company-wide offsite locations, up to 4x per year.

You’ll be part of a team working to solve problems like:
Increasing the number of patients who receive the opportunity to do a medication reconciliation, which can help keep them out of the hospital.
Streamlining and automating processes so that our pharmacists can serve as many patients as possible.
Predicting patient events likely to occur so that our clinical team can adequately prevent them before it’s too late.
Helping our pharmacists make the most of data and insights that will help them be as effective as possible in serving patients.
Evaluating the effectiveness of our clinical services impact at the population level.

What You’ll Do:
Design, implement, and maintain our data infrastructure, pipelines, automation, and ETL processes to support our data needs.
Build and maintain scalable data models and databases.
Collaborate with Data Science and Analytics teams to understand their data needs and provide data solutions accordingly.
Ensure data quality and integrity by implementing data validation and cleansing processes.
Ensure data security and compliance with healthcare regulations.
Monitor and optimize data performance and scalability.
Design and implement data governance policies and procedures.
Evaluate and implement new data technologies and tools to improve our data capabilities.

Your Skills:
3+ years of experience as a Data Engineer or similar role.
Strong programming skills in languages such as Python and SQL.
Experience using git to track changes.
Experience with data modeling and database design.
Experience with the “modern data stack” preferred
Experience with ETL and data pipeline development.
Knowledge of healthcare data and related standards (e.g., HL7, FHIR) is a plus.
Familiarity with AWS.
Strong problem-solving and analytical skills.
Excellent communication and collaboration skills.

Technology Stack:
Python
RedShift
dbt
Dagster
MongoDB
Looker
AWS

Educational Requirement: Bachelor’s Degree or relevant work experience

Salary: $110,000 - $140,000 plus full benefits

Why work with us?
Make a measurable impact on patients’ lives
Be part of a company dedicated to living its core values:
Empathy
Honesty
Integrity
Impact
Fun
Equity in Healthcare
Work in the company without micromanagement
Share your vision and make an impact on the product
Opportunities for career growth
Remote work
Competitive salary
Premium benefits (Health, Dental, Vision, 401k, unlimited PTO, more)

Equal Employment Opportunity Policy
Cureatr is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, pregnancy, race, religion or religious creed, color, gender, gender identity, gender expression, national origin, ancestry, physical or mental disability, medical condition, genetic information, marital status, registered domestic partner status, age, sexual orientation, military or veteran status, protected veteran status, or any other basis protected by federal, state, local law, ordinance, or regulation and will not be discriminated against on these bases
Disclaimer: we are aware of a scam targeting applicants using a cureatrcareers.com email address. Please block and report this sender, as they are not affiliated with Cureatr. If you have any questions, please contact us at jobs@cureatr.com. Thank you!
uBajh3OfeT",125000,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,Less than $1 million (USD),Remote,11,data engineer,na,"['sql', 'python']","['aws', 'redshift']",['looker'],"['mongodb', 'dbt']",[],[],bachelor,
SECURE RPO,4.3,Manhattan,Senior data engineer,"Must have skills:
8+ years of experience building high performance scalable enterpris`e analytics or data centric solutions
8+ or 5+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines
At least 5 years of experience implementing complex ETL pipelines preferably in connection with Glue and/or Spark
Exceptional coding and design skills in Python or Java/Scala
Hands-on experience with AWS (i.e. Glue, Aurora Postgres, Lambda, EMR, EKS, Redshift, etc.)
Experience with visualization tools like QuickSight, PowerBI, Looker or Tableau
Experience with Talend (ETL) is a big plus
Roles and Responsibility:
Drive a high impact and high visibility project that enables data availability, encompasses data analytics, machine learning, and petabyte scale datasets, and provides reliable and timely access to thousands of data sources
Design, architect and support systems for collecting, storing, and analyzing data at scale
Recommend improvements and modifications on new and existing data and ETL pipelines. Create optimal data pipeline architecture and systems using Apache Airflow
Create data analytics for d ata scientists to innovate, build and optimize our ecosystem
Assemble large, complex data sets that meet functional and non-functional business requirements
Be responsible for ingesting data into our data lake and providing frameworks and services for operating on that data including the use of Spark
Analyze, debug and correct issues with data pipelines
Operate on or build solution required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, AWS and Spark technologies
Job Type: Full-time
Salary: $56.80 - $80.16 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 7 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: On the road",123264,1 to 50 Employees,Unknown,,,-1,Unknown / Non-Applicable,Manhattan,-1,data engineer,senior,"['sql', 'java', 'scala', 'python']","['aws', 'redshift']","['looker', 'tableau']",[],['spark'],[],,5-10 years
Atalan Tech,4.0,Remote,Data Engineer,"Atalan Tech is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. We are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Scientific Officer (UCLA Ph.D.), Chief Product Officer (Oxford Ph.D.), and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
We are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with us. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Some knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and CSO to build data pipelines for machine learning
Assisting the CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Working closely with other key members on other projects
Developing and designing data security standards and policies in line with industry best practices
About Us:
Atalan Tech’s mission is to improve the wellness of clinicians and their patients. We blend labor economics, machine learning, and clinical psychology with a team from Harvard, Yale, and UCLA. Atalan Tech is looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon our proof-of-concept success, we have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand our reach.
Contact Us:
If you’re interested in this position, please send us your CV/resume and cover letter.
Job Type: Full-time
Pay: $108,448.20 - $130,604.27 per year
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you know the role is full time?
Can you start within the next month?
What are your salary expectations?
Have you held the title of a data engineer? If not, what experience do you have as a data engineer? Please be very specific.
Have you released Python code to a production environment in the last year?
Experience:
Data analytics: 2 years (Required)
Work Location: Remote",119526,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'r', 'python']","['aws', 'redshift']",[],[],[],[],,2-5 years
Radiant System,4.0,"Pleasanton, CA",Data Engineer,"First Priority: Pleasanton, CA –
Second Priority for a superstar: Plano, TX
Experience in developing and deploying data pipelines, preferably in the Cloud
Experience working with Spark, Databricks, and other relevant technologies in Microsoft Azure cloud
Experience in productionizing and deploying Big Data platforms and applications
Expert in writing SQL statements
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Work Location: In person",112889,,,,,-1,,CA,-1,data engineer,na,['sql'],"['databricks', 'azure']",[],[],['spark'],[],,
"Integrated Technology Strategies, Inc.",4.0,"New York, NY",Data Engineer,"Integrated Technology Strategies is a provider of information technology consulting. Digital transformation, improving business performance, developing strategies and enhancing value is at the core of what we are.

Integrated Technology Strategies is looking to hire a Data Engineer to be a part of our Consulting team.

We are looking for someone passionate about data is focused on developing the platform for critical data products, including real time business metrics and analytics capabilities. The role requires supporting and collaborating with groups including Data Analytics/BI/Product, as well as our core backend API team. The individual will not be afraid to think out of the box and will play a key role in technical decision making. We are highly focused on giving ownership and responsibility to autonomous teams, using the right tools for the job, and building flexible architectures.

Responsibility:
Continue to evolve the internal Reporting and Analytics platform on top of Snowflake on AWS infrastructure.
Experience in Architect, design and implementing scalable ETL and data processing systems to handle the big data ecosystem including data collection, processing, ETL and Data warehouse.
Build soft real time capabilities and insight into product metrics to help product managers and BI/Analytics understand and optimize product features and guide product decisions.
Participate and contribute to the capabilities and engineering priorities across the organization.
Contribute to the codebase and participate in code review.
Build analytics tools that utilize the data pipeline to
provide actionable insights into operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product,
Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Reporting to the Senior Director, Software Engineering, you’ll be responsible for overseeing engineering product quality and delivery and setting and overseeing technical standards for teams who are working on everything from customer-facing applications.

Skill Requirements:
Solid understanding of real time data processing with Kafka, Spark and Flink and batch data processing frameworks on EMR and Snowflake.
Passion for building world-class data platforms that support a global customer base
Solid engineering background and understanding of programming languages such as Python, Java or equivalent
5+ years of progressive experience in data infrastructure development, with a track record of successful high-quality deliveries
Experience of working in an agile environment and embracing engineering best practices
Ability to apply both technical competence and interpersonal skills to achieve business outcomes
High emotional intelligence, sound temperament, and professional attitude
Strong understanding of SQL, experience with key databases such as Snowflake, MS-SQL and Postgres
Knowledge of the internals of how database systems work to design models for varied use cases.
Experience with CI and CD in an AWS environment with Terraforms
Experience with key Data technologies, such as Sqoop. Kafka will be a plus
Proven experience in building secure data platforms
Bachelor’s degree in Computer Science or equivalent",110128,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),NY,16,data engineer,na,"['sql', 'java', 'python']","['snowflake', 'aws']",[],['snowflake'],"['kafka', 'flink', 'spark']",[],bachelor,+10 years
Egrove Systems Corporation,4.4,"Dallas, TX",Data Engineer Flink,"Java, Bigdata, Azure & Flink
Flink Sr. Developer, who has implemented and dealt with failure scenarios of processing data through Flink.
A senior who can help his Flink development team, guiding and helping them implement custom solutions through Flink.
Seasoned Java developer who knows about all aspects of SDLC.
Worked on integrations of other technologies with Flink, eg: Kafka, MongoDB, etc
Azure experience in the areas of Messaging, Data processing, preferably on Flink or on Databricks.
Job Type: Contract
Salary: From $51.83 per hour
Compensation package:
Monthly bonus
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
Work Location: On the road
Speak with the employer
+91 732)-860-7733",93294,51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,['java'],"['databricks', 'azure']",[],['mongodb'],"['kafka', 'flink']",[],,0-2 years
DocsInk LLC,4.0,"Wrightsville Beach, NC",Data Engineer,"DocsInk is on a mission to provide a collaborative and exciting alternative to the traditional healthcare model. Through the complete virtualization of healthcare organizations, we are equipping providers with the ability to deliver better patient care with speed, efficiency, and accuracy. With the widespread focus on providing value-based care, DocsInk is making it possible to provide superior care while also maximizing reimbursements. In today’s environment, shifting the way we approach healthcare is essential and DocsInk is on the cutting edge of where the industry is headed. We are excited to be growing our team and are looking for driven individuals with a passion for making a difference in the healthcare industry.

As our Data Engineer, you will be in the driver’s seat as our team creates elegant and forward-thinking software solutions for our healthcare customers. You will work with fellow developers, designers, training & support, quality assurance, and management in a fast-paced environment to take our products to the next level and exceed expectations. We are looking for someone who is incredibly talented to do what most cannot and yet humble enough to easily work with peers, accept guidance from more experienced staff, and mentor more junior staff. If designing and developing complex integration efforts is what gets you out of bed each morning, then we want you to help us reshape healthcare.

People are our most important asset and the number one reason we all love working at DocsInk. As a team, we value accountability, transparency and collaboration. We have agile teams, with clear, outcomes-focused goals. Delivering innovative solutions and superior service are at the forefront of what we do, and the collective ideas of the team allow us to continue to excel in a fun and energetic environment.

Professional Requirements and Responsibilities:

Role / Responsibilities:

Implement robust ETL solutions that integrate healthcare data feeds using tools such as Apache NiFi, NextGen Connect (JavaScript), PostgreSQL, MySQL, and Linux shell scripting
Configuring and managing deployments to AWS infrastructure consisting of both traditional servers and serverless components. CI/CD tools include Bitbucket and AWS Code Deploy
Building direct interfaces with hospitals and EMR vendors utilizing industry standards such as HL7 2.x, C-CDA, and FHIR as well as implementing interfaces that consume formats such as XML, JSON, and delimited files
Utilizing APIs and web services to permit bi-directional flows of data to and from our partner organizations
Monitoring operational ETL processes, triaging and resolving issues as they arise to meet SLAs
Writing PL/SQL functions and packages to automate workflows in a Postgres environment linked with web technologies
Maintaining data standards (including adherence to HIPAA), documenting work, and championing process improvements
Work as a project team member to jointly deliver high-quality business solutions consistent with project objectives and constraints

Knowledge and Skills Required:

4+ years experience with HL7 2.x
2+ years experience with Apache Nifi
2+ years experience with NextGen Connect
Advanced knowledge of PostgreSQL, MySQL, AWS Code Deploy, HL7 2.x, C-CDA, FHIR, and HIPAA data compliance

Qualifications and Other Attributes Required:

Degree in Computer Science or equivalent field, or equivalent work experience
Highly self-motivated team player who takes initiative
Strong sense of personal accountability; contributes beyond job role and responsibilities
Excellent problem-solving skills; meticulous & methodical",85000,,,,,-1,,NC,-1,data engineer,na,"['sql', 'shell']",['aws'],['excel'],"['postgresql', 'mysql']",[],[],,+10 years
N9 it solutions,4.6,Remote,Data Engineer,"Job title: Data Engineer
Visa's: CPT, OPT-EAD, H1B transfer
Employment: W2 position ( should be ok with Marketing)
Location: Hybrid Or Remote
(Authorized to work anywhere in the USA and for only those who are staying in the USA)
It's a long term project
Job description
Data Engineer (primary technologies: Azure Data Factory, Synapse, Synapse Pipelines, ADLS Gen 2, understanding of DataWarehouse concepts, ELT, Azure DevOps, Azure resource groups)
Develop architectural strategies for data modeling, design and implementation to meet stated requirements for metadata management, operational data stores and Extract Transform Load environments
Work with business leaders and teams to collect and translate information requirements into data to develop data-centric solutions
Apply industry-accepted data architecture principles and standards for modeling, stored procedures, replication, regulations, and security, among others, to meet technical and business goals.
Work to streamline data flows and models; improve consistency, quality, accessibility, and security; unify data architecture; remove unnecessary costs; and, optimize database activity across company needs.
Analyze and understand Data sources & APIs
Design and Develop methods to connect & collect data from different data sources
Design and Develop methods to filter/cleanse the data
Work closely with Data Scientists to ensure the source data is aggregated and cleansed
Work with Cloud and Data architects to define robust architecture in cloud setup pipelines and workflows
Benefits:
H1B and GC filling
Free training and Placement
E-verified
On-job technical support
Guesthouse facilities are also available
Skill Enhancement
Opportunity to work with Fortune 500 Companies
Job Type: Full-time
Salary: $40.26 - $56.00 per hour
Experience level:
6 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91",86634,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2016,$5 to $25 million (USD),Remote,7,data engineer,na,['sql'],['azure'],[],[],[],[],,0-2 years
Staff Bees Solutions,4.0,"Dallas, TX",Data Engineer,"We are looking for OPT/CPT individuals, Helping Them to Train, Providing knowledge, and Placing Them in Fortune companies with the help of our Direct Clients in Big Data, Machine Learning, And Data Engineering Suitable Positions. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics..
Qualifications for Data Engineer
An individual who has valid visa, Opt/Cpt are Applicable
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong project management and organizational skills.
Job Types: Full-time, Part-time, Contract
Salary: $70,000.00 - $80,000.00 per year
Benefits:
Employee assistance program
Health insurance
Professional development assistance
Relocation assistance
Compensation package:
Yearly pay
Experience level:
1 year
No experience needed
Under 1 year
Schedule:
10 hour shift
4 hour shift
8 hour shift
Monday to Friday
Ability to commute/relocate:
Dallas, TX 75243: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",75000,,,,,-1,,TX,-1,data engineer,na,['sql'],['aws'],[],[],[],[],,0-2 years
Quadrant Resource,4.1,Remote,Data Engineer,"Job Description:
4-6 years of experience as a Data Engineer in the Snowflake-Fivetran-dbt stack
• Possess a solid understanding of end to end Data ingestion, ETL, data analysis, Reporting processes
• Expert in ETL development using Fivetran and experienced in implementing data pipelines for Snowflake Cloud DWH
•Experienced in using dbt for data transformation
•Expertise on Tableau/Metabase for building reports/dashboards
•Expert in troubleshooting and resolving ETL issues, data load failures/problems and transformation translation problems
•Excellent communicator with the ability to work effectively with distributed teams
Job Type: Full-time
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Data Engineer: 9 years (Required)
Fivetran: 8 years (Required)
DBT: 7 years (Required)
Snowflake: 6 years (Required)
Work Location: Remote",112889,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,Unknown / Non-Applicable,Remote,19,data engineer,na,[],['snowflake'],['tableau'],"['snowflake', 'dbt']",[],[],,5-10 years
Gridiron IT,4.5,Remote,Data Engineer,"GridironIT is seeking a Data Engineer.
Responsibilities:
We are looking for seasoned Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications:
US Citizen Only
Ability to hold a position of public trust with the US government.
8+ years industry experience coding commercial software and a passion for solving complex problems.
8+ years direct experience in Data Engineering with experience in tools such as:Big data tools: Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra
Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
AWS cloud services: EC2, EMR, RDS, Redshift
Data streaming systems: Storm, Spark-Streaming, etc.
Search tools: Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Amazon S3, Athena, Redshift Spectrum, AWS Glue, AWS Glue Catalog, AWS Functions, and Amazon EC2 with SQL Server Developer
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Experience with Informix and Data Stage
Job Type: Full-time
Pay: $140,000.00 - $150,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Hourly pay
Yearly pay
Experience level:
10 years
11+ years
7 years
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Due to the nature of the role, US Citizenship is required. Do you possess US Citizenship?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 7 years (Required)
SQL: 7 years (Required)
AWS: 4 years (Required)
Big data: 5 years (Preferred)
NoSQL: 5 years (Preferred)
Work Location: Remote",145000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable,Remote,6,data engineer,na,"['python', 'scala', 'java', 'sql', 'nosql']","['azure', 'aws', 'redshift']",[],"['sql server', 'elasticsearch']","['kafka', 'spark', 'hadoop']",[],bachelor,5-10 years
Kastech Software Solutions Group,4.0,"Dallas, TX",Data Engineer,"In KastechSSG we are are seeking for a Cloud DB Platform Engineer with Data Enablement Services experience ($50/hour), to work in a Hybrid Schedule (3 days on-site, 2 days home office) for one of our partner companies!
Hybrid Schedule info: onsite 3 days per week from list of locations: Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ - Must be local to one of these locations.
** Contract would be $50 per hour on W2 (18 months). American Citizens or GC Holders Only**
3-5 years experience in:
Terraform, Azure/GCP, DevOps Engineering, ETL, Client/AI, Data lake, Data architecture.
GCP Key Services Experience (ie Big Query, Composer, Vertex AI, DataProc, Data Catalog, Notebooks).
Azure Key Service Experience (ie Synapse, Purview, Data Lake, Logic Apps, Cognitive Services).
Job Type: Full-time
Salary: $50.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you American? If not, please specify your visa type
Would you be willing to work under W2?
Experience:
Terraform: 3 years (Preferred)
Azure Data Lake: 3 years (Preferred)
GCP Key Service Exp: 3 years (Preferred)
DevOps: 3 years (Preferred)
Work Location: In person",90000,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2008,$25 to $100 million (USD),TX,15,data engineer,na,[],['azure'],[],[],[],['terraform'],,2-5 years
Virtualan Software LLC,4.5,"Atlanta, GA","Sr. Data Engineer (Python,Spark & Databricks)","Role : Senior Data Engineer (Python, Spark & Databricks)
Location : Hybrid (Atlanta, GA)
Pay: 60-70$ on C2C / 1099
RESPONSIBILITIES:
Create or modify the conceptual, logical and physical data models.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources like Hadoop, Spark, AWS Lambda, etc.
Lead and/or mentor a small team of data engineers.
Design, develop, test, deploy, maintain and improve data integration pipeline.
Develop pipeline objects using Apache Spark , PySpark and/or Python.
Design and develop data pipeline architectures using Spark and related AWS Services.
Communicate effectively with client leadership and business stakeholders.
Participate in proposal and/or SOW development.
REQUIREMENTS:
5+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
2+ years of experience migrating/developing data solutions in the AWS cloud is required.
1+ years of experience building/implementing data pipelines using DataBricks is required.
Expert level knowledge of using SQL to write complex, highly-optimized queries across large volumes of data.
Hands-on object-oriented programming experience using Python is required.
Professional work experience building real-time data streams using Spark.
Knowledge or experience in architectural best practices in building data lakes.
Bachelor or Master degree in Computer Science, Engineering, Information Systems or relevant degree.
Hybrid work schedule, and must live in the Atlanta, GA metro area. Must be open to up to 25% national travel to client locations when on engagements outside of the Atlanta, GA area.
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Apply only if you are interested in Hybrid Role in Atlanta, GA. Interested ?
Experience:
Data Engineer: 4 years (Required)
Databricks: 1 year (Required)
Willingness to travel:
25% (Required)
Work Location: On the road",117000,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2019,Less than $1 million (USD),GA,4,data engineer,senior,"['sql', 'python']","['databricks', 'aws']",[],[],"['hadoop', 'spark']",[],bachelor,0-2 years
GOBankingRates,3.3,"North, SC",Staff Data Engineer,"GOBankingRates™ is unique in the digital marketing and media industry - we combine marketing, digital, content and fintech. Our performance based approach increases brand awareness and generates targeted audience engagement on our internal web properties and partner sites.
Learn More About What We Do

What's interesting about this role?
GOBankingRates has big growth plans ahead and is looking for a strong Staff Data Engineer experienced in Data warehousing and Python to join our Data Engineering team. The GOBankingRates Data Engineering team is responsible for designing and developing the Data lake, enterprise database, data warehouse, reporting solutions, and pipelines for data processing. If you are a critical thinker with a solid track record of developing data solutions and solving complex problems with SQL and Python, we want you to join our team! You will play a vital role in designing and developing our next generation data pipelines and data platform. Join our team and prototype new data product ideas and concepts!
How will you make an impact?
Build and maintain multiple data pipelines to ingest new data sources (API and file-based) and support products used by external users and internal teams.
Optimize by building tools to evaluate and automatically monitor data quality and develop automated scheduling, testing, and distribution of feeds.
Work with data engineers, data scientists, and product managers to design, rapid prototype, and productize new data product ideas and capabilities.
Design and build cloud-based data lakes and data warehouses.
Conquer complex problems by finding new ways to solve them with simple, efficient approaches focusing on our platforms' reliability, scalability, quality, and cost.
Collaborate with the team to perform root cause analysis and audit internal and external data and processes to help answer specific business questions.
What will you bring to us?
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Experience with dimensional data modeling and schema design in a database or data warehouse
Expertise with scripting languages such as Python and writing efficient and optimized SQL.
Working experience in building data warehouses and data lakes.
Experience working directly with data analytics to bridge business requirements with data engineering.
Experience with AWS infrastructure
Ability to operate in an agile, entrepreneurial start-up environment and prioritize
Excellent communication and teamwork, and a passion for learning
Curiosity and passion for data, visualization, and solving problems
Willingness to question the validity, accuracy of data, and assumptions
Preferred Qualifications:
Experience building data warehouse, data lake, and data pipeline using Snowflake/Redshift and other AWS Technologies.
Experience with large-scale distributed systems with large datasets.
Experience with event streams and stream processing (e.g., Kafka, Spark, Kinesis)
Hands-on experience with event streaming with modern event streaming tools like Pulsar, Kafka, and Kinesis. Understanding when streaming vs. batch processing is appropriate, and tradeoffs in a given context
Knowledge of advertising platforms.

Benefits
Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity and passion.
Awesome medical, dental and vision plans with heavy employer contribution
Paid maternity leave and paternity leave programs
Paid vacation, sick days and holidays
Company funding for outside classes and conferences to help you improve your skills
Contribution to student loan debt payments after the first year of employment
401(k) - employees can start contributing immediately. After the first year, GOBankingRates matches your contribution up to 4% of your salary
A note about our response to COVID -19 and our new norm: The world has changed and we know it's important to adapt and to do our part to take care of our teams in this global pandemic. Our number one priority is to have our team feel safe, balanced and connected. We're committed to providing our teams with the best resources and tools to navigate this new virtual world that we're living in. We've also reinvented the ways in which we recognize, celebrate, and engage with each other to keep our culture strong!
Here's a peek into our world at GOBankingRates -
Our teams are working remotely 100% for the foreseeable future and have flex time. We're in the digital media space so we're mobile and flexible!
Option to work from an office (if you need to get away!)
Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Airtable, Slack, Zoom and so much more!)
To keep our community engaged and connected, virtual team building events are held weekly and monthly.
For wellness and balance, weekly virtual fitness classes such as yoga are available.
To care for the local communities that we're a part of across the U.S our team members host socially distanced philanthropic events every quarter.
And most importantly, we've committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness.
We are an equal-opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.",134519,51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,-1,$25 to $100 million (USD),SC,-1,data engineer,na,"['sql', 'python']","['snowflake', 'aws', 'redshift']",[],['snowflake'],"['kafka', 'spark']",[],master,
MoneyDolly,5.0,"Salt Lake City, UT",General Data Engineer,"Position Title: General Data Engineer
Location: Remote (US Based only)
Commitment: Full-Time

Job Brief

About Us
MoneyDolly is a fast growing Fintech Saas tech company, headquartered in Sandy, Utah. We are leading innovation in supporter relationship management, wherein teams meet all their fundraising goals. Teams can create their own page for your supporters to visit, offer products and incentives for their contributions, then simply invite their team to spread the word and watch the money roll in. MoneyDolly has already helped thousands of organizations, groups and teams nationwide raise over $20 million. We are still private, and our best work is still ahead of us. This is a massive industry with antiquated methods and no clear market leader. This is the spot for a qualified team player looking to build something new, make a real impact, and actually change the world.

Job Description:
We are looking for a highly motivated and talented Generalist Data Engineer to join our development team. The ideal candidate will be someone who loves to wear multiple hats and thrives in an environment of high autonomy to accomplish our business goals. You will be responsible for collaborating with our business team, creating flexible data models, and maintaining data pipelines to ensure the smooth flow of data in our organization.
Responsibilities:
Interact with our business team to gather requirements and understand data needs
Design and create flexible data models that allow for easy report generation and ad-hoc analysis
Write and maintain DBT transformations to generate flexible data models from our production database
Develop and maintain required data pipelines in Python to collect and process data from various sources
Develop and maintain Tableau and Hex.Tech dashboards to analyze the collected data
Ensure data quality, integrity, and security in all data processes
Collaborate with other team members to implement data solutions and integrate them into the existing infrastructure
Continuously monitor and optimize data models and pipelines to meet changing business requirements
Requirements:
Bachelor's degree in Computer Science, Engineering, or a related field
Strong experience in data modeling, ETL processes, and data pipeline development
Proficiency in Python and SQL
Familiarity with DBT transformations and best practices
Experience working with relational databases and big data technologies
Excellent problem-solving skills and attention to detail
Strong communication and collaboration skills
Ability to work independently and adapt to a fast-paced, dynamic environment
Nice to have:
Experience with data visualization tools (e.g., Tableau, Power BI)
Experience with AWS and GCP (RDS Postgres and BigQuery are our current stack)
Knowledge of the fundraising industry or a strong interest in learning more about it
What We Offer:
A competitive salary ($100k-$140k range) and benefits package
A supportive and collaborative work environment
Opportunities for professional growth and development
The chance to make a significant impact in a growing startup
If you are passionate about data engineering and excited about the prospect of revolutionizing the fundraising industry, we'd love to hear from you. Apply now to join the MoneyDolly team!",120000,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,UT,-1,data engineer,na,"['sql', 'python']",['aws'],"['tableau', 'power bi']",['dbt'],[],[],bachelor,
Tipico - North America,4.0,"Denver, CO",Data Engineer,"Company Description

Tipico is an energetic, passionate, bold tech company, and we love sports! Tipico has recently expanded to the US market, establishing offices in Hoboken, New Jersey and Denver, Colorado. We are driven by our core values: passion, progress and trust. Our goal is to build the best possible mobile sports betting product in the industry, in order to amplify the emotions of millions of sports fans every day. Let’s Make This Interesting ™
Please note: this role is located in our Denver, CO office; we work off a hybrid model and come into the office 2 days per week.

Job Description

The Data Engineer will report directly to the Data Engineering Manager and work together with rest of the data engineers, BI analysts, and product teams. The position holder ensures this is done by doing the following:
Create and maintain both batch (ETL/ELT) and real-time data pipelines and architecture
Ensure all data provided by Data Platform is of the highest quality and is delivered in a timely manner and in line with agreed SLAs
Assemble large and complex data sets that meet functional and non-functional business requirements
Identify, design, and implement internal process improvements, including automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Collaborate with stakeholders including data architects, Executive, Product, Data and IT team members from the beginning to the delivery of a project
Build a scalable, modern, and trusted data ecosystem, making sure it is responsive, available and in line with business requirements
Take part in sprints, workloads, and deliverables through agile methodologies
Manage quality assurance, verification of accuracy and consistency of data, specifically KPI measures sent to the business
Thrive working in a fast-paced and challenging environment
Open to continuous learning individually and with the team
Comfortable with being challenged / seeking challenge
Willing to work flexible hours if needed

Qualifications
3+ years in Data Platforms / Datawarehouse / BI experience
Must have experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
Strong analytical and problem-solving skills that balance with creative approaches
Ability to learn new and complex concepts quickly
Strong experience with large-scale production databases and SQL
Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR, EKS, RDS and Lambda
Proficient with one or more big data & new data technologies such as DBT, NIFI, Airflow, Kafka
Experience with data modelling
Experience with ETL development (extractions, data load, aggregation, transformation)
Knowledge in working with time-series/analytics databases such as Elasticsearch
Understanding of containerization and orchestration technologies like Docker/Kubernetes
Must be able to take ownership of projects with a high level of attention to detail and priority
Team player with a hands-on approach and attitude
Must be comfortable with on call rotation
Nice to have:
Familiarity with Marketing tooling and/or CRM systems is preferred
Familiarity with using business intelligence tools such as Domo, Tableau or Looker
Knowledge about and / or experience with sports betting
Background in digital companies, online, gambling, gaming, e-commerce, marketing, or mobile apps
Familiarity with one scripting language, preferably Python

Additional Information

What's in it for you:
Work in an environment where you, your work and ideas matter and have an impact.
You will be a be part of the newest and up and coming USA online iGaming and sports betting companies in the market.
Work in a new and young business with high growth potential.
Build your own success story together with us.
Work with self-organized, self-responsible and entrepreneurial employees.
Start-up feeling, backed-up by a leading European sports betting house. We are a high-volume business and are taking off in the US!
Competitive salary, Medical, Vision, Dental Benefits, Unlimited PTO, and more
Salary for this position is $120k+ & annual performance bonus",120000,201 to 500 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2004,Unknown / Non-Applicable,CO,19,data engineer,na,"['sql', 'python']","['aws', 'redshift']","['looker', 'tableau']","['elasticsearch', 'dbt']",['kafka'],['docker'],,
National Group Corporation,4.0,"Dallas, TX",Data Engineer,"Who are we?
ReAlign Insurance Holdings is a leading primary underwriter of program administrator managed specialty insurance programs in the U.S. With the extensive knowledge and experience our team has in building and managing carriers and programs we are able to reach decisions on program opportunities quicker than our competition.
Why choose ReAlign?
At ReAlign, we believe that we are only as strong as our people. We strive to create an inclusive and welcoming culture where employees feel comfortable and empowered to be themselves. We support a diverse and inclusive work environment, with a focus on a healthy work-life balance. We understand that comprehensive benefits are important which is why we offer a benefit package that includes: medical, dental, vision, company paid short and long-term disability, life insurance, health spending account, 401(k) match every pay period, PTO, paid holidays, floating holidays, tuition reimbursement, and referral bonuses.
Job Summary
The Data Engineer position will involve data exchange process analysis, design, development, unit testing, and support. This role would work with other data team members on database ETL processes and reporting needs in both the on-premise database environment as well as the Azure data environment. This position requires strong technical and analytical skills to review business processes and data, recommend solutions, conduct functional and code analysis, and lead and perform database development tasks. This position will also involve production support and require experience within a DevOps environment.
Essential Functions
· Design, development, and support data ETL processing for both external and internal customers
· Support the internal and Azure cloud-based data warehouse platforms
· Work with DBA/System Admin to develop and maintain automated data processing
· Design and analyze queries and stored procedures
· Write and support SSRS and PowerBI reports
· Work in a data development life cycle environment
Work with stakeholders and data analysts to understand business requirements
Work with end users and the testing team to perform unit, functional, and integration testing and validation prior to production implementation
Troubleshoot and resolve operational problems with a focus on delivery
· Communicate solutions and architectural design to IT leadership
· Work in both Kanban and Scrum Agile environments
· Continually improve processes and solutions, seeking ways to use new tools, features, and capabilities
Job Requirements
3-10 years of experience as a Database Developer
Highly proficient in writing advanced SQL queries, stored procedures, SSRS reports, and ETL processes
Strong knowledge of relational and transactional database architecture
Strong technical and analytical skills
Experience with the Microsoft Azure and PowerBI platforms and Agile development methodology a plus
Bachelor’s Degree in computer science, database development, or related field preferred
Detail oriented with the objective of delivering a solution to a problem in a timely and efficient manner
Self-driven, willing to take initiative when an opportunity presents itself
Strong collaboration and communication skills and the ability to work in a team environment a must
Ability and willingness to work in a partial on-prem and partial remote office environment
Experience in the Property and Casualty Insurance industry a plus
Proficient with standard desktop software used in a business environment (MS Word, Excel, Visio, etc.)
Must be able to work in a hybrid in-office and remote work environment
Must be authorized to work in the United States
Job Type: Full-time
Pay: $90,000.00 - $125,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you currently local to the Dallas area and able to commute to the office 2 to 3 times per week? (Deal breaker)
Experience:
writing advanced SQL queries and ETL processes: 3 years (Required)
cloud database platform (i.e. Azure, AWS): 1 year (Preferred)
Azure Data Factory: 1 year (Preferred)
Work Location: In person",107500,,,,,-1,,TX,-1,data engineer,na,['sql'],"['azure', 'aws']",['excel'],[],[],[],bachelor,0-2 years
Stonehenge Technology Labs,5.0,"Bentonville, AR",Data Engineer (ETL),"At Stonehenge Technology Labs, we help leading CPG teams win in the omni-commerce space by bringing together data that powers insights and drives autonomous actions. The faster STOPWATCH™ Members identify opportunities and command gaps across their total online and in-store assortment, the faster they win! Learn more here: https://stopwatch.tech
The team at Stonehenge Technology Labs operates off three distinct principles: 1) Empower Unique Talent; 2) Operate as a World-Class Team; 3) Solve Complex Problems with Speed & Simplicity. Stonehenge recruits, employs, trains, compensates, and promotes regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Visa sponsorship is not available for this role.
Purpose:
As the Data Engineer at STOPWATCH™, you will be a data engineering powerhouse for the organization, tasked with pulling in data from a variety of sources, storing them in their raw format for long term accessibility, and then ingesting and transforming the data for easy consumption by teams downstream.
You will:
· Develop, monitor, maintain, optimize, and orchestrate end-to-end data pipelines while leveraging best practices.
· Develop data solutions to provide relevant, timely, and insightful information supporting both upstream and downstream teams.
· Leverage APIs to their full potential and for the betterment of our platform offerings.
· You may be tasked with deploying cloud resources as needed within the processes and procedures established.
· Leverage our stack, including but not limited to Databricks and SQL, in the most optimal way, making recommendations and executing on them in close partnership with the Lead Data Engineer and other functions.
· Take on projects hands-on with a high level of professionalism and varying levels autonomy.
· Be able to independently make decisions around tradeoffs between different methods for building pipelines and data extraction, with a core emphasis on data safety, cost optimization, and data accuracy.
· Partner closely with security engineer, cloud architect, product teams, and Power BI engineers to achieve business goals.
We expect you to operate with speed, simplicity, and store your work on the group system with a focus on process mapping and documentation enabling rapid scale and interconnected efficiencies between work groups.
As part of the application process, all candidates are required to complete a skills test.
SKILLSET/EXPERIENCES:
· Expert level understanding data engineering, specifically in end-to-end pipeline solutions is a must
· 2+ Years of data engineering experience is a must
· 2+ Years of cloud experience, preferably in an Azure environment is a must
· 2+ Years of SQL experience is a must
· 1+ Years of Python or Scala experience is a must
· 1+ Years of owning data pipeline orchestration is a must
· 1+ Years of DevOps experience or comparable is a must
· 1+ Years of Databricks experience is a must
· 1+ Years of experience with Azure Data Factory or equivalent is a plus
· Experience working with Typescript/Node.js is a plus
· Extreme attention to detail including self-monitoring QA
· Penchant for documentation, reproducibility, and standardization especially in terms of standard data model matrices
· Familiarity with retail/retailer/supply chain data is a plus; experience working for a major retailer or CPG brand even bigger plus
· Scrappy mindset; resourceful and relentless in finding answers
This role (and every role at Stonehenge Technology Labs) is expected to read, notate and arrive ready to discuss THE GOAL by Eliyahu Goldratt on Day 1.
Here’s what we offer in addition to competitive base pay:
Stock incentives based on individual and team performance
5 weeks paid vacation, 11 paid holidays
Flexible (albeit intense) work schedule
100% Company Paid Health Care for Employee and Spouse/Partner
Available Dental and Vision Plans
Available 401K Program
Contribution matching to qualified 501c3 organization (team member’s choice),
Kid/pet/partner focused culture
Paid maternity/paternity leave and financial assistance in support of adoptive & fostering activities
Executive coaching (certain roles)
Job Type: Full-time
Pay: $110,000.00 - $135,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Health savings account
Paid time off
Parental leave
Retirement plan
Vision insurance
Experience level:
2 years
Schedule:
Monday to Friday
Application Question(s):
Do you now or in the future require visa sponsorship to continue working in the United States?
Experience:
Databricks: 1 year (Required)
Python: 1 year (Required)
Data Engineering: 2 years (Required)
Cloud infrastructure: 2 years (Required)
SQL: 2 years (Required)
Work Location: Hybrid remote in Bentonville, AR 72712",122500,1 to 50 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2019,Unknown / Non-Applicable,AR,4,data engineer,na,"['sql', 'scala', 'python']","['databricks', 'azure']",['power bi'],[],[],[],,2-5 years
DataArt,4.6,Remote,Data Engineer,"CLIENT
Our client is a pioneer in US schools’ education since 2000, it is leading the way in next-generation curriculum and formative assessment. They develop a number of solutions and interactive web products for teachers, students and their parents.

The company is technology-driven with a huge number of software engineers involved in product development. They have a very solid approach to technology, employ best practices and processes, with a focus on cutting-edge frameworks, languages and tools.

We invite to the company, not a project
TEAM
There are multiple autonomous and empowered teams who work with their own part of the product and choose their own way of doing things and they expect people to be proactive and good team players.
Responsibilities
Building well-tested and optimized ETL data pipelines for both full and delta extraction
Collaborating with data analysts and learning scientists to store, aggregate, and calculate captured students’ work
Contributing to leading industry data standards, such as Caliper Analytics or xAPI
Improving our deployment and testing automation data pipelines
Requirements
3+ years of professional software development or data engineering experience
Strong CS and data engineering fundamentals
Proven fluency in SQL and a development language such as Python
Experience with the following storages: Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB)
Experience with ETL/BI: Airflow, DBT, Matillion, Looker
Experience with Cloud Infrastructure: AWS Kinesis, Lambda, API Gateway, Terraform
Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake)
BS in Computer Science, Data Science, or equivalent
Experience in education or ed-tech
Proven passion and talent for teaching fellow engineers and non-engineers
Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
Strong communication skills in writing, conversation",112889,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1997,$100 to $500 million (USD),Remote,26,data engineer,na,"['sql', 'python']","['snowflake', 'aws']",['looker'],"['dynamodb', 'snowflake', 'dbt']",[],['terraform'],,
Staffbee Solutions Pvt Ltd,5.0,"Dallas, TX",Data Engineer,"Dear OPT/CPT candidates,
We currently hiring OPT/CPT Individuals , we also specialized in training ( data engineering course , How to handle the interviews etc...) and placement for Data Engineering positions, With many benefits like
Free accomodation untill you get free accomodation
100% placement
Modification of resumes according to the market standards
Best Package
H1b sponsership
Travel allowances
+1469 902 8976 (wapp/call)
Or share resume at vijay.komma@staffbees.com
Job Type: Full-time
Salary: $60,000.00 - $70,000.00 per year
Application Question(s):
This requirment is for only OPT/CPT visa status candidates , Do you aware?
Are you sure you have read the JD?
Work Location: One location",65000,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2018,Unknown / Non-Applicable,TX,5,data engineer,na,[],[],[],[],[],[],,
Vibrant Planet,4.0,California,Data Engineer,"About Vibrant Planet
We are a team of leaders in science, forestry, policy, and tech, building a cloud-based, data-driven platform to increase the pace and scale of forest restoration and reduce catastrophic wildfire, tree mortality, forest degradation, and deforestation. Our current software modernizes land management planning and monitoring with AI-driven data development, user friendly scenario building and decision support, and forest resilience trends and treatment outcome detection. The system quantifies potential and actual treatment benefits, helping to grow markets for ecosystem services (carbon, water, biodiversity, sustainable forest-derived products).
Our initial platform (launched in September 2021) is focused on temperate, “fire adapted” landscapes in California and the Western US. Our mission is global; our roadmap expands into other geographies and forest types accordingly.
Driven by our sense of urgency to protect vital forests and the services they provide, we use our expertise to build sophisticated, AI/ML-driven, user friendly products to democratize the use of data and accelerate the process of stabilizing and restoring our forests.
Vibrant Planet is backed by climate and ecosystem resilience solutions leaders, including Grantham Foundation, Earthshot, Elemental Excelerator, Ecosystem Integrity Fund, Chris Cox (CPO at FB), Neil Hunt (ex CPO of Netflix), Cisco, Valia Ventures, and Halogen Ventures.
For further information please visit: VibrantPlanet.net
Equal Opportunity Employer: Vibrant Planet is committed to diversity. We encourage applicants from all cultures, races, colors, religions, sexes, national or regional origins, ages, disability status, sexual orientation, gender identity, military, or other status protected by law to apply.
We are most interested in finding the best candidate for the job, and that candidate may come from a less traditional background, but have capacity to grow into and thrive in the position after some mentoring. We do not require that you have experience with every job description task. We will consider any equivalent combination of knowledge, skills, education, and experience to meet minimum qualifications. We encourage each candidate to think broadly about their unique background and skill set and how it may relate to the role. This is important to us. We aren’t just saying this, we mean it.
For further information please visit: https://vibrantplanet.net
About the Role
We are looking for a versatile, hands-on data focused engineer to help us scale and build our geospatial pipelines and tooling. You will be part of a small team of engineers and scientists, tackling some of the most important climate change related issues facing the world today. This is a remote role (and company) so being comfortable and effective working in a distributed team is crucial.
Key Responsibilities:
Design, develop, scale, and maintain data pipelines that ingest, transform, and store large volumes of geospatial data from multiple sources.
Optimize data processing and storage performance and cost efficiency by leveraging cloud-based technologies and services.
Collaborate with engineers, scientists and other stakeholders to share knowledge and build expertise.
Lead and participate in development life cycle activities like design, coding, testing, and production release.
Contribute to our evolving engineering culture, standards, tooling, and processes.
Mentor and support other engineers and deeply review code.
Technical Qualifications
Strong software engineering skill set.
2+ years experience in data engineering or a related field.
Experience with distributed data processing frameworks and tools (e.g., Airflow, Hadoop, Spark).
Proficiency with Python or an equivalent language. Ability to write clean, high-quality code and tests to keep our system fast, reliable, and monitorable.
Bonus Qualifications - Absolutely not required, but nice to have
Airflow
Pandas / Geopandas
Jupyter notebooks
Geospatial processing
Lidar data
Satellite imagery
Experience with ArcGIS/QGIS
Other Expectations:
Strong communication skills; discussing complex technical concepts to engineers and non-engineers is no problem to you.
Collaborative and supportive team player, with a desire to enrich our engineering culture.
Eagerness to learn, think creatively, and share knowledge with others.
Ability to write understandable, testable code with an eye towards maintainability.
Proactive and empathetic mindset - you love to roll up your sleeves to fix problems
Location - We are a remote first company with the majority of the team in the US west coast time zone. We also have a small and growing presence in New Zealand. Location can be flexible, but we are primarily targeting those two time zones and anything in between.",112889,,,,,-1,,California,-1,data engineer,na,['python'],[],[],[],"['spark', 'hadoop']",[],,+10 years
Siri info solutions inc,4.0,"Beaverton, OR",Sr. Data Engineer,"Must Have Skills
PySpark
Python on AWS
Responsibilities:
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Advanced SQL skills with experience querying large datasets from multiple sources and developing automated reporting
Python skills for scripting, data manipulation, custom extract, transform and loads and statistical/regression analysis particularly, as they apply to sales and marketing operations and performance
Job Type: Full-time
Salary: $47.00 - $50.00 per hour
Experience level:
7 years
Work Location: On the road",87300,,,,,-1,,OR,-1,data engineer,senior,"['sql', 'python']",['aws'],[],[],[],[],,5-10 years
American Power & Gas,2.5,"Largo, FL",Data Engineer,"Because of expansive growth American Power & Gas is seeking a Data Engineer to add to our technical team. This is a fulltime permanent on-site role.
We have been offering Green Energy solutions to both residential and small commercial customers for over 20 years and have won the award for fastest growing company in the Tampa Bay Business Journal as well as being featured in Forbes and the Huffington Post.
**
**
Key Responsibilities
Support operational executives in solving business problems by designing, developing, troubleshooting, and implementing data driven solutions to complex technical objectives.
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Deploy sophisticated analytics programs, machine learning and statistical models to predict business outcomes and continually optimize performance through data science.
Gather and summarize technical requirements associated with strategic business initiatives.
Document, maintain, support and enhance company technology platforms and analytics applications.
Look for opportunities to streamline and automate various reporting processes to help support internal teams.
Assist with the research and development of internal business case studies to support onboarding new data tools.
Extract, manipulate and cleanse raw data from various data sources – call center, web, and CRM systems.
Work independently as well as collaboratively with other team members and key stakeholders as required to troubleshoot and resolve data issues.
Provide comprehensive technical and consultative services to support development and maintenance of internal and third-party platforms.
Create functional test cases/criteria to verify all functionality adheres to specifications and create end user manuals.
Routinely represent the Analytics department in cross-functional status & data strategy meetings.
Assist business analysts in provision of regular performance trends reporting, forecasts, and insights for marketing and sales team leaders as well as senior executive management to maintain a tight finger on the pulse of emerging performance trends and opportunities.
Partner with internal cross functional teams to identify business needs and analytics opportunities, developing tools and techniques to analyze and provide performance-improving recommendations.
Partner with the Operations team to optimize data workflows from sourcing to storage to reporting to deliverables to maximize for value-added and time-efficiency.
Develop, enhance, and manage various analytical solutions in support of business objectives.
Determine what data is needed and how to consume and store this data to support reporting needs and ad hoc performance-improving analysis for internal stakeholders.
Ensure deliverables are adapted properly to stakeholder audience; adapting terminology and visuals as needed to “speak the stakeholder’s language”, thus communicating with maximum effectiveness.
Troubleshoot and QA data, reporting, and tracking anomalies as needed, with proactive communication to stakeholders.
Relentlessly challenge the status quo. Always be critical of how we can be more effective or more efficient as an individual, as a team, and as a business.
Provide ongoing and proactive client service to your internal customers as required to continue elevating the performance of the business.
Regularly work with and analyze data across marketing channels and the customer journey through website analytics, call center activity, and CRM systems.
Requirements
University degree or college graduate in Engineering, Computer Science, Mathematics, Statistics, related technical/programming discipline, or the equivalent hands on experience in Data Engineering or Software Development.
Proficiency with coding in SQL is required. Ability to also write in Python, R, Java, or similar programming languages is preferred.
Strong technical prowess, including an understanding of algorithms, systems architecture and end user experience.
Experience with modern source, build, and deploy tools such as Git, Grub, Maven, Yeoman, etc. is a plus.
Ability to think unconventionally to derive innovative and creative solutions.
Competency in accurately estimating development timelines.
Experience with data warehouse design, relational databases, SQL/NoSQL data modeling, RESTful API standards and large scale data processing solutions.
Demonstrated skill in database development with solid understanding of schema design, stored procedure development, query optimization and ETL processes.
Excellent troubleshooting ability. Must be able to resolve issues tied to capturing and processing data in a timely manner.
Excellent English written and verbal communication skills, especially explaining technical concepts to non-technical business leaders.
Exceptional critical thinking and problem-solving skills; able to distill overall objectives into the actionable steps required to achieve those objectives.
Capable of effectively managing projects, priorities, timelines, and working relationships.
Must possess the intellectual curiosity to succeed in a dynamic, entrepreneurial, fast paced, sales driven organizational culture.
Excited by the opportunity to disrupt the status quo and uncover the eureka moment insights that will take the business to the next level – proactively searching for problems to solve, knowing there is so much to learn.
We offer Health, Dental, Optical and Life Insurance, PTO (paid time off) and the opportunity for promotions and room to advance.
For immediate consideration please send a resume to Carl Schumacher the Manager of Recruiting CarlS@goapg.com
Job Type: Full-time",92733,201 to 500 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,2010,$100 to $500 million (USD),FL,13,data engineer,na,"['sql', 'java', 'nosql', 'r', 'python']",[],[],[],[],[],,
Kanini,4.0,"North Druid Hills, GA",Senior Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for Data Engineer who has a deep experience in Data Engineering, Agile planning, Snowflake, SQL, MySQL, PostgreSQL, Git, AWS Data Tools, Python, Jira
Job Description
The Senior Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Senior Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies.
Responsibilities
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understands and enforces appropriate data master management techniques.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Work with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Understands the challenges that the analytics organization faces in their day-to-day work and partner with them to design viable data solutions.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Installs, maintains, monitors, and supports business intelligence, distributed computation, and big data analytics tools.
Provides ongoing support, monitoring, and maintenance of deployed products.
Actively works with less experienced data engineers providing technical guidance and oversight.
Actively participates in the engineering community, staying up to date on new data technologies and best practices and shares insights with others in the organization.
Qualifications:
Bachelor’s degree in computer science or related field and 4 years work experience
Working experience with batch and real-time data processing frameworks
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience in design, development, and implementation of highly scalable, high-volume software systems and components, client-facing web applications, and major Internet-oriented applications and systems
Working experience working with relational databases such as SQL, MySQL, Postgres/PostgreSQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Working experience with application lifecycle methodologies (e.g. waterfall, agile, iterative)
Experience with ETL processes and tools
Experience working with Git.
Desirable Qualifications
Scrum Developer Certification or equivalent
Working experience with Sisense platform
Working experience with SQL Server Integration Services
Working experience with AWS data tools (Database Migration Service)
Working experience with Python
Experience working with Jira, Rally or similar tools
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $65.00 per hour
Application Question(s):
Current Location?
Work Location: Remote",112500,,,,,-1,,GA,-1,data engineer,senior,"['sql', 'python']","['snowflake', 'aws']",[],"['sql server', 'postgresql', 'snowflake', 'mysql']",[],[],master,2-5 years
Hiscox,4.1,"Atlanta, GA",Senior Data Engineer,"job description:
You will be a senior member of our US IT Team data engineering practice. You will be an early member of this team and will work closely with the Director of Data Engineering to help establish it. We will have our work cut out for us delivering planned work, building the practice and evolving our technical landscape. This is an exciting opportunity for the right individual to join at the very beginning and grow with the new team.
As a senior you will be expected to lead, mentor others and deputise for the director of data engineering when required. Yoi will also continue to contribute as a hands-on engineer. You will have the knowledge, best earned through 5+ years of experience, to truly inhabit this senior role in an ambitious team.
What you’ll be doing:
Working with business stakeholders and our delivery team to understand high value business problems that can be solved through the application of data processing and analytical systems.
Developing, expanding and evolving our existing databases and ETL pipelines.
Working with architects to design, build and support a transformational cloud-based data platform for the business.
Being a core and professional member of the new data engineering practice
Understand business requirements and help refine into development tasks and estimate their complexity.
Research, evaluate and adopt new technologies with a right tool for the job mentality.
Focus on both speed of delivery and quality, with suitable pragmatism – ensuring your solutions are always appropriate and not too complex or over-engineered.
Quick progression of projects from proof-of-concept to post-production stage.
Communication and presentation of ideas to colleagues in all parts of the wider tech team.
Participating in code reviews for the data engineering practice.
Act as a servant leader in our practice – ensuring team members have
person specification:
Knowledge of, or ability to rapidly adopt our core languages for data engineering – SQL and/or Python.
Experience working using a modern DevOps approach. You should not just have written data code but also the CI/CD pipelines necessary to test and deploy that code in a professional environment.
A robust understanding of core data engineering topics – ETL vs ELT, structured and unstructured data, data quality and data governance.
Ability to contribute to all aspects of a solution – design, infrastructure, development, testing and maintenance.
The ability to design and advocate for technical solutions to business problems.
Effective collaboration with technical and non-technical team members through agile ceremonies – roadmap planning, feature workshops, backlog elaboration, code review.
Understanding of cloud technology and significant hands-on experience working with it. You must understand how cloud-native solutions must be built differently from traditional ones.
Track record of taking initiative and delivering projects end-to-end; clear evidence of being self-driven and motivated
Immense curiosity, high energy and desire to go the extra mile to make a difference.

Desirable Skills
Beyond the required skills we are open to individuals of diverse talents. Experience with additional technologies, data science knowledge or a business background are all valued. We want to know how your unique abilities can contribute to our team.
Our Technology
We operate in a diverse technical landscape and are looking for flexible engineers who can adapt to and use many different tools. We would not expect any engineer to be familiar with the entire tech stack – no engineer can. Instead, we seek people with a good understanding of data structures and algorithms and the ability to apply this knowledge in learning new tools.
We have an existing data landscape involving both MS SQL Server and Oracle databases. We use SSIS for ETL. We will also need to source data from our integration layer – webMethods.
If some of that sounds like it needs bringing up to date, we know! We hope to start building a modern cloud data platform to realise the full value of our data. The technologies we use for that platform are still to be decided. Likely it will be based on Snowflake with Azure Data Factory for ELT. Other parts of our business have had success using Prefect for orchestration and dbt for transformation and we will reuse where it is sensible.
We prepare data for use by analysts working with a variety of tools – Tableau, PowerBI, and even Excel.
We take a DevOps approach and strive for continuous integration / continuous deployment. We use Azure Pipelines to deploy our code. We deploy infrastructure the same way using Terraform and Docker.
Our technology landscape is not fixed. Our engineering and architecture teams drive the technology we adopt.


Salary range $140,000 - $170,000",155000,1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1901,$1 to $5 billion (USD),GA,122,data engineer,senior,"['sql', 'go', 'python']","['snowflake', 'azure']","['excel', 'tableau', 'ssis']","['sql server', 'snowflake', 'oracle', 'dbt']",[],"['terraform', 'docker']",,
Gold Coast Health Plan,3.8,"Camarillo, CA",Sr. ETL DEV/Data Engineer,"Data Engineers will be responsible for transformation and modernization of enterprise data solutions on Cloud Platforms integrating Azure services and 3rd party data technologies. Data Engineer will work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions.
ESSENTIAL FUNCTIONS
Reasonable Accommodations Statement
To accomplish this job successfully, an individual must be able to perform, with or without reasonable accommodation, each essential function satisfactorily. Reasonable accommodations may be made to help enable qualified individuals with disabilities to perform the essential functions.
Essential Functions Statement
As a Data Engineer, you will be responsible for assisting our clients envision, design, and deploy data engineering workloads as part of our solutions. As part of a small, dynamic team, you will have the opportunity to contribute to multiple phases of the solution life cycle including designing and implementing models and processes for large-scale datasets used for descriptive, diagnostic, predictive, and prescriptive purposes
Work with the team to evaluate business needs and priorities, liaise with key business partners and address team needs related to data systems and management.
Translate business requirements into technical specifications; establish and define details, definitions, and requirements of applications, components and enhancements.
Generate design, development, test plans, detailed functional specifications documents, user interface design, and process flow charts for execution of programming.
Develop data pipelines / APIs using Python, SQL, potentially Spark and AWS, Azure or GCP Methods.
Build large-scale batch and real-time data pipelines with data processing frameworks in Azure cloud platform.
Assist in the migration from on-prem SQL Server data analytics platform to MS Azure cloud platform.
Work as part of a team to build upon ingestion framework to intake new data sources.
Analyze, design, code and test multiple components of application code across one or more clients.
Perform maintenance, enhancements and/or development work

Qualifications
BA/BS in computer science, mathematics, information management, business, or equivalent experience
6+ years of experience in SQL
4+ years of experience in Cloud Platforms: Azure or AWS or GCP
4+ years of experience in Python and Pyspark
4+ years of experience in Synapse highly preferred
Experience using SQL, dB Visualizer, AWS, Azure, Cloud technologies
Experience with Power BI or similar data visualization tools
knowledge of HL7 v2, HL7 CDA and FHIR interface mapping highly preferred
Exposure to non-relational databases and tools, such as Cassandra, JSON, JAVA, Python, and Spark
In-depth knowledge of healthcare interoperability and patient data aggregation
Ability to effectively communicate, at times in a non-technical language, with customers at all levels of the organization.",127500,Unknown,Self-employed,Insurance,Insurance Agencies & Brokerages,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['sql', 'java', 'python']","['azure', 'aws']",['power bi'],['sql server'],['spark'],[],,+10 years
Barracuda Networks Inc.,3.6,"Alpharetta, GA",Data Engineer,"Job ID: 23-579

Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
We currently have an exciting new position open for a Data Engineer. This person will assist in driving technology innovation for engineering projects and work on architecture of AWS, Elastic and Kubernetes. They will also work closely with the Technology Architect to create and drive the Architecture Review Board.
What you’ll be working on
Determine internal / external customer data needs and identify system specifications
Analyze the needs of large systems and breaking them down into smaller manageable parts
Responsible for designing the journey of data from various sources to the database and everything in between
Plan and design the structure of data, brainstorm with team
Analyze best technology products and systems for internal teams
Help provide build vs buy decisions on data architecture
Communicate data requirements to engineers; explain data architecture to them and provide assistance throughout SDLC
Choose / maintain suitable software, hardware and integration methods
Help resolve technical problems as and when they arise
Review testing procedures to ensure data performant and error free
Ensure that database satisfy quality standards and procedures. Setup such standards.
Work with senior IT personnel to devise plans for future IT requirements of the organization
Produce progress reports on key infrastructures - Elastic and Data Pipelines
What you bring to the role
Bachelor Degree or Post Graduate in Computer Science, or relevant work experience
5+ years of experience in an Engineering / Operations heavy organization
1+ years of experience as a Data Analysis and Data Design responsible for data architecture of a SaaS application
Experience in understanding and designing data sharding in a distributed database
Some Hands-on experience in software development and system administration
Knowledge of strategic IT solutions
Familiarity with programming languages like JavaScript or Java or Python
Familiarity with Linux OS and Windows alike
Experience in cloud computing and cloud Technologies (AWS, GCP, Azure)
Excellent communication skills – you should be adept at listening to, understanding, and explaining key concepts to managerial and technical resources.
Good Anticipation and problem-solving skills
Ability to prioritize and manage time – responsible for multiple streams of work at the same time
Relationship building - Ability to form a good rapport with internal and external stakeholders
In-depth knowledge about quality standards and security best practices

Preferred Skills:
Expert level Understanding of Elastic eco system - Elasticsearch, Logstash, Kibana, Filebeats
Understanding of secure tunnel communication using VPN
Experience designing data at scale
AWS experience is preferred in Cloud Technologies
What you’ll get from us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualified options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
#LI-Remote",98161,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2004,$100 to $500 million (USD),GA,19,data engineer,na,"['java', 'python']","['azure', 'aws']",[],['elasticsearch'],[],[],bachelor,+10 years
Encore Technologies,4.4,"Atlanta, GA",Senior Data Engineer,"Encore Technologies is seeking a Senior Data Engineer to work for a client in Atlanta, GA (zip code 30339). This is a Direct Hire role that will be worked in a hybrid schedule, with 2-3 days a week onsite.
Summary:
The Senior Data Engineer is responsible for managing and organizing enterprise data. They will translate requirements and designs into functional data pipelines while ensuring the continued quality and completeness of the information. Senior Data Engineers will combine raw information from different sources to create consistent and machine-readable datasets that are easy to analyze and support company initiatives. They will support other Data Engineers and Data Analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They will also implement methods to improve data reliability and quality, improve data visibility and reduce effort through automation.
Essential Duties:
Contribute to a team of data engineers through design, demand delivery, code reviews, release management, implementation, presentations, and meetings.
Mentor fellow data engineers and contribute to ongoing process improvements for the team
Evaluate business needs and objectives and align architecture/designs with business requirements
Build the data pipelines required for the optimal extraction, transformation, integration, and loading of raw data from a wide variety of data sources
Assemble large, complex data sets and model our data in a way that meets functional / non-functional business requirements
Create data tools for analytics team members that assist them in generating innovative industry insights that provide our business a competitive advantage
Implement data tagging mechanisms and metadata management so data is accurately classified and visible to the organization
Build processes to help identify and improve data quality, consistency, and effectiveness
Ensure our data is managed in a way that conforms to all information privacy and protection policies
Use agile software development processes to iteratively make improvements to our data management systems
Identify opportunities for automation
Be an advocate for best practices and continued learning
Requirements:
Bachelor's/Tech School degree in Computer Science, Information Systems, Engineering or equivalent and/or commensurate years of real-world experience in software engineering.
4+ years of relevant experience in data management
3+ years in data engineering with detailed knowledge of data warehouse technical architectures, infrastructure components, and ETL/ ELT.
Experience with performance analysis and optimization.
Experience in data acquisition, transformation, and storage design using design principles, patterns, and best practices.
Data engineering certification is a plus.
Experience with Informatica, Kafka, CDC, SQL, Irwin, Python, AWS (S3, Athena, Glue, Kinesis, Redshift), Spark, Scala, AI/ML, Modern data platforms, Snowflake, dbt, Fivetran, and Airflow.
Encore Technologies is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills, and experiences within our workforce.
Job Type: Full-time
Pay: $120,000.00 - $160,000.00 per year
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Atlanta, GA 30339: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
ETL/ELT: 3 years (Required)
data management: 4 years (Required)
Work Location: Hybrid remote in Atlanta, GA 30339",140000,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2014,$100 to $500 million (USD),GA,9,data engineer,senior,"['sql', 'scala', 'python']","['snowflake', 'aws', 'redshift']",[],"['snowflake', 'dbt']","['kafka', 'spark']",[],bachelor,2-5 years
Cox powered by Atrium,4.0,"Atlanta, GA",AWS Data Engineer- Hybrid,"Minimum Qualifications:
Bachelor’s degree or equivalent work experience
A minimum of 3+ years’ experience in Microsoft Windows/ SQL Server Technologies, .Net development, AWS Administration.
Experience working on 24x7 environments oriented towards a zero downtime target.
Working knowledge or previous administration of SQL 2016-SQL 2022 and Windows Server 2012+ preferred.
Ability to work with minimal direction, in a team environment.
Performance tuning for AWS/DataLake systems.
Some Experience with SQL in virtual, physical and cloud-based environments.
Experience with Athena and data modeling for cloud technologies.
Proven ability to quickly learn and implement new technologies.
Experience with Administration, Security/Identity Management and Terraforms in AWS.
Preferred Qualifications:
Experience with SentryOne, a plus.
Ability to code Powershell commands and maintain code in GitHub, a plus.
Some Experience with Metabase and Collibra, a plus.
Experience with ETL in AWS, a plus.
Pay Range:
$60-$68/hr
Requisition Disclaimer:
This job posting is for a temporary role as an employee of Atrium on assignment at Cox. The individual selected for this role will be offered the role as an employee of Atrium; compensation, medical benefits, fringe benefits and other terms and conditions of employment shall be presented by Atrium upon offer. The pay rate range provided is a reasonable estimate of the anticipated compensation range for this job at the time of posting. The actual pay rate will be based on a number of factors, including skills, competencies, experience, location and/or being pursued and other job-related factors permitted by law. In addition, this role will be eligible for overtime pay, in accordance with federal and state requirements

By applying for this position you agree to the Atrium Terms and Conditions. Agreeing to these terms, includes permission to use the email address and mobile phone number you provide during the application process or throughout the duration of your prospective or actual employees to notify you of job openings, profiles, articles, news, and other employment-related information, as well as to notify you of special promotions or additional products and services offered by us or our affiliates and partners (collectively, “Atrium Alerts”). Atrium Alerts may be sent by email, phone or text message. Your personal information will be safely stored in our database. Atrium does not sell your personal information to third parties. Text message and data rates may apply. To OPT OUT of text messaging or to modify your communication preferences for Atrium Alerts at any time, please contact us at privacyadministrator@atriumstaff.com.

If you do not agree with the Atrium Terms and Conditions, you can still complete your application for this position by emailing your resume to our team at coxrecruit@atriumworks.com. Please include the job title in the subject of your email.

As a woman-owned firm, Atrium values diversity. We are an equal opportunity employer and will consider all applications without regard to race, sex, age, color, religion, national origin, veteran status, disability, genetic information or any other characteristic protected by law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Posting: #zip",115200,,,,,-1,,GA,-1,data engineer,na,['sql'],['aws'],[],['sql server'],[],[],bachelor,+10 years
The Data Sherpas,4.0,Remote,Data Engineer - LATAM,"Who We Are:
Working at The Data Sherpas is like being part of a dynamic and collaborative team of talented individuals passionate about helping clients navigate the ever-changing information technology landscape. At The Data Sherpas, you can work with cutting-edge tools and technologies, tackle challenging data problems, and continuously develop your skills in a supportive environment. As a Data Sherpa, you'll be empowered to lead projects, take ownership of your work, and make meaningful contributions to our client's success.
What We Are Looking For:
We're seeking an ambitious and driven Data Engineer to join our team. As a Data Engineer, you can work on exciting, challenging projects supporting our client's needs. You will be part of a dynamic team of experts dedicated to improving business performance and driving data-driven results.
What You'll Do:
Design and implement data pipelines, ETL processes, and data warehousing solutions
Develop and maintain attribution and measurement models for ad campaigns
Perform data matching and segmentation techniques to create customer profiles and behavior patterns
Configure and maintain AWS infrastructure and services to support data engineering processes
Collaborate with cross-functional teams to identify data needs and develop data-driven solutions.
Stay up-to-date with the latest technologies and industry trends related to Data Engineering and Ad Tech.
What You Have:
Proficiency in data modeling, ETL development, and data warehousing
Ability to design and maintain data pipelines for large-scale data sets
Proven experience with Python programming language for data engineering solutions
Knowledge of data matching techniques for identifying duplicate data and inconsistencies
Experience with data deduplication techniques
Ability to work with large datasets to segment and group data
Experience with clustering algorithms and methodologies
Understanding of customer segmentation and persona development
Familiarity with data visualization tools to showcase segmentation results
Strong experience with data engineering in cloud-based environments, primarily in AWS
Experience using AWS data analytics services like EMR, Redshift, Kinesis, and Glue",112889,,,,,-1,,Remote,-1,data engineer,na,['python'],"['aws', 'redshift']",[],[],[],[],,
Tech Mahindra / Microsoft,3.2,Remote,Data Engineer - Cosmos,"Hi,
One of my direct client is looking for Data Engineer in Redmond, WA. If you are interested, please share me your updated resume.
Title: Data Engineer
Location: Redmond, WA
Direct Client: Microsoft
Job Description:
Primary Requirement:
1. Cosmos Scope Scripting
2. Azure Data Lake, Pipelines
3. ADLS, ADLA
4. SQL querying experience
5. Microsoft projects experience
Optional Requirement:
1. ADO
2. PowerShell or Python Scripting language
3. Project manager
Thanks & Regards
K. ManiMegalai
Phone: (206) 337-5702 Ext 241
Zen3 is now a Tech Mahindra Company.
Tech Mahindra is a strategic Microsoft Partner and is a proud partner in implementing the World’s largest Azure Migration program.
Job Type: Full-time
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote",112889,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']",['azure'],[],[],[],[],,
Alter Tech Solutions,4.0,"Jersey City, NJ",Senior Data Engineer,"Job Title: Data Engineer
Location: Jersey City, NJ
Job Type: Hybrid
Job Description:
Position is more about scale - application monitoring, automation framework, test automation - data engineering - data pipelines - rest api's
system design - experience with distributed systems, design the right components, push out to the AWS cloud, data structures/algorithms
Experience:
10+ years of experience cross functional experience - data engineering and sw engineering background (working with risk, audit teams) need hands on coding but also design
Qualifications & Requirements:
TOP SKILLS java preferred language but would consider strong python - Apache Flink, Kafka, Cassandra, GraphQL, SPARK, MACHINE LEARNING.
Job Types: Full-time, Permanent
Salary: $60.00 - $75.00 per hour
Experience:
Java: 8 years (Preferred)
GraphQL: 8 years (Preferred)
Python: 5 years (Preferred)
Work Location: On the road",121500,,,,,-1,,NJ,-1,data engineer,senior,"['java', 'python']",['aws'],[],[],"['kafka', 'flink', 'spark']",[],,5-10 years
"Intertech, Inc",4.4,Minnesota,Sr. Data Engineer,"Sr. Data Engineer
US Citizenship Required
Contract to Hire Opportunity
Fully Remote

The Senior Data Engineer will oversee the department's data integration work, including developing data models, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. This role will work closely and collaboratively with members of other areas to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of the analytics needs.

Responsibilities
Maintain and build our data warehouse and analytics environment
Design, implement, test, deploy, and maintain stable, secure, and scalable data engineering solutions and pipelines in support of data and analytics projects, including integrating new sources of data into our central data warehouse, and moving data out to applications and affiliates as needed
Make data available for the reporting and analytics teams
Produce scalable, replicable code and engineering solutions that help automate repetitive data management tasks
Implement and monitor best in class security measures in our data warehouse and analytics environment, with an eye towards the evolving threat landscape
Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security
Other duties as assigned
Provide technical assistance in an on-call rotation



Job Qualifications

Required:
Bachelor’s Degree in Computer Science or Management Information Systems (MIS) or Business, Finance or Accounting with an emphasis in MIS
Minimum 5 years experience of developing and supporting enterprise level data warehouse systems
Strong knowledge of relational databases and SQL. Extract, Transform, and Load (ETL) data into a relational database
General data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets together, reformat data between wide and long, etc.
Demonstrated ability to learn new techniques and troubleshoot code without support, ex. find answers to common programming challenges
Strong knowledge of T-SQL language as evidenced by ability to write complex SQL queries, Microsoft SQL Management Studio, SQL Analysis Services and SQL Server Integration Services
Demonstrated ability to work independently and be a self-starter
Demonstrated ability to work effectively in teams, in both a lead and support role
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision

Preferred:
Experience working with Data Vault 2.0
Experience working with cloud infrastructure services like Amazon Web Services and Google Cloud
Experience with advanced data visualization and mapping",112889,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1991,$5 to $25 million (USD),Minnesota,32,data engineer,senior,['sql'],['google cloud'],[],['sql server'],[],[],bachelor,
Accentvision Technology Inc,4.0,"Plano, TX",Senior Data Engineer - Confluent Kafka,"Job Description:
Apply Confluent Kafka API lifecycle development and management.
Administer and improve Kafka use throughout organization including Kafka Producers, Kafka Consumers, Kafka Connect, KsqlDB, KStreams, and custom implementations.
Work with multiple teams to ensure best use of Kafka and data-safe event streaming.
Understand and apply event-driven architecture patterns and Kafka standard processes.
Model system behaviors using standard process methods for communicating architecture and design.
Develop or assist developers in choosing correct patterns, event modeling, and ensuring data integrity.
Work with Kafka APIs to provide pro-active insights and automation.
Acquire a deep understanding of source and sink connector technical details for a variety of platforms including Snowflake, Sales Force, MongoDB, PostgreSQL, MS SQL Server, Azure, and others as required.
Experience:
Senior Data Engineer: 10+ years proven experience in data engineering.
Data Engineer: 5+ years of proven experience in data engineering.
Solid understanding of Kafka (Consumption, publishing, and streaming) architecture and integrations.
Familiar with 3rd Party Confluent Kafka Connectors, Kafka Connect and its connectors for integrating with external systems.
Development experience using Confluent Kafka producers, consumers, and streams.
Experience with building streaming applications with Confluent Kafka
Experience with Java coding, CI/CD processes, microservices is required.
Good exposure to various Azure cloud platforms to play a key role in Application Modernization
Experience in CI/CD, DevOps tool chain, GIT, docker, Jira, and a test-driven approach to agile delivery.
Ability to participate in and contribute to code management in GitHub including actively collaborating in peer-reviews, feature branches, and resolving impediments and commits.
Job Types: Full-time, Contract, Temporary, Permanent
Pay: $110,000.00 - $150,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Vision insurance
Compensation package:
1099 contract
Hourly pay
Yearly pay
Experience level:
6 years
Schedule:
Monday to Friday
Experience:
Data Engineering: 6 years (Required)
Confluent Kafka: 5 years (Required)
KSQL: 5 years (Required)
KStreams: 5 years (Required)
Work Location: Hybrid remote in Plano, TX 75024",130000,,,,,-1,,TX,-1,data engineer,senior,"['sql', 'java']","['snowflake', 'azure']",[],"['sql server', 'mongodb', 'snowflake', 'postgresql']",['kafka'],['docker'],,5-10 years
AMBE Engineering,4.4,"Rome, GA",Data Engineer,"DATA ENGINEER
FULL TIME
LOCATION: Rome, GA
Works with various stakeholders across the organization to create dashboards and interactive visual reports using Power BI.
Designs, extends and enhance Power BI data models and development of new cubes.
Maintains functionality and relevance of reports and problem solve as issues arise.
Interprets trends and patterns, conduct complex data analysis and report on results.
Develops profiling scripts, data quality reports. Collaborate with business to develop DQ rules to run profiling.
Links Power BI with ERP system for efficient and automated report generation.
Combines raw information from different sources.
Works with users and team members at all levels to elevate existing reports and reporting capabilities and distribution.
Develops, test, and deploy DAX based scripts, queries and functions in Power BI.
Drives continuous improvement in IT processes and increases operational efficiencies
Works closely with project teams in gathering requirements, standing up and configuring the platform.
Other duties can be assigned based on company needs and employee capabilities.
REQUIREMENT
Understands business requirements in BI context and convert data into meaningful insights.
Be able to write and execute DAX measures, queries, and functions in Power BI.
Strong SQL skills (complex queries on relational databases, stored procedures, automation).
Strong Excel skills (experience migrating data from Excel to Power BI strongly preferred)
Understands business requirements in BI context and convert data into meaningful insights
Benefits
Who we are:
Ambe Engineering is a dual-certified W/MBE (woman and minority owned business) Diversity Supplier continuing to grow its ecosystem and extend their expertise globally. We bring the right resource for supplier development, high-impact project management, holistic cost savings, lean manufacturing and quality systems/problem solving solutions.
Quality, Logistics & Production | Crisis Management / Critical Situations | Cost Reduction | HR Services
__
_www.ambeeng.com_
Job Type: Full-time
Pay: $130,000.00 - $160,000.00 per year
Schedule:
8 hour shift
Day shift
Experience:
Power BI: 2 years (Required)
Automotive: 2 years (Preferred)
Work Location: In person",145000,Unknown,Company - Private,Manufacturing,Transportation Equipment Manufacturing,-1,Unknown / Non-Applicable,GA,-1,data engineer,na,['sql'],[],"['power bi', 'excel']",[],[],[],,2-5 years
Kroenke Sports Enterprises,3.4,"Denver, CO",Data Engineer,"Job Title: Data Engineer
Department: Hockey Operations
Business Unit: Colorado Avalanche
Location: Denver, CO or Remote
Reports To: Director of Analytics
Employment Type: Full Time – Salaried - Exempt
Supervisor Position: No
_____________________________________________________________________________________
Kroenke Sports & Entertainment (KSE) is an American Sports and Entertainment holding company based in Denver, Colorado. KSE is committed to providing world class sports and entertainment for both live and broadcast audiences. We are the employer of choice as the owner and operator of Ball Arena, DICK’S Sporting Goods Park, the Paramount Theatre, 1STBANK Center, Denver Nuggets (NBA), the Colorado Avalanche (NHL), Colorado Mammoth (NLL), Colorado Rapids (MLS), KIMN,KXKL, KKSE (FM/AM), Altitude Sports & Entertainment, Major League Fishing/Fishing League Worldwide (MLFLW), Winnercomm, Outdoor Sportsman Group and SkyCam.

Nature of Work:
The Colorado Avalanche are looking to hire a full-time Data Engineer to work within the team’s Hockey Operations Department. This person will be responsible for maintaining and expanding the Avalanche hockey operations database. They will be tasked with importing and integrating data from external providers and interacting with the rest of the hockey operations department to ensure optimal dissemination of information to the appropriate parties. This person will also have the opportunity to analyze data and share insights with members of the Analytics department as well as the broader Hockey Operations department if desired. They will report to the Director of Analytics.

Examples of work performed:
Manage and improve the organization’s data storage, structure, and ETL pipeline while optimizing query performance for large datasets
Design automated processes to oversee data integrity and query performance on a regular basis, as well as being available to spot and resolve data issues that may arise at any time
Ensure that our automated processes run on schedule without issue
Incorporate expansive new datasets from disparate sources into our structure in a seamless fashion

This description is a summary only and is describing the general level of work being performed, it is not intended to be all-inclusive. The duties of this position may change from time to time and/or based on business needs. We reserve the right to add or delete duties and responsibilities at the discretion of the supervisor and/or hiring authority.

Working Conditions & Physical Demands:
Typical Office Conditions
Travel may be required

Qualifications:
Required
Strong knowledge of ETL architecture and development in a cloud-based environment
Academic and/or industry experience in database architecture, back-end software design, and query optimization
Expertise in SQL and Spark
Excellent attention to detail, problem-solving abilities, and work ethic
Preferred
1-3 years of work experience in a database-related position
An advanced degree in software engineering, computer science, information technology, or a related field
Familiarity with R and Python, as well as R Studio / Posit
Experience with databricks and Linux operating system
Knowledge of NHL players and teams as well as a passion for hockey and hockey analytics
Some front-end development and/or data analysis experience is considered a plus

Competencies/Knowledge, Skills & Abilities:
Ability to maintain positive attitude and demonstrate professionalism
Ability to maintain a high level of confidentiality
Ability to complete work accurately and in a timely manner
Ability to work independently & in a group setting and demonstrate good judgment skills
Ability to communicate effectively orally and in writing
Possesses excellent interpersonal skills
Ability to multi-task, prioritize and adapt to changing environments

Compensation:
Salary Range $80,000 - $110,000 per annum

Benefits Include:
12 Paid Company Holidays
Health Insurance (Medical, Dental, Vision)
Paid Time Off (PTO)
Life Insurance
Short and Long-term Disability
Health Savings Account (HSA)
Flexible Spending plans (FSAs)
401K/Employer Match

Equal Employment Opportunity
Kroenke Sports & Entertainment (KSE) provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.",95000,1001 to 5000 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,-1,Unknown / Non-Applicable,CO,-1,data engineer,na,"['sql', 'r', 'python']",['databricks'],[],[],['spark'],[],,2-5 years
Diverse Lynx,3.9,"Charlotte, NC",Azure Data Engineer,"Production support Engineer
Job summary required skill:
ITIL and Prod Support experience
Powershell • Scripting knowledge
Should be able to check server and service health
Should be able to check certificates
Good to have knowledge of SQL (preferred)
Development experience
Experience: 5+ years
Required Skills:
1. Powershell
2. Any Scripting knowledge
3. SQL
Preferred Skills: Good to have knowledge of SQL
Technical Skills: PowerShell Domain Skills, ITIL Domain Skills, Technology Infrastructure Services
Development experience required skill: PowerShell
ITIL and Prod Support experience
Powershell
Scripting knowledge
Should be able to check server and service health
Should be able to check certificates
Job Type: Full-time
Salary: $78,245.06 - $171,281.86 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28203: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person",124763,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2002,$100 to $500 million (USD),NC,21,data engineer,na,['sql'],[],[],[],[],[],,5-10 years
RevolutionParts,4.0,United States,Data Engineer,"RevolutionParts is dedicated to modernizing the auto industry through our parts e-commerce platform. And we are pretty great at it too! We have enabled thousands of dealerships to sell auto parts online by transforming the way buyers and sellers connect.

And not only are we dedicated to revolutionizing the auto industry; we are also passionate about building a revolutionary team. Our Revolutionaries (as we call ourselves) are talented humans who have a shared goal of delivering an exceptional product and customer experience. Plus, we have fun while doing it!

The Role
RevolutionParts is on a mission to take our data to the next level. In this key role, you will help design and implement the next generation of our ETL pipeline for our parts catalog, pricing, and inventory data, which powers all of our eCommerce solutions. You will also be involved in establishing an enterprise-grade data platform for our largest partners. This is a high-impact role where you will be driving initiatives affecting teams and decisions across the company and setting standards for all our data stakeholders. Does the idea of spearheading a data practice in a high-growth e-commerce business sound exciting? If so, read on.
Responsibilities
We’re pulling in diverse data sources. You’ll need to learn our data and bring a strong grasp of ETL & ELT, workflows, AWS Glue, and data organization via efficient data lake and relational designs.
You will help design and build all stages of data from access to transformation and modeling.
You will build quality into the pipeline from day one using automated tests and data validation.
You will work with stakeholders in Product and data science to run ad hoc analysis of our data to answer questions and help prototype solutions.
You must own business problems through to resolution both individually and as part of a data team.
You will support product engineering teams by performing query analysis and optimization, as well as work with product teams to implement data driven product features.
Requirements
You should have 4+ years experience as a data engineer; or at least 2 years experience as a Data Engineer and 3+ as a software engineer.
You need to show us that you know what good looks like. This means experience implementing automated tests in a multi-stage data pipeline to ensure quality.
You are highly analytical and curious by nature.
You must have the ability to own business problems and the design and solutions that drive business outcomes.
You must be a team player with the ability to work with others and know when to support and when to push.
This role requires strong communication and collaboration skills; comfortable discussing projects with anyone from end users up to executive leadership.
Fluency with the programming language of your trade. Our primary languages for data are Golang and Python, but we use others as well. You must be comfortable learning new skills on the job.
We require fluency with best practices in an object-oriented design and programming; experience as a backend software engineer is necessary. Demonstrable experience with a functional paradigm is also valuable.
The ability to write and optimize complex SQL statements is a base requirement.
Familiarity with ETL/ELT pipelines and modern tools is fundamental to this role. We are building workflows managed in Argo utilizing Docker containers deployed within Kubernetes.
You should have experience working in a cloud-based software development environment, preferably with AWS.
Familiarity with no-SQL databases such as ElasticSearch, DynamoDB or others is helpful.
Bachelor’s Degree in Computer Science or equivalent is required.
RevolutionParts is proud to provide all full-time Revolutionaries with a comprehensive employment package including competitive compensation, career development, benefits, 401K match, parental leave, and many more valuable perks. You can learn more about our core-value driven culture at our career page.

RevolutionParts is an Equal Opportunity Employer; we value diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, gender orientation, gender identity or expression, sexual identity, sexual orientation, age, marital status, family status, genetic information, veteran status, or disability status.",112889,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2013,$5 to $25 million (USD),TX,10,data engineer,na,"['sql', 'python']",['aws'],[],"['dynamodb', 'elasticsearch']",[],['docker'],bachelor,2-5 years
National Conference of Bar Examiners,4.1,"Madison, WI",Data Engineer,"JOB SUMMARY:
The National Conference of Bar Examiners (NCBE) is a nonprofit organization that provides high-quality assessment products, services, and research for the benefit and protection of the public and the legal profession. We assist state courts and licensing authorities with fulfilling their responsibility to determine minimal competence for entry to the legal profession.

Diversity, fairness, and inclusion are central to NCBE’s mission and to our vision for a competent, ethical, and diverse legal profession. NCBE provides an inclusive and family-friendly environment, flexible schedules, remote work options, and competitive salary and benefits. NCBE’s headquarters is located in Madison, Wisconsin, a vibrant community with excellent municipal services and educational opportunities.

The Data Engineer is responsible for creating, editing, and maintaining SQL functionality in the relational database platforms. The data engineer is responsible for managing and developing code on Oracle, MySQL, and Postgres platforms both on-prem and in the cloud. The data engineer also works to support application developers with database expertise regarding data architecture, SQL construction, application/database integration, as well as developing and maintaining complex procedures for the software backend. The position is also responsible for providing business management information to business leaders via ad-hoc queries and reports. Some traditional DBA server management skills such as creating and restoring data, creating / updating development and test database environments, maintaining backups, tuning for optimal performance and security awareness are also used.
Work is performed under the supervision of the IT Manager of Infrastructure.

ESSENTIAL DUTIES and RESPONSIBILITIES:
Create databases with efficient structures.
Develop, test, maintain and document changes to databases, both internal and public facing.
Develop queries, views and triggers for integration with other applications.
Assist developers and others in the department with database related needs.
Perform customer support for applications as needed.
Monitor health and security of the databases. Maintain high standards of data quality and integrity.
Develop and maintain queries and complex reports as needed.
Develop, debug and maintain procedures, functions and other DDL as needed; understand issues related to performance and security.
Understand and apply the SDLC, specifically Scrum and Agile methodology.
Perform other duties as assigned.

QUALIFICATIONS:
S:
Education and Experience Required:
4+ years of experience working on progressively more complex systems.
PL/SQL, Transact SQL, SQL 92 or other database language required.
Familiarity with scripting languages.
Familiarity with Linux.
Familiarity with Agile development principles.
Strong work ethic, professional, organized, and flexible; able to work independently to solve issues through self-directed research and troubleshooting.
Ability to handle and prioritize multiple tasks.
Bachelor’s Degree or relevant work experience.

Preferred Qualifications:
Design and development of database functions, procedures and packages.
Experience with reporting tools such as Crystal Report.
Experience working in a team environment.
Experience with modern systems development model.

The Data Engineer position may work remotely.

Mission
NCBE promotes fairness, integrity, and best practices in admission to the legal profession for the benefit and protection of the public. We serve admission authorities, courts, the legal education community, and candidates by providing high-quality
assessment products, services, and research;
character investigations; and
informational and educational resources and programs.

EEO Statement
NCBE is proud to be an equal employment opportunity organization. We are committed to providing equal employment opportunity to all applicants and employees regardless of their race, color, religion, age, sex, national origin, disability, military service, protected veteran status, genetic information, sexual orientation, gender identity, or any other characteristic protected by federal, state or local law.

Please note that applicants may be contacted via email throughout the hiring process. We suggest that you add BambooHR (@bamboohr.com and @app.bamboohr.com) to your Approved/Safe Sender list so that email notifications are delivered to your inbox and not marked as spam.",89236,51 to 200 Employees,Nonprofit Organization,Government & Public Administration,State & Regional Agencies,-1,$5 to $25 million (USD),WI,-1,data engineer,na,['sql'],[],[],"['oracle', 'mysql']",[],[],bachelor,+10 years
DSFederal Inc,4.0,Remote,Data Engineer*,"Description:
We are seeking an experienced Data Engineer to design, build, and maintain our government client’s data infrastructure. The Data Engineer will work closely with cross-functional teams to develop scalable data solutions that support our client’s business needs.
Requirements:
Design, develop, and maintain data pipelines and data storage systems.
Collaborate with cross-functional teams to design and implement data-driven solutions.
Optimize and tune data systems for performance and scalability.
Implement and maintain data quality and validation processes.
Troubleshoot and debug data-related issues.
Participate in code reviews and contribute to best practices for data engineering.
Technical Skills Required:
Proficiency in one or more programming languages (Python, Java, etc.)
Experience with data modeling, database design, data marts, and data warehousing concepts
Knowledge of ETL tools and techniques
Experience with cloud-based data platforms such as AWS or Azure
Strong problem-solving and analytical skills
Desired Skills:
Familiarity with data visualization tools such as Tableau or Power BI.
Knowledge of machine learning concepts and frameworks.
Experience Required:
3+ years of experience in data engineering or related field
Education Required:
Bachelor's in engineering

Who We Are:
DSFederal is a leader in health IT and data analytics that delivers innovative solutions to improve healthcare delivery and address critical challenges, including cancer surveillance, HIV/AIDS, maternal and child health, and COVID-19 emergency response. Our highly experienced professionals serve over 50 different federal clients on over 80 projects. Our people are united by one mission – to improve human life through transformative solutions. www.dsfederal.com.
What We Can Offer You:
DSFederal attracts the best people in the business with our competitive benefits package that includes medical, dental and vision coverage, 401k plan with employer contribution, paid holidays, vacation, and more. If you enjoy being a part of a high performing, professional services, and technology focused organization, we encourage you to apply!
Our Mission: Improve human life through transformative solutions.
Our Core Values: Integrity, People First, Disciplined Business Practices, Customer Centric, Value Creation.
EEO Statement: DS Federal is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status.
E-Verify Statement
DS Federal utilizes the E-Verify program for every potential new hire. This makes it possible for us to confirm that every employee who works for DS Federal is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website by clicking the logo below. E-Verify® is a registered trademark of the United States Department of Homeland Security.
#LI-LP1",112889,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $100 million (USD),Remote,16,data engineer,na,"['java', 'python']","['azure', 'aws']","['tableau', 'power bi']",[],[],[],bachelor,+10 years
Artint Knowledge Tech,4.0,"Charlotte, NC",Azure Cloud Data Engineer,"Role: Azure Cloud Data Engineer (Onsite role)
Location: Mount Laurel, NJ / Charlotte, NC (day 1onsite)
Job Duration: Full Time
Below is the JD:
Keys Skills: Azure, Synapse, ADF, DataBricks, PySpark, Informatica Power Centre or SQL Server SSIS or DataStage
Must Have
* More than 12 years of IT experience in Datawarehouse
* Hands-on data experience on Cloud Technologies on Azure, Synapse, ADF, DataBricks, PySpark
* Prior Experience on any of the ETL Technologies like Informatica Power Centre, SSIS, DataStage
* Ability to understand Design, Source to target mapping (STTM) and create specifications documents
* Flexibility & willingness to work on non-cloud ETL technologies as per the project requirements, though main focus of this role is to work on cloud related projects
* Flexibility to operate from client office locations
* Able to mentor and guide junior resources, as needed
* Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Job Type: Full-time
Salary: $100.00 - $115.00 per year
Work Location: One location",112889,,,,,-1,,NC,-1,data engineer,na,['sql'],"['databricks', 'azure']",['ssis'],['sql server'],[],[],,
Dime Community Bank,3.4,"Hauppauge, NY",Data Engineer,"Summary: Dime Community Bank is looking for a Data Engineer based in Hauppauge, NY.
Salary commensurate with experience, ranging from $100,000 to $135,000 annually. The exact compensation may vary based on relevant experience, skills, education, training, licensure and certifications, and location.
All applicants need to attach a recent resume.
Responsibilities:
Work with relational databases and data warehousing. Add new and modify existing data models to ensure data warehouse is up to date for reporting needs.
Handle data visualization for extract transform and load (ETL).
Handle job pipelining and orchestration. Automate data loads and systems with Powershell scripting and Python. Orchestrate jobs with Dagster to ensure proper data processing, swift execution of tasks, and accurate monitoring. Build ETL pipelines to bring all data sources to the data lake, data warehouse and data marts. Work with python & SQL, and scripting in Powershell for automation.
Build python packages for use by the team for report building, accessing databases, etc.
Develop and maintain operational data stores (ODS) in our datalake and data warehouse architecture. Utilize ETL concepts and handle transformation with data build tool (DBT). 20468.3.6
**Telecommuting may be permitted.
Mail resume to Robin.Derin@dime.com. Must reference job 20468.3.6.
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)",117500,501 to 1000 Employees,Company - Public,Financial Services,Banking & Lending,2021,$100 to $500 million (USD),NY,2,data engineer,na,"['sql', 'python']",[],[],['dbt'],[],[],,
Summit2Sea Consulting,5.0,"Tampa, FL",Data Engineer,"Summit2Sea Consulting, a cBEYONData company, is a technology consulting firm run by hands on technologists that combines people, process and technology to deliver innovative solutions to our clients. We have been named on The Washington Post's list of top work places for the past 3 years! We invest in our biggest asset - our people! You can be a part of a winning team that will contribute to your career growth.
Have you been looking to shift your career into high gear? This is your opportunity to take your ambitions and convert them into a solid career in a supportive and innovative environment!
Summit2Sea, a cBEYONData company is seeking a Data Engineer to support our federal customer's multi-domain technology platform which offers military and business decision makers, analysts, and users at all levels unprecedented access to authoritative enterprise data and structured analytics in a scalable, reliable, and secure environment.
Responsibilities:
Support data collection, ingestion, validation, and loading of optimized data in the appropriate data stores
Work on a team made up of analyst(s), developer(s), data scientist(s), and a product lead
Working directly with the analyst(s) and the product lead, the data engineer identifies and implements solutions for the data requirements, including building pipelines to collect data from disparate, external sources, implementing rules to validate that expected data is received, cleansed, transformed, massaged and in an optimized output format for the data store
Performs validation and analytics corresponding with client requirements and evolves solutions through automation, optimizing performance with minimal human involvement
As pipelines are executed, the data engineer monitors their status, performance, and troubleshoots issues while working on improvements to ensure the solution is the very best version to address the customer need
This role focuses specifically on the development and maintenance of scalable data stores that supply big data in forms needed for business analysis
Apply advanced consulting skills, extensive technical expertise and has full industry knowledge to develop innovative solutions to complex problems
Requirements:
Must Have:
4+ years of experience with SQL
4 years of experience working in a big data and cloud environment
4+ years of experience with a modern programming language such as Python or Java
4+ years of experience developing data pipelines using modern Big Data ETL technologies like NiFi or StreamSets.
Active Secret Clearance or higher
Nice to Have:
2 years of experience working in an agile development environment
Possession of excellent verbal and written communication skills
Preferred experience at the respective command with an understanding of analytical and data paint points and challenges across the J-Codes.
Ability to display a positive, can-do attitude to solve the challenges of tomorrow
Ability to quickly learn technical concepts and communicate with multiple functional groups
Summit2Sea Consulting, a cBEYONData company, is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity:
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.",112889,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD),FL,20,data engineer,na,"['sql', 'java', 'python']",[],[],[],[],[],,2-5 years
Resilience Consultancy Services,4.0,Remote,Azure Data Engineer/Tech Lead,"Job Title: Azure Data Engineer and Tech Lead
12+ Months
Contract
Job Summary: We are looking for an experienced Azure Data Engineer and Tech Lead to join our team. The successful candidate will be responsible for designing and implementing data solutions using Azure services such as Data Factory, Databricks, and other related services. Additionally, the candidate will lead and manage the technical team responsible for developing and maintaining these solutions.
Responsibilities:
Design and implement scalable data solutions on Azure platform using Data Factory, Databricks, PySpark, Python and other related services.
Build data pipelines and workflows to ingest, transform, and load data from various sources.
Develop and maintain data models and schemas for efficient data storage and retrieval.
Manage and lead a technical team responsible for data engineering and analysis.
Develop and maintain CI/CD pipelines for automated deployment and testing of data solutions.
Collaborate with cross-functional teams to ensure data solutions meet business requirements.
Continuously evaluate and recommend new data technologies and approaches to improve data solutions.
Requirements:
Bachelor's or Master's degree in Computer Science, Information Systems, or related field.
5+ years of experience in data engineering with expertise in Azure Data Factory, Databricks, PySpark, Python and related services.
Proven experience in leading and managing technical teams in data engineering and analysis.
Experience in designing and developing data models and schemas.
Strong proficiency in programming languages such as Python.
Experience in developing and maintaining CI/CD pipelines for automated deployment and testing.
Good understanding of cloud computing and its services (e.g., Azure, AWS, GCP).
Excellent problem-solving skills and ability to work in a fast-paced environment.
Strong communication and interpersonal skills.
Job Type: Contract
Pay: $75.00 - $80.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Education:
Bachelor's (Preferred)
Experience:
Data Factory: 6 years (Preferred)
SQL: 8 years (Preferred)
Databricks: 6 years (Preferred)
PySpark: 5 years (Preferred)
Python: 5 years (Preferred)
CI/CD: 5 years (Preferred)
Azure: 5 years (Preferred)
Work Location: Remote",139500,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'python']","['databricks', 'azure', 'aws']",[],[],[],[],bachelor,5-10 years
Express Global Solutions LLC,4.3,Remote,Data Engineer-Databricks Consultant,"5+ years' data engineering experience working with big data.
4+ years' experience developing in Databricks.
Job Types: Full-time, Contract
Pay: $80,631.44 - $97,104.52 per year
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",88868,51 to 200 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,['sql'],['databricks'],[],[],[],[],,0-2 years
FiberSense,5.0,"Oakland, CA",Data Engineer,"Why join us: Our benefits
Global team and remote role
Inclusive and flexible work culture
Motivated and vibrant team
Small team, large career growth opportunities
Opportunities and support to learn and receive mentoring
Fast growing start-up with unique, highly technical and exciting product
We are making the world safer and more sustainable
Our technology is one of a kind – there is no one else in the world doing what we're doing
About the opportunity: Data Engineer
Hybrid (from Oakland) or remote | Very flexible hours | $141,000 - $155,000 USD per annum
The software team works in a hybrid work environment from an office in Oakland, so ideally you would be based in Oakland too. We will consider candidates who are 100% remote within the USA. We encourage all types of flexible working arrangements.
As FiberSense's first dedicated Data Engineer, you will have a significant role in enhancing our existing data pipelines, while also taking charge of designing, developing, and maintaining new robust and scalable data infrastructure.
You will report directly to the head of ML/AI, located in Oakland, California. However, for the right candidate, we support remote work and offer flexible working hours.
Your responsibilities will include:
Design, build, and maintain data infrastructure focusing on reliability and scalability that can accommodate the growing volume of data from the sensor fleet, using AWS cloud technologies (AWS S3, Lambda, RDS, EMR, and Glue/EventBridge).
Extract and integrate data streaming from our rapidly growing fleet of sensors via RESTful APIs into storage buckets and databases
Ensure data quality, accuracy, and completeness by implementing data validation protocols and testing
Provide input on which APIs and data formats to expose for easier ingestion of streaming data from the sensors
Evaluate infrastructure options that fit the business's needs and optimize costs at scale
Stay up to date with emerging technologies and industry trends
Automate data security and develop data microservices
Build scalable and performant databases using SQL, such as PostgreSQL and MySQL.
Implement data monitoring and alerting, ensure the data infrastructure is running smoothly and that any issues are detected and resolved quickly
Within 90 days, you will:
Ensured the robustness of, or rebuild, the current event-triggered data extraction pipelines to S3/RDS
Have met with key stakeholders to scope needs and priorities both long and short-term
Explored existing data pipelines and contribute significantly to a roadmap for data infrastructure and pipelines
Begun implementing proposed infrastructure and pipelines
About you: The type of person who will be a good fit
If you are excited about this role but not sure if you meet all of the criteria, please apply. Research shows women and minority groups are less likely to apply for roles when they don't meet 100% of the criteria. We'd like to change this.
Essential:
Minimum of 3 years of experience in data engineering
Strong programming skills in relevant languages such as Python, SQL (required) and Golang (or other low-level language and a willingness to learn Go), Java (preferred)
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field
Demonstrated proficiency in AWS cloud technologies, including AWS S3, EMR, EC2, RDS, Glue, and Lambda
Experience using workflow managers and building ETL processes
Experience with data pipeline development using tools such as Apache Airflow, AWS Glue, and AWS Data Pipeline
Expertise in data warehousing, database architecture, and SQL/NoSQL databases, such as PostgreSQL, MySQL, MongoDB, Cassandra, and DynamoDB.
Strong understanding of API development and implementation, including RESTful APIs using tools such AWS API Gateway
Hands-on experience with data ingestion tools such as Apache Flume and Kafka to collect, store and process large volumes of data in real-time or batch mode
Excellent problem-solving and analytical skills
Excellent communication and collaboration skills
Highly desired:
Experience working with IoT or sensor fleets particularly time-series data
Experience working with data stored in protobufs
Experience working with large and rapidly increasing data throughput (10s GB/day)
Experience serving data to be consumed by AI/ML teams
Experience building AI/ML models
Experience designing and building data infrastructure/pipelines end-to-end from inception
Experience working in a rapidly-growing start-up
About FiberSense:
FiberSense is a deep tech company with a remarkable set of capabilities and new service offerings that will transform the way we perceive and respond in public spaces. Our mission is to build the first centralized ubiquitous sensing fabric for all moving objects and events in public spaces in all cities around the world. Our expertise and technology sits at the intersection of optical fiber sensing networks, integrated photonics and machine learning. More than a sensor company, FiberSense is building out a global scale sensing platform using our patented VID+R™ (Vibration Detection and Ranging) technology. The company is Australian founded and already has employees across the globe, which is also true of our customers. FiberSense is implementing a giant vision and a culture built on two key pillars of massive innovation and effective execution. We offer an environment that is exciting, inclusive, challenging, fun and extremely rewarding.
FiberSense is an equal opportunity employer committed to providing a working environment that embraces and values diversity and inclusion. If you have any support or reasonable adjustment requirements, we encourage you to advise us at time of application. All qualified applicants will receive consideration for employment without regard to age, ancestry, colour, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.
Apply now. Join us on our growth journey as we transform the way we perceive and respond in public spaces!",148000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'r', 'java', 'nosql', 'go', 'python']",['aws'],[],"['dynamodb', 'postgresql', 'mongodb', 'mysql']",['kafka'],[],bachelor,
Health Federation Of Philadelphia,3.9,"Philadelphia, PA",EHR Data Engineer,"Equal Opportunity Employer
The mission of the Health Federation of Philadelphia is to promote health equity for marginalized communities by advancing access to high-quality, integrated, and comprehensive health and human services. Health equity is at the heart of all our work. We believe in and are firmly committed to equal employment opportunity for employees and applicants. We do not discriminate on the basis of race, color, national or ethnic origin, ancestry, age, religion, disability, sex or gender, gender identity and/or expression, sexual orientation, military or veteran status. This commitment applies to all aspects of the Health Federation of Philadelphia’s employment practices, including recruiting, hiring, training, and promotion.
JOB SUMMARY
The Electronic Health Records (EHR) Data Engineer will be responsible for managing all aspects of EHR related data with a focus on database administration, data integration, and data flow design to deliver advanced clinical and operational analytics for the Philadelphia Department of Public Health, Ambulatory Health Services (AHS), a large community health organization. The EHR Data Engineer will utilize a systematic approach and advanced tools to design and support AHS data management services. The position requires skills and knowledge to connect, store, manage, secure, analyze, report, distribute and govern EHR related data. This position works directly with AHS Leadership, EHR Program Management Office (EHR PMO) Program staff, Health IT (HIT), City Office of Innovation and Technology (OIT) and respective partner personnel to support comprehensive planning and tactical delivery of data management services in production operations. In addition, responsibilities include Tier 2 and Tier 3 support for EHR production issues and Helpdesk tickets regarding data, database, and respective services.
Strategic direction, programmatic and operational prioritization will be specified through the authority of AHS Leadership. Accountability is managed through the EHR PMO with AHS oversight.
JOB SPECIFICATIONS
Responsibilities/Duties
Support the design, development, deployment, and ongoing management of AHS data. Work with EHR PMO to integrate data management strategies for projects, programs, and initiatives.
Assist in the design, development, deployment, and support of relational and non-relational database designs in a team-oriented environment using various data visualization tools in accordance with City OIT standards.
Responsible for overseeing patch management and maintenance on reporting infrastructure and data related systems. Develop version and release migration methodologies.
Design, implement and control access to data and databases with/for approved AHS partners and City entities.
Assure systems and data integrity.
Oversee functional configuration components to support monitoring and administering assigned systems and applications.
Serve as the technical intermediary between departments and vendors for various database solutions and datasets within on-premise and cloud hosted systems.
Assist to design, implement, and manage integration points for automated retrieval, sharing and delivery of incoming and outgoing data.
Assist with data conversion, migration, and validation of data to ensure quality and accuracy.
Develop Standard Operating Procedures (SOPs) for EHR data, database, and system administration establishing standards and procedures to maintain consistent practices.
Abide by the EHR PMO change control policies and procedures while ensuring change control methods are congruent with overall Health IT and OIT procedures; Submit and present various changes related to AHS data management.
Support the backup and recovery strategy for database environments. Oversee and administer business continuity and disaster recovery tests and methods.
Perform daily administration and performance monitoring of database related systems.
Monitor and proactively support capacity and performance planning and analysis.
Identify database sizing requirements based on AHS patient population, provider, user and legal retention requirements; monitor space usage and alert appropriate staff members to resolve sizing issues and conditions that may cause application failures.
Perform root cause analyses and troubleshoot internal and external data-related problems.
Aid in the support/writing/configuration of data transformation services (DTS) and extract, transform, load (ETL) packages, Web Services, and SQL database objects to both monitor and maintain the database and support applications.
Actively participate in the creative process to continuously improve architectural designs and implementation to ensure optimal efficiency and cost effectiveness.
Assist AHS in developing and conducting presentations related to data management.
Maintain working relationships with AHS, EHR, HIT, City OIT, and Partner staff including regular status communications using EHR and AHS methods of communication.
Provide off-hours support for upgrades, problem resolution, outages, or disaster recovery events as needed.
Stay current with data management tools as departmental infrastructure evolves.
Assist AHS leadership & staff as needed.
Other data analysis and management duties as required.
Education
Bachelors Degree in Computer Science/Information Systems, Data Analytics, or related experience in the field.
Database and/or Server Administration related Certification
Industry related certificates or certification a plus
Skills/Experience
Demonstrated knowledge of the Systems Development Life Cycle
5-7 years of experience working with SQL Server (2016 or later) and related products; special consideration given to Microsoft Business Intelligence Platform/Development Studio suite (SSRS, SSIS, and SSAS)
5-7 years of experience working with Windows Server (2016 or later) and related products
Five (5) years working in health services related field
HIPAA/HITECH trained
In depth understanding of conceptual, logical, and physical database structures, architectures, and integration
Ability to translate, integrate, and export data from various cross-platform software and databases
Comfortable leading the design of both relational and OLAP databases
Database related expertise working with SQL Server (2012 or later), Oracle SQL developer, and related products; special consideration given to Microsoft Business Intelligence Development Studio suite (SSRS, SSIS, and SSAS)
Experience with working in physical and virtual server environments
Experience with log shipping, SQL back-up restore, and SFTP
Strong analytical skills including experience with tools for: Business Intelligence; Analytics; Dashboards; Reporting
Well-developed computer skills including: Office365, Visio, entity relational diagrams
Effective organizational skills, detail-oriented
Excellent communication and presentation skills, both written and oral
Well-developed interpersonal skills including proper phone etiquette
Ability to work independently, be flexible and handle multiple tasks
Ability to work with a variety of cultures and diverse audiences
Additional proficiencies not required, but preferred
Experience with FQHC, ambulatory, municipal government and/or public health
Knowledge of healthcare, medical, lab and/or public health terminology
Experience with eClinicalWorks, i2i Tracks, Rhapsody and VMware
Experience with Microsoft SQL, Oracle database, Azure Synapse Analytics, and other multi-model database management or data warehouse systems
Experience with Microsoft PowerShell, SharePoint, and Visual Studio
Experience with data visualization software such as Tableau, Power BI, Red Cap, Quickbase, etc.
Web Analytics; batch processing development; HTML; XML; IIS server administration
Work Environment
Standard office setting with extended periods at work station and periodic use of office equipment
Position Type and Work Schedule
Full time position, typical hours are Monday through Friday 8:30 am to 5:00 pm. Flex office schedule options available with supervisor approval.
Travel
Local travel to multiple sites could be required for periodic implementations, upgrades, outages or onsite meetings.
Physical Demands
Ability to manage in an office environment located in a City health center.
Ability to work in a cubicle or shared space environment.
Ability to travel between city locations.
Ability to lift up to 50 pounds.
Ability to work in a fast paced, high pressure setting.
Ability to tolerate extensive use of keyboard, typing, computer.
Salary and Benefits Our employees are our most valuable resource, so we offer a competitive and comprehensive benefits package, which can include:

Medical with vision benefits
Dental insurance
Flexible spending accounts
Life, AD&D and long term care insurance
Short- and long-term disability insurance
403(b) Retirement Plan, with a company contribution
Paid time off including vacation, sick, personal and holiday
Employee Assistance Program
Eligibility and participation is consistent with the plan documents and HFP policy.
DISCLAIMER
The Health Federation reserves the right to modify, interpret, or apply this job description in any way the Company desires. The above statements are intended to describe the general nature and level of work being performed by an employee assigned to this position. This job description in no way implies that these are the only duties, including essential duties, responsibilities and/or skills to be performed by the employee occupying this position. This job description is not an employment contract, implied, or otherwise. The employment relationship remains “at will.” The aforementioned job requirements are subject to change to reasonably accommodate qualified disabled individuals.",100000,51 to 200 Employees,Nonprofit Organization,Nonprofit & NGO,Civic & Social Services,1983,$5 to $25 million (USD),PA,40,data engineer,na,['sql'],['azure'],"['power bi', 'tableau', 'ssis']","['sql server', 'oracle']",[],[],,5-10 years
World Finance,3.3,"Greenville, SC",Data Engineer,"**Hybrid role open to local Greenville, SC or Candidates relocating to Greenville, SC**
The Data Engineer implements data-centric solutions to complex business problems. This individual will work alongside a data architect and the data engineering team to expand and optimize data delivery architecture. Providing expertise in data warehousing and data delivery, this individual will work with cross-functional teams including marketing, analytics, operations, and software engineering to build data-aware systems and process flows across the enterprise.
Essential Duties and Responsibilities:
· Protect confidential company information.
· Assist business, technology, and support partners/stakeholders to deliver secure data solutions.
· Design, build, and maintain data delivery solutions in accordance with governing data architecture patterns.
· Model and assemble data sets that meet functional and technical business requirements.
· Implement infrastructure required to optimize ETL and ELT operations across a variety of data sources.
· Process file-based data extracts using data retrieval and management tools to provide timely loading of critical business data.
· Identify, design, and implement process improvements in data flows and data pipelines, focusing on automating data tasks.
· Integrate external systems with internal systems to ensure proper data flow between systems.
· Maintain an accurate and comprehensive inventory of data, data systems, and data storage.
· Communicate with non-technical stakeholders to determine technical solutions to business problems.
· Perform data load, data extraction.
Qualifications:
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required:
· Expertise with relational databases.
· Advanced SQL proficiency.
· Experience working with data warehousing systems for large enterprises with both internal and customer facing applications, preferably with near real time transactional data.
· Expertise in creating and maintaining data structures in SQL.
· Experience structured and non-structured data paradigms, relational databases, data lake and data warehouse technologies, data vault and dimensional data models.
· Ability to define problems, collect data, establish facts, and draw valid conclusions.
· Ability to interpret an extensive variety of technical instructions in mathematical or diagram form and deal with several abstract and concrete variables.
· Ability to conduct research into systems issues and products to determine integration requirements.
· Ability to communicate ideas in both technical and user-friendly language.
· Ability to work with IT operations to quickly come to resolution of open support requests.
Self-motivated and able to work within a project-based environment.
Possesses strong oral and written communication skills, clearly and accurately communicating complex and/or technical information to both technical and nontechnical audiences.
Python for building and redefining data pipelines is required.
Education and/or Experience:
· Bachelor’s Degree in MIS, Computer Science, or related field, or equivalent professional experience.
· Minimum 6 years of designing and implementing Data engineering solution.
· Hands on experience with building productionized data ingestion and processing pipelines.
· Experience working in business intelligence and data warehousing environments.
· MS SQL Server, Integration Services (SSIS) required.
· Strong TSQL scripting abilities and understanding of complex stored procedures, views, data. aggregation/manipulation through table joins/queries, database design, normalization, and de-normalization techniques.
· Experience with Snowflake.
· Experience using Python.
· Experience with ETL tools (Matillion, Fivetran, Stich, etc) is a plus.
· Familiarity with Dev OPS, Airflow and DBT is a plus.
· Industry experience working with large data sets.
· Experience with Microsoft PowerBI or any BI visualization tool is a plus.
Physical Demands:
· Must be able to constantly remain in a stationary position.
· The person in this position needs to occasionally move about inside the office to access file cabinets, office machinery, etc.
· Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and computer printer.
· Occasionally may require light lifting to 25 pounds.
Work Environment:
· Office environment.
· Occasional travel may be required.
This job description reflects management’s assignment of essential functions; and nothing in this herein restricts management’s right to assign or reassign duties and responsibilities to this job at any time.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Vision insurance
Compensation package:
Performance bonus
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Greenville, SC 29601: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 4 years (Required)
Data warehouse: 4 years (Required)
Work Location: Hybrid remote in Greenville, SC 29601",95216,1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,1962,$500 million to $1 billion (USD),SC,61,data engineer,na,"['sql', 'python']",['snowflake'],['ssis'],"['sql server', 'snowflake', 'dbt']",[],[],bachelor,2-5 years
Maven LLC,4.3,Remote,Senior Data Engineer,"Description:
Please apply to job with this link: https://portal.dynamicsats.com/Application/WebForm/3e11b0af-f443-4cfb-ac80-066fe882c27b?portal=17b1dd1f-7af3-4dae-96aa-8723901631fa&job=f5ac005e-44e8-ed11-a7c6-002248081803
All About Us
Our client is a family community of banking teams that deliver an exceptional depth of resources to individuals and businesses, with a hands-on, personalized approach. Our client offers great benefits including health insurance for you and your whole family, 11 paid holidays, PTO, and tuition reimbursement to name a few.
About the Role
You'll be responsible for building, managing, and optimizing data pipelines and effectively converting into production for key data and analytics consumers. The Senior Data Engineer is also collaborating with Data Engineers, Data Architects, and Data Scientists across the enterprise, as well as Enterprise Architecture, to follow the standardization of data engineering integration architectures.
Job Essentials
Design, create and maintain projects involving data collection and storage systems
Recommend and deploy data models and solutions for existing data systems
Collaborate across teams on the design and maintenance of our client's Operations data mart (ETL, data modeling, metric design, metric design, reporting/dashboarding) to assure a stable reporting infrastructure
Work with data architects and data analysts to determine design needs
Mentor and provide guidance to junior engineers and new team members
Ensure data users and consumers utilize data responsibly through data governance and compliance initiatives
Perform data validation testing to ensure accurate data flows
Promote data and analytics capabilities and awareness to business unit leaders in order to leverage opportunities to achieve strategic business initiatives
Define coding standards, including Object Oriented design standards and design patterns
Implement a standardized library of reusable objects across the enterprise, such as the core banking platform
Maintain awareness of and adherence to Bank’s compliance requirements and risk management concepts, expectations, policies and procedures and apply them to daily tasks
Deliver a consistent, high level of service within our client's standards
Other duties as assigned
Requirements:
Bachelor's degree in computer science, information technology, or related field
Technical Expertise with SQL, PL SQL
Previous knowledge and experience integrating Informatica ETL
Experience with Tableau, Cognos, SSRS or equivalent BI tool
Previous experience using SSIS, Informatica or equivalent ETL
Skilled in Oracle, Snowflake, and SQL servers
Experience in AWS and Azure
Interpersonal/Customer Service Skills
Written and Verbal Communication. Ability to work as part of a team
Adaptable to change. Able to Multi-Task or Juggle Priorities
Analytical Thinking and Problem/Situation Analysis
Experience with Master Data Management (MDM) is preferred
An Equal Opportunity Employer
We do not discriminate based on race, color, religion, national origin, sex, age, disability, genetic information, or any other status protected by law or regulation. It is our intention that all qualified applicants are given equal opportunity and that selection decisions be based on job-related factors.",112889,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,['sql'],"['snowflake', 'aws', 'azure']","['tableau', 'ssis']","['snowflake', 'oracle']",[],[],bachelor,
Pomeroy,3.1,"Cincinnati, OH",Data Engineer,"General Function:
The Data Engineer will play an important role in our growing Enterprise Data and Analytics
team. The person in this role will build out a new centralized Analytics Data Lakehouse,
help maintain our existing Operational Data Warehouse, and the infrastructure that
underlies both. We are looking for a candidate with experience creatively solving data
complexities of various sizes and levels of cleanliness - with the goal of enabling data
analysts and business users throughout Pomeroy to make decisions backed by data.
Job Type: Full-time
Pay: $70,000.00 - $80,000.00 per year
Benefits:
401(k)
Dental insurance
Employee assistance program
Flexible schedule
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
4 years
5 years
6 years
7 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
SQL: 4 years (Required)
Oracle Cloud Integrator: 4 years (Required)
Data warehouse: 4 years (Required)
Work Location: In person",75000,1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable,OH,41,data engineer,na,['sql'],[],[],['oracle'],[],[],,2-5 years
Seamless.AI,3.4,"Columbus, OH",Senior Data Engineer - Remote US,"Role: Senior Data Engineer
The Opportunity
We are seeking a Senior Data Engineer with a strong background in data ingestion and processing for large data sets. The ideal candidate is maniacal about data accuracy and has a proven track record of building robust and scalable data pipelines. This role requires an experienced professional who can work collaboratively with cross-functional teams to design and implement solutions that meet business requirements and adhere to data security and privacy policies.
If you are a senior data engineer with a passion for data accuracy and a proven track record of building robust and scalable data pipelines, we encourage you to apply for this role. We offer competitive compensation, a flexible work environment, and opportunities for growth and advancement within the organization.
About Seamless
Seamless delivers the world's best sales leads. Through our product, we help sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence; by development of a robust real-time contact and company search engine as well as a suite of technically-advanced tools to support sales and lead generation. We have been recognized as one of Ohio's fastest growing companies and have been recently ranked No. 7 in LinkedIn's Top 50 Startups of 2022, featured in Forbes as #1 Software company in Ohio in 2022, and on G2's ""Top 100 Highest Satisfaction Products for 2022"" list!
The Seamless Team
We have an amazing culture and work environment that anyone would want to be a part of. We encourage a culture of positivity. We thrive off of continuous feedback and do whatever it takes to help our team and customers be successful. You will grow as an individual, professionally, and be able to see and feel the impact you are making to the growth of Seamless every day.
Role Responsibilities
Design and develop data ingestion and processing pipelines that ensure data accuracy and completeness.
Work closely with cross-functional teams to understand business requirements and design solutions that meet those requirements.
Lead efforts to identify and implement data quality control measures to ensure consistency and reliability of data.
Monitor data pipelines to identify issues and proactively resolve them.
Evaluate and recommend new technologies and techniques to improve data pipeline performance and scalability.
Ensure compliance with data security and privacy policies and procedures.
Mentor junior engineers and provide technical leadership in data engineering best practices.

Candidate Requirements
Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.
3+ years of experience in data engineering, with a focus on data ingestion and accuracy.
Strong experience with large-scale data processing frameworks such as Hadoop, Spark, or Flink.
Proficiency in programming languages such as Python, Java, or Scala.
Experience with SQL and NoSQL databases.
Familiarity with cloud-based data processing and storage platforms such as AWS, Google Cloud, or Azure.
Strong experience in designing and implementing data quality control measures.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Experience mentoring and leading teams of data engineers.
Seamless.AI has been delivering the world's best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio's fastest growing companies and won 2020 Best Places to Work and LinkedIn's Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.",93575,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,OH,8,data engineer,senior,"['sql', 'scala', 'java', 'nosql', 'python']","['azure', 'aws', 'google cloud']",[],[],"['flink', 'spark', 'hadoop']",[],bachelor,+10 years
Wish,3.2,"San Francisco, CA",Data Engineer,"Company Description

Wish is a mobile e-commerce platform that flips traditional shopping on its head. We connect hundreds of millions of people with the widest selection of delightful, surprising, and—most importantly—affordable products delivered directly to their doors. Each day on Wish, millions of customers in more than 60 countries around the world discover new products. For our over 250,000 merchant partners, anyone with a good idea and a mobile phone can instantly tap into a global market.
We're fueled by creating unique products and experiences that give people access to a new type of commerce, where all are welcome. If you’ve been searching for a supportive environment to chase your curiosity and use data to investigate the questions that matter most to you, this is the place.

Job Description

Our engineers move extremely fast, while solving unique and challenging problems. Our team is small and nimble. We release every day to ensure that engineers are able to iterate quickly, and make an impact immediately. We’re looking for engineers to work on our massive semi-structured datasets.
You'll develop software to process, transform and analyze the data to identify signals from billions of events we collect every day. You'll provide insights that improve the experience of hundreds of millions of users worldwide. You should be results-driven, highly motivated, and have a track record of using data analytics to drive the understanding, growth, and success of a product.
What you'll be doing:
Design and Develop data collecting and processing systems to handle large data sets. You’ll have the opportunity to design innovative data solutions and solve challenging problems.
Design, Develop and Support highly-parallel, and fault-tolerant applications.
Build and integrate scalable backend systems, services, platforms, and tools
Contribute to the design and code of complex data pipelines operating on production data
Optimize current approaches to efficiently handle ever-increasing volumes of data
Build proof of concept using modern technologies and convert them into production-grade implementation.
Create best-practice reports and dashboards based on data mining, analysis, and visualization

Qualifications
5 + years of experience as a Software Engineer or Data Engineer using Python, java or any other programming language
Expertise with SQL and data storage systems
Experience and knowledge of modern data warehouse, pipeline and reporting/analytic techniques and tools such as Airflow, Presto/Hive, Spark, or any other scheduling frameworks, Tableau or other reporting tools
Experience working on Amazon Web Services or other cloud computing platforms
Bachelor's degree in Computer Science or related field.
Preferred Qualifications:
Experience in data visualization a plus.
Here at Wish, you are joining a team and company at a time of growth and transformation. You will love being surrounded by people who are as passionate as you are about e-commerce, technology, and a data-driven culture. Even if you don't meet 100% of the above, we encourage you to still apply!
The estimated base salary range for this position varies by the candidate's location as follows:
Candidates located in California: $178,000 - $241,000 annually
Candidates located in New York and Washington: $201,000 - $241,000 annually
Candidates located in Colorado: $155,000 - $186,000 annually
For states not listed, the estimated base salary range for this position starts around $155,000 annually

Please note that individual total compensation for this position will be determined at the Company's sole discretion and may vary based on several factors, including but not limited to, location, skill level, years and depth of relevant experience, qualifications and other business considerations.

Additional Information

Wish values diversity and is committed to creating an inclusive work environment. We provide equal employment opportunities for all applicants and employees. We do not discriminate based on any legally-protected class or characteristic. Employment decisions are made based on qualifications, merit, and business needs. If you need assistance or accommodation due to a disability, please let your recruiter know. For job positions in San Francisco, CA, and other locations where required, we will consider employment for qualified applicants with arrest and conviction records.
Individuals applying for positions at Wish, including California residents, can see our privacy policy here.",155000,501 to 1000 Employees,Company - Public,Information Technology,Internet & Web Services,2010,$1 to $5 billion (USD),CA,13,data engineer,na,"['sql', 'java', 'python']",[],['tableau'],['hive'],['spark'],[],bachelor,+10 years
Colorado Community Managed Care Network,3.2,"Denver, CO",Data Engineer,"CCMCN focuses on improving the lives of Coloradoans through innovative solutions. Our areas of focus include Population Health, Care Coordination, Social Health Information Exchange, Interoperability, and Clinical Quality Improvement. Through working with health centers and community partners, CCMCN provides best-in-class solutions that create a more comprehensive and collaborative network of care for our most needed Coloradoans.
WHY
Work for CCMCN
At CCMCN, we strive to support our employees and their growth within their career life-cycle. We are committed to a diverse and inclusive work environment where new ideas and innovation are not only invited, but celebrated. We value collaboration, innovation, adaptability, and a great work-life balance.
At CCMCN, you will see the impact of your work directly in the improvements that we make with our partners and their communities.
Our Benefits:
Remote work (depending on position) with access to CCMCN's physical office
Friday flextime and volunteer time off
Health, dental, and vision insurance plans
HSA, FSA, DCA, and employer-sponsored HRA
Life, AD&D, and long-term disability insurance plans
401K retirement plan with employer match
Employee Assistance Program (EAP)
Paid leave including vacation, sick, holidays, & 2 floating holidays

Category: Full-time, Exempt
Salary: $70,000 - $85,000 (DOE)
Reports To: Data Engineering Director
At CCMCN, our mission is to provide services that enable our members and their community partners to succeed as efficient, effective, and accountable systems of care.
CCMCN’s vision is that all Coloradans have access to high-quality, integrated, accountable health care. Areas of focus include population health, accountable care, shared services, health information technology, and clinical quality improvement programming. CCMCN is governed by a Board of Directors comprised of organizational representatives from each of its health center members as well as representation from Colorado Community Health Network (CCHN) and clinician representatives. Through working with health centers and community partners, CCMCN provides technological and analytical tools that help create a more comprehensive and collaborative network of care for Coloradans.
Position Description:
The Data Operations department provides data management, integration, and reporting services for multiple external and internal consumers. The Data Engineer will be responsible for all aspects of data management that support CCMCN’s production services. Candidate must have experience in Microsoft SQL database development, data integration, ETL (extract, transform, and load) tools and methods, analytics, reporting, and documentation.
Essential Functions:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures, and code sets as well as applicable data privacy practices and legal requirements.
Required Skills and Experience:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures, and code sets as well as applicable data privacy practices and legal requirements.
Additional Preferred Qualifications:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures, and code sets as well as applicable data privacy practices and legal requirements.",77500,1 to 50 Employees,Nonprofit Organization,,,-1,Unknown / Non-Applicable,CO,-1,data engineer,na,['sql'],[],[],[],[],[],,
DocuSign,3.7,"San Francisco, CA",Data Engineer,"Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

DocuSign is seeking a talented and results-oriented Data Engineer to focus on delivering trusted data to the business. The Data Engineer delivers data for analytics using our Enterprise Data Warehouse, enabling the global DocuSign analytics community via curated, governed and cleansed data. As a member of the Global Data and Analytics team, the Data Engineer leverages a variety of technologies to accomplish this goal, ranging tools like Airflow, Matillion, dbt, Snowflake and Fivetran to languages like SQL and Python. The successful candidate will develop solutions with innovative cloud technologies, work on a variety of fast-paced assignments, and partner with world-class technical and business teams to maximize the value of data.

This position is an individual contributor role reporting to the Manager, Data Analytics.

Responsibility
Build data pipelines using Fivetran, dbt/Matillion, Snowflake and Airflow
Develop and maintain data documentation including ERD, data dictionaries, data lineage, and metadata
Ensure data quality and integrity by implementing appropriate data validation and cleansing techniques
Monitor and troubleshoot data pipelines to identify and resolve issues in a timely manner
Build POCs to validate new concepts and new technologies
Collaborate with business, engineering, and data science teams to understand their data needs and design efficient solutions to support their requirements


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)


What you bring

Basic
Bachelor’s Degree in Computer Science, Data Analytics, Information Systems or similar, or equivalent work experience
5+ years of relevant experience
2+ years of dimensional and relational data modeling experience
Experience with modern data integration and transformation tools such as Fivetran, Dbt, and Matillion
Experience with workflow orchestration tools such as Airflow
Experience with MPP databases like Snowflake, Redshift and BigQuery
Experience with cloud platforms like AWS, Azure and GCP
Experience with versioning tools like git
Experience working with tools like Jira and Confluence
Experience with SQL and Python
Experience with document and data debugging

Preferred
Good knowledge of database and data warehouse concepts such as facts and dimensions to design and develop data models that support enterprise reporting and analytics needs
Ability to work independently with minimal supervision, as well as in a team environment
Excellent communication skills
Eye for detail, good data intuition, and a passion for data quality
Comfortable working in a rapidly changing environment with ambiguous requirements
Organizational and time management skills, with the ability to prioritize tasks and meet deadlines


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $118,300 - $182,775 base salary

Washington: $111,600 - $162,625 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.",137113,5001 to 10000 Employees,Company - Public,Information Technology,Information Technology Support Services,2003,$1 to $5 billion (USD),CA,20,data engineer,na,"['sql', 'python']","['snowflake', 'aws', 'azure', 'redshift']",[],"['snowflake', 'dbt']",[],[],bachelor,+10 years
Spencer Gifts & Spirit Halloween,3.6,"Egg Harbor Township, NJ",Azure Data Engineer,"We are currently looking for a Data Engineer to optimize our data integration at scale. Inside our Data Integration team, you will be designing pipelines and warehouses to model data from multiple sources that will allow us to derive business insight. Using Azure and other open-source technologies, such as Azure Data Factory and PySpark, you will design and build our next-generation ETL pipelines and data models.
Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data
Develop data models and schemas in our data warehouse that enable performant, intuitive analysis
Handle the challenges that come with managing terabytes of data
Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance
Develop the server applications and APIs that are used by our Data Team
Responsibilities:
Build data pipelines and python-based ETL tools for acquiring, processing, and delivering data
Develop data models and schemas in our data warehouse that enable performant, intuitive analysis
Handle the challenges that come with managing terabytes of data
Collaborate with business leaders and analysts to define key metrics and build reporting to monitor and understand company performance
Develop the server applications and APIs that are used by our Data Team
Requirements:
Bachelor’s degree or higher in Computer Science, Computer Engineering, Information Technology or related field
Fluent in several programming languages such as Python, R, or Scala
6+ years of work experience in building ETL pipelines in production data processing and analysis
Experience designing SQL tables, choosing indexes, tuning queries, and optimizations across different functional environments.
Hands-on experience writing complex SQL queries and using a BI tool
Experience with data lakes and designing and maintaining data solutions using Spark and Azure serverless services such as ADF
Familiarity with data ingestion APIs, data sharing technologies, and warehouse infrastructure and development
* Proof of vaccination for COVID-19 required for employment, reasonable accommodations considered for medical, pregnancy or sincerely held religious beliefs.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Egg Harbor Township, NJ: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Work Location: In person",112889,5001 to 10000 Employees,Company - Private,Retail & Wholesale,"Gift, Novelty & Souvenir Stores",1947,$100 to $500 million (USD),NJ,76,data engineer,na,"['sql', 'r', 'python', 'scala']",['azure'],[],[],['spark'],[],bachelor,5-10 years
Shutterfly,3.3,"Eden Prairie, MN",Senior Data Engineer,"At Shutterfly, we make life’s experiences unforgettable. We believe there is extraordinary power in the self-expression. That’s why our family of brands helps customers create products and capture moments that reflect who they uniquely are.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.
What You'll Do as a Sr. Data Engineer II
Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines am improve the operation and performance of the Data Warehouse
The Skills You'll Bring
Expert knowledge Python, Spark, SQL scripting and Java
7+ years of hands-on experience with building data ingress frameworks and experience with dealing with a wide variety of source data formats
Hands-on experience in building data applications, including design, implementation, debugging, and support.
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it’s the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.",132354,10000+ Employees,Company - Private,Information Technology,Internet & Web Services,1999,$1 to $5 billion (USD),MN,24,data engineer,senior,"['sql', 'java', 'python']",['aws'],[],[],['spark'],[],,+10 years
RelMap Consulting,4.8,"Addison, TX",Senior Data Engineer (Hybrid),"The data engineer entails:
Advanced SQL Server Development
Ability to create and modify views
Ability to create and modify custom functions
Ability to write complex queries, including joins, temp table processing, and common table expressions (CTE’s)
Ability to create and modify stored procedures using TSQL
Ability to analyze and optimize query performance using indices
Ability to use Red Gate’s SQL Compare to perform a schema (AKA “do a manual roll”) between one server an another, and between revision control and a database
Ability to stand-up new database server VM’s in Linux and Windows using VMWare
Ability to backup and restore a database in a Linux and Windows environment
Ability to manage and script automated database jobs
Ability to use PowerShell / Bash to interact with databases from the command line
Addition skills that would be desired but not required
· Experience with Redis
· Experience with MongoDB / CosmosDB
· Experience with Azure and Azure SQL
· C#, JavaScript (NodeJS / ES6), or Python experience
Job Type: Contract
Pay: $70.00 - $80.00 per hour
Compensation package:
Hourly pay
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Addison, TX: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",135000,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2010,$1 to $5 million (USD),TX,13,data engineer,senior,"['sql', 'python']",['azure'],[],"['sql server', 'mongodb', 'redis']",[],['bash'],,5-10 years
Transfr,4.5,"New York, NY",Sr Data Engineer,"Transfr creates job-training simulations in virtual reality designed by each industry that teaches novices in the same manner that experts master their crafts - through trial and error. Data-driven simulations work like virtual coaches, adapting to every person’s pace and skills while responding to their mistakes. Our immersive Experience Training method helps trainees build confidence in their knowledge, skills, and abilities.
We provide a better way to train young adults for the skills they need to succeed on the job. We focus on developing a pipeline of talent for jobs that are going unfilled, significantly reducing cost and risk.

Job Summary:

Transfr is looking for a highly motivated, self-driven Data Engineer to join the team. This individual will have the opportunity to work with researchers, engineers, and product designers across the company and engage in cutting-edge research to contribute to learning research and analytics.

This position can be 100% remote or work with our team from our headquarters in NYC on a flexible, hybrid schedule.

Responsibilities:
Instrument data solutions from multiple sources that enable fast, data-driven decision making on issues including useability, learning, product engagement, and quality.
Build the infrastructure required for optimal extraction, transformation, and loading of data from multiple data sources using SQL, PostgreSQL, and AWS technologies
Build real-time, scalable data solutions that support A/B product testing and support data and learning science through enabling visualizations and views into complex data sets
Create and maintain data pipeline architecture, configuration and implementation
Work with internal teams to assist with data-related technical issues and support their data infrastructure, access and visualization needs.
Support multiple teams with the creation of data strategies and data guidance documentation and architectures
Adhere to data privacy and security guidelines and regulations analysis
Build data expertise and own data quality for allocated areas
Create efficient processes for acquiring, extracting, integrating, transforming, and modeling data to derive useful information
Qualifications:
Minimum 4+ years of experience in a Data Engineer role
Strong preference for candidates with a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases including Postgres and MySQL.
Strong programming and analytics experience with Python and R
Experience with using Python to build ETL pipelines
Strong knowledge and hands-on experience building data lakes and architecting warehouses
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
Architect and implement CI/CD strategy for Data platform
A successful history of manipulating, processing and extracting value from large datasets
Working knowledge of message queuing, stream processing, and highly scalable big data data stores
Experience supporting and working with cross-functional teams in a dynamic environment

What We Offer:

The base salary range for this position is expected to be between $130K - $150K, with the actual base salary amount dependent on a number of factors, including but not limited to a candidate’s credentials, relevant experience, and primary job location. In addition to salary this role will be eligible for additional company benefits such as stock options, 401(k), paid vacation and sick time, and medical/dental/vision insurance.

In Closing:

We invite you to join us. Be a part of creating pathways to prosperity by helping to develop training simulations to teach skills that lead to well-paying jobs, for all.

At Transfr, we embrace diversity because it breeds innovation. Transfr is an equal opportunity employer that participates in E-Verify committed to providing equal employment opportunities to all applicants, consultants, and employees, and prohibits discrimination and harassment of any type without regard to race, color, religion, age, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

**Must be authorized to work in the United States without restriction**

#LI-Remote

Learn more at https://transfrinc.com/",125505,51 to 200 Employees,Company - Private,Education,Education & Training Services,2018,$1 to $5 million (USD),NY,5,data engineer,senior,"['sql', 'r', 'python']","['aws', 'redshift']",[],"['postgresql', 'mysql']",[],[],master,+10 years
CrossAmerica Partners,3.2,"Allentown, PA",Microsoft SQL Data Engineer,"We are looking for a talented Microsoft SQL Data Engineer to join our team. The successful candidate will be responsible for designing and implementing scalable, reliable, high-performance data solutions using Microsoft SQL Server. The ideal candidate should have a proven track record of delivering successful projects strong expertise in database design, data modeling, and data integration.
Key Accountabilities:
Design and develop data models, database schemas, and ETL processes using Microsoft SQL Server.
Build and maintain data warehouses and data lakes to support business intelligence and analytics
Collaborate with cross-functional teams to understand data requirements, and design solutions that meet business needs.
Develop efficient SQL queries, stored procedures, and triggers to support data access and analysis.
Perform data profiling and data analysis to identify data quality issues and develop strategies to resolve them.
Optimize database performance by tuning SQL queries, database indexes, and query execution plans.
Implement data security, access controls, and auditing mechanisms to protect sensitive data.
Ensure data quality and consistency by performing regular data validation and testing.
Develop and maintain technical documentation, including data dictionaries, database schemas, and ETL workflows.
Qualifications:
Bachelor's degree in Computer Science, Information Systems, or related field.
At least 5 years of experience in Microsoft SQL Server development, including database design, data modeling, and ETL.
Strong proficiency in T-SQL programming and SQL Server Integration Services (SSIS).
Experience with data warehousing, data integration, and data migration projects.
Working knowledge with Microsoft Azure Cloud Services is a plus.
Strong problem-solving skills and ability to work independently and as part of a team.
Excellent communication and interpersonal skills.
If you are passionate about data and have a strong technical background in Microsoft SQL Server, we encourage you to apply for this exciting opportunity.
We offer:
Competitive compensation
Health insurance (Medical, Dental, Vision)
STD/LTD, Life Insurance
401k with employer match
Vacation/Sick time
Paid holidays
& More!
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: In person",87950,1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,-1,$1 to $5 billion (USD),PA,-1,data engineer,na,['sql'],['azure'],['ssis'],['sql server'],[],[],bachelor,
Talent Group,2.3,"Austin, TX",data engineer with ML,"Required Skills :
A background in computer science, engineering, mathematics, or similar quantitative field with a minimum of 2 years professional experience .
Experience in implementing data pipelines using python.
Experience with workflow scheduling / orchestration such as Kubernetes, Airflow or Oozie.
Extract Transform Load (ETL) experience using Spark, Kafka, Hadoop, or similar technologies.
Experience with query APIs using JSON, Protocol Buffers, or XML.
Experience with Unix-based command line interface and Bash scripts
Job Type: Full-time
Salary: Up to $130,000.00 per year
Benefits:
401(k)
Experience level:
11+ years
Ability to commute/relocate:
Austin, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130000,1 to 50 Employees,Company - Public,Media & Communication,Broadcast Media,-1,Less than $1 million (USD),TX,-1,data engineer,na,"['sql', 'python']",[],[],[],"['kafka', 'hadoop', 'spark']",['bash'],,0-2 years
"Point Predictive, Inc.",4.0,"San Diego, CA",Senior Data Engineer,"Senior Data Engineer, San Diego, CA based OR willing to relocate to San Diego in 60 Days.
Company
PointPredictive is a fast-growing technology start-up that leverages a patented combination of artificial and natural intelligence [Ai+Ni] to provide risk assessments in the auto lending, mortgage, and retail space. The platform has been proven to reduce lender loan losses by 40-60% with review rates of 5-10% of their applications, resulting in higher productivity of lender risk management departments, significantly lower losses to their bottom lines, and improved customer experience. The company was founded in 2013 by a seasoned team of technology entrepreneurs with over 20 years of experience in the startup space (including several acquisitions) and has financial backing from top tier investors.
Role:
The company is looking for an outstanding Senior Data Engineer to focus on its Data Asset, scale the Database and Data Asset for high performance and reliability. You will also serve as an engineering resource for data related questions, issues, and bugs. Core skills include Python, Snowflake, database systems and SQL, and Amazon Web Services (AWS).
Responsibilities:
· Developing new extract-transform-load (ETL) processes and pipelines. Must be able to manage large volumes of data flowing in from a variety of formats and into a variety of location.
· Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Automation experience highly desired.
Write complex SQL; be fluent in relational based systems; have strong analytical and problem-solving skills.
Ability to represent complex algorithms in software; bring strong understanding of database technologies, management systems, data structures, and algorithms; a deep understanding in database architecture testing methodology.
Develop and execute test plan, debugging, and testing scripts and tools.
Building real-time streaming data pipelines; building and deploying data pipelines, data streams, and extract-transform-load (ETL) processes.
Manage Data Governance and Data Cleansing, as well as supporting production issues and customer requests.
Provide engineering support to customer issues and bugs. Research and implement fixes.
About you:
You have 5+ years of relevant software development industry experience building and operating scalable, fault-tolerant, distributed systems.
Database and software development experience with Python, SQL, Redshift, and experience with Amazon Web Services, Snowflake along with pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming, etc.
Experience with container services.
Fluid with Amazon Web Services.
Experience with concurrency, multithreading, and the deployment of distributed system architectures.
Experience leading and shipping large scope technical projects in collaboration with multiple experienced engineers.
You have excellent communication skills and the ability to work well within a team and across engineering teams.
You are a strong problem solver and have solid production debugging skills.
You Thrive in a fast-paced environment and see yourself as a partner with the business with the shared goal of moving the business forward.
You have a high level of responsibility, ownership, and accountability.
Job Type: Full-time
Competitive pay, bonus, equity, and benefits:
Benefits:
401(k)
Health insurance
Dental insurance
Flexible spending account
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Education:
Bachelor's or Master’s (Preferred)
Work Location: San Diego (Del Mar)
Job Type: Full-time
Pay: $105,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Stock options
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
Are you willing to be in the office 5 days a week to influence culture and capabilities ?
Are you willing to move to San Diego within 60 days if not within the commuting region ?
Work Location: Hybrid remote in San Diego, CA 92130",132500,,,,,-1,,CA,-1,data engineer,senior,"['sql', 'python']","['snowflake', 'aws', 'redshift']",[],['snowflake'],"['kafka', 'spark']",[],bachelor,5-10 years
plaxonic,4.6,Remote,Senior Data Engineer,"We are seeking an experienced Senior Data Engineer.
This position is a remote opportunity.
Qualification
Bachelor’s degree in Computer Science or a related field
Minimum of 4 years of experience in building data driven solutions.
Applicants must be authorized to work in the US without requiring employer sponsorship currently or in the future. CSS does not offer H-1B sponsorship for this position.
Specialized Knowledge & Skills
Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.
Strong scripting experience using Python
Working knowledge of foundational AWS compute, storage, networking and IAM.
AWS scripting experience using lambda functions and knowledge of CloudFormation is nice to have.
Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.
Experience with one or more data integration tools viz. Attunity (Qlik), AWS Glue ETL, Talend, Kafka etc.
Strong understanding of data security – authorization, authentication, encryption, and network security.
Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services
Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.
Demonstrated experience in data management with a strong understanding of process re/design.
Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.
Knowledge and understanding of data standards and principles to drive best practices around data management activities and solutions.
Strong understanding of the importance and benefits of good data quality, and the ability to champion results across functions.
Ability to lead collaborative meetings which result in clearly documented outcomes, a concrete understanding of meeting attendee performance/reliability, and ongoing management & follow-up for action items.
Acts with integrity and proactively seeks ways to ensure compliance with regulations, policies, and procedures.
Requirements of this position?
- Python
- AWS (Build data pipeline in AWS environment, Foundational AWS services (s3, VPC, IAM). and Programming in AWS )
- Snowflake or Redshift -Data Integration tool
– PySpark or Glue
- USC (US Citizen) or GC (Green Card holder)
Job Type: Full-time
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
Informatica: 5 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
AWS: 5 years (Preferred)
Python: 5 years (Preferred)
Redshift: 5 years (Preferred)
Snowflake: 5 years (Preferred)
PySpark: 5 years (Preferred)
Glue: 5 years (Preferred)
Work Location: Remote",112889,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),Remote,10,data engineer,senior,"['sql', 'python']","['snowflake', 'aws', 'redshift']",['qlik'],['snowflake'],['kafka'],[],bachelor,5-10 years
The Swift Group,5.0,"Chantilly, VA",Senior Data Engineer,"Job Description
The Swift Group is seeking Data Analytic expert to support and advance a learning data analytic program that collects and analyzes training data, measures learning related organizational drivers of success and provides information to facilitate data driven decisions.
Essential Job Responsibilities:
Extract, Transform, and Load (ETL) unstructured and structured data.
Transform unstructured and structured data into new formats facilitating analysis and reporting.
Write script (SQL Developer, Python) to ingest, process, clean, transform, and organize data from the Data Lake and other sources.
Assess data quality, verify data accuracy, and correct data issues.
Set up data models, infrastructure, pipelines, and frameworks.
Monitor and troubleshoot scripts for ingestion of business data from source systems.
Create data flow and data model documentation including data dictionaries.
Participate in development/maintenance/management of a data lake.
Work with team to create, update, and performance tune feeds of learning data from source systems.
Work with team to administer, performance tune, and configure cloud and data lake environments.
Work with team to receive, track, prioritize, develop, test, implement, and deploy new requirements for data feeds, web applications, and cloud services.
Troubleshoot solutions to data related issues.
Required Skills
5 to I0 years of data engineering experience and solid educational training in data engineering.
Advanced skills in data engineering and ability to assess data quality and verify data accuracy.
Advanced ability to Extract, Transform, and Load (ETL) unstructured and structured data.
Advanced experience using SQL Developer and Python programming languages to script the extraction and transformation of data.
Advanced experience with data infrastructure development and data lake management.
Advanced skills in data collection, cleaning, and maintenance.
Advanced ability to assess data quality and verify data accuracy.
.
Desired Skills
Preferred experience with data science techniques including data visualization.
About iC-1 Solutions, LLC.
iC-1 Solutions LLC. is a wholly owned subsidiary of The Swift Group. The Swift Group is a privately held, mission-driven and employee-focused services and solutions company headquartered in Reston, Virginia. Founded in 2019, The Swift Group supports Civilian, Defense, and Intelligence Community customers, across the country and around the globe.

The Swift Groups is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",112276,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2019,Unknown / Non-Applicable,VA,4,data engineer,senior,"['sql', 'python']",[],[],[],[],[],,
Sadup Softech,4.2,United States,Data Engineer,"USA ,
Contract to Hire
5 M ago
Experience
3+ Years
Positions opened
11
Job Description
· Strong knowledge of Java, Java based frameworks like Spring · Good knowledge in Python programing & SQL concepts · Experience in Big data technologies such as Hadoop, Spark / Hive and scheduling (UC4) is preferred · Experience with RESTful web services and SOA concepts · Strong communication skills in a collaborative environment · Strong critical thinking skills, Ability to devise innovative solutions · Should be a strong advocate of code craftsmanship, good coding standards
Desired Candidate Profile
Work with the Agile team to clarify the new products and features requested by the Product team.
Collaborate with Data science & other dependent teams to design and implement the required solutions.
Explore new industry standard practices and implement for the project.
Participate in pair programming in the delivery of both POC and targeted features.
Understand and apply our technical architecture to ensure consistent, reliable, and secure deployments.
Enhance and maintain existing product capabilities.
Participate in formal and informal code reviews to ensure code quality.
Actively contribute to the automated test suite to enable continuous integration.",112889,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'java', 'python']",[],[],['hive'],"['spark', 'hadoop']",[],,+10 years
Everybody Votes Campaign,4.0,United States,Data Engineer,"ABOUT EVERYBODY VOTES CAMPAIGN
Everybody Votes Campaign (EVC) is a national non-partisan, not-for-profit hiring staff for a large-scale coordinated civic engagement campaign active through the 2024 election cycle. The campaign aims to create a more representative democracy by registering millions of underrepresented voters across the country. This effort focuses on voter registration in a targeted fashion by conducting at-scale, effective, efficient, metrics-driven registration work. Through this work, we seek to fundamentally change the make-up of the electorate and to increase the political power of traditionally underrepresented communities in our democracy.
We directly fund organizations who execute voter registration and run quality control operations. We are dedicated to being active participants with the organizations to ensure their programs are effective and promote an investment in the future of emerging communities.


ABOUT THE OPPORTUNITY
EVC seeks a Data Engineer to implement, maintain, and optimize voter registration data pipelines. The Data Engineer will be responsible for large scale reporting automation with a particular focus on standardizing complicated voter registration data across multiple databases, working closely on a multidisciplinary team of engineers, analysts, project managers, and field practitioners. This individual will be at the forefront of the campaign’s effort to develop advanced voter registration dashboards and pipeline solutions, working closely with partner organizations. The Data Engineer will report to the Director of Data Products.

WHAT YOU WILL DO IN YOUR ROLE

Centralize data from disparate sources across multiple databases and use innovative hygiene solutions to clean up traditionally complicated voter registration data
Optimize and automate the campaign’s voter registration data pipeline, integrate new external data sources, and help our data team automate and streamline manual reporting processes
Set best practices for data standardization and refinement
Participate in maintaining a well-documented, consistent codebase
Work as a team: pair-program, review code, co-design and plan, develop a shared vision for and an understanding of the work, document progress in JIRA and Confluence
This is a great opportunity for someone who:

Enjoys coming up with creative solutions to big questions through collaboration, and is able to use immediate challenges as windows into future opportunities.
Notices and fixes errors that others might overlook. Acknowledges mistakes and turns them into learning opportunities. Has a track record of leaving things better than they found them – simplified pipelines, strong documentation, code sharing discipline.
Plans ahead and finds alternative paths, when needed, to get to the finish line. Bounces back from setbacks and rejections. Holds a high bar when things are hectic.
Brings civic engagement experience working for or with groups that serve communities of color.
Stays ahead of the curve in an ever-changing technology environment.
Identifies decisions, policies, or practices that have disparate impacts based on identity. Is driven to make changes in systems and practices to operationalize equity.
CORE COMPETENCIES

Growth mindset: demonstrated ability to take and receive feedback with professionalism and grace from peers and staff as well as supervisors
Relentlessly goal-oriented: enjoys working toward and achieving ambitious goals; willing to go over, under, around, or through any obstacle that gets in the way of meeting goals with a proven track record of creating and executing/managing comprehensive strategic goals
Cultural competency: able to build relationships and collaborate with colleagues, partners, and stakeholders across multiple lines of identity difference
Keeps Calm in Stressful Situations: demonstrated capacity and willingness to work long hours during peak season, rolling up their sleeves and getting the work done
REQUIREMENTS FOR THIS ROLE

Proficiency and experience using APIs to push and pull data
Strong knowledge of SQL and management of relational databases (in particular, Redshift and PostgreSQL)
Experience working with messy data, and creating and maintaining data pipelines
Strong proficiency in programming (preference for Python)
Experience writing and editing clear, clean code and a willingness to use version control systems like git and Github
Interest in being a member of a diverse, multidisciplinary team that communicates technical concepts to non-technical audiences
Helpful but not required:

Previous experience working in the field of voter registration or voting rights
Familiar with cloud infrastructure (e.g. AWS, GCP)
Understanding of Terminal and the command line interface
Experience creating reports using data visualization & business intelligence tools (e.g. Tableau, PowerBI, Periscope, Google Data Studio)
BENEFITS AND CULTURE
We offer flexible remote forward work, and a generous benefits package; including 100% cost coverage of employee health benefits, 401K with an automatic employer contribution regardless of employee contribution level, virtual therapy, stipend for ergonomic office set ups and generous vacation and leave policies.


All employees must be eligible to work lawfully within the United States upon the commencement of employment. The organization does not sponsor visa applications for prospective or current staff.


Salary Range: $88,000-$102,500
Our work is centered on creating a deeply inclusive and significantly more representative electorate. In order to be successful in this role, the candidate must have the cultural competence to successfully work with a diverse group of staff, partners and stakeholders. We especially strongly encourage applicants with close ties to Black, Latinx, Indigenous, non-English-speaking, disability, and LGBTQ+ communities to apply. We are proudly an Equal Opportunity Employer.",95250,1 to 50 Employees,Nonprofit Organization,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'go', 'python']","['aws', 'redshift']",['tableau'],['postgresql'],[],[],,
"ZAGG, Inc.",3.6,"Midvale, UT",Senior Data Engineer,"Position Summary
ZAGG is looking for an individual who can drive business change through insightful development of systems and applications. This position will work closely with business owners to architect, develop, and implement solutions across ERP, CRM, EDW, and other applications. A successful candidate will have excellent communication and technical skills to help implement business requirements into working solutions.
Salary: $120K to $135K
Responsibilities:
Administration – 30%
Work closely with multiple departments locally and overseas contributing and solving complex issues that will directly affect business operations and outcomes
Gather/evaluate requirements for business processes and technology enhancements while uncovering areas for improvement
Manage and prioritize projects and resources to ensure business goals are met and maximum value is created
Evangelize applications through effective process design and user training
Create and maintain documentation for operational and security audits

Development – 60%
Architect and manage solutions across multiple applications ensuring efficient integrations that provide redundancy, visibility, and extensibility
Utilize technology to improve the quality of life by automating and enhancing the ability of users/departments
Enhance cloud based data silos supporting Microsoft Dynamics CRM technologies
Conduct application testing and provide database management support
Create and maintain integrations between core applications, services, databases, etc.
Consolidate multiple data silos into a single EDW used for companywide Reporting and Analytics
Model and build data structures to support multi-dimensional data discovery

Other duties as required – 10%

Qualifications:
7+ years of similar experience in an engineering role utilizing an EDW system
Bachelor’s degree in Computer Science or related area
Exceptional analytical and conceptual thinking skills with a detail oriented and inquisitive personality
Proven experience gathering and interpreting business requirements and converting them to technical blue prints
Knowledgeable in enterprise technology stacks (Servers, Database, Network, EDI, etc)
Strong experience in cloud architecture and development (Azure, AWS)
Strong experience in supporting data structures supporting Microsoft Dynamics Technologies including MSSQL and Cloud CDS
Strong experience with integration tools and web service protocols such as SSIS, Fivetran, Synapse, Jitterbit, Smart Connect, Scribe, SOAP, REST, etc.
Experience in a development/scripting language such as python, powershell, etc.
Strong SQL Skills along with an understanding of data warehouse methodologies
Strong experience with data warehousing platforms (Azure DW, Redshift, Matrix, Snowflake)
Strong experience with visualization tools (PowerBI, Tableau, SSRS, etc)
Strong knowledge of SDLC and Agile/Scrum Framework
Strong understanding of operations in a consumer goods industry – sales forecasting, inventory, etc. is a plus
Be able to communicate with customers and co-workers in an effective, timely and professional manner
Strong interpersonal and meeting facilitation skills with technical project management experience being a plus
Must have a collaborative style and be able to cultivate and maintain an open environment where ideas are shared, questioned and tested",127500,501 to 1000 Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,2005,$500 million to $1 billion (USD),UT,18,data engineer,senior,"['sql', 'python']","['azure', 'aws', 'snowflake', 'redshift']","['tableau', 'ssis']",['snowflake'],[],[],bachelor,
"Integrated Technology Strategies, Inc.",4.0,"New York, NY",Data Engineer,"Integrated Technology Strategies is a provider of information technology consulting. Digital transformation, improving business performance, developing strategies and enhancing value is at the core of what we are.

Integrated Technology Strategies is looking to hire a Data Engineer to be a part of our Consulting team.

We are looking for someone passionate about data is focused on developing the platform for critical data products, including real time business metrics and analytics capabilities. The role requires supporting and collaborating with groups including Data Analytics/BI/Product, as well as our core backend API team. The individual will not be afraid to think out of the box and will play a key role in technical decision making. We are highly focused on giving ownership and responsibility to autonomous teams, using the right tools for the job, and building flexible architectures.",110128,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),NY,16,data engineer,na,[],[],[],[],[],[],,
DSMH LLC,5.0,"Peoria, IL","Sr. Data Engineer (API, Cloud)","Education
- Bachelor degree in Computer Science, Information Systems, or a
related field; a Master degree is preferred
- At least 15 years of professional experience, with a focus on API
design and development, and extensive knowledge of the
Snowflake Data Cloud platform
Technical Skills
(Required)
- Proven experience in leading large-scale API and Snowflake
implementation projects
- Strong knowledge of API technologies, such as REST, GraphQL, or
gRPC
- Experience with ETL and data integration tools, such as Talend,
Informatica, or Matillion
- Deep understanding of data modeling, data warehousing, and data
quality concepts
- Familiarity with cloud platforms, such as AWS, Azure, or GCP, and their
respective services
- Excellent communication, leadership, and problem-solving skills
- Ability to work effectively in a fast-paced, collaborative environment.
- Strong attention to detail and commitment to delivering high-quality work
- Experience with agile methodologies and project management tools,
such as JIRA or Trello
Job Type: Contract
Pay: $55.00 - $60.00 per hour
Experience level:
11+ years
Schedule:
Monday to Friday
Experience:
APIs: 10 years (Required)
AWS: 5 years (Required)
Snowflake: 10 years (Required)
Work Location: In person",103500,1 to 50 Employees,Company - Private,,,-1,$1 to $5 million (USD),IL,-1,data engineer,senior,[],"['snowflake', 'aws', 'azure']",[],['snowflake'],[],[],bachelor,0-2 years
DSFederal Inc,4.0,Remote,Junior Data Engineer*,"Description:
We are seeking a Junior Data Engineer to support the design, development, and maintenance of our government client’s data infrastructure. The Junior Data Engineer will work under the guidance of senior team members to implement and maintain data solutions to support business needs.
Requirements:
Develop, maintain, and optimize data pipelines.
Collaborate with cross-functional teams to design and implement data-driven solutions.
Assist in the development and maintenance of data storage systems.
Troubleshoot and debug data-related issues.
Participate in code reviews and contribute to best practices for data engineering.
Technical Skills Required:
Proficiency in at least one programming language (Python, Java, etc.).
Understanding of data modeling, database design, data marts, and data warehousing concepts.
Familiarity with ETL tools and techniques.
Experience with cloud-based data platforms such as AWS or Azure.
Strong problem-solving and analytical skills.
Desired Skills:
Familiarity with data visualization tools such as Tableau or Power BI.
Knowledge of machine learning concepts and frameworks.
Experience Required:
1-2 years of experience in data engineering or related field.
Education Required:
Bachelors in Engineering

Who We Are:
DSFederal is a leader in health IT and data analytics that delivers innovative solutions to improve healthcare delivery and address critical challenges, including cancer surveillance, HIV/AIDS, maternal and child health, and COVID-19 emergency response. Our highly experienced professionals serve over 50 different federal clients on over 80 projects. Our people are united by one mission – to improve human life through transformative solutions. www.dsfederal.com.
What We Can Offer You:
DSFederal attracts the best people in the business with our competitive benefits package that includes medical, dental and vision coverage, 401k plan with employer contribution, paid holidays, vacation, and more. If you enjoy being a part of a high performing, professional services, and technology focused organization, we encourage you to apply!
Our Mission: Improve human life through transformative solutions.
Our Core Values: Integrity, People First, Disciplined Business Practices, Customer Centric, Value Creation
EEO Statement: DS Federal is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status.
E-Verify Statement
DS Federal utilizes the E-Verify program for every potential new hire. This makes it possible for us to confirm that every employee who works for DS Federal is eligible to work in the United States. To learn more about E-Verify you can call 1-800-255-7688 or visit their website by clicking the logo below. E-Verify® is a registered trademark of the United States Department of Homeland Security.
#LI-LP1",112889,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$25 to $100 million (USD),Remote,16,data engineer,na,"['java', 'python']","['azure', 'aws']","['tableau', 'power bi']",[],[],[],,2-5 years
United Digestive,4.0,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs",112889,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,GA,-1,data engineer,na,[],[],[],[],[],[],,
Swish Analytics,5.0,United States,Data Engineer,"Company Overview
Swish Analytics is a sports analytics, betting and fantasy startup building the next generation of predictive sports analytics data products. We believe that oddsmaking is a challenge rooted in engineering, mathematics, and sports betting expertise; not intuition. We're looking for team-oriented individuals with an authentic passion for accurate and predictive real-time data who can execute in a fast-paced, creative, and continually-evolving environment without sacrificing technical excellence. Our challenges are unique, so we hope you are comfortable in uncharted territory and passionate about building systems to support products across a variety of industries and consumer/enterprise clients.
Job Description
The Swish Analytics team is seeking Data Engineers to have direct impact on the infrastructure and delivery of our core consumer and enterprise data offerings. We're a team passionate about accurate predictions and real-time data, and hope you find satisfaction in building new products with the latest and greatest technologies. This is a remote position.
Duties
Architect low-latency, real-time analytics systems including raw data collection, feature development and endpoint production
Build new sports betting data products and predictions offerings Integrate large and complex real-time datasets into new consumer and enterprise products
Develop production-level predictive analytics into enterprise-grade APIs
Contribute to the design and implementation of new, fully-automated sports data delivery frameworks
Requirements
BS/BA degree in Mathematics, Computer Science, or related STEM field
Experience writing production level code
Proficiency in Python
Proficiency in SQL (preferably MySQL)
Experience building end-to-end ETL pipelines
Experience utilizing REST APIs
Experience in SQL database management, shema design, index structuring
Experience with version control (git), continuous integration and deployment, shell scripting, and cloud-computing infrastructures (AWS)
Experience with web scraping and cleaning unstructured data
Knowledge of data science and machine learning concepts
A strong interest for US sports and sports betting. You love sports, particularly the NFL, NBA, MLB, NHL, College Football, College Basketball, and Tennis, and can use your knowledge of the sport to inform your work with complex datasets
Base salary:$90-145,000
Swish Analytics is an Equal Opportunity Employer. All candidates who meet the qualifications will be considered without regard to race, color, religion, sex, national origin, age, disability, sexual orientation, pregnancy status, genetic, military, veteran status, marital status, or any other characteristic protected by law. The position responsibilities are not limited to the responsibilities outlined above and are subject to change. At the employer's discretion, this position may require successful completion of background and reference checks.",117500,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Gambling,2014,Unknown / Non-Applicable,TX,9,data engineer,na,"['sql', 'shell', 'python']",['aws'],[],['mysql'],[],[],,
CareFirst BlueCross BlueShield,3.5,"Washington, DC",Principal Data Engineer (Remote),"Resp & Qualifications
PURPOSE:
Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources. Creates data collection frameworks for structured and unstructured data. Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent. Employee is recognized as an expert within the organization and has in-depth and/or breadth of expertise in own discipline and broad knowledge of other disciplines within the function. Anticipates internal and/or external business challenges and/or regulatory issues; recommends process, product or service improvements. Solves unique and complex problems that have a broad impact on the business. Contributes to the development of functional strategy. Leads project teams to achieve milestones and objectives.

ESSENTIAL FUNCTIONS:
Leads project teams to achieve milestones and objectives. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources. Develops data models by studying existing data warehouse architecture; evaluating alternative logical data models including planning and execution tables; applying metadata and modeling standards, guidelines, conventions, and procedures; planning data classes and sub-classes, indexes, directories, repositories, messages, sharing, hiding, replication, back-up, retention, and recovery.
Solves the most complex problems; takes a new perspective on existing solutions. Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Recommends process, product or service improvements. Solves unique and complex problems that have a broad impact on the business. Contributes to the development of functional strategy. Acts as a mentor for colleagues with less experience.
Manage the data collection process providing interpretation and recommendations to management. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using various technologies (e.g. Hadoop or equivalent MapReduce platform). Defines, designs and build dimensional databases and data pipelines to support analytics projects.
Oversees the delivery of engineering data initiatives and projects. Supports long term data initiatives as well as Ad-Hoc analysis and ETL activities. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies. Provide technical guidance and support to developers, data engineers and data administrators. Develop strategies for data acquisitions, recovery and implementations.

SUPERVISORY RESPONSIBILITY:
Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.

QUALIFICATIONS:

Education Level: Bachelor's Degree in Information Technology or Computer Science OR inlieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.
Licenses/Certifications Upon Hire:
Data Management\Certified Analytics Professional (CAP)
Experience:
10 years Experience leading database design and developing modeling tools.
Experience in leading data engineering and cross functional teams to implement scalable and fine tuned ETL/ELT solutions for optimal performance.
Experience developing and updating ETL/ELT scripts.
Hands-on experience with application development, relational database layout, development, data modeling.
Knowledge, Skills and Abilities (KSAs)
Expert in at least one programming language (i.e., SQL, NoSQL, Python).
Knowledge of multiple database technologies - structured and un-structured.
Ability to quick learn new technology and take direction.
Strong customer service orientation.
Provide direction to and lead technical teams.
Requires strong organizational with the ability to handle multiple priorities.
Excellent communication skills both written and verbal.
Must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. Must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging.

Department
Department: CBIW Mandates Development
Equal Employment Opportunity
CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Hire Range Disclaimer
Actual salary will be based on relevant job experience and work history.
Where To Apply
Please visit our website to apply: www.carefirst.com/careers
Federal Disc/Physical Demand
Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
PHYSICAL DEMANDS:
The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
Sponsorship in US
Must be eligible to work in the U.S. without Sponsorship
#LI-LD1",125053,5001 to 10000 Employees,Company - Private,Insurance,Insurance Carriers,1942,$100 to $500 million (USD),DC,81,data engineer,senior,"['sql', 'python', 'nosql']",[],[],[],['hadoop'],[],bachelor,0-2 years
Purple Drive Technologies,4.2,"Chicago, IL",AWS Data Engineer,"Looking for a strong AWS Data Engineer who has hands on experience with AWS data stack (AWS Glue, Glue Catalog, S3, AWS Hudi, EMR, Appflow and airflow). Successful candidate will be responsible for the following activities:
i. Identify, Build the foundational blocks of the Data Lake Platform such as Processing layer, CI/CD pipelines, Orchestration, template pipeline etc.
ii. Work closely with Infrastructure/Architects/key stakeholders to finalize the foundational blocks on various aspects of the platform
iii. Automate pipeline for Moxie data source by using the foundation that is built on.
iv. Work with Business stakeholders to understand the data needs and build data tech debt.
v. Build reusable data pipelines/framework as a solution accelerator
vi. Configure automated pipelines using the framework for the data sources (CC1, MedForce, Nice).
vii. Build Data Quality/Governance framework to make data complete and trustworthy.
viii. Reference Architecture, Documentation of Data Platform
ix. Configure automated pipelines for the data sources identified from tech debt.
x. Production roll-out, knowledge sharing and hyper-care
Job Type: Full-time
Schedule:
Monday to Friday
Work Location: One location
Speak with the employer
+91 (609) 796-2792",97051,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,[],['aws'],[],[],[],[],,
OpenGov,4.5,Remote,Senior Data Engineer,"Imagine yourself here!

OpenGov is a mission driven fast-growth, Series D, venture backed startup (includes Andreessen Horowitz, Formation 8, and Emerson Collective). Our Board of Directors includes iconic Silicon Valley executives John Chambers (former Cisco Chairman and CEO) and Marc Andreessen (Time Magazine’s list of the 100 most influential people in the world).

OpenGov is the leader in modern cloud software for local governments and state agencies. We have surpassed 1,600+ governments (and growing fast!) using our products in our mission to power more effective and accountable government.

OpenGov is a 2022 Top Workplaces USA award winner and a Forbes 2022 America's Best Startup Employer!

The Senior Data Engineers primary responsibility will be to balance the health of our data platform with the development of new integrations, pipelines, and data models. In this role, you will also manage the production support and enhancement of our existing BI and Data Warehouse tech stack. This role will be expected to learn and support Enterprise Data processes and software practices to proactively manage platform processes and ensure the highest quality of data. Developing strong relationships with business partners in the GTM, G&A, and R&D organizations is essential to success in this role. Prior experience in Data Warehousing and Data Integration/ELT technologies, coupled with your ability to understand complex data models and flows, will enable the incumbent to excel in this role.

Responsibilities:

Design and implement data solutions and related integrations with industry best practices
Design and implement efficient ETL and ELT processes to manage the data flowing into and out of the Data Warehouse in accordance with data governance and security standards
•Monitor and maintain data pipelines and system integrations proactively to ensure high service availability. •Systems may include Salesforce, Jira, Intacct, Netsuite, Zuora, Mavenlink (PSA), Zendesk, Pendo, ProductBoard, MixMax, Gainsight, Gong (CI), and Marketo
Help to define and improve our internal standards for style, maintainability, and best practices for a reliable data infrastructure
Build and leverage internal and external partnerships across GTM, G&A and R&D organizations, turning business goals into technical solutions
Assist in driving instrumentation and optimization of the data warehouse and systems integrations processes
Ensure data accuracy by following standardized structures and practices for dissemination and communication with appropriate departments
Comfortable breaking down complex data and integrations to technical and non-technical staff members
Drive business stakeholder meetings to gather requirements and make system integration, pipeline and reporting/BI recommendations
Conduct analysis and ongoing monitoring of available data, refinement, data and BI roadmap and feedback, scrubbing to improve existing data integrity
Understand what good looks like and the change management required to get there
Develop a deep understanding of the various business processes, data sets and data team deliverables

Requirements and Preferred Experience:

Bachelor’s degree in Information Technology, Computer Sciences or related field (equivalent experience may be substituted for formal education)
8+ years of experience in Business Systems, IT, Data Engineering or similar technical role
5+ years of experience in Data Engineering, Warehousing and Systems Integrations; 2+ years of experience with DBT
Experience with Salesforce, Support Platforms (Zendesk or Service Cloud), Mixmax, Outreach or Salesloft, Gainsight, JIRA, Intacct or Netsuite, Zuora, Professional Services Platforms (Mavenlink, FinancialForce, etc.)
Knowledge of enterprise SaaS applications and integrations
Experience working with
business intelligence tools and concepts (Domo, Tableau, Looker, etc.)
data warehouses (Redshift or Snowflake)
working with ETL and/or ELT tools (Snaplogic, Workato, and Fivetran)
iPasS experience maintaining and building integrations (Mulesoft, Snaplogic, Dell Boomi, etc.)
Demonstrated proficiency working with SQL, Python, GIT and CI/CD pipeline
An “end-to-end” data owner who believes in owning the whole stack from data source to cleaning to dashboard and stakeholder communication
Propensity and passion for problem solving, both conceptually and analytically, focused on optimizing strategy and process to continually provide valuable insights.
Organized professional, with the ability to manage multiple projects for multiple stakeholders with varying specifications
Ability to remain focused and flexible, working in a fast-paced environment with rapid change
Strong process and documentation skills, with the ability to connect process gaps with data integrity needs and to manage across teams to refine workflows
Strong written and verbal communication skills
Experience working with SaaS applications built on AWS or Azure platforms is preferred
#LI-DNI
What makes OpenGov unique

» Leadership: CEO Zac Bookman (MPA from Harvard and JD from Yale) is truly a mission-driven CEO. He was named one of the 100 most Intriguing Entrepreneurs by Goldman Sachs, a Tech Pioneer by the World Economic Forum, and SF and Silicon Valley Business Times' 40 under 40 class of 2018!

» Funding: Over $250 million, Series D company, from top tier investors including Andreessen Horowitz, 8VC, Cox Enterprises, and Emerson Collective.

» Board of Directors: Includes iconic executives John Chambers (former Cisco Chairman and CEO), Marc Andreessen (Time Magazine’s list of the 100 most influential people in the world), Katherine August-deWilde (Vice Chair of First Republic Bank), and Amy Pressman (co-founder, former president, and a current board member of Medallia).

» Growth: Record breaking growth with 1,600+ governments (and counting) using our products and seven acquisitions in the past six years! Click here to read more.

» Culture: Winner of Forbes 2022 Best Startup Employers, Winner of 2022 Top Workplaces USA award, 50 Best Workplaces award. Check out our Careers Video!

» Perks: 90% paid Medical/Dental/Vision premium for employees, fully paid Life and Short/Long term disability insurance, Unlimited PTO, Parental Leave policy, monthly fitness stipend, anniversary awards, and more!

» Product: Named to the GovTech 100 (seven consecutive years), we are the leader in cloud software for our nation's cities, counties, and state agencies.

» Mission Driven: We are a technology company with a passion for the mission. We're powering more effective and accountable government.

Come join us and make a positive social impact!

OpenGov is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.",112889,201 to 500 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2012,Unknown / Non-Applicable,Remote,11,data engineer,senior,"['sql', 'r', 'python']","['snowflake', 'aws', 'azure', 'redshift']","['looker', 'tableau', 'excel']","['snowflake', 'dbt']",[],[],bachelor,+10 years
JTEC Consulting,5.0,"Crystal City, VA",Data Engineer,"Data Engineer
Crystal City, VA 22202

JTEC Consulting LLC focuses on successfully delivering solutions to meet our clients’ most critical needs. Our founding members have decades of experience delivering a wide range of solutions to Air Force and DOD clients. We are a Veteran-Owned Small Business.

Security Clearance Requirement: Current TS/SCI
Location Note: Must reside in Wash DC Metro area and provide on-site support, relocation will be considered



Position Description:
JTEC Consulting is hiring a Mid-Senior level Data Engineer to work in support of a large-scale data analytics platform. This position requires excellent analytical skills and the ability to work directly with the government customer in a dynamic mission driven environment. Qualified candidates will have experience developing solutions for high volume, low latency applications and abilities to operate in a fast-paced environment.

Duties and responsibilities may include (but are not limited to):
Work as part of an enterprise-wide team collaborating with other data engineers, data scientists and product leads to develop innovative solutions for data requirements.
Build pipelines to collect data from disparate external sources.
Implement rules and perform validation and analytics to ensure expected data is received, cleansed, transformed, and in an optimized output format.
Support automation and performance optimization.
Monitor pipeline status and performance.
Support troubleshooting and issue resolution.
Other duties as assigned.
Education and qualifications include (but are not limited to):
U.S. citizenship
Current TS/SCI security clearance
Understanding of distributed computer systems
Experience working in big data environments and understanding of data challenges (DoD command experience preferred)
Experience working in cloud environments
4+ years of experience in the following areas:
Experience with modern programming languages such as Java and Spark
Experience using SQL and ETL
Experience working in a distributed environment using Apache Kafka and Apache Airflow
Experience with stream processing using Apache Flink or StreamSets
Experience working in an Agile development environment is preferred
Proficiency working across MS Office Suite including advanced Excel skills
Experience working with data visualization tools and dashboards is a plus
Effective written and verbal communication skills to work in a collaborative environment with government and contractor teams
Availability to work on-site
JTEC Consulting LLC provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.

Salary:
JTEC offers competitive compensation and benefits to all employees. The estimated salary range displayed in this posting is dependent upon position requirements, individual qualifications, education, experience, location, and other job related factors. Our recruiting team will review candidates' related experience and salary targets during the evaluation process. All qualified candidates will be considered.",145000,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,VA,-1,data engineer,na,"['sql', 'java']",[],['excel'],[],"['kafka', 'flink', 'spark']",[],,+10 years
Unilever,4.1,Remote,Senior Data Engineer (Multiple Openings),"We have multiple Data Engineer roles open across the US to be located preferably in Eastern Standard Timezone; near our HQ office in New Jersey; or near satellite offices in Rogers, AR; Minneapolis, MN and Cincinnati, OH.
Background & Purpose of the Job
The North America Data Engineer will work with peer data engineers to develop data solutions to support product analytics managers and data scientists. This role will be partnering with internal data scientists, product managers and external stakeholders across the business to deliver advanced analytics. The ideal candidate is an experienced data engineer with vast hands-on experience and leadership experience in leading projects and teams from the ground up.
Who You Are & What You’ll Do
Facilitate the team’s delivery of scalable, maintainable, performant data engineering solutions. Work through your teams to define/design solution options, evaluate technical feasibility, and provide estimates on effort and risk. Drive the design and implementation of new data projects and the optimization of existing solutions. Work with architecture and engineers to ensure quality solutions are implemented, and engineering best practices are defined and followed. Drive collaborative reviews of designs, code, and test plans. Define the team’s technical roadmap and influence its adoption. Anticipate, identify and solve issues concerning data management to improve data quality. Work across teams to resolve operational & performance issues. Identify and remove technical bottlenecks for your engineering teams.
In this role you’ll be working closely with passionate and driven Product Management team, Architects, Engineers, Data Analysts, Internal partners and other departments. They will span across various functional groups across the enterprise. You will be in contact with a wide range of great people. In all interactions, you will find success in teamwork, a positive attitude, and hard work. As a Data Engineer, in partnership with diverse team of smart, intelligent, and creative people. This allows us to be a driving force for building first-class solutions for North America Data & Analytics team and its business partners, working on development projects related to customer development, supply chain, commerce, consumer behavior and web analytics among others.
You’re a dot connector and storyteller: You like to unravel complex data, - building analytical tools that utilize the data pipeline to provide actionable insights, operational efficiency, and other key metrics.
You’re a changemaker: You are a self-starter you can work independently to identify, design and implement internal process improvement; driving change with new innovative solutions, optimizing data delivery, re-designing for greater scalability.
You’re a paradox navigator: Enable data driven decision-making across customer development, supply chain, consumer marketing insights, and disrupting data silos.
You’re a culture & change champion: Be an advocate and set an example of digital cultural.
You love to win, and have fun doing it: Transform the way Unilever operates with a proactive, informed risk-taking, winning attitude!
What You Will Bring
Bachelor’s Degree in Computer Science, Information Systems, Business or related field or equivalent combination of education and experience with 3+ years of experience in large-scale software development and data engineering
Ability to define/design solution options, evaluate technical feasibility, and provide estimates on effort and risk
Experience working with data sources such as IRI, Nielsen, Retailer POS systems, Panel data such as Numerator and Nielsen/IRI Panel required, in support of retail such as Walmart, Target, Amazon, Dollar General, Kroger a huge plus!
Solid foundation in data structures, algorithms, and architecture patterns
Experience building distributed / cloud scalable, high-performance data solutions
Experience with batch processing frameworks and programming in tools such as Python, SQL, Spark and Hive
Experience with messaging/streaming/complex event processing tooling and frameworks with an emphasis on Spark Streaming or Structured Streaming or Apache Nifi
Experience with data warehousing related concepts - e.g. SQL and SQL Analytical functions
Experience in Agile/Scrum application development
Familiarity with the principles of Domain Driven Design
The following skills and experience are also relevant to our overall environment, and nice to have:
Experience working in a public cloud environment, particularly Microsoft Azure
Experience building RESTful API’s to enable data consumption
Experience in developing Power BI dashboards using DAX, Power Automate flows, Web Applications
Experience with practices like Continuous Development, Continuous Integration and Automated Testing
Pay: The pay range for this position is $83,200 to $124,700. Unilever takes into consideration a wide range of factors that are utilized in making compensation decisions including, but not limited to, skill sets, experience and training, licensure and certifications, qualifications and education, and other business and organizational needs.
Bonus: This position is bonus eligible.
Long-Term Incentive (LTI): This position is LTI eligible.
Benefits: Unilever employees are eligible to participate in our benefits plan. Should the employee choose to participate, they can choose from a range of benefits to include, but is not limited to, health insurance (including prescription drug, dental, and vision coverage), retirement savings benefits, life insurance and disability benefits, parental leave, sick leave, paid vacation and holidays, as well as access to numerous voluntary benefits. Any coverages for health insurance and retirement benefits will be in accordance with the terms and conditions of the applicable plans and associated governing plan documents.
-
Unilever is an organization committed to diversity and inclusion to drive our business results and create a better future every day for our diverse employees, global consumers, partners, and communities. We believe a diverse workforce allows us to match our growth ambitions and drive inclusion across the business. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

Employment is subject to verification of pre-screening tests, which may include drug screening, background check, credit check and DMV check.

If you are an individual with a disability in need of assistance at any time during our recruitment process, please contact us at
NA.Accommodations@unilever.com
. Please note: This email is reserved for individuals with disabilities in need of assistance and is not a means of inquiry about positions or application statuses.
#LI-Remote",103950,10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1872,$10+ billion (USD),Remote,151,data engineer,senior,"['sql', 'python']",['azure'],['power bi'],['hive'],['spark'],[],bachelor,+10 years
Intone Networks,4.5,"Dallas, TX",Data Engineer,"Walmart Dallas, TX (Must be onsite from Day - Hybrid Schedule onsite a couple times per month) JOB DESCRIPTION: Looking for a Data Engineer who is very strong with Spark, Scala, Hive, GCP , Bit query. (They are commercializing data. Lots of pipeline work. Nice to have are - Druid, Azure, on GCP currently",97101,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$5 to $25 million (USD),TX,-1,data engineer,na,['scala'],['azure'],[],['hive'],['spark'],[],,
SEFCU,3.4,"Albany, NY",AWS Cloud Data Engineer,"If you are ready to join a company that truly cares about its employees, our members, and our community then you have come to the right place!
Summary of Role:
Broadview is looking for an outstanding Cloud Data Engineer to join the Data team. This is your opportunity to be a core part of the team that has direct impact on the organization’s day-to-day and strategic decision-making processes. As a Cloud Data engineer, you will be responsible for helping to accelerate the organization’s migration to the cloud, moving data from source systems to the AWS environment, managing data into a conformed data model, developing business KPIs and integrating them with front-end BI dashboards. You will get the exciting opportunity to work on large data sets in a cutting-edge data warehouse cloud environment. You will work closely with the business and technical teams to solve many non-standard and unique business problems and use creative problem solving to deliver actionable output.
Our team is serious about great design and redefining best practices with a cloud-based approach to scalability and automation. A successful candidate will be a self-starter, comfortable with ambiguity, with strong attention to detail, an ability to learn and work in a fast-paced environment, and an ability to work effectively with cross-functional teams
Essential Job Functions/Responsibilities:
Design, develop, test, and maintain cloud-based data solutions and infrastructure
Collaborate with cross-functional teams, including business analysts, data engineers, analysts and Cloud infrastructure team to ensure data solutions meet business requirements
Develop data ingestion pipeline to load data from various data sources
Develop and maintain scalable ETL pipelines to extract, transform, and load data
Implement data governance policies and ensure compliance with data security regulations of Broadview
Build and maintain data warehouses and data lakes on AWS
Develop insights into existing data warehouses and optimization approaches
Automate data processes using scripts and workflows to improve efficiency and reduce errors
Optimize data storage and retrieval processes to ensure high performance and scalability
Troubleshoot data-related issues and provide solutions to resolve them
Continuously monitor and improve data quality and data governance processes
Stay up-to-date with emerging technologies in cloud computing, data engineering, and data architecture and recommend best practices to the team.
Minimum Job Qualifications:
Bachelor's degree in Computer Science, Information Technology, or related field
Minimum of 3 years of experience in cloud computing and data engineering
3+ years of related work experience in a similar role. Preferred experience in banking, capital markets, insurance or asset management sectors
Experience with cloud-based data solutions like AWS S3, DMS, Redshift, Snowflake, Athena, EMS, API gateway
Hands-on experience with ETL tools like Apache Airflow or AWS Glue
Hands-on experience working with Quicksight
Knowledge of data modeling and database design principles
Familiarity with data governance and data security policies on AWS
Excellent problem-solving and analytical skills
Strong communication and collaboration skills to work with cross-functional teams
Practical experience in a data architecture/integration engagement across the end to end product life cycle – strategy, roadmap, requirements, design, development, testing, deployment, production support
Starting Compensation: $97,371 - $126,582 plus a competitive benefits package
Bilingual individuals who are fluent in a second language in addition to English are highly encouraged to apply.
We are an equal opportunity employer. We do not discriminate on the basis of race, creed, color, national origin, religion, sex, age, veteran status, disability, genetic information, gender identity, or any other protected class.
Broadview FCU is committed to ensuring individuals with disabilities and/or those who have special needs participate in the workforce and are afforded equal opportunity to apply and compete for jobs. If you would like to contact us regarding the accessibility of our Website or need assistance completing the application process, please contact us at
bv-talentacquisition@sefcu.com
.",111977,201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1953,Unknown / Non-Applicable,NY,70,data engineer,na,[],"['snowflake', 'aws', 'redshift']",[],['snowflake'],[],[],bachelor,+10 years
Barclays,4.0,"Whippany, NJ",Data Engineer,"Data Engineer
Whippany, NJ
As a Barclays Data Engineer, you will be contributing directly to the execution of the business strategy and play a key role in development of future state data science platform. You will also work collaboratively with a cross- functional team of data scientists and database developers, business intelligence designers, architects, business analysts and infrastructure engineers.
Barclays is one of the world's largest and most respected financial institutions, with 329 years of success, quality, and innovation behind us. We've helped millions of individuals and businesses thrive, creating financial and digital solutions that the world now takes for granted. An important and growing presence in the USA, we offer careers providing endless opportunity.
We are currently in the early stages of implementing a hybrid working environment, which means that many colleagues spend part of their working hours at home and part in the office, depending on the nature of the role they are in. We’re flexible on how this works and it may continue to change and evolve. Depending on your team, typically this means that colleagues spend a minimum of between 20% to 60% of their time in the office, which could be over a week, a month or a quarter. However, some colleagues may choose to spend more time in the office over a typical period than their role type requires. We also have a flexible working process where, subject to business needs, all colleagues globally are able to request work patterns to reflect their personal circumstances
Please discuss the detail of the working pattern options for the role with the hiring manager.
What will you be doing?
Designing scalable and secure engineering solutions that will be leveraged by Banking colleagues and customers
Working collaboratively with cross-functional team of data scientists & database developers, business Intelligence designers, architects, business analysts and infrastructure engineers
Being responsible for full life cycle development and design of new data science platform with Python and AWS based applications and components
Working as a team player in a global development group, and participating in requirements and data analysis, design as well as development
Communicating and collaborating between the infrastructure, development, and business groups
Having excellent analytical skills, being self-motivated and capable of working in a dynamic environment that demands multi-tasking
Having the ability to generate ideas and efficiently mock-up proposals and demos
What we’re looking for:
B.S. degree in computer science or related field with emphasis on technology
Five years of experience in Python
At least two years of AWS experience
Two plus years of SQL experience
Skills that will help you in the role:
Working experience in financial industry, especially IB applications is a plus
Prior experience with AWS Services such as Lambda, Glue, Athena, ECS, Cloud Formation
Working Experience in developing REST APIs using Python
Experience in building and deploying services using any container orchestration tools such as Kubernetes, ECS, Docker Swarm, OpenShift
Where will you be working?
At Barclays, we are proud to be redefining the future of finance and here at Whippany we are defining the future of the workplace and the future of the way we work and live. We are creating a unique community, one of four strategic tech-enabled hubs that will redefine opportunity for everyone who works here. Whatever you do at Whippany, you’ll have every chance to build a world-class career in this world-class environment.

#LI-Hybrid
#data",96821,10000+ Employees,Company - Public,Financial Services,Banking & Lending,1690,$10+ billion (USD),NJ,333,data engineer,na,"['sql', 'python']",['aws'],[],[],[],['docker'],,
The Washington Post,4.3,"Washington, DC","Data Engineer, Election Platforms (all-levels)","Job Description
The Washington Post is hiring a data engineer for our Election Platforms team, to work on our data pipelines, tools, and infrastructure ahead of the 2024 presidential election.
The Election Platforms team focuses on leveraging data, partnering with stakeholders, and pioneering serviceable tools to facilitate The Post’s election coverage. This role is part of Elections Engineering, a cross-functional group of full-stack engineers and data scientists, with varied technical and academic backgrounds, responsible for delivering live election results across The Post’s platforms.
The role will be focused on contributing to our technical stack and developing new technology to facilitate robust and lively election coverage. In particular, we’re looking for candidates who would be comfortable working on deploying high-impact code in production and have an interest in data and newsroom tooling. We also work on other data engineering projects related to elections, such as voter registration data and campaign finance, and occasionally write
open
source
software
. Most of our work is in Python and TypeScript/React/Node.
We typically work at a higher level of abstraction than a single story, and aim to create general purpose tools that are reusable and easy to deploy. Our ideal candidates are creative problem-solvers, who are comfortable writing production level code to solve real-world newsroom problems. This position is remote-friendly for work in the US.
Motivation
You have a desire to contribute to The Post’s journalism through technology.
You are passionate about refining and improving team process, culture, documentation and communication while tackling complex data science and engineering challenges.
You want to work at a deeper level of abstraction than a single story or beat.
You are excited about learning new technologies and applying them to the practice of journalism.
Responsibilities
Work on the software components of our live election architecture including deployment, observability, participating in code reviews, and more.
Work closely with our engineers and data scientists to develop and maintain pipelines and administrative apps/dashboards for election data.
Work on tooling to make data preparation easier and more efficient for our live election model.
Work with newsroom stakeholders to assess needs, build useful and reusable tooling, and contribute to storytelling.
Foster effective team collaboration and communication by improving and documenting internal tools, processes and guidelines.
Assist in attracting and hiring team members who want to work at the intersection of journalism and technology by actively participating in recruiting – and contributing feedback on our overall hiring process.
Qualifications
A minimum of one year of experience building and maintaining backend and data engineering applications.
Proficiency across some of the languages and frameworks used in our engineering stack such as Python, TypeScript/React/Node, and AWS services.
A desire to work on cutting edge, cross-functional tools to help with features, data science, and machine learning/big data work in a team setting.
Experience with election data is not necessary, but is a plus.
Experience developing and deploying newsroom tools and applications is not necessary, but is a plus.
Levels
If you have less than three years, but at least one year, we’d love to evaluate you for a data engineer role.
If you have more than three years but less than about seven years, we’d love to evaluate you for a senior data engineer role.
If you have more than about seven years, we’d love to talk to you about a principal data engineer role.
Wherever you are in your life or career, The Washington Post offers comprehensive and inclusive benefits for every step of your journey:
Competitive medical, dental and vision coverage
Company-paid pension and 401(k) match
Three weeks of vacation and up to three weeks of paid sick leave
Nine paid holidays and two personal days
20 weeks paid parental leave for any new parent
Robust mental health resources
Backup care and caregiver concierge services
Gender affirming services
Pet insurance
Free Post digital subscription
Leadership and career development programs
Benefits may vary based on the job, full-time or part-time schedule, location, and collectively bargained status.
The Post strives to provide its readers with high-quality, trustworthy news and information while constantly innovating. That mission is best served by a diverse, multi-generational workforce with varied life experiences and perspectives. All cultures and backgrounds are welcomed.
The innovation doesn’t end in the Newsroom – dozens of teams power The Washington Post. We are now hiring the next innovator – how will you Impact Tomorrow?
#washpostlife",83249,1001 to 5000 Employees,Company - Private,Media & Communication,Publishing,1877,Unknown / Non-Applicable,DC,146,data engineer,na,['python'],['aws'],[],[],[],[],,
CareFirst BlueCross BlueShield,3.5,"Reston, VA","Senior Data Engineer, (Hybrid)","Resp & Qualifications
PURPOSE:
The Senior Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on developing solutions and helping transform the company's platforms deliver data-driven, meaningful insights and value to company and clients.

ESSENTIAL FUNCTIONS:
Develops and maintains auditing systems (e.g., data warehouses, data lakes) using Ab Initio. Prepares and manipulates data using multiple technologies. Creates data collection frameworks for structured and unstructured data.
Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
Supports Production and PLT environments; addresses issues in a timely fashion.
Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.
Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
Applies and implements best practices for data auditing, scalability, reliability and application performance.

SUPERVISORY RESPONSIBILITY:
Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.

QUALIFICATIONS:

Education Level: Bachelor's Degree, Details: Computer Science, Information Technology or Engineering or related field OR In lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.

Experience: 5 years Experience with database design and developing modeling tools. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.
The incumbent must have expereince in Ab Initio, Unix shell scripting, SQL, Axway, Cognos. Exposure to Big Data technologies (Cloudera) and AWS is a big plus. Must be willing to support off hours (weekends) implementation to Production and Plan Test environments and address issues as needed.
Preference will be given to candidates having hands-on experience with FEPOC and FEPDO systems such as HEDIS, FEDVIP Dental and Vision reports, Truven, IBM Watson Health etc.
Knowledge, Skills and Abilities (KSAs)
Knowledge and understanding of at least one programming language (i.e., Ab Initio, SQL, NoSQL, Python).
Knowledge and understanding of database design and implementation concepts.
Knowledge and understanding of data exchange formats.
Knowledge and understanding of data movement concepts.
Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.
Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.
Must be able to effectively work in a fast-paced environment with frequently changing priorities, deadlines, and workloads that can be variable for long periods of time. Must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. Must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging.

Department
Department: (EIS Technical Delivery)
Equal Employment Opportunity
CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Hire Range Disclaimer
Actual salary will be based on relevant job experience and work history.
Where To Apply
PeopleSoft/Self Service/Recruiting
Federal Disc/Physical Demand
Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
PHYSICAL DEMANDS:
The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
Sponsorship in US
Must be eligible to work in the U.S. without Sponsorship
#LI-KT1",128662,5001 to 10000 Employees,Company - Private,Insurance,Insurance Carriers,1942,$100 to $500 million (USD),VA,81,data engineer,senior,"['sql', 'nosql', 'shell', 'python']",['aws'],[],[],[],[],bachelor,5-10 years
Brillio,3.8,"Austin, TX",AWS Data Engineer – R01525162,"AWS Data Engineer - R01525162
About Brillio:
Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded ‘Great Place to Work’ in 2021 and 2022

Consultant
Primary Skills

Specialization

Job requirements
Job Description: We are looking for an experienced AWS Data Engineer to join our team and help us build and maintain our data infrastructure on AWS. As an AWS Data Engineer, you will work closely with data Architect, Business and Data analysts, BI Developer, Customer Business and engineering team, and other stakeholders to design, implement, and manage data pipelines and systems that support our business needs.

Key Job Responsibilities:
Design, implement, and maintain data pipelines and data processing systems, ETL Infrastructure on AWS using technologies like Apache Spark, AWS Glue, AWS Lambda, and AWS S3 and strong Python and Pandas hands on experience.
Collaborate with data Architect and analysts/BI Developer to understand their data requirements and design data models that meet their needs.
Work with DevOps engineers to ensure that data pipelines are reliable, scalable, and secure.
Monitor and troubleshoot data pipelines to ensure that data is flowing correctly and on time.
Develop automation scripts to streamline deployment, testing, and maintenance of data pipelines and systems.
Document technical designs, standard operating procedures, and best practices for data engineering on AWS.
Keep up to date with emerging technologies and best practices in data engineering and recommend new tools and technologies as appropriate.
Excellent communication and collaboration skills.
Need strong commitment to project, Problem solving, Team Player

Preferred qualifications:
Bachelor’s or master’s degree in computer science, Information Technology, or a related field.
3+ years of experience in data engineering with a focus on AWS technologies.
Strong knowledge of AWS services such as S3, EC2, Glue, Lambda, and Athena.
Experience designing and building data pipelines using Apache Spark.
Experience working with SQL and NoSQL databases.
Familiarity with ETL processes and tools such as Talend or Informatica.
Strong scripting skills in Python or Java.
#LI-CH1
Know what it’s like to work and grow at Brillio: Click here",88650,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2014,$100 to $500 million (USD),TX,9,data engineer,na,"['sql', 'java', 'nosql', 'python']",['aws'],[],[],['spark'],[],bachelor,+10 years
Johnson Controls,3.6,"Milwaukee, WI",PLM Data Engineer (Remote),"Job Details
What you will do
As the PLM Data Engineer, you are a leader within the Engineering IT team. Working under the Engineering Services IT leader, you will partner with peers in the Engineering Services IT organization as well as business and IT leaders globally. You will provide thought leadership on PLM data along with associated technologies and drive successful delivery of IT investments and services. You will develop, manage, and maintain relationships with business and IT leaders across the organization and participate in the development of IT strategies in support of business plans.
In this role you will support the data quality and the data health of the organization. This includes supporting the launch and operationalization of a PLM Data Factory enabling a large multi-year Windchill PLM deployment. You will support the development engineering data standards and data transformation as part of the Engineering Data Factory.
You will participate in the successful planning and execution of all IT related aspects of assigned projects and services as well as contributing to the business and project management aspects. You will coordinate and lead teams consisting of, but not limited to, Global Infrastructure, Enterprise Architecture, Application Delivery, Solution Development, Testing, Performance, Business, and IT Subject-Matter Experts, and including direct and indirect resources.
How you will do it
Serve as a liaison and project leader in partnership between the Business Unit (BU) IT department, Corporate IT department, BU Engineering Services department, Product Business teams, and cross-Product Business functions. Effectively collaborate with teams that span IT and the business.
Express a clear understanding of business structures, hierarchies, products, and services supported. Demonstrate knowledge outside of the technology arena with the Product Business supported.
Partner with other IT leaders, communicate and discuss technology trends, specific changes, and technology roadmaps.
Understand short- and long-term Engineering Services business plans and objectives, IT investments and assets. Participate in development of an IT strategy that enables Engineering Services business strategies. Lead the creation and continuous improvement of a subset of a business capability and IT enablement roadmap. Revise roadmap on at least an annual basis and support investment proposal development.
Support capture, aggregation, and prioritization of a portfolio of IT investments to enable needs as defined on the Engineering Services capability roadmaps. Champion Engineering Services priorities through IT investment portfolio governance process to ensure alignment of projects with business objectives, leverage of assets and practices, and synthesis with broader corporate-wide priorities. Champion understanding, support for, and value of Engineering Services IT investment portfolios.
Lead the ideation and high-level planning of IT projects including specification of high-level business requirements, and formulation of business process and technology solution options. Support creation and vetting of business cases, and high-level project planning (scope, schedule, & budget).
Project responsibilities include analysis, documentation, system design, development of project plans, time and cost estimates, and Capital Appropriation Requests.
Incorporate process standardization and simplification improvements in high-level designs to enable Global Products-wide leverage and streamline the IT applications landscape over time.
Gather requirements, perform analysis, and make recommendations for design of data models and software specifications; create project schedules; provide training; and coordinate implementation and support of developed applications.
Spearhead trade-off analyses and assessments (project timing, technology approaches, requirements prioritization, etc.), support solution design and testing activities, assess overall quality of solution and fit with business capability needs, ensure go-live readiness of Business and IT communities, and measure business results after solution stabilization.
Evaluate emerging technologies or methodologies and develop knowledge in these areas to apply to company projects.
Work closely with and drive accountability and results from internal and external service providers and support teams.
May provide third level support for applications, including working with technical teams, solving issues, routing to IT management, and providing documentation for other team members to use to respond to support issues.
Make decisions and achieve results while being a role model for company values.
Set challenging SMART personal goals that improve both personal skills and team deliverables.
What we look for
Required
Bachelor's Degree in Information Technology, Computer Science, Engineering, or a closely related subject area.
Minimum of 3 years' experience successfully delivering sophisticated Business Technology projects and achieving quantified business outcomes for a related business/industry.
Minimum 3 years' experience in technical data architect role.
Minimum 5 years' experience in product development domain.
Must have a thorough understanding of a broad cross-section of modern Information Technology concepts.
Experience developing solutions with large sophisticated global organizations.
Experience with implementation and roll-out of scalable out-of-the-box solutions as well as custom application development.
A foundational understanding of Agile concepts.
Demonstrated abilit to produce clear, concise, and accurate documentation detailing business processes and requirements.
Position requires knowledge of product development principles, processes, and techniques.
Position requires thorough understanding of Engineering tools including PLM, Informatica tools and PTC Windchill. Working knowledge of PTC Windchill required.
Ability to work collaboratively with a global community including Business Unit and Corporate IT, horizontal and vertical business functions, and end users.
Must have strong ability to communicate effectively with technology, business, and leadership audiences.
Travel varies from 25% to as much as 50% (at peak) including both domestic and international. Typical travel is 1-2 weeks.
Work with global teams, requiring flexibility of working hours.
Proficient English required.
Preferred:
PMI or SAFe certification
BRMi certification.
#DICE",112889,10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,1885,$10+ billion (USD),WI,138,data engineer,na,['go'],[],[],[],[],[],bachelor,5-10 years
Parker Wellbore,3.9,"Houston, TX","IT Data Engineer - (Houston, TX)","Company Description Parker Wellbore helps energy companies accomplish their drilling and production goals efficiently, reliably, and safely. Our global team supports oil and gas operators with innovative land and offshore drilling services, premium rental tools and well services, and advanced operations and management support. Founded in 1934, Parker Wellbore helps customers manage their costs and mitigate their risks, to achieve their operational goals in a safe and efficient manner. With experience in both harsh-environment regions and complex drilling situations, you can trust Parker Wellbore to get the job done.
Job Description

The IT Data Engineer will work closely with the business and IT team to build high-quality data pipelines supporting both our corporate and product divisions and will be key in supporting our current and future reporting and analytics solutions. This role requires a deep understanding of data architecture, data engineering, data analysis, reporting, and a basic understanding of data science techniques and workflows. This role will work with software developers, architects, and data analysts on expanding and optimizing our data and data pipeline architecture, as well as streamlining data flow and collection in support of our cross-functional teams.
Essential functions
Work with business to gather requirements and translate business needs to technical specifications
Builds and maintains Azure data platform including enterprise data lake and data warehouse in alignment with our strategic objectives and organizational goals
Create and maintain optimal data pipeline architecture
Collaborate with Cloud Solution Architects in implementing complex end-to-end Enterprise solutions on Microsoft Azure platform.
Develop policies and implement mechanisms for data ingestion into Azure data platform
Configure, validate, and implement various Azure tools such as but not limited to Databricks, Data Factory, Data Lake, Synapse Analytics, and Data Catalog as appropriate
Works with the Enterprise Information management manager, and enterprise architects to define data architecture and high-level solution design.
Actively collaborates with the business intelligence team, business teams, and project teams to understand data requirements, integration needs, constraints, and performance requirements
Works with Parker’s technology team to understand mathematical models and optimize data solutions accordingly.
Developing and maintaining Data Lake and data warehouse schematics, layouts, architectures, and relational/non-relational databases for data access and Advanced Analytics.

Qualifications

Necessary qualifications, skills and abilities
Bachelor’s Degree in Computer Science, or 3-5 years of equivalent work experience
Analytic Problem-Solving: Approaching high-level challenges with a clear eye on what is important; employing the right approach/methods to make the maximum use of time and human resources.
Effective Communication
Explore new territories to find creative and unusual ways to solve problems.
Data Analysis Knowledge: Understanding how data is collected, analyzed and utilized, Data Ingestion and Orchestration from on premise to Azure, Data Ingestion from Azure Blob Storage to Azure SQL DW
3-5 years of experience in Azure Data Factory, Azure Data Lake Analytics (USQL), Data processing in Azure, Azure DevOps CICD Pipelines, Azure SQL DW, Azure Blob Storage or Azure Data Lake Store, Azure Logic App/Functions, Azure Event Hub/IoT
Data Modeling, architecture and storage experience
3-5 years of experience in Power BI – DAX, SSAS tabular, PySpark/scripting, PowerShell, Stream Analytics
3-5 years of experience in Microsoft traditional data warehousing/BI (SQL Server, SSIS, SSAS, SSRS)
Exposure to Azure Services and big data processing solutions

Position competencies
Initiating & Driving Change • Acts as a catalyst for and takes responsibility for leading, directing, and managing organizational change • Develops new insights into situations and applies innovative solutions • Creates work environment that encourages creative thinking and innovation • Drives step changes in how the company operates • Understands how to change and addresses not only systems and processes, but also cultural aspects of change • Is good at bringing the creative ideas of others to market • Develops a change strategy that includes milestones and timelines • Accurately assesses the potential barriers and resources necessary for change initiatives • Understands and supports the need for change • Envisions and articulates the intended result of the change process • Provides direction and focus during the change process • Helps to generate support of the changes throughout the organization • Identifies and enlists allies who support the change process • Provides resources, removes barriers, and acts as an advocate for those initiating change
Result Focused • Establishes clear, specific performance goals, expectations, and priorities • Can be counted on to exceed goals successfully • Is constantly and consistently one of the top performers • Very bottom-line oriented • Steadfastly pushes self and others for results • Navigates quickly and effectively to resolve problems and obstacles • Persists to complete tasks / responsibilities, even in the face of difficulties • Develops a sense of urgency in others to complete tasks • Operates with personal ownership and looks for ways to improve performance all the time • Challenges him- or herself and others to raise the bar on performance • Focuses people on critical activities that yield a high impact • Holds self and others accountable for delivering high-quality results on time and within budget (e.g., models high work standards and demands the same from others)
Team Work • Blends people into teams when they are needed • Creates strong morale and spirit in his/her team • Shares wins and successes • Fosters open dialogue • Lets people finish and be responsible for their work • Seeks consensus among diverse viewpoints as a means of building group commitment • Defines success in terms of the whole team • Creates a feeling of belonging in the team • Values the contributions of all team members • Creates an environment that encourages open communication amongst team members • Creates an environment that encourages collective problem solving amongst team members
Customer Focus • Is dedicated to meeting the expectations and requirements of internal and external customers • Gets first-hand customer information and uses it to understand customers' business issues and needs for improvements in products and services • Acts with customer in mind • Establishes and maintains effective relationships with customers and gains their trust and respect • Genuinely enjoys working with customers to build long-term partnerships • Creates a sense of customer focus throughout their team/ department/ business unit
Physical demands and work environment
Ability to gather, analyze, and interpret data.
Ability to perform under stress, under pressure, and/or in emergency situations.
Ability to multitask, work in a fast-paced environment, meet deadlines, reason logically, and make sound decisions.
Ability to comprehend, remember, and follow verbal and written directions and comply with Company policies, procedures and standard.
Use repetitive wrist, hand or finger movements at a computer.
Ability to work as a team, communicate and interact with others in a professional manner, and consider alternative and diverse perspectives.

Additional Information

All your information will be kept confidential according to EEO guidelines.
Parker Wellbore provides equal opportunity for all people and will not discriminate on the basis of race, color, religion, sex, gender, sexual orientation, pregnancy, age, marital status, national origin, citizenship status, disability, genetic information, military service, veteran’s status or any other characteristic protected by applicable law.
If an applicant has a disability, the applicant may request accommodations when needed to enable that person to perform their essential job functions or to allow that person to participate in employment.",55584,1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1934,Unknown / Non-Applicable,TX,89,data engineer,na,['sql'],"['databricks', 'azure']","['power bi', 'ssis']",['sql server'],[],[],bachelor,5-10 years
"Press Ganey Associates, Inc.",3.4,Remote,Senior Data Engineer,"About Press Ganey:
Press Ganey , the leading Human Experience (HX) healthcare performance improvement company, offers an integrated suite of solutions that address safety, clinical excellence, patient experience and workforce engagement. The company works with more than 41,000 healthcare facilities in its mission to reduce patient suffering and enhance caregiver resilience to improve the overall safety, quality, and experience of care. Press Ganey is a PG Forsta company.
Position Description:
Our company is looking for a Senior Data Engineer who will be responsible for designing and building scalable and robust data systems. The ideal candidate will be an expert in data engineering technologies, have a strong understanding of data architecture, and be able to work with large and complex data sets. The Senior Data Engineer will also work closely with the data science and analytics teams to ensure data integrity and develop data pipelines.
Duties & Responsibilities:
· Design and build large scale data pipelines to process and analyze large volumes of data.
· Build and maintain efficient data infrastructure, ensuring data quality and consistency.
· Work with the data science and analytics teams to ensure data accuracy and completeness.
· Collaborate with other engineers to build scalable and maintainable data systems.
· Develop and implement data governance and security policies.
· Continuously evaluate and improve data processes to optimize system performance.
· Keep up to date with the latest data engineering technologies and trends.
Technical Skills:
· Expertise in SQL and NoSQL databases, data warehousing, and data modeling.
· Fluency in Python and SQL. Additional data or system languages (e.g. Java, Scala, Go, R) a plus.
· Experience using data pipeline frameworks such as Apache Beam or Apache Spark at scale.
· Experience using data orchestration / automation frameworks such as Airflow, Databricks and MLFlow.
· Hands on experience with one or more of the major cloud providers (GCP, AWS, Azure). Experience with infrastructure-as-code (e.g. Cloud Formation, Terraform) a plus.
· Experience with data visualization tools such as Tableau or Power BI.
Minimum Qualifications:
· Bachelor’s degree in Computer Science, Engineering, or a related field.
· 5+ years of experience in data engineering or related field.
· Excellent problem-solving skills and ability to work independently.
· Strong communication and collaboration skills.
Preferred Qualifications:
· Master’s degree in Computer Science, Engineering, or a related field.
· Experience in machine learning or data science.
All positions at Press Ganey require an applicant who has accepted an offer to undergo a background check. The specific checks are based on the nature of the position. Background checks may include some or all of the following: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, fingerprint verification, credit check, and/or drug test. By applying for a position with Press Ganey, you understand that you will be required to undergo a background check should you be made an offer. You also understand that the offer is contingent upon successful completion of the background check and results consistent with Press Ganey's employment policies. You will be notified during the hiring process which checks are required for the position.
Press Ganey Associates LLC is an Equal Employment Opportunity/Affirmative Action employer and well committed to a diverse workforce. We do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, veteran status, and basis of disability or any other federal, state or local protected class.
Pay Transparency Non-Discrimination Notice – Press Ganey will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information.
The expected base salary for this position ranges from $96,000 - $150,000. It is not typical for offers to be made at or near the top of the range. Salary offers are based on a wide range of factors including relevant skills, training, experience, education, and, where applicable, licensure or certifications obtained. Market and organizational factors are also considered. In addition to base salary and a competitive benefits package, successful candidates are eligible to receive a discretionary bonus or commission tied to achieved results.
#LI-Remote",123000,1001 to 5000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,1985,$500 million to $1 billion (USD),Remote,38,data engineer,senior,"['scala', 'r', 'java', 'nosql', 'sql', 'go', 'python']","['databricks', 'azure', 'aws']","['tableau', 'power bi']",[],['spark'],['terraform'],bachelor,+10 years
Watauga Group,3.6,"Atlanta, GA",Associate Data Engineer,"WHO WE ARE:
At Watauga Group, we leverage two decades of specialized media expertise and our love for the outdoors and entertainment to help outdoor recreation & attraction brands maximize their sales and elevate advertising ROI.Our unique blend of marketplace intelligence and deep insights into the media behaviors and preferences of outdoor participants and attraction visitors enables us to surgically target untapped consumer audiences and connect brands with the greatest number of potential customers at every step of their buying journeys.Watauga’s brand and performance advertising experts navigate today’s complex media landscape to create fully integrated strategies encompassing the most effective mix of digital and traditional media channels, platforms, data, and technologies. Our end-to-end media solutions include Broadcast TV & Radio, OTT & Streaming Audio, Out-of-Home, Digital Display, Paid Search, Paid Social, Sponsorships, and more.
Certified by the WBENC, Watauga is one of the largest women-owned media agencies in North America with offices in Orlando, Atlanta, and Birmingham.
WHY JOIN US:
Generous health benefits package including employer contribution to medical insurance, employer-paid life insurance and disability insurance
Employer match to 401(k) retirement plan
Flexible PTO
Flexible Hybrid Schedule
Paid Parental Leave
Health Savings Account
Tuition Reimbursement
Career progression
Bonus and incentive plans
ABOUT THE JOB:
PRIMARY DUTIES:
The duties and responsibilities of this position include but are not limited to those listed below. These duties and responsibilities may be modified at any time by Management. Modifications will be in writing and will be acknowledged by both parties.
Assist with maintenance of Python/SQL ETL pipelines
Assist with administration of PostgreSQL databases
Assist with design of database architecture to support business analysts
Assist with maintenance of Azure Cloud environment
Create and maintain internal and external dashboards for reporting & data visualization in Power BI
Transform, improve, and integrate data from multiple sources, into accessible, understandable, and usable datasets.
Assist in building and maintaining DataMart tables to optimize BI performance.
Maintain thorough documentation of dashboard data requirements.
Provide quality assurance of imported data.
Assist with pipeline and database development and maintenance.
Work with Digital Media team to ensure the proper data needs are delivered with focus on accuracy and attention to detail.
CORE COMPETENCIES:
Achievement and Results Orientation
Adaptability and Flexibility
Analytical and Strategic Thinking
Attention to Detail with Accuracy
Communication – Written, Oral, Presenting
Learning Support and Continuous Learning
Process Orientation
Problem Solving
Teamwork, Cooperation, and Working with Others
EDUCATION AND EXPERIENCE REQUIREMENTS:
Bachelor’s degree (Computer Science, Data Analytics, Accounting, or Finance a plus)
Strong data analysis skills
Strong data visualization skills
Proficient in Microsoft Excel
Working knowledge of MS Power BI Development, Deployment, and Integration
SQL and Python experience is a plus
Familiarity with CM360 or other tag management systems is a plus
1 year relevant work experience with digital marketing or related industry is a plus
PHYSICAL CRITERIA:
Able to lift and carry 20 pounds
Able to sit for prolonged periods of time at a computer
Industry
Marketing & Advertising
Employment Type
Full-time",74258,1 to 50 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2004,$5 to $25 million (USD),GA,19,data engineer,na,"['sql', 'python']",['azure'],"['power bi', 'excel']",['postgresql'],[],[],bachelor,0-2 years
Parkland Health and Hospital System,3.7,"Dallas, TX",Data Engineer - PCHP,"Interested in a career with both meaning and growth? Whether your abilities are in direct patient care or one of the many other areas of healthcare administration and support, everyone at Parkland works together to fulfill our mission: the health and well-being of individuals and communities entrusted to our care. By joining Parkland, you become part of a diverse healthcare legacy that’s served our community for more than 125 years. Put your skills to work with us, seek opportunities to learn and join a talented team where patient care is more than a job. It’s our passion.

Primary Purpose
The Parkland Community Health Plan’s (PCHP’s) Data Engineer is responsible for maintaining the data systems including business intelligence, ETL, and supporting backup strategies to provide PCHP with secure, dependable, and accurate data including data transfer, data integrity, and data storage responsibilities. The Data Engineer will collaborate with Database Administrators, server team, storage team, and other teams to plan maintenance activities and with PHCP’s analytics team for report or universe deployments. The Data Engineer will also be involved in dashboard and report development activities.

Minimum Specifications

Education
Bachelor’s degree in computer science, management information systems, information technology, statistics, mathematics, or related discipline.

Experience
Seven years of experience in maintaining business intelligence, data warehouse solutions, or ETL in a Run or Production environment.
Six years of experience troubleshooting ETL load related issues (SSIS or Data Solutions).
Six years of experience with ETL development and maintenance experience in a data warehouse environment.
Experience with systems engineering (hardware / software) capacity.
Experience with database or report portal tool administration is preferred.
Experience at a healthcare or managed care organization is preferred.

Certification/Registration/Licensure
System Administration or Reporting Tool Administrative Certification is preferred. (i.e., Epic Cogito or Clarity, SAP Business Objects, Tibco Composite, Microsoft Certified Solutions Engineer (MCSE), Oracle Certified Professional (OCP), etc.)
PMP or other project management certificate or training is preferred.

Skills or Special Abilities
Proficiency with ETL tool Build or Run activities.
Ability to create reports and/or build virtual data environments.
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
Proficiency with Microsoft Office Excel, Word and Outlook is required; Access and PowerPoint are preferred.
Demonstrated critical thinking and troubleshooting skills accompanied by a high level of detail.
Demonstrated ability to plan and manage multiple processes and projects simultaneously.
High level of attention to detail.
Strong verbal and written communication skills.
Demonstrated ability to collaborate effectively and work as part of a team.
Independent worker and self-starter, having the ability to provide internal motivation and drive.
Proficiency with server or application patching, backups, scripting is preferred.
Understanding of SSIS and Apache NiFi is preferred.
Proficiency with Business Objects Administration is preferred.

Responsibilities
Implements and maintains high-value business intelligence environments.
Maintains the data systems including business intelligence, ETL, and supporting backup strategies.
Has a strong understanding of all the tools within the environment, regardless of vendor, and quickly and efficiently triages, troubleshoots, and restores services during outages or service degradation.
Responsible for being on-call for Business Intelligence and ETL cycles.
Works with the Database Analyst and storage teams to ensure proper backups are taken, test back-ups periodically, and ensures that the system can be restored in the time of a disaster.
Proactively identifies areas for improvement in our Business Intelligence environment.
Documents all routine processes and cross-trains other team members.
Improves function, speed, and accuracy of data distribution methods.
Develops automated reports and dashboards.

Job Accountabilities
Identifies ways to improve work processes and improve customer satisfaction. Makes recommendations to supervisor, implements, and monitors results as appropriate in support of the overall goals of PCHP.
Stays abreast of the latest developments, advancements, and trends in the field by attending seminars/workshops, reading professional journals, actively participating in professional organizations, and/or maintaining certification or licensure. Integrates knowledge gained into current work practices.
Maintains knowledge of applicable rules, regulations, policies, laws, and guidelines that impact the area. Develops effective internal controls designed to promote adherence with applicable laws, accreditation agency requirements, and customer requirements. Seeks advice and guidance as needed to ensure proper understanding.

Parkland Health and Hospital System prohibits discrimination based on age (40 or over), race, color, religion, sex (including pregnancy), sexual orientation, gender identity, gender expression, genetic information, disability, national origin, marital status, political belief, or veteran status. As part of our commitment to our patients and employees’ wellness, Parkland Health is a tobacco and smoke-free campus.",97618,5001 to 10000 Employees,Hospital,Healthcare,Health Care Services & Hospitals,1894,$500 million to $1 billion (USD),TX,129,data engineer,na,[],[],"['sap', 'excel', 'ssis']",['oracle'],[],[],bachelor,
Atos,3.7,"Troy, MI",DATA ENGINEER,"Publication Date:
May 10, 2023

Ref. No:
480012

Location:
Troy, MI, US, 48083

The future is our choice

At Atos, as the global leader in secure and decarbonized digital, our purpose is to help design the future of the information space. Together we bring the diversity of our people’s skills and backgrounds to make the right choices with our clients, for our company and for our own futures.

Position : Cloud Data Engineer

Location : Westlake Village, CA

Job Description

Design and Build scalable, secure, low latency, resilient and cost-effective solutions for enabling predictive and prescriptive analytics across the organization
Design/ Architect frameworks to Operationalize ML models through serverless architecture and support unsupervised continuous training models
Take over and scale our data models (Tableau, Dynamo DB, Kibana)
Experience in shipping low-latency massive scale systems to production
Communicate data-backed findings to a diverse constituency of internal and external stakeholders
Build frameworks for data ingestion pipeline both real time and batch using best practices in data modeling, ETL/ELT processes and hand off to data engineers
Participate in technical decisions and collaborate with talented peers.
Review code, implementations and give meaningful feedback that helps others build better solutions.
Helps drive technology direction and choices of technologies by making recommendations based on experience and research. MINIMUM
REQUIREMENTS & SPECIAL ATTRIBUTES

5 or more years of experience working directly with enterprise data solutions
Hands on experience working in a public cloud environment and on-prem infrastructure.
Specialty on Columnar Databases like Redshift Spectrum, Time Series data stores like Apache Pinot and the AWS cloud infrastructure
Experience with in-memory, serverless, streaming technologies and orchestration tools such as Docker, Spark, Kafka, Airflow, Kubernetes is needed
Excellent SQL skills and Python coding is a must
Current hands-on implementation experience required, possessing 5or more years of IT platform implementation experience.
AWS Certified Big Data - Specialty desirable
Experience designing and implementing AWS big data and analytics solutions in large digital and retail environments is desirable
Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases, data lakes, and schemas.
Experience with AWS Cloud Data Lake Technologies and operational experience of Kinesis/Kafka, S3, Glue and Athena.
Experience with any of the message / file formats: Parquet, Avro, ORC
Design and development experience on subscribing to a Streaming Service, EMS, MQ, Java, XSD, File Adapter, and ESB based applications
Experience in distributed architectures such as Microservices, SOA, RESTful APIs and data integration architectures is a plus
Hands on experience migrating On-Prem Data solutions to cloud
Prior experience managing On-prem Enterprise Data Warehouse solutions like Netezza is a plus
Experience with a wide variety of modern data processing technologies, including
Big Data Stack (Spark, spectrum, Flume, Kafka, Kinesis etc.)
Data streaming (Kafka, SQS/SNS queuing, etc)
Expert in Columnar databases primarily, Redshift or like technologies lile Snowflake, Firebolt
Expert in Commonly used AWS services (S3, Lambda, Redshift, Glue, EC2, etc)
Expertise in Python, pySpark or similar programming languages is a must have
BI tools (Tableau, Domo, MicroStrategy) is a plus
Skilled in AWS Compute such as EC2, Lambda, Beanstalk, or ECS
Skilled in AWS Management and Governance suite of products such as CloudTrail, CloudWatch, or Systems Manager
Skilled in Amazon Web Services (AWS) offerings, development, and networking platforms
Skilled in AWS Analytics such as Athena, EMR, or Glue
Proficiency in Oracle, MYSQL and Microsoft SQL Server Databases is a plus
Understanding Continuous Integration / Continuous Delivery with experience in Jenkins
Here at Atos, diversity and inclusion are embedded in our DNA. Read more about our commitment to a fair work environment for all.

Atos is a recognized leader in its industry across Environment, Social and Governance (ESG) criteria. Find out more on our CSR commitment.

Choose your future. Choose Atos.

Nearest Major Market: Troy

Nearest Secondary Market: Detroit",90916,10000+ Employees,Company - Public,Information Technology,Information Technology Support Services,1997,$10+ billion (USD),MI,26,data engineer,na,"['sql', 'java', 'python']","['snowflake', 'aws', 'redshift']",['tableau'],"['sql server', 'snowflake', 'oracle', 'mysql']","['kafka', 'spark']",['docker'],,
HORNE LLP,3.5,"Ridgeland, MS",Data Engineer,"The Data Engineer will develop, optimize, and support Horne’s data integrations, APIs and ETL processes. The Data Engineer should know how to examine new data system requirements and implement processes. The ideal candidate will also have proven experience in cloud data management, with excellent analytical and problem-solving abilities.
Responsibilities:
Creates data pipelines, big data platforms and data integrations in databases, data warehouses and data lakes, and works with various cloud and on-premises technologies.

Gathers and analyzes data from databases and other source systems, runs machine learning algorithms and predictive models, creates data visualizations for business users.

Extracts data from business systems, cleanses, analyzes, and creates reports and dashboards to highlight trends and other business information for end users.

Create complex functions, scripts, stored procedures, and triggers to support application development.

Work on multiple, small to large projects as a team member, or independently on small projects.

Troubleshoot applications and provide corrective measures.

Monitor the system performance by performing regular tests, troubleshooting, and integrating new features.

Offer support by responding to system problems in a timely manner.

Requirements:
Bachelor’s degree in computer science, Computer Engineering, or relevant field.

Strong knowledge of relational databases, data warehouses.

Strong knowledge in developing complex SQL scripts.

Proficiency in Python and/or Scala, machine learning.

Experience in data visualization, and data analytics.

Knowledge of data integration and ETL tools Informatica IICS is a preferred skill.

Experience with Business Intelligence tools such as Tableau, Power BI is a preferred skill.

Experience with cloud computing environments, particularly Microsoft Azure, is a preferred skill.

Knowledge of big data computing on Spark framework.

Some knowledge of data modeling

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)",89047,501 to 1000 Employees,Company - Private,Financial Services,Accounting & Tax,1962,$25 to $100 million (USD),MS,61,data engineer,na,"['sql', 'scala', 'python']",['azure'],"['tableau', 'power bi']",[],['spark'],[],bachelor,
Hollstadt & Associates,4.5,Minnesota,Senior Data Engineer,"Minnesota - Developer
Hollstadt Overview
Hollstadt Consulting is a management and technology consulting firm dedicated to placing professionals at engagements where they will excel. When you work with us, you'll work with a refreshingly real company led and staffed by seasoned experts who are also down-to-earth, good people. We're committed to treating you with respect and helping you achieve your career aspirations.

Since 1990, Hollstadt has been a trusted partner to more than 150 domestic and global companies and has successfully completed over 2,000 projects. Our continued growth has created challenging and rewarding opportunities for accomplished IT and Business Consultants. Hollstadt Consulting is an equal opportunity employer including disability/veteran.

Job Description

The Senior Data Engineer will oversee the department's data integration work, including developing data models, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. This role will work closely and collaboratively with members of other areas to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of the analytics needs.

Requirements
Bachelor’s Degree in Computer Science or Management Information Systems (MIS) or Business, Finance or Accounting with an emphasis in MIS
Minimum 5 years experience of developing and supporting enterprise level data warehouse systems
Strong knowledge of relational databases and SQL. Extract, Transform, and Load (ETL) data into a relational database
General data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets together, reformat data between wide and long, etc.
Demonstrated ability to learn new techniques and troubleshoot code without support, ex. find answers to common programming challenges
Strong knowledge of T-SQL language as evidenced by ability to write complex SQL queries, Microsoft SQL Management Studio, SQL Analysis Services and SQL Server Integration Services
Demonstrated ability to work independently and be a self-starter
Demonstrated ability to work effectively in teams, in both a lead and support role
Strong analytical and problem-solving skills with the ability to work independently with minimal supervision
Cloud knowledge or exposure
Experience working with cloud infrastructure services like Amazon Web Services and Google Cloud
Agile LeSS (seems like this would make the candidate stand out and they'd be very interested in interviewing)
Mentorship experience
Preferred
Experience working with Data Vault 2.0
Experience with advanced data visualization and mapping




Benefits + Perks


Comprehensive Benefit Plan
Hollstadt offers a competitive and comprehensive benefit package which includes Medical, Dental, Vision, Long Term/Short Term Disability, and Life Insurance. With three different medical plans to choose from, you can enroll in the coverage you need from single to family, or anywhere in between!

Remarketing Process
Hollstadt is based on retention and relationships. We get to know your strengths and career wishes throughout your assignment and then start remarket discussions 6-8 weeks prior to your end date. By being proactive, we are able to keep your down time between assignments as short as possible, unless you choose otherwise.

Professional Development
Hollstadt offers free bi-weekly training courses for our consultants as well as on-demand access to past sessions through our consultant portal. Trainings give our consultants the continuing education they need to excel on their projects.

401k + Matching
One popular benefit is our 401(k) match on the first 4% of your contributions. Hollstadt wants to help you reach your long-term financial goals and understands that planning for your future is critical. Consultants also have access to support from a Financial Advisor.

Bonus Opportunities
We appreciate and reward loyalty. Join Hollstadt, stay for 5 years, and we’ll give you a $5,000 Longevity Award bonus! Additionally, we know great talent knows other great talent. If you are on contract with Hollstadt and refer one of your connections who gets placed, we’ll pay you $1,000!

Ongoing Support & Networking
We have made a significant investment in building a support program for our consultant team - so you never have to feel like you are going it alone. We also have a Consultant Coach program which acts like a 'work buddy' to provide a safe ear for questions or concerns at your client site.",112889,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1990,$25 to $100 million (USD),Minnesota,33,data engineer,senior,['sql'],['google cloud'],['excel'],['sql server'],[],[],bachelor,5-10 years
Syntelligent Analytic Solutions,3.9,"Washington, DC",Data Engineer,"Syntelligent Analytic Solutions, LLC provides uniquely qualified personnel with the expertise and tools needed to fulfill our customers’ management and technical requirements in the intelligence, defense, homeland security and commercial market space.
Our customers’ and Syntelligent’s success are built upon the core values of People First, Integrity & Accountability, Mission Driven, Community Focus and Team Oriented.
Syntelligent is seeking a Data Engineer with an active TS/SCI clearance for upcoming work with a federal client in Washington, DC.
Description
Syntelligent is seeking a data engineer to work in a variety of settings to develop and design data pipelines to support end-to-end solutions. Candidate will build systems that collect, manage, and convert raw data into usable information for data scientists and business analysts to interpret.
Duties
Develop and design data pipelines to support an end-to-end solution.
Develop and maintain artifacts i.e., schemas, data dictionaries, and transforms related to ETL processes.
Integrate data pipelines with AWS cloud services to extract meaningful insights.
Manage production data within multiple datasets ensuring fault tolerance and redundancy.
Design and develop robust and functional dataflows to support raw data and expected data.
Provide Tier 3 technical support for deployed applications and dataflows.
Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.
Skills
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred
Database administration and development experience will be a plus for consideration.
Experience with cloud message APIs and usage of push notifications.
Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security.
Qualifications:
Active TS/SCI clearance ONLY
Bachelor’s degree
Three (3) to five (5) years of relevant experience.
Clearance level required: TS/SCI
Location:
For any unclassified development efforts, remote work is available. Position does require client site work at DHS HQs on Nebraska Ave NW, DC
Online applications, please.
The salary ranges for these positions will be set based on experience, geographic location and possibly contractual requirements and could fall outside of this range. Other rewards may include annual bonuses, Spot bonuses, and program-specific awards. In addition, Syntelligent provides a variety of benefits to all our Full-Time employees.
When we review candidates' information, we are looking for the best matches for the position based on the qualifications listed in the job posting. If your skills and experience appear to match an open position, a recruitment services professional or a hiring manager may contact you.
Syntelligent Analytic Solutions, LLC is an Equal Employment Opportunity and Affirmative Action employer. It is the policy of the company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation and gender identity or expression, national origin or protected veteran status and will not be discriminated against on the basis of disability. If you are a qualified disabled veteran or individual with a disability and need reasonable accommodation to use or access our online system, please contact our Human Resources department at 540-736-4570, Extension #2",100358,1 to 50 Employees,Company - Public,Telecommunications,"Cable, Internet & Telephone Providers",-1,Unknown / Non-Applicable,DC,-1,data engineer,na,[],['aws'],[],[],[],[],bachelor,
WEX Inc.,3.7,Remote,Data Engineer,"We are seeking a Data Engineer to play a critical role in the development of WEX's enterprise data & analytics capabilities. You will be part of an organization focusing on the development and delivery of data solutions and capabilities for WEX’s data platform. The successful candidate is motivated by thinking big data, technically proficient, and enjoys working in a fast paced environment.
The base pay range represents the anticipated low and high end of the pay range for this position. Actual pay rates will vary and will be based on various factors, such as your qualifications, skills, competencies, and proficiency for the role. Base pay is one component of WEX's total compensation package. Most sales positions are eligible for commission under the terms of an applicable plan. Non-sales roles are typically eligible for a quarterly or annual bonus based on their role and applicable plan. WEX's comprehensive and market competitive benefits are designed to support your personal and professional well-being. Benefits include health, dental and vision insurances, retirement savings plan, paid time off, health savings account, flexible spending accounts, life insurance, disability insurance, tuition reimbursement, and more. For more information, check out the ""About Us"" section.
Salary Pay Range: $80,500.00 - $108,000.00",94250,1001 to 5000 Employees,Company - Public,Financial Services,Investment & Asset Management,1983,$1 to $5 billion (USD),Remote,40,data engineer,na,[],[],[],[],[],[],,
TekValue IT Solutions,4.0,"Houston, TX",Data Engineer with PL/SQL,"Data Engineer
Day 1 Onsite
Loc Houston Texas
Required Skills:
Requirements:
Recent PL/SQL development work experience with Oracle 19.
Must have working experience with tools like Airflow, Git.
Must have python coding experience and REST API development experience.
Plus would be experience with microservices, docker, Kubernetes.
Plus would be experience with accounting and financial data
Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.
Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL.
Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations. Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts.
Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
2+ years’ experience in the design, implementation and support of full life-cycle data engineering projects.
Bachelor's degree in Computer Science, Engineering or other equivalent degree, with 5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree with 2+ years data engineering preferred.
High school diploma or GED is required.
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Houston, TX 77003: Reliably commute or planning to relocate before starting work (Required)
Experience:
PL/SQL: 9 years (Preferred)
Python: 8 years (Preferred)
Database development: 8 years (Preferred)
Work Location: One location",130500,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python']",[],[],"['mongodb', 'elasticsearch', 'oracle']",[],['docker'],bachelor,5-10 years
OM Group Inc.,3.5,Remote,Senior Data Engineer,"Data Engineer
OM Group, Inc. is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal & DoD clients. We are currently expanding support for the Enterprise Data Warehouse to continue and evolve on-prem/cloud/hybrid data migration and enterprise reporting platforms.
We are hiring a Data Engineer with experience creating and maintaining complex shell and SQL scripts used to expand and populate data into the enterprise data warehouse. The successful candidate will be responsible for managing and maintaining the Extract, Transform and Load (ETL) processes to populate the Enterprise Data Warehouse/ Enterprise Virtual Viewer (EDW/EVV). The EDW/EVV is powered by a multi-tier environment encapsulation of Windows/Unix and Linux Environments. The primary data repository, as well as host to the virtualization/governance and catalog layers live on the IBM Cloud Pak for Data System (CP4D) and Netezza Performance Server (NPS) PostgreSQL based database. The CP4D system is Linux-based on a combined OpenShift and Redhat 7 platform. The EDW/EVV also utilizes SAP Business Objects (BOBJ) to do reporting based on a Windows Virtual Machine (VM) architecture. The Oracle 19c database is on Oracle Solaris VMs and acts as repository databases for SAP BOBJ.
This position is remote on Eastern Time zone schedule.
Responsibilities
Work with stakeholders to evaluate business needs and develop tasks to meet requirements and objectives
Provide quality assurance and identify bugs or required fixes and communicate to respective teams. Ensure data being migrated to the EDW/EVV production environment is documented
Enforce EDW/EVV standards / policies and provide quality assurance for universe, star schema/build, report, and dashboard migrations from the EDW/EVV development environment into the EDW/EVV production environment (see Appendix A for migration metrics)
Maintain existing Unix Shell and Structured Query Language (SQL) script based ETL processes. Design, Develop, document, and maintain new and/or expanding script based ETL processes e.g., Unix Shell and SQL scripts
Coordinate, test, implement, document, and manage required upgrades and capability enhancements for the EDW/EVV. Perform tasks for data repository expansion as more data content is transitioned to the warehouse
Work with infrastructure team to ensure that all the required monitoring, exception handling and fault tolerance is in place for a production-quality data platform
Team up with analysts, product managers, and other stakeholders to understand evolving business needs and translate reporting capabilities accordingly
Assist with process improvement with a customer-focused, progressive mindset
Understand data classification and adhere to the information protection and privacy restrictions
Troubleshoot and provide technical support for staff and back-end system users
Document and support code migrations and provide quality assurance / control of EDW/EVV star schemas/builds, universes, local data, and reports

Requirements
Active DoD Secret Security clearance
Hold DoD IAT-III, IAM-II and IASAE-II certifications and Data related industry certifications
5+ years experience with Linux Shell Script development, testing, debugging and deployment
5+ years experience with SQL, relational database and data warehouse technologies
3+ years experience developing and maintaining Python or similar scripting language
3+ years experience in multiple subversion technologies such as Subversion, GitHub or Tortoise
Knowledge of AWS technologies such as S3, EC2, Redshift, Glue, Athena and Step Functions preferred
Experience in data mining and development of ETL processes, distributed data architectures and big data processing technologies
Knowledge of production / environment control
Knowledge of Agile software development lifecycle
Excellent problem-solving skills and attention to detail
Strong documentation and training skills
OM Group, Inc. recently voted one of the top workplaces in the Washington, DC area, is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal & DoD clients. We are a growing company that values your skills, training, and ideas and strives to foster a welcoming, diverse, and inclusive environment. OM Group provides competitive compensation and benefits including health insurance coverage, 401(k), paid time off, as well as support for continuous education and training.
OM Group, Inc. is an equal opportunity employer (EEO) and does not discriminate on the basis of race, color, religion, sex, national origin, age, disability, veteran status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive workplace where all employees are treated with respect and dignity",112889,1 to 50 Employees,Company - Public,Management & Consulting,Business Consulting,2001,Unknown / Non-Applicable,Remote,22,data engineer,senior,"['sql', 'shell', 'python']","['azure', 'aws', 'redshift']",['sap'],"['postgresql', 'oracle']",[],[],,+10 years
vebyond corp,4.0,"Charlotte, NC",AWS Data Engineer,"AWS Data Engineer Lead, Core Technical Skills
5+ years of AWS experience
Experience in a regulated environment
AWS services - S3, EMR, Glue Jobs, Lambda, Athena, CloudTrail, SNS, SQS, CloudWatch, Step Functions
Experience with Kafka/Messaging preferably Confluent Kafka
Experience with databases such as DocumentDB, MySQL, Postgres, Glue Catalog, Lake Formation, Redshift, DynamoDB and Aurora and SQL
Tools and Languages – Python, Spark, PySpark
Experience with Secrets Management Platform like Vault and AWS Secrets manager
Experience with Event Driven Architecture
Experience with Rest APIs and API gateway
Experience with AWS workflow orchestration tool like Airflow or Step Functions
AWS Data Engineer Lead Additional Technical Skills (nice to have, but not required for the role)
Experience with native AWS technologies for data and analytics such as Kinesis, OpenSearch
Databases - Document DB, MongoDB Atlas
Data Lake platform (Hive, Druid, Apachi Hudi/Apache Iceberg/Databricks Delta)
Java, Scala, Node JS, Pandas
Workflow Automation
Experience transitioning on premise big data platforms into cloud-based platforms such as AWS
Strong Background in Kubernetes, Distributed Systems, Microservice architecture and containers
Day to Day Responsibilities/project specifics:
Provides technical direction, guides the team on key technical aspects and responsible for product tech delivery
Lead the Design, Build, Test and Deployment of components
i. Where applicable in collaboration with Lead Developers (Data Engineer, Software Engineer, Data Scientist, Technical Test Lead)
Understand requirements / use case to outline technical scope and lead delivery of technical solution
Confirm required developers and skillsets specific to product
Provides leadership, direction, peer review and accountability to developers on the product (key responsibility)
Works closely with the Product Owner to align on delivery goals and timing
Assists Product Owner with prioritizing and managing team backlog
Collaborates with Data and Solution architects on key technical decisions
i. The architecture and design to deliver the requirements and functionality
Mentor other developers in development of components and related processes
Job Type: Full-time
Salary: $80.00 - $85.00 per hour
Experience level:
10 years
11+ years
9 years
Schedule:
8 hour shift
Experience:
AWS Data Engineer: 5 years (Preferred)
Work Location: On the road",148500,,,,,-1,,NC,-1,data engineer,na,"['sql', 'java', 'scala', 'python']","['databricks', 'aws', 'redshift']",[],"['dynamodb', 'mongodb', 'hive', 'mysql']","['kafka', 'spark']",[],,5-10 years
Wallero,4.4,"Seattle, WA",Data Engineer,"Hi,
Pretty much interested in your resume, we have an urgent opening for Data Engineer role with one of our clients based in Seattle WA.
Position: Data Engineer
Location: Seattle WA - Remote (100%)
Description:
Good Experience in reporting space including PowerBI, ADF, and possibly standing up a data warehouse/data mart.
The main scope centering is around reporting.
Experience with SQL, and agile framework including requirement gathering and story/feature writing skills.
Need someone strong who is comfortable leading and guiding teams in that capacity.
Migrating data from third party to Azure Cloud
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",135000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,$1 to $5 million (USD),WA,8,data engineer,na,['sql'],['azure'],[],[],[],[],,0-2 years
Codinix Technologies,5.0,"Boston, MA",Cloud Data Engineer - AWS,"Only US Citizen
Strong in warehouse technologies (Hadoop, Spark, AWS, etc.)
Bachelor's degree in Computer Science, Information Systems, or another related field
5+ years of data engineering, and cloud engineering experience
Experience in Agile project methodologies and DevOps
Experience with AWS (certification preferred; including Athena, Redshift, Glue), Cloudera
Experience building a CI/CD pipeline to deploy applications on AWS (GitLab, Terraform)
Advanced experience programming and scripting on Unix/Linux (i.e. Python, Bash)
Experience with CTRL-M, Cron (other) scheduling of batch jobs
Experience migrating legacy systems to cloud-ready architectures
Job Type: Contract
Pay: Up to $75.00 per hour
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",135000,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MA,-1,data engineer,na,"['sql', 'python']","['aws', 'redshift']",[],[],"['spark', 'hadoop']","['gitlab', 'terraform', 'bash']",bachelor,0-2 years
Synovize,5.0,Remote,Big Data Engineer,"Synovize is a cutting-edge technology company that specializes in providing advanced data analytics and intelligence solutions. Our mission is to empower businesses with the tools and insights they need to make data-driven decisions and drive innovation. We are committed to delivering exceptional results by combining our expertise in big data technologies with our passion for creating impactful solutions.
As a Big Data Engineer at Synovize, you will have the opportunity to work on exciting projects that involve designing and implementing scalable data solutions. You will collaborate with a talented team of professionals, including software developers, data scientists, and domain experts, to tackle complex data engineering challenges and deliver high-quality solutions to our clients.
Job Title:
Big Data Engineer
Company: Synovize
Number of Positions: 2
Location: Remote
Length: 12 Months
Tax Terms: 1099/C2C or W-2
Rate/Range: $70-$90/hr
Work Authorization: US Citizen
Security Clearance: None
Skills Needed:
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS (Amazon Web Services) and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Responsibilities:
Design and develop scalable data pipelines to process, integrate, and analyze large volumes of structured and unstructured data.
Implement data ingestion, transformation, and storage mechanisms using big data technologies and frameworks.
Collaborate with data scientists and analysts to understand data requirements and optimize data models for analytics and machine learning.
Utilize AWS services and tools to build and maintain data infrastructure, ensuring data quality, integrity, and security.
Troubleshoot and resolve data-related issues, optimizing performance and efficiency.
Stay up-to-date with emerging trends and advancements in big data technologies, recommending and implementing improvements to existing systems and processes.
Requirements:
Bachelor's or master's degree in computer science, engineering, or a related field.
Proven experience as a Big Data Engineer or similar role, with a strong understanding of big data technologies and architectures.
Proficiency in Java and Kotlin programming languages.
Strong knowledge of AWS and its big data offerings.
Experience with BigData technologies, frameworks, and tools.
Familiarity with Kafka for streaming data processing.
Proficiency in Python programming language.
Strong problem-solving skills and ability to work in a fast-paced, collaborative environment.
Excellent communication and interpersonal skills.
Join Synovize and be part of a dynamic team that is at the forefront of data innovation. Together, we will drive meaningful change and unlock the full potential of data-driven decision-making.
Job Types: Full-time, Contract
Pay: $70.00 - $90.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Schedule:
10 hour shift
4 hour shift
8 hour shift
Choose your own hours
Application Question(s):
Are you a US Citizen?
Years of experience in Java?
Years of experience in Kotlin?
Years of experience in big data?
Years of experience in AWS?
Security Clearance
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",144000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'java', 'python']",['aws'],[],[],['kafka'],[],bachelor,0-2 years
Maven Workforce,4.1,"McLean, VA",Data Engineer,"MUST HAVES: Hands-on experience with Informatica PowerCenter and/or Informatica Intelligent Cloud Services (IICS), Cloud Data Integration (CDI) , Cloud AWS,UNIX scripting, and any other ETL tools.
Job responsibilities: –
The candidates will be having the IICS module of Informatica more than any other platform modules.
Strong hands-on experience building ETL pipelines and Data Replication
3+ yrs of experience with IICS is required.
Must be hand Ons with AWS services like ECS,S3,EKS etc.
Cloud Data Integration is also required.
Data Engineering Background experience with Spark, Hive is also required.
Preferred:
SQL is preferred, but basic knowledge is required
Job Type: Contract
Salary: $60.00 per hour
Ability to commute/relocate:
Mclean, VA 22101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica PowerCenter: 7 years (Required)
Informatica Intelligent Cloud Services (IICS): 7 years (Required)
Cloud Data Integration (CDI): 7 years (Required)
UNIX scripting: 7 years (Required)
ETL tools: 7 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",108000,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable,VA,15,data engineer,na,['sql'],['aws'],[],['hive'],['spark'],[],,5-10 years
Kaizen Analytix,3.9,"Dallas, TX",Cloud Data Engineer: AWS,"Cloud Data Engineer: AWS
Kaizen Analytix LLC, an analytics services company, is seeking a qualified Cloud Data Engineer Contractor to support data science or data analytics projects. Responsibilities, capabilities for success, and job requirements are below. This role requires an estimated 40 hours/week commitment for approximately 24 weeks.
Responsibilities:
Candidate will be responsible for supporting a data science or data analytics project.
Work with client IT to understand access and use of source-of-truth data systems and downstream data sources.
Work with data scientist or visualization experts to determine data requirements such as structure, latency, availability.
Work with IT Security to assess risk of using sensitive data and how these data can be used throughout the project life cycle.
Develop an as-is and to-be state for data architecture.
Capabilities for Success:
In addition to key responsibilities, Candidate should demonstrate an ability to…
Work with the project manager and the business stakeholders to define upstream requirements for data.
Contribute to estimating input and time required for data engineering development tasks.
Contribute to client demonstrations of solution or presentations on architecture.
Proactively raise risks and issues to internal project manager
Work with data scientist or analysts to proactively assist in data concerns
Job Requirements:
Education: Bachelor's degree required
Extensive Experience in Data Engineering – consuming, wrangling, validating, developing pipelines for data.
Extensive Experience with AWS
Must have Solutions Architect Certification
Strong knowledge of databases, ETL concepts, SQL, and NoSQL sources.
Experience in: Python (data packages such as Pandas, PySpark, SQL Alchemy, pydantic, etc.) and SQL.
Position Type: 1099 Contractor",103500,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python', 'nosql']",['aws'],[],[],[],[],bachelor,
Kanini,4.0,Remote,Data Engineer,"About Kanini
Kanini provides Agile Software Development, Cloud Computing, Data Science, and Location Intelligence services to public and private organizations. We have successfully served our clients in government, finance, transportation, utility, and software industries since 2003.
Why you should join
Working at Kanini is flexible and personal. We are a highly motivated, collaborative team experimenting with the latest technologies. We are committed to everyone having a healthy work/life balance, and we provide extensive mentorship and training resources to help you succeed.
Kanini is looking for a Big Data Engineer who has a deep experience in Data Engineering, AWS, Python, Data Lakes.
Required Skills
At least 10 years software development experience
At least 5 years leading at least one Scrum team of data engineers building data-intensive products with a modern tech stack.
Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies
Significant experience with a general-purpose programming language such as Python, Scala, or Java
Experience with Spark framework and related tools (PySpark, Scala, SparkR, Spark SQL, Spark UI)
Experience with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3
Experience with data visualization development using Python, Tableau, or PowerBI
Experience with Azure, AWS or GCP
Solid understanding of performance tuning concepts for relational and distributed database systems
Familiarity with distributed programming, big data concepts, and cloud computing
Education Qualifications
Bachelor’s degree in computer science/Engineering or Technology related field or possess equivalent work experience.
Preferred Qualifications
Cloud certifications from Azure, AWS or GCP
Big data, data engineering or data science certifications from recognized vendors such as Databricks & Cloudera
Kanini Software Solutions, Inc. does not discriminate in employment matters based on race, gender, religion, age, national origin, citizenship, veteran status, family status, disability status, or any other protected class. We support workplace diversity. If you have a disability, please let us know if there is anything we can do to improve the interview process for you; we’re happy to accommodate.
Kanini Software Solutions, Inc., 25 Century Blvd., Ste. 602, Nashville, TN 37214
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Work Location: Remote",117000,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'java', 'scala', 'python']","['databricks', 'azure', 'aws']",['tableau'],['hive'],"['hadoop', 'spark']",[],bachelor,5-10 years
Provista Software Corporation,4.0,Remote,Data Engineer,"As a successful candidate, you’ll play a pivotal role in the data conversion and business intelligence solution efforts during project implementation for our state and local clients. We are looking for you to join our software implementation team as a Senior Data Engineer.
Qualification:
5+ years of experience working independently as ETL developer on data migration projects that covered diverse database platforms and complex data conversions.
Must have experience across leading Database Platforms especially Oracle, Azure SQL, Snowflake.
Expert level experience with Extraction Transform Load (ETL) plan, protocols, and tools.
Expert level experience with programming languages SQL, Phyton, Java, C#.
Good at communication with clients and internally.
Expert at problem solving and troubleshooting skills.
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
Must be a U.S. Citizen or Green Card Holder.
Functional Responsibility:
Data Conversion -
In collaboration with Data Stewards, discover inventory of legacy data sources and all data the client expects to be migrated to destination Modified Off the Shelf (MOTS) solution.
Complete gap assessment between legacy data and MOTS data requirements, and options to address the gaps.
Develop data migration map to minimally include data dictionary, detailed data map of all elements of the current database, data transformation rules, relationship rules, validation rules, process of migrating images, and process for recovering missing or erroneous data.
Identify issues, risks, and/or barriers that may interfere with data migration and recommendations to resolve or mitigate.
Conduct testing and quality assurance to ensure the data is migrated accurately.
Migrate data from legacy to destination MOTS solution progressively using a sample set of data and the entirety of the data prior to deployment into relevant go-live environments (e.g., staging, production, etc.)
Prepare data migration reports that includes – record counts of source vs migrated vs modified; field level validation of data at row and column; discovered exceptions.
Business Intelligence -
Develop ETL pipelines in and out of Snowflake data warehouse using combination of Python, SnowSQL, and SQL.
In collaboration with Data Stewards, create Snowflake data mart as curated subset of data to support the analytics and business intelligence needs of subgroup of users.
Translate requirements for business intelligence and reporting to database design and reporting design.
Conduct discovery sessions with users to capture business intelligence reports.
Develop Tableau self-service ad hoc reports to support operational users.
Develop Tableau Dashboards to support executive, management, and operational users.
About us:
Provista Software Corporation is fast growing Software Solution provider for US State and Local Government Clients.
Job Type: Full-time
Pay: $67,107.98 - $134,285.66 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Parental leave
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
8 hour shift
Application Question(s):
What is your most preferred work time zone ?
Experience:
ETL: 5 years (Required)
Data warehouse: 1 year (Preferred)
Tableau: 1 year (Preferred)
Talend: 1 year (Preferred)
Microsoft SQL Server: 5 years (Required)
Work Location: Remote",100697,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'java', 'go', 'python']","['azure', 'snowflake']",['tableau'],"['sql server', 'snowflake', 'oracle']",[],[],bachelor,5-10 years
Spar Information Systems,3.7,Remote,Sr Data Engineer- (Visa Sponsorship Not Available),"Sr Data Engineer- (Visa Sponsorship Not Available)
Location: Remote
Duration: 3 Months Contract to hire
Required Skills:
10+ years of experience in data software development, programming languages and developing with big data technologies
2+ years of experience designing and building on existing and new data applications
2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework
1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express.
Experience in data software development, using data technologies such as Relational & NoSQL databases, open data formats, and programming languages such as Python, Scala, and/or other frameworks, building data pipelines (ETL and ELT) with batch or streaming ingestion, error handling, loading, and transforming data, and developing with big data technologies such as Spark, Hadoop, and MapReduce. Experience with analytics solutions.
Experience in development using Python or PySpark, Spark, Scala.
Advanced understanding of designing and building for data quality assurance, reliability, availability, and scalability, on existing and new data applications.
Advanced understanding of DevOps Concepts, Cloud Architecture, and Azure DevOps Operational Framework, Pipelines, Kubernetes.
Advanced understanding of designing and building solutions for data quality and observability, metadata management, data lineage, and data discovery.
Advanced understanding of building products of micro-services oriented architecture and extensible REST APIs.
Advanced understanding of open-source frameworks.
Experience with continuous delivery and infrastructure as code.
Experience in existing Monitoring Portals: Splunk or Application Insights.
Advanced understanding of Security Protocols & Products: Understanding of Active Directory, Windows Authentication, SAML, OAuth.
Advanced understanding of Azure Network (Subscription, Security zoning, etc) & tools like Genesis.
Advanced understanding of existing Operational Portals such as Azure Portal.
Knowledge of CS data structures and algorithms.
Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication).
Practical knowledge of working in an Agile environment (Scrum/Kanban/SAFe).
Strong problem-solving ability.
Ability to excel in a fast-paced, startup-like environment.
Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience.
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Experience:
Data Engineer: 10 years (Preferred)
Cloud: 3 years (Required)
Work Location: Remote",117000,Unknown,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['nosql', 'scala', 'python']","['azure', 'aws']",['excel'],[],"['hadoop', 'spark']",[],bachelor,2-5 years
Zenotis Technologies inc,4.0,"Addison, TX",Big Data Engineer,"Role: Big Data Engineer
Location – Addison, TX/ Charlotte, NC / Chandler, AZ (Hybrid – 3 days onsite 2 days remote)
Position Summary:
Looking for a big data engineer who loves solving complex problems across a full spectrum of technologies. The ideal candidate is excited by experimentation and looking for a new challenge that stretches their talents. The big data engineer will help ensure that our technological infrastructure operates seamlessly in support of business objectives.
The engineer must be able to:
· Translate complex technical and functional requirements into detailed designs
· Guide the development and testing teams in developing and implement pipelines that extract, transform, and load data into an information product that helps the organization reach its strategic goals
· Focus on ingesting, storing, processing, and analyzing large datasets
· Deep understanding on how to create scalable, high-performance web services for tracking data
· Investigate alternatives for data storing and processing to ensure the implementation of the most streamlined solutions
Primary Skill
Hadoop
Secondary Skill
Kafka
Tertiary Skill
Oracle Exadata
Required Skills
· Translate complex technical and functional requirements into detailed designs
· Design data pipelines using design tools and provide solution intent in provided templates
· Work closely with data science team to implement data analytics pipelines
· Analyze vast number of data stores to uncover insights
· Define data governance policies and support data-versioning processes
· Maintain security and data privacy, working closely with data protection guidelines.
· Desired Skills
· Experience with Python, Spark, and Hive
· Understanding of data-warehousing and data-modeling techniques
· Knowledge of industry-wide visualization and analytics tools (ex: Tableau, R)
· Strong data engineering skills with Hadoop platform
· Experience with streaming frameworks such as Kafka
· Knowledge of Core Java, Linux, SQL, and any scripting language
· Good interpersonal skills and positive attitude
Job Type: Full-time
Salary: Up to $70.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Work Location: On the road",126000,201 to 500 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'java', 'r', 'python']",[],['tableau'],"['hive', 'oracle']","['kafka', 'spark', 'hadoop']",[],,
"PRIMUS Global Services, Inc",4.1,"Minneapolis, MN","Data Engineer – Hadoop, Python – REMOTE WORK 43198","We have an immediate long-term opportunity with one of our key clients for a position of Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Python, Hadoop, Hive and SQL.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tejaswini
PRIMUS Global Services
Direct: 972-798-2662
Desk: 972-753-6500 Ext: 204
Email: jobs@primusglobal.com",91633,501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD),MN,21,data engineer,na,"['sql', 'python']",[],[],['hive'],['hadoop'],[],,
Southern Glazer’s Wine and Spirits,3.6,"Dallas, TX",Data Engineer,"What You Need To Know
Open the door to a groundbreaking tech career with an industry leader. Southern Glazer’s Wine & Spirits is North America’s preeminent wine and spirits distributor, as well as a family-owned, privately held company with a 50+ year legacy of success. To create a new era in alcohol beverage sales and service, we’re heavily invested in the most transformative new technologies – and the most brilliant tech professionals. Southern Glazer’s was named by Newsweek as a Most Loved Workplace and is included on the Forbes lists for Largest Private Companies and Best Employers for Diversity.
As a full-time employee, you can choose from a full menu of our Top Shelf Benefits, including comprehensive medical and prescription drug coverage, dental and vision plans, tax-saving Flexible Spending Accounts, disability coverage, life insurance plans, and a 401(k) plan. We also offer tuition reimbursement, a wellness program, parental leave, vacation accrual, paid sick leave, and more.
We offer continuous learning and career growth in a fast-paced environment where you are respected, your voice is heard, and technology is part of our strategy for success. If you’re looking to fill your glass with opportunity, come join our FAMILY.
Overview
The Data Engineer's role is to design, develop, maintain and enhance interfaces and connectivity to the Data Warehouse ecosystem by coding with a technical language to meet business requirements and business objectives. This can include taking technical specifications and developing an application or integration of data between applications, testing, as well as, completing the appropriate technical documentation. The Data Engineer will use best practices in software development and adhere to SGWS development standards, as well as, focus on quality and innovation. The Data Engineer may also be responsible for delivering support to end users in the organization for specific code, including troubleshooting code.
Specialized Skills and Technologies
Strong PL/SQL skills
Experience in ETL Tools (Preferrable Informatica)
Data Warehouse techniques will be a plus
Experience in cloud platforms like Azure or AWS will be a plus
Knowledge of UNIX/Linux, shell scripting, Python will be a plus
Experience developing Application Programming Interfaces (API's) will be a plus
Experience in Hadoop will be a plus
Primary Responsibilities
Design, develop, implement, and support software applications
Drive technical validity of solution.
Develop user documentation as well as in-code documentation to explain designs and participate/support user training
Structure requirements to facilitate automation of acceptance tests
In conjunction with Data Management Group, develop routines and procedures that provide data quality checks and balances on data delivery/ingestion
Collaborate across the BI / Analytics, Data Management Group, Enterprise Insights and Analytics teams to establish standards, reusable data models and best practices for delivery/ingestion of data from/to Data Warehouse - This includes Publish/Subscription and API options
Obtain any certifications needed to effectively support applications in scope
Support the development of business and technical process documentation and training materials
Structure requirements to facilitate automation of acceptance tests
Provide support for software applications under area of responsibility
Drive Behavior-Driven-Design (BDD) process
Perform other job-related duties as assigned
Minimum Qualifications
Bachelor’s Degree or a combination of work experience and education
Knowledge in application and software development
Knowledge of software design and programming principles
Proficient oral and written communication skills, ability to influence outcomes, and strong attention to detail
Strong analytical, mathematic, and problem-solving skills
Strong team player with ability to demonstrate Agile delivery values working both within a team and working independently
Strategic thinker – can develop a plan to meet a long-term objective
Agile Delivery Values
Openness – Team and stakeholders agree to be open about all work and challenges
Commitment – Personally commit to achieving the goals of the team
Respect – Respect your team members to be capable and independent
Courage – You have courage to do the right thing and work on tough problems
Focus – Everyone focus on the work in the sprint and the goal of the scrum team. Rise and fall as a team
Physical Demands
Physical demands include a considerable amount of time sitting and typing/keyboarding, using a computer (e.g., keyboard, mouse, and monitor), or mobile device
Physical demands with activity or condition may occasionally include walking, bending, reaching, standing, squatting, and stooping
May require occasional lifting/lowering, pushing, carrying, or pulling up to 20lbs
EEO Statement
Southern Glazer's Wine and Spirits, an Affirmative Action/EEO employer, prohibits discrimination and harassment of any type and provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Southern Glazer's Wine and Spirits provides competitive compensation based on estimated performance level consistent with the past relevant experience, knowledge, skills, abilities and education of employees. Unless otherwise expressly stated, any pay ranges posted here are estimates from outside of Southern Glazer's Wine and Spirits and do not reflect Southern Glazer's pay bands or ranges.",97837,10000+ Employees,Company - Private,Retail & Wholesale,Wholesale,1968,$10+ billion (USD),TX,55,data engineer,na,"['sql', 'shell', 'python']","['azure', 'aws']",[],[],['hadoop'],[],bachelor,
Breadboard,4.0,"New York, NY",Data Engineer,"About Us
We’re on a mission to revolutionize the global electronics supply chain.
During the pandemic we saw how the inefficiencies in electronics manufacturing can have massive global impact. From over $200B lost in car manufacturing to delays on laptops, gaming systems and much more.
At Breadboard we are rethinking the electronics supply chain down to the finest detail. We’re designing the new operating system in which this entire system will run. We’ve already secured seed funding and are now looking to expand our engineering team to help us set the foundation for the future.
Why work with us?
As a part of our team, you will have the unique opportunity to design and implement the data platform for Breadboard, a critical component in our business.
As one of the first data engineers to join the Breadboard team you will be partnering with the CTO, our founding engineers, and will set the foundation for the design of the operating system for the electronics supply chain.
You will be involved in all aspects of the software development process, architectural decisions, recruiting, and building the organization in general.
We’re a tight-knit, intense team and you will have a ton of end to end ownership.
Work with seasoned founders who have had successful exits.
This is not only a unique opportunity for someone who wants to get their hands dirty, but also take that step back and collaborate with the executive team on setting a strategic future.
You’ll be responsible for
Data Management: Develop and automate robust data acquisition, processing, and normalization pipelines from various sources to ensure data quality and consistency.
Database and ETL Architecture: Design, implement, and optimize a database system for efficient data storage, retrieval, and analysis.
Collaboration on Machine Learning: The prospects of working with data scientists to leverage extracted data features, contributing to the development and refinement of a machine learning model for component matching.
API and UI Integration: Collaborate with the software development team to facilitate seamless integration between the database, machine learning models, and user interface.
Continuous Improvement and Compliance: Monitor system performance and user feedback for continuous improvements, while ensuring adherence to data privacy and security standards.

Requirements
Experience: A minimum of 3-5 years of experience in a data engineering role, preferably in the electronics, manufacturing industry or supply chain industry.
Technical Skills: Proficiency in SQL and experience with database design, data modeling, and performance optimization. Strong coding skills in a major programming language, such as Python or TypeScript. Experience with data processing tools and libraries (e.g., pandas, Spark). Experience with pipeline and workflow orchestration tools, such Airflow. Experience with Snowflake or similar cloud-based data warehousing solutions.
Data Acquisition and Processing: Experience with data acquisition methods, such as APIs and web scraping. Familiarity with data cleaning, normalization, and feature extraction techniques.
Machine Learning: Familiarity with machine learning concepts and algorithms, and experience collaborating with data scientists or machine learning engineers.
API Development: Experience with API development and integration, preferably in a microservices architecture.
Data Governance: Knowledge of data privacy and security regulations, and experience implementing data governance measures.
Bonus Points
Entrepreneurial mindset (we encourage all employees to be future founders and this can be a great stepping stone towards that goal).
Customer-centric and passionate about helping small businesses grow.
Previous experience at a high-growth, fast-paced startup.
Experience with our stack (and some others that may be included shortly):
React, Material UI, Typescript, CSS
NestJS, GraphQL, TypeORM, Knex.js, Objection.js, PostgreSQL, gRPC, Kafka
AWS, Docker, Kubernetes, Helm, Github Actions, CI/CD pipelines.
Compensation + Benefits
Competitive cash compensation
Stock ownership at a fast growing company
Health/dental/vision insurance—100% coverage option
The ability to push for other benefits you think are important
Our Engineering Culture
Openness to diverse opinions and backgrounds - Testing our assumptions
Supported career growth with a high emphasis on learning and mentorship
Lean development
End to end ownership and accountability. We’re all going to make mistakes. It’s about how you own up to them and learn from them.

Equal Opportunity Employer
Breadboard Software does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.",112889,,,,,-1,,NY,-1,data engineer,na,"['sql', 'python']","['snowflake', 'aws']",[],"['postgresql', 'snowflake']","['kafka', 'spark']",['docker'],,5-10 years
"Pallidus, Inc",3.7,Remote,Data Engineer,"Pallidus, Inc. is a fast-growing producer of large diameter Silicon Carbide (SiC) wafers for high power and high frequency semiconductor applications in transportation, green energy, telecommunications, and Industrial markets. Pallidus’ M-SiC™ technology delivers step change improvements in SiC quality and cost effectiveness to drive wafer to device yield performance. Pallidus is a high energy, technology driven and innovative startup company looking for like-minded people to join our team.

SCOPE
Pallidus is looking for a Data Engineer to deliver business value through the design and integration of information models that originate from multiple data sources. You will be responsible for expanding and optimizing our data and data pipeline architecture and data flow and collection for cross-functional teams. The Data Engineer will be part of the growing the data science team and be responsible for the day-to-day activities related to the implementation of new services and support for existing services.

RESPONSIBILITIES
Partner with key business stakeholders to create blueprints for data solutions & services that align with overall corporate objectives and support enterprise scale reporting capabilities
Translate simple to complex user requirements into functional and actionable analytics
Provide guidance on architecture, data-store selection, data modeling, and query optimization for existing and future applications
Execute the design, development and support of data solutions including configuration, administration, monitoring, performance tuning, and debugging for efficient data handling & analytical reporting
Ensure an appropriate infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources into cloud computing platforms such as Snowflake and Databricks
Support data quality issue resolution while managing SLAs for services owned
Participate in the development lifecycle using Agile / DevOps methodologies and support testing & deployment as part of the full release cycle.
Become an internal subject matter expert on various datasets and support other departments on usage of those datasets
Collaborate with applications teams and business partners to assist in developing data governance framework & data retention policies
REQUIREMENTS
Bachelor's degree in computer science, engineering, or related field of study
Minimum of 5 years of experience in a data engineering role working with diverse data sets ideally with modern data warehousing/data lake technologies
A successful history of ETL manipulating/processing/extracting value from large disconnected and diverse datasets
Comfortable building pipelines with industry best practices to load huge volumes of data into a Data Warehouse
Strong analytic skills related to working with unstructured datasets
Experience with relational SQL and NoSQL databases, and capable of writing complex queries in SQL, spark, pandas, etc.
Knowledge of data modeling, star schema, incremental load
Ability to quickly build effective working relationships across the organization
Solid software engineering skills and advanced knowledge of at least one language, preferably Python. Appreciation of and dedication to clean coding principles is important
Effective communication skills, including the ability to articulate complex technical requirements to business and end-users in a clear and non-technical manner
Must be a US Citizen or be a lawful permanent resident of the U.S. to meet Export Control requirements",135000,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python', 'nosql']","['databricks', 'snowflake']",[],['snowflake'],['spark'],[],bachelor,
"Wisetek Providers, Inc",4.2,Remote,Principle Data Engineer,"** We do transfer / Sponsor visa
Role: Principle Data Engineer Duration: 6 months.
Looking for Senior candidates with 10+ years of experience. .
Required 3-5 years of experience with the following technologies:
o Hadoop, Python, Hive, SQL, Shell scripting.
o Apache Spark.
o NoSQL and relational databases.
Required 1-3 years of experience with the following technologies:
o Scheduling tools like Airflow / Tivoli Work scheduler.
o Working in Agile/Scrum environment.
o Jenkins or similar CICD tool, GitHub.
Preferred 2-5 years of experience with the following technologies:
o Apache Kafka.
o API’s.
o Kubernetes.
Job Types: Full-time, Contract
Pay: $80,000.00 - $105,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
COVID-19 considerations:
Yes
Experience:
Hadoop / Python: 5 years (Required)
Jenkins / CICD: 1 year (Required)
SQL: 5 years (Required)
Apache/ Spark: 5 years (Required)
Work Location: Remote",92500,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),Remote,-1,data engineer,na,"['nosql', 'sql', 'shell', 'python']",[],[],['hive'],"['kafka', 'spark', 'hadoop']",[],,5-10 years
WorkCog,4.3,"Atlanta, GA",Sr. AWS Data Engineer,"Position: AWS Data Engineer
Location: Atlanta ,GA
Duration: 12+ Months
Interview: Video
JD
Responsibilities
Bachelor’s degree in Computer Science, Software Engineering, or other related field, or equivalent work experience.
Extensive experience in data modeling, writing & performance tuning complex SQL queries & user-defined functions
Hands-on experience in design, implementation and orchestration of data pipelines and ETL batch Jobs.
Hands-on experience on all aspects of data handling: cleansing and standardization, normalization, merging, change data capture, data integration, reconciliation, and transformations
Strong experience in working with Big Data technologies such as Hive/Spark, Data lake and Data warehouse using GCP and/or AWS Services
Ability to write robust code in scripting languages like Python, Scala, PySpark or similar.
Good to Have
Associate Architect or equivalent cloud certification
Experience with processing large datasets using AWS and GCP UDFs (User defined functions)
Experience in ML probability & statistics algorithms, frameworks like Keras or PyTorch and tools like Jupyter notebooks.
Job Type: Contract
Pay: $45.83 - $52.42 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL (Preferred)
Informatica (Preferred)
Data warehouse (Preferred)
Work Location: In person",88434,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable,GA,6,data engineer,senior,"['sql', 'scala', 'python']",['aws'],[],['hive'],['spark'],[],bachelor,0-2 years
FACEBOOK APP,5.0,Remote,Azure Data Engineer,"Responsibilities:
Create ER diagrams and write relational database queries
Create database objects and maintain referential integrity
Configure, deploy and maintain database
Participate in development and maintenance of Data warehouses
Design, develop and deploy SSIS packages
Creating and deploying reports
Provide technical design, coding assistance to the team to accomplish the project deliverables as planned/scoped.
Ability to talk to client and get the Business Requirements
Skills:
Azure Data Factory
Azure Devops
Azure Storage/ Data Lake
Extraction, Transformation and Loading
Analytics development
Report Development
Relational database and SQL language
Other Requirements:
· Should be well versed with Data Structures & algorithms
· Understanding of software development lifecycle
· Excellent analytical and problem-solving skills.
· Ability to work independently as a self-starter, and within a team environment.
· Good Communication skills- Written and Verbal
Job Type: Full-time
Salary: $84,454.31 - $190,806.62 per year
Benefits:
Flexible schedule
Health insurance
Compensation package:
1099 contract
Yearly pay
Experience level:
10 years
9 years
Schedule:
Day shift
Experience:
Azure Data engineer: 9 years (Preferred)
SQL: 9 years (Preferred)
Data warehouse: 10 years (Preferred)
Work Location: Remote",137630,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,['sql'],['azure'],['ssis'],[],[],[],,0-2 years
Urbane Systems LLC,4.0,"Washington, DC",Senior Cloud Data Engineer,"Seeking a Cloud Data Engineer that will assist in maintaining and monitoring infrastructure as well as build or assist in building data transformation pipelines.
Candidate must have prior experience with AWS or Azure and extensive knowledge of python for ETL development. Additional Cloud-based tools experience is important (see skills section)
Additional desired skills include experience with the following:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive experience leveraging Python to build data transformation pipelines (ETL) and experience with libraries such as pandas and NumPy.
Experience with pyspark.
Experience using AWS Glue and EMR to construct data pipelines
Experience building and optimizing ‘big data’ data pipelines, architectures, and datasets.
Job Type: Contract
Pay: $80.00 - $90.00 per hour
Schedule:
Day shift
Work Location: Hybrid remote in Washington, DC 20001",153000,,,,,-1,,DC,-1,data engineer,senior,"['sql', 'python']","['azure', 'aws']",[],[],[],[],,
Ripple Effect Consulting,5.0,"Denver, CO",Data Engineer - 004/005,"Data Engineer
Hybrid-remote based in Denver, CO | $80-$160k
Job Overview:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
1) Experience with big data tools
2) Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
3) Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
4) Experience with cloud services
5) Experience with stream-processing systems: Storm, Spark-Streaming, etc.
6) Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Job Type: Full-time
Pay: $80,000.00 - $160,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Denver, CO: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",120000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,CO,-1,data engineer,na,"['scala', 'java', 'nosql', 'sql', 'python']",['aws'],[],[],['spark'],[],,0-2 years
DiamondPick,4.5,"Edison, NJ",Senior Data Engineer,"Hi ,
Greetings from Diamond pick inc.
We are currently looking for the below position for one of our clients... Please let me know your interest along with your updated resume ASAP..
Role:Data engineer
Location: Berkley Heights, NJ(Onsite)(locals only)
9+ years of experience is must
Description
Skills: strong Java,Azure,Spark & sql
Company Description:
Apexon is a digital-first technology services firm backed by Goldman Sachs Asset Management and Everstone Capital. We specialize in accelerating business transformation and delivering human-centric digital experiences. For over 17 years, Apexon has been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
RESPONSIBILITIES
Basic Qualifications for consideration:
5+ Overall industry experience
3+ years' experience with building large scale big data applications development
Bachelors in Computer Science or related field
Provide technical leadership in developing data solutions and building frameworks
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Hands-on experience with Springboot framework, MVC architecture, Hibernate, API creation, Restful services, SOAP using Java
Java experience with OOPS concepts, multithreading
Experience deploying code on containers
Hands on experience with leveraging CI/CD to rapidly build & test application code
Conduct code reviews and strive for improvement in software engineering quality
Review data architecture and develop detailed implementation design
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance
QualificationsBachelor's Degree in Computer Science or Computer Engineering is required
Job Type: Contract
Salary: $43.96 - $70.65 per hour
Ability to commute/relocate:
Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL (Required)
Java (Required)
Azure (Required)
Work Location: One location",103140,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,-1,Unknown / Non-Applicable,NJ,-1,data engineer,senior,"['sql', 'java', 'nosql']","['azure', 'aws']",[],['dynamodb'],"['kafka', 'spark']",[],,+10 years
LandGate Corp,4.0,Remote,Data Engineer,"LandGate is a fast growing renewable energy and carbon data & analytics platform. We are looking for a Database Engineer with experience using Python and SQL to process data. You will be expected to be a savvy developer who can work tasks from start to finish, applying your in depth experience of data processing using Python and SQL, and be prepared to hit the ground running. Must have excellent attention to detail, communication skills, and willingness to go the extra mile.
Compensation
LandGate offers a very competitive salary (commensurate with experience), commission, bonus, and shares providing ownership in the company. Benefits include 5 weeks PTO, work from home, platinum health/vision/dental insurance 100% paid for employees and dependents, disability/life insurance 100% paid, phone/internet 100% paid, and other benefits.
About LandGate
LandGate is the leading provider of data solutions, and an online marketplace for US commercial land and its resources: solar, wind, carbon, minerals, and water. The company helps investors, developers, real estate agents, and landowners understand energy & environmental resource values and connect on its online marketplace for land-related transactions.
LandGate enables energy and carbon professionals to run economic engineering studies in minutes; access land leads and MLS listings; and manage their leads in a land CRM web app connecting their team. The company opens energy and carbon commission opportunities to real estate agents. LandGate applies its technology to provide the most advanced analytics for renewable energy M&A deals, market & price trends, operators’ benchmark and performance indicators.
Founded in 2016 in Denver, Colorado, LandGate received Series A funding in 2019 from Rice Investment Group, a widely-respected energy technology investor, and Series B funding in 2022 from Nextera Energy, the world’s largest generator of renewable energy, and from Kimmeridge, a leading carbon solutions private equity firm. The company has partnered with the Realtors Land Institute (RLI), the American Association of Professional Landmen (AAPL), and the Texas Engineering Executive Education (TxEEE) from the University of Texas at Austin.
Skills
Experience with Python and SQL (2+ years)
Proficiency in working with PostgreSQL, writing SQL (queries, functions, stored procedures)
Strong math background with ability to apply scientific formulas to large datasets
Experienced using Linux command line
Proven work experience as a Back-end developer
Familiarity with testing and debugging
In-depth understanding of the entire development process (design, development and deployment)
An ability to perform well in a fast-paced environment
Excellent analytical and multitasking skills
BSc degree in Computer Science or relevant field
Job Type: Full-time
Pay: $100,000.00 - $180,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Paid time off
Retirement plan
Vision insurance
Compensation package:
Performance bonus
Experience level:
4 years
Schedule:
Monday to Friday
Experience:
SQL: 2 years (Required)
Python: 2 years (Required)
Work Location: Remote",140000,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'go', 'python']",[],[],['postgresql'],[],[],,2-5 years
ProIT Inc.,5.0,"Bellevue, WA",Azure Data Engineer,"Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Spark ,Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power
Job Type: Full-time
Pay: $100,154.64 - $104,132.47 per year
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Bellevue, WA 98004",102144,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,WA,-1,data engineer,na,['sql'],['azure'],['power bi'],['sql server'],['spark'],[],,0-2 years
Purple Drive Technologies,4.2,"Cupertino, CA",Data Engineer,"Terraform along with synapse Azure cloud
Deep expertise in Data Engineering and Data Warehousing (minimum 4+ years)
Azure synapse , Azure Data Factory , Spark Pool , SQL , SQL Pool (minimum 4+ yrs)
CI/CD and Python/Java programming experience (minimum 3+ years)
Ideal to have a 24 x 7 development i.e offshore presence or different time zones
Job Type: Full-time
Salary: $115,871.32 - $207,675.38 per year
Ability to commute/relocate:
Cupertino, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",161773,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['sql', 'java', 'python']",['azure'],[],[],['spark'],['terraform'],,0-2 years
AbleTo,3.7,"New York, NY",Data Engineer,"About AbleTo
Join our mission-driven organization, where your work matters and a diversity of ideas and backgrounds are welcomed. AbleTo is the leading provider of high quality, technology-enabled behavioral health care. We believe that everyone deserves access to high-quality care and offer a suite of technology-enabled services to empower people to lead better lives through better mental health. A proprietary platform connects individuals with AbleTo highly trained licensed providers who deliver weekly sessions by phone or video supported by an integrated digital experience. Members also have access to mental health coaches, and digital support programs. Our outcomes-focused approach is proven to improve both behavioral and physical health and lower medical costs.

Overview
Data is integral to nearly everything that we do at AbleTo, and as a result the data team plays a key role at our company. As a Data Engineer at AbleTo, you will grow and develop your skills as a data engineer to build a world class data platform to support our mental health care mission.

What You Will Do
Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, Informatica, Python, etc.)
Identify code quality issues and implement tests to improve future processes.
Document orchestration work-flows and create run books
Implement data integrity tests to ensure we are ingesting accurate data.
Implement high-quality test-driven code.
Participate in daily team stand ups and other Agile ceremonies

Key Initiatives over the next 12 months
Data Mart client migration
Data Mart enhancements
Automation of manual day to day operational tasks
Decommission legacy pipelines

Who You Are:
1+ years of experience coding in Python.
Experience working with SQL (structured query language).
Familiarity with structuring and writing ETLs.
Experience working with Orchestration tools (Airflow) and Cloud Databases (BigQuery) is a plus.
Experience working in an Agile Development environment is a plus.
BS in Information Systems, Computer Science or related field is preferred.
You are excited to work with data!
Curiosity to dive deeply into issues and feel empowered to make a meaningful impact at a mission-driven company.
Committed to agile development and value delivery and solid engineering principles, as well as continuously improving our craft.
You are a collaborator. You build and maintain strong, productive working relationships with your customers and stakeholders.
You are empathetic and seek to understand each individual's diversity of background and experience contributes to making up a great team.
You drive to improve yourself and others.
You are excited about learning new languages, tools, techniques and technologies.

Why You Should Join Our Team:
We're proud to be a Great Place to Work-Certified™ company. We want you to show up and feel your best at work, and that means respecting your time outside of work. Our inclusive, flexible workspace prioritizes a work/life balance. We offer competitive salaries, comprehensive health benefits (for full-time employees), and professional perks such as 401K matching, fully funded HRA, and generous time off, including mental health days because your well-being is important to us.
At AbleTo, we're empowering people to get better and stay better. Want to join us? Take the next step in your career by applying for this role today.
The salary range is $40,000 to $90,000. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, AbleTo offers benefits such as a comprehensive benefits package, incentive and recognition programs, and 401k contribution (all benefits are subject to eligibility requirements).

#LI-Remote
Follow AbleTo on LinkedIn, Twitter, and Instagram!
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
AbleTo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. AbleTo is an E-Verify company.",65000,501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2008,$25 to $100 million (USD),NY,15,data engineer,na,"['sql', 'python']",['google cloud'],[],[],[],[],,+10 years
Fathom Management LLC,2.0,Remote,Sr. Data Engineer Remote Opportunity,"Sr. Data Engineer

seeking a Senior Data Engineer who possesses expert level knowledge of appropriate data sources to address the specific requirements of projects for data modeling. Understand business requirements and translating into technical work. Design and implement features in collaboration with team engineers, product owners, data analysts, business partners using Agile/SCRUM Methodology.

This is a full- time position / 100% Remote.
The salary range of $140,000 - $160,000 will be based on technical experience and technical interview.

Responsibilities:

Ability to build programs or systems that can take data and turn it into meaningful information that can be studied.
Build ETL/ELT jobs and workflows to combine data from disparate sources.
Install continuous pipelines of huge pools of filtered information so that data analyst/scientists can pull relevant data sets for their analyses.
Build data workflows using SQL Server Integration Services (SSIS)
Build data workflows using Microsoft Azure (Azure Data Factory, Storage Accounts, Synapse)
Build data workflows using Databricks
Collaborate with business stakeholders, business operations, and product engineering teams in analyzing business problems, in building and testing solutions and data models.
Experience implementing and operating analytic models and services.
Document the current-state and target-state software architecture and create roadmap plans for success on various software components.
Assist in the design, implementation, and maintenance of complex solutions.
Build systems that collect, manage, and convert raw data into usable information for business analysts to interpret.
Make data accessible for evaluation and optimization
Collaborate with business stakeholders, business operations, and product engineering teams.
Coordinate activities with other technical personnel as appropriate.
Works with back-end data and develops tables using SQL scripts, SSIS, and SSMS.
Experience with Azure cloud platforms and Data bricks

Required Experience and Education:
Master's degree in computer science, systems engineering, or related technical discipline is preferred with 7-10 years of experience as a Data Engineer/Administrator or similar role. OR , B.S. in Computer Science with 15 years of relevant experience.

Benefits Overview: Full-time employees are offered comprehensive and competitive benefits package including paid vacation, sick leave, holidays, health insurance, life insurance, military leave, training, tuition reimbursement, a wellness program, short- and long-term disability, 401(k) retirement plan with company matches/immediate vesting, commuter benefits, and more.
EEO Policy: It is our policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits, and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability, or any other characteristic protected by applicable federal, state or local law.",150000,1 to 50 Employees,Self-employed,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,['sql'],"['databricks', 'azure']",['ssis'],['sql server'],[],[],master,5-10 years
Go Intellects Inc,4.0,"Washington, DC",Data Engineer,"Work Location: REMOTE (1 – 2 Days On-site/Week may require)
Required Skills:
· Collect, manage, and convert raw data accurately and reliably
· Organize data systems for subgroup access and analyses
· Configure and sustain data cloud structures
· Must have expertise in Data Visualization Tools (Tableau)
· Data Modeling/Science as Python/SAS
· Should have AWS cloud native services, security, data pipeline
· Able to work with structured and unstructured data.
· Validate outputs of data pipelines
· Degree in Data Engineering preferred.
Job Type: Full-time
Pay: Up to $150,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Application Question(s):
Do you have, ""Active Secret (or) Top Secret Security Clearance""?
Work Location: In person",150000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,['python'],['aws'],['tableau'],[],[],[],,0-2 years
CONN-SELMER INC,3.3,Remote,Data Engineer,"Reports to: Director of IT
**Remote position**
Perform data analysis and provide Business Intelligence support to the organization
Responsibilities:
Perform data analysis on ERP systems including Infor XA and D365.
Perform data analysis on financial systems and custom developed applications.
Develop PowerBI reports for financial analysis and management reporting.
Create SQL-statement-based queries to facilitate data analysis.
Support data models to enhance BI reporting.
Engage and interact with business users to identify requirements and develop documents for the reporting needs of the business.
Engage and interact with business users to troubleshoot and resolve reporting issues.
Work with users to address ad-hoc data analysis requests.
Prioritize tasks and responsibilities to properly manage one’s own workload.
Troubleshoot data and BI application issues.
Fully analyze the impact and determine the strategy for all report requests.
Escalate top priority or production-critical issues to the appropriate support staff.
Make recommendations of perceived solutions to potential problem areas and methods for improvement.
Communicate effectively.
Other duties as assigned by supervisor
Qualifications and Requirements:
Bachelor of Science in Information Systems/MIS, computer science, business or related field or equivalent experience
4 years Business Intelligence experience preferred
Microsoft Dynamics 365 experience preferred
Courses are desired and/or will be required within the first six months of employment: PL300, DP500, DP203
Microsoft PowerApps and PowerPortal applications preferred
Infor XA or any IBM (AS/400 or iSeries) based ERP preferred
Azure Synapse Experience preferred
Power BI required
SQL required",112889,501 to 1000 Employees,Subsidiary or Business Segment,Manufacturing,Consumer Product Manufacturing,1875,Unknown / Non-Applicable,Remote,148,data engineer,na,['sql'],['azure'],['power bi'],[],[],[],bachelor,2-5 years
BIGCLFY,4.0,"Dallas, TX","W2/ 1099, No C2C - SSIS ETL Data Engineer (Minneapolis, MN/ Dallas-Irving)","No C2C. It's W2/1099 Requirement
Only taking USC, GC or H4 EAD, H1 Transfer, L2 Visa or TN Visa - they will not take OPT EAD or CPT
Need to have confirmation in writing that you will be onsite by Day 1
ALL ROLES ARE HYBRID IN THE OFFICE (2-3 DAYS/WEEK) UNLESS REMOTE IS NOTED
Please submit qualified candidates and include FULL LEGAL NAME as it appears on the passport
If your candidate is relocating, need to have confirmation in writing that they will be onsite by Day 1
Senior SSIS/ETL Data Engineer
12 months – 3 days in office
Minneapolis, MN or Dallas-Irving, TX
5+ years of experience with SQL Server 2016 (or newer) and SSIS
7+ years of data engineering
Must be capable of leading teams
2+ years of experience working with large scale SQL implementations
Experience designing, developing, and maintaining complex stored procedures, functions, and views using T-SQL
Experience designing, developing, and maintaining ETL processes using SSIS
Should understand how to unit test their product
Hands on experience with Visual Studio to document
Experience in financial or regulatory environments highly preferred
Ideal candidates will have experience creating conceptual, logical, and physical database designs
Preference given to candidates with extensive knowledge and experience with performance monitoring, database and SQL tuning
Bachelor’s or Master's in computer science or related field or equivalent work experience
Job Type: Contract
Pay: From $73.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Chandler, AZ 85225: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Expected Payrate on W2/1099 Tax Term? Please mention. Thanks.
Experience:
Risk Analytics: 5 years (Preferred)
Work Location: In person",131400,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,['sql'],[],['ssis'],['sql server'],[],[],bachelor,5-10 years
"IT Engagements,Inc.",4.0,"Dallas, TX",Big Data Engineer,"Greeting from IT Engagements.
IT Engagements is a global staff augmentation firm providing a wide-range of talent on-demand and total workforce solutions. We have an immediate opening for the below position with one of our premium clients.
Job Title: Big Data Engineer
Location: Minneapolis, MN (Hybrid 3 days onsite) 2nd location Dallas TX.
Duration: 12 Months (Potential FTE)
Required Qualifications
4+ years of Software Engineering experience
4+ years of Data engineering experience
4+ years of experience in any or all Big-Data stack such as Hadoop, Hive, Spark, python
4+ years of Relational data base experience
4 + years of ETL (Extract, Transform, Load) development experience using any Big-Data technology
2+ years of Agile software development experience
Hands-on experience with cloud-based data environments
Technical Documentation
Thanks and Regards
Divya Kumari
Technical Recruiter
divya(at)itengagements(dot)com
Job Type: Contract
Pay: From $65.00 per hour
Experience level:
4 years
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'python']",[],[],['hive'],"['spark', 'hadoop']",[],,0-2 years
Inovalon,3.1,United States,Software Data Engineer,"Inovalon was founded in 1998 on the belief that technology, and data specifically, would empower the transformation of the entire healthcare ecosystem for the better, improving both outcomes and economics. At Inovalon, we believe that when our customers are successful in their missions, healthcare improves. Therefore, we focus on empowering them with data-driven solutions. And the momentum is building.
Together, as ONE Inovalon, we are a united force delivering solutions that address healthcare's greatest needs. Through our mission-based culture of inclusion and innovation, our organization brings value not just to our customers, but to the millions of patients and members they serve.
Overview: The Software Data Engineer is responsible for contributing to data pipelines, ETL, data warehouse, jobs, data operations. They will be part of the team which is building next generation data & reporting platform. This platform will cater internal business stakeholders and external customers to provide insights & forecasting to understand current state of business, improve decision-making for their tactical and strategic goals & KPIs. This position may require independent work, sharing information and assisting others with work request.
Duties and Responsibilities:
Work with the agile team to participate in agile ceremonies like grooming, planning, standup, retrospective, demos
Actively contribute to grooming, and standup, create & update tasks, estimate and status
Work with data architects and business analysts to create a logical data model and create DDL scripts for physical database creation
Work on large data to ensure ingestion of data, dynamic rule & validation of data, cleansing, transforming, and loading into the data warehouse
Write complex queries, stored procedures, functions, SSIS Packages for various job execution
Develop modern ETL framework utilizing tools like ADF (Azure Data Factory), MS-SSIS etc
Develop STAR or SNOWFLAKE database schema utilizing industry best practices to build Data warehouse, data marts, views, and data sets/products
Develop ETL pipelines, using SQL, Stored procedures/functions to extract data from various sources and load into warehouse
Develop Symantec layer and data export frameworks to extract the data from the warehouse, transform, pre-aggregate, perform calculations and load into various data marts for Analytics use
Develop configurable export framework to extract data from Data warehouse and data marts to generate reports for internal and external customers in .csv, flat files and
Design and implement data validation and quality checks to ensure the accuracy and completeness of the data in the data warehouse
Perform performance of queries and data processing, identify and resolve any issues
Work and communicate in a cross-functional geographically dispersed team environment comprised of software engineers and product managers; and
Ensure compliance to company procedures when making changes and implementing code.
Maintain compliance with Inovalon's policies, procedures and mission statement;
Adhere to all confidentiality and HIPAA requirements as outlined within Inovalon's Operating Policies and Procedures in all ways and at all times with respect to any aspect of the data handled or services rendered in the undertaking of the position; and
Fulfill those responsibilities and/or duties that may be reasonably provided by Inovalon for the purpose of achieving operational and financial success of the Employer.
Job Requirements:
Minimum two (2) years related experience required; healthcare industry experience preferred.
Strong understanding to develop SQL queries for data analysis.
Experience working on Azure Cloud is preferred
3+ experience in MS SQL, T-SQL, ETL Jobs
3+ experience in Microsoft tools like SSMS, SSIS, SQL Server
Strong understanding of database concepts and schema (like star, snowflake schema)
Ability to learn quickly and independently
Ability to effectively communicate with internal and external customers
Experience with test driven development methodologies.
Education:
Bachelor's degree in Computer Science, Software Engineering, or Information Technology.
Physical Demands and Work Environment:
Sedentary work (i.e., sitting for long periods of time);
Exerting up to 10 pounds of force occasionally and/or negligible amount of force
Frequently or constantly to lift, carry push, pull or otherwise move objects and repetitive motions
Subject to inside environmental conditions; and
Studies have shown that women and people of color are less likely to apply for jobs unless they believe they meet every one of the qualifications listed in a job description. If you don't meet every qualification listed but are excited about our mission and the work described, we encourage you to apply regardless. Inovalon is most interested in finding the best candidate for the job and you may be just the right person for this or other roles.
By embracing diversity, equity and inclusion we enhance our work environment and drive business success. Inovalon strives to reflect the diversity of the communities where we operate and of our clients and everyone whom we serve. We endeavor to create a culture of inclusion in which our associates feel empowered to bring their full, authentic selves to work and pursue their professional goals in an equitable setting. We understand that by fostering this type of culture, and welcoming different perspectives, we generate innovation and growth.
Inovalon is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirement.
The Company maintains a drug free work environment for all of its associates, which includes employees, contractors and vendors. It is unlawful for associates to manufacture, sell, distribute, dispense, possess or use any controlled substance or marijuana in the workplace and doing so will result in disciplinary action, up to and including termination of employment or the contracted relationship.",112889,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,1998,$500 million to $1 billion (USD),TX,25,data engineer,na,['sql'],"['azure', 'snowflake']",['ssis'],"['sql server', 'snowflake']",[],[],bachelor,
Gleecus TechLabs Pvt. Ltd,4.5,"Glen Allen, VA",Lead Azure Data Engineer,"Position Title: Lead Azure Data Engineer - Glen Allen VA (Hybrid)
Tech Skills:
· Concepts to be covered ? (if any).
· Databricks(Python, SQL).
· Hands on Coding.
· Databricks optimization/Pyspark Transformations/PySpark Architecture.
· Azure Data Factory.
· Scenario-Based Questions and Practical Application.
· Azure Data Factory activities/Use of ADF Monitor/CI CD concepts.
· Azure ML, Databricks ML, Azure SDK.
· Scenario-Based Questions and Practical Application.
· Optimized Apache Spark environment, Machine Learning Integration, Experiment tracking, Model training, Feature development, Model serving.
· Synapse Analytics, Parque and Delta tables.
· Scenario-Based Questions and Practical Application.
· Synapse Analytics related concepts.
Job Type: Contract
Salary: $130,000.00 - $150,000.00 per year
Benefits:
Flexible schedule
Health insurance
Paid time off
Schedule:
Day shift
Monday to Friday
Ability to commute/relocate:
Glen Allen, VA 23058: Reliably commute or planning to relocate before starting work (Required)
Experience:
ADF: 4 years (Required)
Azure Data Lake: 4 years (Required)
Databricks: 5 years (Required)
Work Location: One location",140000,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,Unknown / Non-Applicable,VA,8,data engineer,senior,"['sql', 'python']","['databricks', 'azure']",[],[],['spark'],[],,5-10 years
Schober Consulting LLC,4.0,"Minneapolis, MN",Data Engineer (Junior),"We are seeking a talented and experienced Data Engineer to join our team. The ideal candidate will have 2-3 years of professional experience in data engineering, with a bonus if they have worked in the real estate industry. Additionally, an interest in machine learning is highly desirable.
Responsibilities:
Design, develop, and maintain scalable and efficient data pipelines and ETL processes
Build and optimize data models and data storage solutions for large-scale datasets
Collaborate with cross-functional teams, including data scientists and analysts, to gather requirements and design data-driven solutions
Develop and maintain data infrastructure to support advanced analytics and machine learning initiatives
Ensure data quality and integrity by implementing data validation and cleansing processes
Continuously monitor and optimize data pipelines and systems for performance and reliability
Stay updated with the latest industry trends and technologies in data engineering and machine learning
Qualifications:
Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field
2-3 years of professional experience as a Data Engineer, preferably with exposure to the real estate industry
Strong understanding of data engineering principles, ETL processes, and data warehousing concepts
Proficiency in SQL and experience with relational databases
Solid programming skills in Python, Java, or other relevant languages
Familiarity with cloud computing platforms, preferably AWS or Azure
Experience with data modeling and designing efficient database schemas
Knowledge of machine learning concepts and a strong interest in applying machine learning to data engineering tasks
Excellent problem-solving and analytical abilities
Strong communication and collaboration skills
If you are a self-motivated individual with a passion for data engineering and an interest in the real estate industry and machine learning, we encourage you to apply. We offer a competitive salary, comprehensive benefits package, and a collaborative work environment that fosters professional growth and development.
Job Type: Full-time
Pay: $70,000.00 - $100,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Compensation package:
Employee stock ownership plan
Experience level:
2 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Minneapolis, MN 55402: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
SQL (Preferred)
Data warehouse (Preferred)
Python (Required)
Work Location: Hybrid remote in Minneapolis, MN 55402",85000,,,,,-1,,MN,-1,data engineer,na,"['sql', 'java', 'python']","['azure', 'aws']",[],[],[],[],bachelor,2-5 years
Innova IT Services LLC,4.0,"Raleigh, NC",Network Data Engineer,"W2 only
==========
HP ARUBA experience is a must.
Develop and install network infrastructure, configurations and equipment such as routers and switches
Monitor networks and troubleshoot issues or outages.
LAN, WAN, SWITCH & ROUTING experience is needed.
Consult with clients to suggest network solutions
Manage junior employees and provide training resources for team members
Test and install new computer systems, hardware, software and applications
Develop engineering design packages to integrate new processes into existing ones
Collaborate with clients, other tech support services and network providers to ensure the quality of networks
Team player and support on-call responsibilities as HP ARUBA NMS team member
Coordinate upgrades/updates and maintenance of HP ARUBA NMS tool solutions
Collaborate across multiple technical teams and lines of business with focus on implementing enterprise NMS solutions.
Strong problem-solving and troubleshooting skills
Excellent communication and interpersonal abilities
Ability to work independently and as part of a team
Ability to multi-task, prioritize, and manage time efficiently
Highly organized and detail-oriented
Certifications:-
CCNA minimum CCNP preferred.
-7+ years of experience with network planning, design, and implementing LAN, WAN, network security, and wireless network infrastructures.
Job Type: Contract
Pay: $40.00 - $45.00 per hour
Experience level:
7 years
Schedule:
8 hour shift
Ability to commute/relocate:
Raleigh, NC 27606: Reliably commute or planning to relocate before starting work (Required)
Experience:
Network engineering: 4 years (Required)
Routing protocols: 4 years (Required)
Data network design: 2 years (Required)
Work Location: In person",76500,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,NC,-1,data engineer,na,[],[],[],[],[],[],,2-5 years
Radiant System,4.0,Remote,Snowflake Data Engineer,"REQUIRED SKILLS
- Strong/ Proven Data Engineering experience with Snowflake
-Strong Azure Experience is required
-Experience configuring private networks on the cloud
-Experience moving data from Snowflake to Azure
Job Type: Contract
Application Question(s):
Are you willing to work on W2?
Experience:
Snowflake: 1 year (Required)
Work Location: Remote",112889,,,,,-1,,Remote,-1,data engineer,na,[],"['snowflake', 'azure']",[],['snowflake'],[],[],,0-2 years
PrizePicks,4.8,"Atlanta, GA",Data Engineer,"At PrizePicks, we are the fastest growing sports company in North America, as recognized by Inc. 5000. As the leading platform for Daily Fantasy Sports, we cover a diverse range of sports leagues, including the NFL, NBA, and Esports titles like League of Legends and CS : GO. Our team of over 200 employees thrives in an inclusive culture that values individuals from diverse backgrounds, regardless of their level of sports fandom. Ready to reimagine the DFS industry together?
Our Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
What you'll do:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Improve and streamline data systems to drive innovation within Prizepicks
What you have:
3+ years of experience building frameworks for data ingestion pipelines but real time and batch using data modeling, ETL/ELT processes
Stellar SQL skills, experience building DBT pipelines in production and know your way around structured, semi-structured and unstructured data.
Have built and optimized 'big data' data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience with Python and other object oriented scripting languages
Cloud experience: AWS, EC2, EMR, RDS, Redshift, GCP, etc..
Not required but would be awesome if you have experience with any of the following data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Where you'll live:
Anywhere in the US is fine (we are based in Atlanta, GA)
Benefits you'll receive:
In addition to your great compensation package, company subsidized medical/dental/vision coverage plans and matching 401(k), we'll shower you with perks including:
Break room with ping pong, endless snacks and in-office lunch once a week
Unlimited PTO to encourage a healthy work/life balance (2 week min required!)
Modern work schedule focused on getting the job done, not hours clocked
Workplace flexibility
Company and team outings, we encourage a tight-knit workplace
Generous Maternity AND Paternity leave (16 weeks!)
Annual bonus & stock options
Wellness program
Company equipment provided (Windows & Mac options)
Annual performance reviews with opportunity for growth and career development
#LI-REMOTE

You must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship or an employment Visa at this time.
PrizePicks is an Equal Opportunity Employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.",97814,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['sql', 'go', 'python']","['aws', 'redshift']",[],['dbt'],[],[],,
Green Worldwide Shipping LLC,3.7,"Aurora, CO",GEM Data Engineer,"About Green
“Global Reach with a personal touch”, that’s the foundation on which four partners (and long-time friends) agreed to when they started Green Worldwide Shipping back in 2008. This ethos has allowed Green to Grow to a 200+ strong team with 15 offices across the US and growing.At green we are a family of innovative problem-solvers who communicate freely to present creative solutions in an environment of trust, respect, and integrity. We are dedicated to customer service, passionate about logistics, and committed to sustainable growth while inspiring others.
Why this role is special..
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping.We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
How you will be successful…
Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our databases using Altova Mapforce ETL software.
Collaborate with cross-functional teams to understand data requirements and provide optimal data solutions.
Develop and optimize SQL queries to retrieve and manipulate data from databases, ensuring efficient data retrieval and storage.
Implement and integrate EDI/API connection protocols to enable seamless data exchange between internal and external systems.
Monitor and maintain the performance, availability, and integrity of the databases and data infrastructure.
Support for reporting, document modification, workflow and database support for internal ERP.
Troubleshoot and resolve data-related issues, including data quality and data integration problems.
Implement data security measures and ensure compliance with data privacy regulations.
Stay updated with industry best practices and emerging technologies in data engineering and incorporate them into the data infrastructure.
Provide technical guidance and support to other team members on data-related projects.
Collaborate with data analysts and data scientists to understand data requirements and support their data exploration and analysis needs.
Create and maintain documentation related to data pipelines, database schemas, and data transformation processes.
What you will bring….
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Prefer strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
*
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
What we offer:
Employee High Deductible Health Plan (HDHP) currently paid for 100% by Green.
$1,800 contributed to employee HSA Account, if on HDHP.
Employee Dental provided by Green.
Option to purchase disability insurance.
Paid life insurance and option to buy-up.
401(K) and company match.
Parental Leave Program.
Monthly recognition points to give/receive, which can be converted to gift cards.
Tuition Reimbursement Program.
Weekly fruit baskets in office locations
A tree planted for every new employee to Trees.org
Hybrid work schedule.
Wellbeing Program.
Training and Development.
Access to Mentorship Program.
Green Worldwide Shipping, LLC (Green) is committed to the full inclusion of all qualified individuals. As part of this commitment, Green will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process and/or to perform essential job functions, please contact Faye Hughes, Head of People and Culture at faye.hughes@greenworldwide.com.Green is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Requirements
Bachelor's degree in Computer Science, Information Systems, or a related field.
Preferred experience in CargoWise (or related ERP) with reporting, document modification, workflow and database support
Proven experience working with SQL and databases, preferably MySQL.
Strong knowledge and hands-on experience with Altova Flowforce Server and Altova Mapforce ETL software, or related applications.
Experience in integrating and working with EDI/API connection protocols.
Proficiency in Python and/or C# (optional but preferred).
Familiarity with data modeling concepts and techniques.
Experience with data warehousing concepts and technologies.
Excellent problem-solving and analytical skills.
Strong attention to detail and the ability to work in a fast-paced environment.
Effective communication skills to collaborate with cross-functional teams.
Ability to work independently and prioritize tasks effectively.
Preferred Skills:
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud Platform).
Knowledge of big data technologies (e.g., Hadoop, Spark) and distributed computing.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git).
Understanding of Agile development methodologies.
Summary
GEM (Green Engineered Management) is a technology solutions department that supports various commercial technology products, data management, and development for Green Worldwide Shipping. We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure and various technology systems to support efficient data processing and analysis. Your expertise in SQL, CargoWise, database management, Altova Flowforce Server, Altova Mapforce ETL software, Python and C#, along with experience in EDI/API connection protocols integration, will be instrumental in achieving success in this role.
Job Type: Full-time
Pay: $60,000.00 - $75,000.00 per year
Benefits:
401(k) matching
Dental insurance
Employee discount
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Tuition reimbursement
Vision insurance
Experience level:
4 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Aurora, CO 80014: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL/MySQL (Required)
ETL Software (Preferred)
Data warehouse (Preferred)
Python/C# (Preferred)
Work Location: Hybrid remote in Aurora, CO 80014",67500,51 to 200 Employees,Company - Private,Transportation & Logistics,Shipping & Trucking,2008,$25 to $100 million (USD),CO,15,data engineer,na,"['sql', 'python']","['azure', 'aws', 'google cloud']","['tableau', 'power bi']",['mysql'],"['spark', 'hadoop']",[],bachelor,2-5 years
Repstor,4.3,"Charlotte, NC",Data Engineer,"The Role:
We're hiring a hands-on Data Engineer to be part of Data Solutions organization. The data solutions team builds out tools and infrastructure needed to source, validate, clean, and process the data and build compelling reports for leaders and integrating our systems. Our team is looking for a Data Engineer to help scale our data efforts. If you have passion for data and want to help build Intapp’s next gen data platform that provide actionable insights to drive customer and business outcomes, we’d love to hear from you.
You will contribute to the full Data development and apps integrations life cycle, including design, modeling, data integrations, unit testing, performance tuning, and deployment activities.
What you’ll do:
Work collaboratively with business leaders to identify and define the business requirements, design technical solution, and data architecture for self-serve and provide actionable insights.
You can build highly scalable end-to-end data integrations pipelines using different open source tools and operationalizing them.
Define data integration points, data flow, data strategy and develop functional and technical specifications test plans/scripts to validate the data integration needs, Build and sanity test release libraries.
Build infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, cloud based relational or non-relational databases employing ETL tools (Informatica, Talend, Workato) and/or scripting languages like Python and streaming technologies like Kafka, Kinesis that allows users to self-serve themselves.
Apply technologies to solve complex data problems with expert knowledge in programming languages like SQL, Python, Linux, SQL, Hive, Spark and serverless services like Athena, Spectrum
Build and deploy dashboards using one or more visualization tools such as tableau, Power BI
Define optimized DB schemas/semantic layer that power the compelling story-telling dashboards.
Hands on experience with web services (AWS, Azure, etc..).
Experience in REST APIs (i.e. ingesting and creating data through the APIs)
Define SLA and acceptable time lags by data source, define QA process, and socialize resolution process to ensure data accuracy and consistency.
Ideal Candidate Must-Haves
BA/BS Degree in Computer Science, any Engineering discipline, and 2+ years of experience in
relevant Data engineering and Business Intelligence platforms or Master’s degree in Computer
Science, any Engineering discipline / information technology and 2+ years in relevant Data
engineering and Business Intelligence platforms experience
Scripting languages like Python, Go, bash, etc...
Scripting languages packages for data manipulation (i.e., Python’s Pandas, etc.)
Scripting languages with packages for REST data ingestion
Relational SQL and NoSQL databases
Tableau, Power BI, Sisense or other reporting tools
Working knowledge of DW Concepts (Star vs snowflake schema, CDC, SCD, etc...)
ETL Tools such as Informatica, Talend, Dell Boomi, Workato
Serverless query service – Athena, Spectrum
Experience working with AWS components [EC2, S3, DynamoDB]
Data visualization experience.
Preferred Qualifications:
Big data tools Hadoop, Hive, HDFS, Spark etc.
Experience in CI/CD pipelines
Demonstrated proficiency implementing self-service solutions to empower an organization to generate valuable actionable insights.
Setup, maintain, and implement Kafka topics and processes. #LI-JS2
Intapp provides equal employment opportunities to all qualified applicants and will make hiring decisions without regard to race, color, sex, sexual orientation, gender identity or expression, religion, national origin or ancestry, age, disability, marital status, pregnancy, protected veteran status, protected genetic information, political affiliation, or any other characteristic protected by federal, state or local laws. All offers are contingent upon passing a criminal history and other background checks if applicable to the position.
Please note: Intapp will not hire through text message, social media, or email alone. We will never extend a job offer unless you have been contacted directly by an Intapp recruiter and have participated in the interview process which will generally consist of 3 or more virtual or in person meetings. We post all legitimate job openings on the Intapp Career Site at
https://www.intapp.com/working-at-intapp/
.",100789,501 to 1000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2000,$100 to $500 million (USD),NC,23,data engineer,na,"['nosql', 'sql', 'go', 'python']","['azure', 'aws', 'snowflake']","['tableau', 'power bi']","['dynamodb', 'snowflake', 'hive']","['kafka', 'hadoop', 'spark']",['bash'],master,+10 years
1point system,4.0,Remote,Senior Data Engineer,"REQUIRED Top 3 Technical skills:
1. MS Sql and .Net experience
2. PostGras, Mongo
3. Experience with BI tools – Tableau,
4. Cloud Technologies – Azure and AWS
5. Python, Java – Nice to haves
Typical Day to Day: They need someone who can come in and roll up their sleeves, get in the trenches and getting things done. Need equal parts soft skills like attention to detail, time management, customer service; lots of technical skills. They will be working with the data team to build, design and develop the data infrastructure systems. This person will work with others and to gather data requirements, implement data solutions and ensure performance of the data to the pipelines. They will be working through an Azure Data Lake.
Job Title: Data Engineer
Job Location: Fully Remote – But ideally someone who can work east coast hours.
Roles
and Responsibilities
· Design and develop scalable, efficient, and reliable data pipelines and ETL processes to process large volumes of structured and unstructured data.
· Collaborate with stakeholders to understand data requirements and translate them into technical specifications and data models.
· Architect, implement, and manage data storage and retrieval systems, including databases, data warehouses, and data lakes.
· Optimize data pipelines and processes for performance, scalability, and data quality, ensuring timely and accurate data delivery.
· Perform data profiling, validation, and cleansing to maintain data integrity and consistency.
· Identify and resolve data-related issues, ensuring the accuracy and reliability of data.
· Implement and maintain data governance practices, data security measures, and data access controls.
· Stay up to date with the latest advancements in data engineering technologies and methodologies and evaluate the potential impact on our data infrastructure.
· Mentor and provide guidance to data engineers, fostering their professional growth and development.
· Collaborate with cross-functional teams to drive data-driven decision-making and provide actionable insights.
· Communicate effectively with technical and non-technical audiences.
· Compile and maintain clear and comprehensive documentation for all data products, services, and platforms.
Working Conditions:
· Ability to collaborate in a team working environment.
· Demonstrates strong collaboration and communication skills (written and verbal).
· Active listener able to translate problems into creative solutions.
· Possesses excellent customer service skills.
Qualifications and Requirements:
· Bachelor’s degree in computer science, Information Systems, or a related field.
· Proven experience as a Data Engineer or similar role, with at least 7 years of experience in data engineering.
· Strong programming skills in languages like Python, Java, .NET, with experience in data manipulation and transformation.
· Proficiency in SQL and experience with relational databases (e.g., MS SQL, MySQL, PostgreSQL) and NoSQL databases (e.g., MongoDB) and common data formats (e.g., JSON, XML, CSV).
· In-depth knowledge of data modeling, data warehousing, and data integration techniques and best practices.
· Hands-on experience with big data technologies and distributed computing frameworks is highly desirable.
· Familiarity with cloud-based data platforms (e.g., AWS, Azure, Google Cloud) and services.
· Strong understanding of data quality, data governance, and data security principles.
· Experience with data visualization tools (e.g., Tableau, Power BI) is a plus.
· Excellent problem-solving and analytical skills, with a strong attention to detail.
· Effective communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.
· Proven ability to work collaboratively in a team environment and lead technical initiatives.
Job Type: Contract
Salary: $84,454.31 - $190,806.62 per year
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",137630,,,,,-1,,Remote,-1,data engineer,senior,"['sql', 'java', 'nosql', 'python']","['azure', 'aws', 'google cloud']","['tableau', 'power bi']","['postgresql', 'mongodb', 'mysql']",[],[],bachelor,0-2 years
Charles Schwab,4.1,"Westlake, TX",Associate - Data Engineer,"Your Opportunity

This full-time role is part of a nine-month NERD (New Employee Recruitment and Development) program that blends on-the-job experience with an extensive training curriculum that covers tools, technologies, processes, and soft skills required to be successful in Schwab Technology Services. By pairing the curriculum, on-the-job experience, and a web-of-support from others, NERDs are well prepared to
succeed in their role which sets the table for future opportunities at Schwab.

What you’ll do
As a member of the NERD program, you will be working in our Data and Rep Technology (DaRT) organization that governs the strategy and implementation of the enterprise data warehouse and emerging data platforms. You will collaborate with partners across the firm to build the next generation analytics platform and capabilities for Charles Schwab. DaRT supports executive leadership, Sales, Marketing, and Finance teams by integrating and analyzing data to help them make data-based decisions through four primary opportunities:

Data Science Frameworks
This team partners with our data scientists and business stakeholders to design intuitive data solutions and create the best-suited serving pipelines for their modeling and analysis needs. You will help drive the deployment of cross-functional applications, determine platform design and architecture, and set the vision to build and scale our machine learning and experimentation platforms.

Data Engineering
This team designs, develops, and implements enterprise data integration solutions. You will have the opportunity to partner with other developers to set the future of the Data Warehouse through exciting and challenging projects and learning and using emerging technologies.

Data Analytics
This team performs data analysis of enterprise integration solutions to transform data into actionable insights leveraging best-in-class technologies, analytics, and visualization tools.

Platform and Production Support
This team builds next-gen enterprise data platforms, manages currency upkeep of existing platform assets, and matures those investments. The core functions include platform engineering, tenant-focused governance, capacity management, software upgrades, performance optimization, administration, lifecycle management, and maintenance.

What you will get out of the program
Mentorship
Challenging Career Opportunities
Continuous Training and Development
Certification Opportunities
Hands-on Technology Experience
Knowledge Sharing and Presentation Opportunities
Exposure to Leadership
Community of Dedicated and Welcoming Peers

Workplace Flexibility Program
We're proud to support our employees in a working approach that allows you to bring your best self to work – whether that’s in the office or remote.
Employees will have the flexibility of a hybrid work environment, spending some time working remotely and sometimes in the office.
Employees and managers can discuss and decide what works best for them, with flexibility available based on their role, business needs, and individual circumstances.
What you are good at

You enjoy problem solving
You have a hunger for knowledge
You work well with others
You are a good communicator
What you have

Undergraduate or graduate degree in Computer Science, Management Information Systems, or related discipline with a graduation date of August 2023 or earlier and/or
Workforce training certifications through coding bootcamps with a graduation date of August 2023 or earlier
Ability to start full-time with the program on September 18, 2023
Basic understanding of data modeling
Understanding of SQL development principles
Experience with algorithm design
Basic understanding of object-oriented analysis and design
Familiarity with data structures
Experience with data engineering, Extract, Transform, Load (ETL), Hadoop, MongoDB, Unix, Sqoop, HiveQL, or Pig Scripting is a plus
Inventiveness and eagerness to work with experimental technology
Demonstrated leadership potential
Passion and interest in solving problems applying innovation and experimentation",88000,10000+ Employees,Company - Public,Financial Services,Investment & Asset Management,1973,$10+ billion (USD),TX,50,data engineer,na,['sql'],[],[],['mongodb'],['hadoop'],[],,
Core4ce,4.3,"Herndon, VA",Data Engineer,"The Data Engineer will provide the engineering support to data science and software engineering team members.
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers
Make sure pedigree and provenance of the data is maintained such that the access to data is protected
Clean and preprocess data to enable analytic access
Architects complex, repeatable ETL processes
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic and others
Supports Experience with Targeting using Sponsor Tools, Reverse Engineering
Develop API connectors to enable ingest of new data catalog entries from databases and files
Ensure that data mappings will provide the best performance for expected user experience
Augmenting technical support in the area of data science, data engineering, and systems engineering, and reviewing and providing technical assessments.
Provides current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments, effective communication with users and managers, and server administration, to include hardware and software support to existing servers.
Support ad-hoc data analysis requirements defined by the client's Leadership.
Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives.
Will work on Data cleaning and transformation efforts in delivery of CDRLs.
Supports Deliverables and Reports

Requirements
5+ years experience working with Data Sets
Experienced in extracting and aggregating structured and unstructured data.
Experienced in data programming languages and tools such as Python and R.
Experience with SQL or similar database language.
Experience designing and implementing data models to enable, sustain and enhance the value of information they contain
Knowledge of business intelligence reporting tools and data visualization software including Tableau.
Strong analytical and critical thinking skills.
Ability to work collaboratively and effectively in a team environment.
TS/SCI with Full Scope Poly Required

All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status

Required Skills

Required Experience",103496,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2018,$100 to $500 million (USD),VA,5,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],"['mongodb', 'oracle', 'mysql']",['spark'],[],,+10 years
Lcp Tracker Inc,4.4,"New Braunfels, TX",Data Engineer,"Company Summary
LCPtracker, Inc. is a leading software service provider specializing in construction site compliance related software, headquartered in Orange, CA. Our main solution, LCPtracker Pro, is a powerful web-based SaaS solution for collecting, verifying, and managing certified payrolls and other labor compliance related documents. Over 200 government agencies and 100,000 contractors have used LCPtracker for their certified payroll reporting.
In 2023, our growth continues at a rapid pace, making LCPtracker one of the fastest growing small companies in Orange County, California, recognized by the Orange County Business Journal. In 2017, 2018, 2019, 2020, 2021 and 2022, LCPtracker was recognized as an Orange County ""Best Places to Work"" by the Orange County Register.

Position Summary
As a Data Engineer at LCPtracker, you will leverage your experience with SQL, ETL, and reporting to drive the design and development of data-driven solutions. The position is responsible for performing advanced technical and analytical work in the development and support of standardized and customized reports, as well as the testing and maintaining of data integrity.
Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

Core Competencies
Confidentiality: This role may be privy to confidential and/or sensitive information. Must demonstrate integrity in maintaining confidential and sensitive information and demonstrate strict adherence to organizational policies and procedures.
Communication Proficiency: uses friendly and proficient communication to interact with a wide range of people, frequently exchanging information about office operations.
Time Management: Must manage their own time. They use an electronic calendar in an email program to set meetings, to request others to attend and to coordinate their responses. They respond to requests for attendance at various meetings.
Initiative and Proactivity: Correctly anticipates a need, volunteers readily, and acts without being told to do so. Brings new ideas to the company. Undertakes self-development activities; seeks increased responsibilities; takes calculated risks; looks for and takes advantage of opportunities; asks for and offers help when needed.
Drive for Results: Is goal-oriented; maintains focus on the objective.
Problem Solving, Personal Judgment: Identifies and resolves problems in a timely manner; gathers and analyzes information skillfully; develops alternative solutions; works well in group problem-solving situations; uses reason even when dealing with emotional topics. Solicits and applies feedback.
Quality Management: Looks for ways to improve and promote quality; demonstrates accuracy and thoroughness. Does not cut corners; monitors work to ensure quality; applies feedback to improve performance.
Primary Duties and Responsibilities
Develop, modify, maintain, and support custom reports (MS SQL, SSRS) for both ad-hoc and ongoing business needs.
Develop MS SQL objects (tables, stored procedures, functions, views, etc.) as applicable.
Create and customize weekly, monthly, quarterly, and annual reports using Microsoft Excel or other reporting tools as applicable.
Ensure high data quality through regular quality checks.
Extract, filter, and aggregate data through logical queries and programming.
Maintain a high level of confidentiality and use discretion when needed.
Perform other work including specific tasks or special projects as required.
Promote and maintain positive morale through teamwork.
Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

Salary Range
Data Engineer rate $100,000 to $140,000 annual salary
Pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment.

Benefits
Paid Time Off
9 Paid Holidays
Phantom Stock
401k Plan with up to 4% company match
Medical Benefits (Health, Vision and Dental)
Life Insurance
LTD & STD

Work Environment
This position performs its duties from our New Braunfels, TX office. This position operates in a professional office environment and role routinely uses standard office equipment such as computers, phones, mobile devices, photocopiers, filing cabinets and fax machines.

Physical Requirements
While performing the functions of this job, the employee is regularly required to sit; frequently required to talk and hear, use hands and fingers to type, scroll and use computer equipment. The employee is required to have close visual acuity to perform an activity such as: preparing and analyzing data and figures; transcribing; viewing a computer terminal; extensive reading; visual inspection of text/data in both print and electronic forms.
Ability to lift and move up to 25 pounds.
Position Type and Expected Hours of Work
This is a full-time exempt position reporting to our New Braunfels, TX office M-F 8am – 5pm. Days/hours worked are dependent on the workload at the time. General availability and presence in the office is expected during regular business hours Monday-Friday. However, some flexibility is allowed. Occasional evening and weekend work may be required as job duties demand.

Travel
There is no major travel requirement for this position. However, infrequent travel may be necessary to visit remote office(s), attend conferences/industry events, etc. Attendance at our corporate Staff Retreat is required. This event is a 2-3-day retreat. Attendance at our annual User Conference as assigned.

LCPtracker, Inc. is an equal opportunity employer of all qualified individuals. All applicants will be afforded equal opportunity without discrimination because of race, color, religion, sex, sexual orientation, marital status, order of protection status, national origin or ancestry, citizenship status, age, physical or mental disability unrelated to ability, military status or an unfavorable discharge from military service. LCPtracker, Inc. will consider for employment qualified applicants with criminal histories in a manner consistent with all federal, state, and local ordinances.
LCPtracker is committed to the full inclusion of all qualified individuals. In keeping with our commitment, LCPtracker will take steps to assure that people with disabilities are provided reasonable accommodations. Accordingly, if reasonable accommodation is required to fully participate in the job application or interview process, to perform the essential functions of the position, and/or to receive all other benefits and privileges of employment, please contact the LCPtracker Human Resources Department at HR@lcptracker.com.
Education and Experience
MUST HAVE:
Bachelor’s degree in software engineering disciplines, computer science or other related field and/or the equivalent combination of education and experience.
7+ years of experience with SSRS reporting tools and MS SQL server.
7+ years of experience using ETL tools such as SSIS and/or ADF.
7+ years of experience in Data Warehousing with SSAS or AAS.
Experience in applying security to SSAS or AAS models using authentication frameworks such as AAD or Active Directory.
Adept at queries, report writing and presenting findings.
Ability to comprehend, analyze, and systematically compile technical, statistical, and information into comprehensive reports or other formats.
Effective business writing and composition skills with good command of the English language.
Ability to independently plan, organize, and complete a variety of projects within established standards, objectives and time frames.
Ability to work in fast-paced, multi-tasking environment with shifting priorities and demanding deadlines.
Ability to work independently in finding solutions
Ability to work in an agile work environment
Ability to work in a team environment
Must be detailed-oriented and able to effectively prioritize and organize workload, with efficient time management.
Minimum 1-year experience working on Scrum teams.
Basic understanding of the Agile methodology.
Scrum certifications are a plus.
Strong interpersonal communication skills.",120000,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,1992,$1 to $5 million (USD),TX,31,data engineer,na,['sql'],[],"['ssis', 'excel']",['sql server'],[],[],bachelor,+10 years
Premier Consulting Group,4.0,"Boca Raton, FL",Senior Data Engineer,"* The right person could be located in Florida or Massachusetts as company has office locations in Boca Roton and Boston.
* Must be a US Citizen
Overview Summary of the Work:
ETL Azure Data Factory work: A Data Engineer who is to be able to import data from a variety of different sources (SFTP, API, Fileshare) and file types (CSV, Excel, JSON). Company also captures files via email now, but they have a pattern set up to be able to do that, so the person would just repeat how they’ve done that. Check existing Azure Data Factory pipelines for failures and troubleshoot. Familiar with dynamic expression and syntax in Azure Data Factory pipelines.
Backlog of stored procedures: Be skilled in stored procedure development and SQL skills.
Data quality and data cleansing: Be able to evaluate data (incoming and existing). Look for issues and be able to resolve them. In general, someone who can look at the data and mentally make the leap of “hey, this looks wrong.” Look at data and determine how it might be better utilized (examples: 2.5% as a string or 0.025 as a number, identify values as empty strings and save as nulls instead, etc).
Technical Experience Required/Preferred:
SQL, Azure SQL, Azure Data Factory, Data warehousing environments (dims and facts), Snowflake (highly preferred), Python (highly preferred).
Job Types: Full-time, Permanent
Pay: $140,000.00 - $160,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Bonus pay
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Weekend availability
Work Location: Hybrid remote in Boca Raton, FL 33431",150000,,,,,-1,,FL,-1,data engineer,senior,"['sql', 'python']","['azure', 'snowflake']",['excel'],['snowflake'],[],[],,5-10 years
TekValue IT Solutions,4.0,"Houston, TX",Data Engineer with Migration,"Data Engineer Day 1 Onsite Loc Houston Texas Required Skills: Requirements: 8-10 Required Experince on Data Engineer and Revalent Skills Experience with NoSQL (MongoDB) Experience with API'S Good Knowledge on Data Migration like Oracle to Mongo db Experience with Python Programming
Job Type: Contract
Salary: $65.00 - $70.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Houston, TX 77001: Reliably commute or planning to relocate before starting work (Required)
Experience:
MongoDB: 5 years (Preferred)
Data migration: 5 years (Preferred)
NoSQL: 5 years (Preferred)
Python: 3 years (Preferred)
Work Location: One location
Speak with the employer
+91 7328323606",121500,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['nosql', 'python']",[],[],"['mongodb', 'oracle']",[],[],,2-5 years
Seamless.AI,3.4,"Columbus, OH",Data Engineer - Remote US,"The Opportunity
We are seeking an experienced Data Engineer with a minimum of 3 years of experience in building data ingestion pipelines for large datasets. As a key player in our team, you will be responsible for designing, building, and managing our data infrastructure, supporting our data-driven decision-making capability. You'll need to be proficient in Python, AWS, and common frameworks used for data ingestion, transformation, and consolidation. The ideal candidate will be passionate about data, a strong team player, and have a continuous learning mentality.
About Seamless
Seamless delivers the world's best sales leads. Through our product, we help sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence; by development of a robust real-time contact and company search engine as well as a suite of technically-advanced tools to support sales and lead generation. We have been recognized as one of Ohio's fastest growing companies and have been recently ranked No. 7 in LinkedIn's Top 50 Startups of 2022, featured in Forbes as #1 Software company in Ohio in 2022, and on G2's ""Top 100 Highest Satisfaction Products for 2022"" list!
The Seamless Team
We have an amazing culture and work environment that anyone would want to be a part of. We encourage a culture of positivity. We thrive off of continuous feedback and do whatever it takes to help our team and customers be successful. You will grow as an individual, professionally, and be able to see and feel the impact you are making to the growth of Seamless every day.
Role Responsibilities
Design, develop and optimize data ingestion pipelines to handle real-time and batch data streams using a variety of sources.
Utilize your extensive knowledge of Python and AWS to engineer solutions for the transformation, consolidation, and storage of large datasets.
Collaborate with data scientists and other stakeholders to understand data needs and translate them into data systems and pipelines.
Enhance our data ecosystem by leveraging industry best practices for testing, deployment, and runtime environments.
Drive continuous improvements to data reliability, efficiency, and quality.
Document data architectures, procedures, and data flows, maintaining excellent communication with the team and stakeholders.
Monitor data systems performance, troubleshoot data issues, perform root cause analysis, and ensure the implementation of optimal solutions.
Participate in data governance and ensure adherence to data security and privacy standards.
Candidate Requirements
A minimum of 3 years of experience as a Data Engineer or in a similar role.
Strong proficiency with Python, AWS, and common frameworks used in data ingestion and transformation.
Hands-on experience building and optimizing large scale data pipelines, architectures, and data sets.
Knowledge of data warehousing concepts, including data modeling, data cleaning, and ETL processes.
Strong understanding of database design and data management principles.
Experience with AWS cloud services such as EC2, S3, Redshift, DynamoDB, and others.
Strong problem-solving skills, and the ability to analyze data and design solutions to complex data issues.
Excellent communication and teamwork skills, and a passion for data.
Experience with other programming languages (e.g., Java, Scala) is a plus.
Familiarity with big data tools (e.g., Hadoop, Spark) is a plus.
Seamless.AI has been delivering the world's best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio's fastest growing companies and won 2020 Best Places to Work and LinkedIn's Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.",84853,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,OH,8,data engineer,na,"['java', 'scala', 'python']","['aws', 'redshift']",[],['dynamodb'],"['spark', 'hadoop']",[],,2-5 years
HCA Healthcare,3.3,"Nashville, TN",Data Engineer,"Introduction
Last year our HCA Healthcare colleagues invested over 156,000 hours volunteering in our communities. As a Staff Data Engineer with HCA Healthcare you can be a part of an organization that is devoted to giving back!
Benefits
HCA Healthcare, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn more about Employee Benefits
Note: Eligibility for benefits may vary by location.

Would you like to unlock your potential with a leading healthcare provider dedicated to the growth and development of our colleagues? Join the HCA Healthcare family! We will give you the tools and resources you need to succeed in our organization. We are looking for an enthusiastic Staff Data Engineer to help us reach our goals. Unlock your potential!
Job Summary and Qualifications
Data Engineers within HCA’s Information and Analytics organization are responsible for defining and implementing data management practices across the enterprise. This full-time position will focus primarily on enterprise data management and migrating of data to the cloud. Data Engineers are expected to source and incorporate new data sources into the Enterprise Data Ecosystem. The responsibilities will include writing, testing, and reviewing ETL pipelines for defining and implementing data management practices across the enterprise. Due to the emerging and fast-evolving nature of Cloud technology and practice, the position requires that one stay well-informed of technological advancements and be proficient at putting new innovations into effective practice.
As a Data Engineer, you will work closely with all team members to create a modular, scalable solution that addresses current needs, but will also serve as a foundation for future success. The position will be critical in building the team’s engineering practices in test driven development, continuous integration, and automated deployment and is a hands-on team member who actively coaches the team to solve complex problems. This is a leadership position that assumes the responsibility for project success and the upward development of team members. They are the development team's point of contact that must interface with business partners of varying roles ranging from technical staff to executive leadership.
As a Staff Data Engineer level, the role requires 'self-starters' who are proficient in problem solving and capable of bringing clarity to complex situations. It requires contributing to strategic technical direction and system architecture approaches for individual projects and platform migrations. It also requires working closely with others, frequently in a matrixed environment, and with little supervision. This candidate will have a history of increasing responsibility in a small multi-role team. This position requires a candidate who can analyze business requirements, perform design tasks, construct, test, and implement solutions with minimal supervision.
Our Purpose
Applied to this position, your skills will help transform healthcare through technology and solutions that dramatically improve patient care and business operations.
Core Competencies
At HCA ITG, your deliverables will influence patient care. Every process, technology, and decision matters. This role will provide leadership and deep technical expertise in all aspects of solution design and application development for specific business environments. It will focus on setting technical direction on groups of applications and similar technologies as well as taking responsibility for technically robust solutions encompassing all business, architecture, and technology constraints.
Responsible for building and supporting a Cloud based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data. Direct the transformation from HCA Healthcare’s current on premise Teradata platform to Google Cloud Platform to enable analytics and machine learning at scale.
Design the cloud environment from a comprehensive perspective, ensuring that it satisfies all the company’s needs.
Develop, manage, and own full data lifecycle from raw data acquisition through transformation to end user consumption
Share knowledge and experience to contribute to growth of overall team capabilities
Perform activities such as deployment, maintenance, monitoring, and management inside the cloud framework that has been created
Provide guidance on technology choices and design considerations for migrating data to the Cloud
Maintain a holistic view of information assets by creating and maintaining artifacts that illustrate how information is stored, processed, and accessed
Demonstrate deep understanding and act as a leader in the team’s continuous integration and continuous delivery automation pipeline
Collaborate with business analysts, project lead, management, and customers on requirements
Design fit-for-purpose products to ensure products align to the customer's strategic plans and technology road maps
Actively participate in technical group discussions and adopt any new technologies to improve the development and operations.
Assist team members with production issues and offer support, guidance, and assist in communicating issues with appropriate stakeholders when necessary.
Provide leadership on key technology choices for Enterprise Data Ecosystem including data warehouse, analytical and big data platforms.
Ensure architectural, quality, and governance adherence through design reviews.
Education & Experience
Bachelor's degree in computer science or related field - Required
Master's degree in computer science or related field - Preferred
3+ years of experience in Data Engineer/Architect- Required
1+ year(s) of experience in Healthcare - Preferred
8+ years of experience in Information Technology - Required
Knowledge, Skills, Abilities, Behaviors
A successful candidate will have:
Experience developing and supporting data pipelines from various source types (on-prem rdbms, AWS, GCS bucket, flat file) to Big Query utilizing Google Cloud Platform native technologies
Knowledge and experience using the following technologies
o Big Query
o Dataflow, Data Proc, Data Fusion, Cloud Composer
o GSUTIL, GCS, Kafka, Pub/Sub
o Data Catalog/Dataplex
o Python, Unix, Linux
Strong understanding of best practices and standards for cloud application design and implementation.
Extensive experience with relational database management systems; Teradata, Oracle or SQL Server:
o Advanced SQL skills
o Write, tune, and interpret SQL queries
o BTEQs
o Stored procedures
Experience with Unstructured Data
Ability to troubleshoot, maintain, reverse engineer, and optimize existing ETL pipelines.
Requires strong practical experience in agile application development and DevOps discipline, including deployment of CI/CD pipelines in Git
Ability to multitask and to balance competing priorities.
Expertise in planning, implementing, supporting, and tuning Cloud ecosystem environments using a variety of tools and techniques.
Ability to define and utilize best practice techniques and to impose order in a fast-changing environment. Must have strong problem-solving skills.
Strong verbal, written, and interpersonal skills, including a desire to work within a highly-matrixed, team-oriented environment.
A successful candidate may have:
o Experience in Healthcare Domain
o Experience in Patient Data
Certifications (a plus, but not required)
GCP Cloud Professional Data Engineer
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""There is so much good to do in the world and so many different ways to do it.""- Dr. Thomas Frist, Sr.
HCA Healthcare Co-Founder
Be a part of an organization that invests in you! We are reviewing applications for our Staff Data Engineer opening. Qualified candidates will be contacted for interviews. Submit your application and help us raise the bar in patient care!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",93734,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1968,$10+ billion (USD),TN,55,data engineer,na,"['sql', 'python']","['aws', 'google cloud']",[],"['sql server', 'oracle']",['kafka'],[],bachelor,+10 years
NextGenITSupport,4.0,"Raleigh, NC",Network data engineer - onsite,"Job Title: Network Data Engineer.
Must have Skill: HP ARUBA experience.
Rate: $45/hour
Location: On-Site in Raleigh, North Carolina
Job Description:
Experience with HP ARUBA is a must.
Develop and install network infrastructure, configurations and equipment such as routers and switches
Monitor networks and troubleshoot issues or outages.
LAN, WAN, SWITCH & ROUTING experience is needed.
Consult with clients to suggest network solutions
Manage junior employees and provide training resources for team members
Test and install new computer systems, hardware, software and applications
Develop engineering design packages to integrate new processes into existing ones
Collaborate with clients, other tech support services and network providers to ensure the quality of networks
Team player and support on-call responsibilities as HP ARUBA NMS team member
Coordinate upgrades/updates and maintenance of HP ARUBA NMS tool solutions
Collaborate across multiple technical teams and lines of business with focus on implementing enterprise NMS solutions.
Strong problem-solving and troubleshooting skills
Excellent communication and interpersonal abilities
Ability to work independently and as part of a team
Ability to multi-task, prioritize, and manage time efficiently
Highly organized and detail-oriented
Certifications:-
CCNA minimum CCNP preferred.
· 7+ years experience with network planning, design, and implementing LAN, WAN, network security, and wireless network infrastructures.
Job Type: Contract
Pay: $45.00 - $48.00 per hour
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",83700,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,NC,-1,data engineer,na,[],[],[],[],[],[],,+10 years
"DiNi Communications, Inc.",3.0,Remote,Senior Data Engineer,"DiNi Communications, Inc. is seeking a Senior Data Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Skills and experience:
· 12+ years of experience in data engineering with a focus on designing and building scalable data platforms using cloud technologies.
· Strong experience designing and implementing data models and schemas in Snowflake.
· Define Snowflake databases objects to support efficient data storage and retrieval.
· Expert proficiency in SQL and one or more programming languages such as Python, Java, or Scala.
· Knowledge of cloud-based data platforms such as AWS, Azure, or GCP, including experience with cloud-based storage, compute, and data processing services.
Experience designing and implementing ETL and data integration pipelines, and familiarity with data modeling concepts and database design principles.
· Excellent communication skills and the ability to collaborate effectively with cross-functional teams.
· Experience with Agile methodologies and working in an Agile team environment.
· Experience developing production-grade, large-scale data solutions using cloud technologies.
· Experience managing data orchestration at scale using tools such as Airflow and Dagster.
· Familiarity with version control systems (e.g., Git) and CI/CD principles.
Preferred skills:
· Experience developing dashboards and reports in applications such as Oracle Analytics Server (OAS), Microsoft Power BI, and Google Looker.
· Thorough experience with data integration tools such as Informatica Intelligent Cloud Services and MuleSoft.
· Experience using Azure services for Security, Blob Storage, Data Lake, Databricks, Data Factory etc.
· Experience with Azure Monitoring services
· Microsoft Certified Azure Solutions Architect Expert or a Snowpro Certification or a similar one
Tasks include, but are not limited to:
· Collaborate with stakeholders to define and understand data needs.
· Design and develop efficient data architectures that can support large-scale data processing and storage requirements.
· Develop and maintain data pipelines, data models, and ETL processes that align with business requirements, data quality standards, and industry best practices.
· Work closely with other data engineering teams to build and maintain reusable data pipelines and tools, enabling faster time-to-market for data-driven solutions.
· Monitor, troubleshoot, and optimize data pipelines and processes for performance, reliability, and scalability.
· Ensure the quality and integrity of various datasets across different platforms and data sources.
· Continuously evaluate and recommend emerging technologies and methodologies to improve data engineering processes, workflows, and performance.
· Mentor and guide junior data engineers on technical best practices, code reviews, and design patterns to ensure high-quality, scalable, and maintainable data engineering solutions.
· Perform special technology-related projects, as assigned.
Job Type: Full-time
Pay: $80.00 - $95.00 per hour
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Experience:
(12+) Data engineering: 10 years (Required)
Snowflake: 5 years (Preferred)
Expert proficiency in SQL: 10 years (Preferred)
AWS, Azure, or GCP: 5 years (Preferred)
design and implementing ETL and data integration pipelines: 5 years (Preferred)
Agile methodologies: 5 years (Preferred)
developing production-grade, large-scale data solutions: 5 years (Preferred)
managing data orchestration: 5 years (Preferred)
version control systems and CI/CD principles: 4 years (Preferred)
local, state, or federal government entities: 2 years (Preferred)
leveraging Jira, Trello, ServiceNow or other tools: 3 years (Preferred)
Work Location: Remote",157500,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['sql', 'java', 'scala', 'python']","['databricks', 'snowflake', 'aws', 'azure']","['looker', 'power bi']","['snowflake', 'oracle']",[],[],bachelor,2-5 years
INTELETECH GLOBAL INC,3.7,"Altamonte Springs, FL",Data Engineer,"Role: Data Engineer
Location: Florida
Type: Contract ( Only w2 )
Visa: Any
Experience level: Mid-level

The Role: · You'll be developing, deploying, and maintaining our production data pipeline which produces risk scores and patient reports vital to the workflows of doctors and care coordinators.· You'll be ensuring product deliverables are executed reliably and accurately on a regular basis.· You'll be managing and monitoring client interfaces to ensure timely delivery of data.

Qualifications :· Bachelor's Degree in computer science, physics, math, or a related field· Minimum 3+ years experience in data engineering in industrial or clinical settings· Experience in deploying data pipelines to production, including large-scale cloud deployments· Adaptability within a dynamic and collaborative environment· Commitment to improve processes and reduce inefficiencies· Deep curiosity to dive into the details of human research studies

We also appreciate if you have familiarity with the following:

Python
SQL
Cloud platforms (e.g. AWS)
Relational and no-SQL database systems
Multi-modal and sensor data",83235,1 to 50 Employees,Company - Private,Financial Services,Accounting & Tax,-1,Unknown / Non-Applicable,FL,-1,data engineer,na,"['sql', 'python']",['aws'],[],[],[],[],bachelor,+10 years
keasis Inc,4.0,"Wilmington, DE",Big Data Engineer,"Big Data Engineer - (ETL/Data Warehousing/Java)
Wilmington, DE
6+ Month CTH
5 openings
Supporting big data platforms
Going through a modernization and data center migration
Moving from informatica tools to Java/Spark, big data, data modeling etc – Java is a MUST HAVE
Looking for strong sr devs with Java/Spark skillsets
Any modernization or migration experience is a huge plus
Transitioning from ETL skillsets into java/spark
Required Skills:
5+ years of professional experience as a developer in a data warehousing or other data oriented, batch processing environment
5+ years experience in analysis of data or complex processes and systems, demonstrating strong analytical skills
3+ years of hands on experience with Java
2+ years of hands on experience with Spark
3+ years of hands on experience with relational databases such as Teradata or Oracle
3+ years using SQL
2+ years creating complex technical designs which included data mappings
Experience with Unix shell scripting (is a plus)
Experience with Tableau or other BI Reporting tools is a plus
Experience with Ab Initio is a plus
Experience with a scheduling tool, especially Control-M, is a plus.
Experience with issue analysis and resolution including usage of issue resolution processes for application production problems.
Experience working in an Agile setting
Demonstrated ability to work in a team environment with a structured SDLC and interface and coordinate with a variety of business and I/T groups.
Job Type: Contract
Salary: $50.00 - $70.00 per hour
Ability to commute/relocate:
Wilmington, DE 19801: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
How many years of Big Data experience do you have?
Are you ready to work on W2?
Experience:
Informatica: 8 years (Required)
SQL: 5 years (Required)
Data warehouse: 5 years (Required)
Work Location: One location",108000,,,,,-1,,DE,-1,data engineer,na,"['sql', 'java', 'shell']",[],['tableau'],['oracle'],['spark'],[],,5-10 years
Arcon Group Inc,4.0,"Mooresville, NC",Data Engineer,"Job Title: Dot NET Full Stack
Location: Charlotte, NC (Day 1 Onsite)
Duration: 12+ Months
Client: Wells Fargo
only on W2
Interview Mode: Phone & TEAMS
Note: We are looking only at OPT & H1-B
Minimum 3-5 years of experience with C#, .NET
Familiarity with the ASP.NET framework, SQL Server, and design/architectural patterns (e.g. Model-View-Controller (MVC))Experienced in implementing niche solutions with C# and .NET
Abundant experience in designing and writing reusable code with C# and .NET
Experienced with SQL/Oracle/Linux/Windows Servers
Work experience with Oracle, SQL, MySQL Database
Good to Have
Familiarity with Any Cloud Functions
C++/Java/Perl
Power Shell script
SAFE Agile Development
Job Type: Full-time
Salary: $45.00 - $55.00 per hour
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Mooresville, NC 28115: Reliably commute or planning to relocate before starting work (Required)
Education:
Master's (Preferred)
Experience:
Informatica: 1 year (Preferred)
SQL: 2 years (Preferred)
ASP.NET: 1 year (Preferred)
License/Certification:
Driver's License (Preferred)
Work Location: One location",90000,,,,,-1,,NC,-1,data engineer,na,"['sql', 'java', 'shell']",[],[],"['sql server', 'oracle', 'mysql']",[],[],master,0-2 years
Blackspoke,4.9,"Chantilly, VA",Data Engineer,"Own your opportunity to serve as a critical component of our nation’s safety and security. Make an impact by using your expertise to protect our country from threats and help ensure today is safe and tomorrow is smarter. Our work depends on a Data Engineer joining our team to support data analysts, data scientists, and big data engineers in identifying data sources, performing exploratory data analysis, developing data models, ensuring data cleanliness and accuracy to provide new mission-enhancing insights.

What you will be working on:
As the Data Engineer, you will tailor cutting-edge solutions to the unique requirements of our clients and make systems performance and availability your priority.
Expand our mission capabilities in automating data integration and collection strategies
Coordinate system development to include design, modeling, security, integration, and formal testing
Contribute to completion of engineering programs and projects
Develop solutions to a variety of complex engineering problems
Optimize the data ingestion pipeline architecture, develop strategies for efficient ingestion, processing, storage, structuring, and access
Improve responsiveness and overall performance of the data ingestion pipeline architecture
Prepare and maintain documentation for processes and procedures relate to engineering projects

What you will bring to us (Must):
Active TS/SCI with CI Polygraph
Current DoD IAT II Level certification

Would be nice if you bring the following (Highly Desired):
Bachelor's degree in Engineering, Computer Science, or other related analytical, scientific, or technical discipline
3+ years of related experience
Strong Python skills, Experience with messages systems like Kafka, Working experience with ETL processing, Working experience with data workflow products like NiFi
Working knowledge of entity resolution systems, Experience with Lambda functions, Working experience with Python RESTful API services / JDBC

What you will get:
Joining a team of technology experts and partners in the same mission
A high-growth environment with plenty of opportunities to grow your career as the company grows
Owner and Leadership team that come from technical backgrounds so they understand the day-to-day challenges of the technical consulting world and can offer real-life solutions and guidance
Highly competitive benefits package that shows we want to hire the very best

Equal Opportunity Employer/Veterans/Disabled. Individuals with disabilities, including disabled veterans or veterans with service-connected disabilities, are encouraged to apply. If you need assistance applying outside of the online application, please contact recruiting@blackspoke.com for more information.",96671,51 to 200 Employees,Company - Private,Government & Public Administration,National Agencies,2011,$5 to $25 million (USD),VA,12,data engineer,na,['python'],[],[],[],['kafka'],[],bachelor,
Rite Pros,3.4,"Portland, ME",Big Data Engineer,"Big Data Engineer with Bachelor’s degree in Computer Science, Computer Information Systems, Information Technology, or a combination of education and experience equating to the U.S. equivalent of a Bachelor’s degree in one of the aforementioned subjects.
Job Duties:
Define and manage the data architecture framework, standards, and principles, including modeling, metadata, security, reference, and master data.
Define reference architecture, and models to provide a common vocabulary, reusable components, and industry best practices.
Create solution frameworks integrating large or complex data sets.
Lead all data modeling efforts within DataBricks, including the design of data structures and the identification of business transformation logic.
Analyzing and translating business requirements into conceptual and fully detailed logical data models.
Creating logical data models based on existing applications and databases.
Working with business architects, and data stewards to capture business requirements in a Logical Data Model.
Transform the logical representation of the model into a physical representation and work with data engineering team to instantiate and manage the data.
Contribute to assessment of appropriate data platform(s) for solutioning efforts.
Collaborate with the BI and Analytics teams on creating the optimized, reusable semantic model, complete with metadata and lineage information.
Define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models
Maintain the metadata - the ""data about the data"" describing the data model, its structure, and semantics
Recommend data and design patterns for common business functionality
Assist in multi-phase roadmaps within and across functional teams
Provide insight and technology recommendations that result in enterprise solutions and architectural risk mitigation.
Contribute to innovation strategies by exploring, investigating, recommending, benchmarking and implementing new data centric technologies for the platform.
Identify, design, and implement internal process improvements such as automation of manual processes, optimization of data delivery and re-design of existing infrastructure for greater scalability.
Build the infrastructure and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and/or other data technologies.
Create and maintain data-related documentation (e.g. data dictionaries, data maps, data models etc.)
Work with data engineering and analytics experts to strive for greater functionality in our data systems and incorporation of industry best practices
Work experience / Technologies required for the position :
Experience building and optimizing Big Data, Data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, meta data, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected data sets.
Working knowledge of message queuing, stream processing, and highly scalable Big Data, data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience in a Data Engineer or similar roles, who has attained a Bachelor’s degree in Bachelor’s degree in Engineering, Computer Science, Information Technology, related field or equivalent work experience.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Work location is Portland, ME with required travel to client locations throughout USA.

Rite Pros is an equal opportunity employer (EOE).

Please Mail Resumes to:
Rite Pros, Inc.
565 Congress St, Suite # 305
Portland, ME 04101.
Email: resumes@ritepros.com",97894,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Less than $1 million (USD),ME,-1,data engineer,na,"['scala', 'java', 'nosql', 'sql', 'python']","['databricks', 'aws', 'redshift']",[],[],"['kafka', 'spark', 'hadoop']",[],bachelor,
Resolution Life,3.5,"Sydney, FL",Senior Data Engineer,"At Resolution Life, we’re proud to have evolved into a global business under the Resolution Life name. For customers, advisers, companies and the industry. We’re making an impact worldwide
Resolution Life Group is a global life insurance group focusing on the acquisition and management of in-force life insurance policies. With assets of $31 billion and 1.5 million customers, Resolution Life are providing existing customers with life insurance, super and investments.

Why us?
Our platform vision is to be the leading in-force specialist life-insurer in Australasia by 2024, by being customer-obsessed and data-driven.
We are one of the first life insurers globally to operate in an entirely Enterprise Agile environment. The strategic priorities of the platform are focused on ensuring the business is future-fit and sustainable, and to grow through bolt-on acquisition of in-force portfolios. We are guided by core behaviours that inform the way Resolution Life team members show up each day and interact with others.
The Opportunity
The individual will play a hands-on role and work closely with business and technology stakeholders to deliver an enterprise-wide data solution in our data platform to grow and retain data domain expertise to support our data driven vision and reduce our ongoing operating costs and technical debit.
Your Story
Align Architecture, Solution Design with business requirements.
Design, Build and Maintain Physical data model in data platform layers.
Map business glossary, data definition and data sets to physical data model
Build ETL that collects, manage, and convert raw data into consumable tables applying technical logic based on business rules.
Responsible to defining and building job orchestration.
Build of database objects by applying application best practices
Build of reusable components and frameworks based on application best practices.
Build a robust codebase to improve data reliability, efficiency, latency, and quality.
Build code and maintain version control by following Software development lifecycle.
Responsible for unit testing and system testing and system integration testing.
Build a test-driven development approach to identify data anomalies and data quality issues.
Support SIT, UAT and Performance Testing
Support Code deployment and Code stabilisation in UAT, Pre-Prod and Prod environments
Present details technical design at DnA design forum and obtain approval.
Be a key technical leader across data platforms technologies including Snowflake, SQL Server, Informatica Cloud, H2O, DBT, advanced SQL, Power BI and Azure / AWS Cloud
Critical Skills
At Resolution Life, we have identified the following critical skills which are key to success in our culture:
Customer Focused: Passionate drive to delight our customers and offer unique solutions that deliver on their expectations.
Critical Thinking: Thoughtful process of analysing data and problem-solving data to reach a well-reasoned solution.
Team Mentality: Partnering effectively to drive our culture and execute on our common goals.
Business Acumen: Appreciation and understanding of the financial services industry in order to make sound business decisions.
Learning Agility: Openness to new ways of thinking and acquiring new skills to retain a competitive advantage.
What Will We Do For You:
Our culture underpins our values and guides our decision making. It's also what makes Resolution Life a great place to work.
Resolution Life Australasia supports virtual working, and our enduring primary place of work continues to be “virtual” with the physical office and home office used interchangeably. We recognise that our workers can contribute and connect equally regardless of where they are located, and we have seen and experienced the wellbeing and benefits that come from working at home. This means some of us work at home most of the time, in the office most of the time or a balanced mix.
Every day is an opportunity to grow – and we hope to offer our people a career, not just a job.
The learning and development opportunities we offer include supporting the completion of executive-level short courses, access to leading online learning tools, on the job training, and mentoring by highly experienced business leaders.
Join us
Before commencing employment in this role you will need to provide two references, full working rights and complete police and credit checks through an online provider.
As an equal opportunity employer strongly committed to working in a diverse and inclusive workforce you will be provided with any support or accessibility requirements throughout your interview process. Please feel free to contact our Talent Team directly at talent@resolutionlife.com.au.
Privacy Policy
Please refer to our
Privacy Policy
to learn about how we use the information you give us, alternatively you can view the same information by navigating to the page
https://www.resolutionlife.com.au/privacy
.",105758,201 to 500 Employees,Company - Private,Insurance,Insurance Carriers,2003,Unknown / Non-Applicable,FL,20,data engineer,senior,['sql'],"['snowflake', 'aws', 'azure']",['power bi'],"['sql server', 'snowflake', 'dbt']",[],[],,
Colorado Rockies Baseball Club,3.8,"Denver, CO",Senior Data Engineer,"Senior Data Engineer

The Role
The Colorado Rockies Baseball Club is building a platform to house all our baseball data, from scouting reports to baseball statistics and rosters, into an all-encompassing application that will help us more effectively and efficiently make baseball decisions. This role will be responsible for building and maintaining the ingestion, transformation, and cloud storage of baseball data that will drive analysis and decision-making in all facets of Baseball Operations. The Senior Engineer will be a technical leader on the team and help grow less-experienced engineers.

Essential Duties and Responsibilities
Implement ETL pipelines from various external vendors for the use of internal statistical analysis.
Facilitate the aggregation of data for consumption by technical and non-technical users.
Maintain existing pipelines and debug problems that arise.
Work closely with a web development team to deliver the data they need for an internal information application.
Assist less-experienced engineers with their projects to increase their skill sets.

Job Requirements
Bachelor’s degree or completion of an immersive technical program in Computer Science, Information Systems, Computer Engineering, Web Development, or a related field preferred.
At least five years of experience working in data engineering.
Experience and strong understanding of AWS cloud services including S3, Cloudwatch, Glue, and CDK.
Expert-level understanding of relational databases, including SQL Server, MySQL, and Postgres.
In depth experience using a scripting language for data management or analysis, such as Python or R.
Strong understanding of handling and parsing various data formats, including XML, JSON, and CSV.
Relocation and on-site work are required for this position.

Preferred Skills
Strongly prefer experience with commonly used baseball data sources, such as StatCast, EBIS, Trackman and CollegeSplits.
Understanding of modern baseball analysis.
Experience working with Apache Airflow and additional ingestion management tools not mentioned above.

SALARY RANGE:
$115,000 - $135,000 a year. This is a regular status, full-time position eligible for all company benefits including but not limited to Health Insurance (medical, dental, vision), Retirement, and accrued time off (Vacation/ Sick/ Holiday).

EQUAL OPPORTUNITY EMPLOYER:
Rockies baseball is for everyone! We pride ourselves on hiring, developing, and promoting talent as an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, national origin, gender, age, religion, disability, veteran status, or any other category protected by law. In addition, we will endeavor to provide reasonable accommodation to otherwise qualified job applicants and employees with known physical or mental disabilities in compliance with the ADA. All employment and promotion decisions will be decided on the basis of qualifications, merit, and business needs",125000,501 to 1000 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,1991,Less than $1 million (USD),CO,32,data engineer,senior,"['sql', 'r', 'python']",['aws'],[],"['sql server', 'mysql']",[],[],bachelor,
Pyramid Global Hospitality Corporate Offices - Boston,4.0,"Boston, MA",Data Engineer Manager,"About Us:
At Pyramid Global Hospitality, people come first. As a company that values its employees, Pyramid Global Hospitality is dedicated to creating a supportive and inclusive work environment that fosters diversity, growth, development, and wellbeing. Our commitment to a People First culture is reflected in our approach to employee development, employee benefits and our dedication to building meaningful relationships.

Pyramid Global Hospitality offers a range of employment benefits, including comprehensive health insurance, retirement plans, and paid time off, as well as unique perks such as on-site wellness programs, local discounts, and employee rates on hotel stays. In addition, Pyramid Global Hospitality is committed to providing ongoing training and development opportunities to help our people build the skills and knowledge they need to advance their careers.

Whether you are just starting out in the hospitality industry or are a seasoned professional, Pyramid Global Hospitality offers a supportive and collaborative work environment that encourages growth and fosters success, in over 230 properties worldwide. Join their team and experience the benefits of working for a company that values its employees and is committed to creating exceptional guest experiences.

Check out this video for more information on our great company!
Location Description:

Pyramid Global Hospitality (“Pyramid”) is a leading hotel management company, operating in the US, Caribbean, and Western Europe. With portfolio revenues exceeding $3 billion, Pyramid manages 230 hotels, resorts, and conference centers, both branded and independent. The firm maintains offices in Boston (Headquarters), Cincinnati, Houston, and London. Additional information about Pyramid can be found at www.pyramidglobal.com

In 2021, Pyramid and Benchmark Resorts and Hotels merged to add an additional 59 Managed or Asset Managed Resorts and over 10,000 additional team members. The two companies share the same company culture, values and philosophies. We are growing and opportunities abound!

What really sets Pyramid apart from our competitors is our reputation as an employer. Professional growth is not just possible throughout the company but planned and encouraged. The Leadership Team at Pyramid consider team member development its first priority, understanding that success is only achieved in a workplace where every contributor is respected and recognized. This is why we deliver superior results.

There is opportunity to work directly with senior leaders, experience stretch assignments and learn hospitality management from industry giants. You will come to know a distinctive people centric culture that is at the core of all we do. The decisions we make and the paths we take are bound by a commitment to our Owners, Associates, Customers and the Communities where we work. We attract the most talented associates in the industry, and actively encourage candidates with a “hospitality spirit” who may be thinking about a career change to join our team.

Overview:
Our organization is seeking a highly motivated Manager of Data Engineering to oversee and advance our data management capabilities. You will be joining a dynamic team and playing a critical role in managing data pipelines that provide decision-making information used by our field leaders, corporate executives, and property owners. Timely, accurate information is our competitive advantage and this role is pivotal to our success.

The primary focus involves leveraging data management best practices to improve data reliability and quality. This includes ensuring day-to-day operational excellence, enabling new data source integration supporting the company’s aggressive growth plans, and evolving our data capabilities.

This leadership role involves strategic design and operational execution. You will join a team led by the Vice President of Data Sciences and play a crucial part in advancing our data strategy and supporting the company's ever-expanding appetite for information.

Essential Functions:

As the Data Engineering Manager, your responsibilities will include but not be limited to:
Support and improve the data pipeline operations – extract/transform/load data from various sources, manage storage, make data available for analytics
Resolve data integrity issues with measures to permanently address the root cause
Architect data management practices to ensure high-quality data
Drive thought-leadership for optimizing our data pipelines for advanced analytics
Implement best-practice methodologies to accelerate our data velocity
Automate data management processes for more efficient data capture, storage, and availability
Establish a data governance capability to solidify ownership across the data life cycle
Facilitate collaboration with stakeholders from across the company to create improvements in effective data use
Stay current with the latest technologies and methodologies
Other responsibilities as assigned
Qualifications:
A Bachelor's degree in Computer Science, Information Systems, or a related field
Demonstrate data management leadership experience
Microsoft Azure Synapse Analytics ecosystem
Azure SQL Data Warehousing experience
ETL expertise with large, complex, heterogeneous datasets
Accomplished work with PowerBI, star-schemas, and analytics data prep
Solid understanding of data analysis and machine learning
A track record of delivering successful data management improvement projects
Excellent communication and interpersonal skills
Strong problem-solving skills and the ability to work independently
A commitment learning about the latest technologies and methodologies

If you're a highly motivated individual with experience in data management and a desire to drive process improvement, we want to hear from you!
Compensation Range: The compensation for this position is $110,000.00/Yr. - $125,000.00/Yr. based on qualifications and experience.",117500,,,,,-1,,MA,-1,data engineer,na,['sql'],['azure'],[],[],[],[],bachelor,
HARAMAIN SYSTEMS INC.,5.0,"New Richmond, WI",ETL-Data Engineer: Direct hire,"Role : Data Engineer
Location : Roseville MN. (Can be mostly remote, but ideally someone could go in occasionally as needed)

Essential Functions and Accountabilities of the Data Engineer:
Design and build data models and data transformations efficiently and reliably to fit the analytical needs of the business
Develop and maintain ETL processes utilizing Azure Cloud tools such as Azure Data Factory, FiveTran, DataBricks as well as efficient cloud data strategies to minimize costs
Engage with teams to push the boundaries of analytical insights, creating new product features using data, and powering data models
Anticipate, Identify and solve issues concerning data management to improve data quality
Understand and use continuous integration, test driven development and production deployment frameworks using DevOps and GIT
Develop and champion modern Data Engineering concepts to technical audience and business stakeholders
Manage and maintain relational databases, including but not limited to SQL Server
Maintain data integrity and transparency to increase data, reporting and dashboard confidence and consistency across all departments
Advocate for new sources of data to create actionable insights and recommendations
Define and communicate business rules and terms governing use of the data including security and data lifecycle management
Communicate technical and non-technical information clearly, concisely, and effectively both orally and in writing. Present findings, recommendations, and specifications in formal reports and/or oral presentations

Knowledge & Education Requirements:
BA/BS degree in Computer Science, Database Administration or similar work experience
3-5+ years of experience in Data Engineering
2 or more years of experience in data modeling, data architecture and ETL
Experience with tools and concepts related to data and analytics, such as dimensional modeling, reporting tools, data governance, data warehousing, structured and unstructured data
Proficient in at least one major programming language (e.g.. Java Script, Python) and comfortable working with SQL
Advanced understanding of analytics tools and methods, best practices and awareness of new techniques of analytics
Experience working with relational databases like SQL, Postgres and Data Bricks
Experience working with ETL tools like Azure Data Factory, Fivetran and SSIS

This is a remote position.",57125,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['sql', 'java', 'go', 'python']","['databricks', 'azure']",['ssis'],['sql server'],[],[],,+10 years
Moser Consulting,4.6,"Indianapolis, IN",Senior Data Engineer,"About Moser
For more than 25 years we have formed partnerships and grown through open and honest collaboration with our clients, partners, and employees. We are best known for taking great care of our clients, our dedication to creating a work environment where employees do their best work, and our deep commitment to continuous improvement. Our consultants work in a collaborative and fast-paced environment, are self-motivated, and are passionate about evolving technology. It is no accident that we are recognized as one of the Best Places to Work in Indiana for 10 consecutive years.
Internally, we believe in building strong teams from the top down with a focus on values in our Model-Coach-Care philosophy. Our leadership are encouraged and trained to model good practices, mentor other employees and each other, and show empathy and caring in all interactions. This is the base of our core values: Accountability, Balance, Collaboration, Focus, Integrity, Social Responsibility, Support and Transparency.
Moser Consulting believes in equal opportunity for all people and is committed to enabling a diverse, equitable, and inclusive culture. We foster a spirit of unity that respects the remarkable individuality of everyone's culture, history, and service.
Description
We are seeking an experienced Senior Data Engineer to join our team. As a Senior Data Engineer, you will be responsible for designing and implementing complex data pipelines that enable efficient data processing, storage, and retrieval. You will work closely with internal and external stakeholders to ensure that data is available and accessible for business analysis and decision making. You will also be responsible for mentoring and coaching data engineers and collaborating with cross-functional teams to drive data strategy and innovation.
Design, Develop, Deploy, and Monitor complex data pipelines which may contain various required transformations of data using various languages and techniques.
Work with client stakeholders to understand their data needs and design data solutions to meet those needs.
Work with Pre-Sales team to prepare level of effort estimates for incoming projects.
Optimize and tune data processing and storage systems for performance, scalability, and reliability.
Ensure data quality and integrity by implementing data validation, cleansing, and transformation processes.
Stay up-to-date with emerging technologies and trends in data engineering and propose new solutions to improve the data infrastructure.
Identify, design, and implement internal process improvements.
Define and drive data engineering strategy, best practices, and standards across the organization
Requirements
At least 7 years of experience in data engineering.
Strong programming skills in Python, SQL, or other programming languages commonly used in data engineering.
Experience with data warehousing, data modeling, and data architecture.
Strong understanding of ETL processes, data integration, and data transformation.
Experience with cloud-based data solutions such as AWS, Azure, or Google Cloud Platform.
Strong problem-solving and analytical skills.
Strong communication and collaboration skills to work effectively with client stakeholders.
Ability to create functioning ETL prototypes to address quickly changing business needs.
Ability to clearly articulate pros and cons of proposed technologies and solutions.
Familiarity with Azure Data Factory, Azure Data Lake Storage, and/or Snowflake.
Preferred
10+ years of experience in data engineering.
Azure, Databricks, or Snowflake certifications.
Experience with technologies such as Hadoop, Spark, and Kafka.
Understanding of DevOps principles and practices, including Continuous Integration, Continuous Deployment pipelines, and configuration management.
Understanding of Azure services, including Azure Compute, Storage, Network, Security, and Management services. Knowledge of Azure architecture patterns and best practices.
Required Location: Indianapolis, IN
Where You'll Work
Moser has two offices in Indianapolis, IN, and one in Baltimore, MD. This position will require you to be in the Indianapolis, IN area.
Benefits
For more than 25 years, Moser Consulting has been the go-to source for exceptional IT talent with the ability to self-manage. At Moser Consulting, our people are our #1 asset. We hire the best people, welcome them like family, connect them with opportunities, and let them do what they do best: produce innovative solutions to technology problems.
Our culture gives us a competitive advantage by keeping our employees happy, healthy, and by lowering stress levels in a very demanding industry. It is no accident that we are recognized as one of the Best Places to Work in Indiana. We focus on giving employees: an incredible work space; a fun, collaborative, and creative atmosphere; an extremely generous compensation package; and dozens of outstanding and unique perks usually not found at one company.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.",114691,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1996,$25 to $100 million (USD),IN,27,data engineer,senior,"['sql', 'go', 'python']","['snowflake', 'google cloud', 'azure', 'aws', 'databricks']",[],['snowflake'],"['kafka', 'spark', 'hadoop']",[],,5-10 years
S&P Global,4.1,"Raleigh, NC",Data Engineer 1,"The Role : Data Engineer 1
The Team: Content Development technology groups develops and support systems which provide high-quality entity, ownership, and compliance data across a wide range of equity and fixed-income asset classes.

The Impact : The candidate will join the Content Development Ownership team which supports mission-critical Fixed Income & Equity data products.
The job will involve hands-on fault diagnosis, resolution, process creation and optimization, knowledge sharing, and delivery in a high-pressure client-focused environment.

What’s in it for you:
Build a career with a global company.
Grow and improve your skills by working on enterprise-level products and new technologies.
Be part of a dynamic and growing organization that values innovation, creativity, and excellence.
Working with a team of highly skilled, ambitious, and result-oriented professionals.
It’s a fast-paced agile environment that deals with a huge volume of data, so you will have the opportunity to sharpen your data skills and work on an emerging technology stack.

Responsibilities:
Content Development team developers use their passion for programming and problem-solving to produce feature-rich applications that deliver value for our clients. With a dedication to continual improvement, our development team challenges themselves each day to expand their knowledge base and produce solutions that are maintainable, extensible, and elegant. We take great pride in our work, share our expertise and ideas to achieve common goals, and aspire to learn more. We have adopted an Agile Development Methodology and are committed to continually improving within this model.
Success in this role does require knowledge of Python, AWS, and Database technologies in the short term; long-term opportunities working with alternative technologies are expected.

What We’re Looking For:
Bachelor’s in computer science, related field, or equivalent experience.
2+ years of experience in Python.
2+ years of experience with relational databases such as SQL Server or Postgres.
1+ years of experience in processing Big Data using any Cloud Native technology.
Hands-on experience with IDEs such as Visual Studio / IntelliJ / PyCharm.
Must possess strong oral and written communication skills.

The following experience would be advantageous:
Familiarity with CI/CD
Familiarity with NoSQL Databases is a plus

Grade/Level (relevant for internal applicants only): 8
The Location: Raleigh, NC. Hybrid 1 day/week.

S&P Global states that the anticipated base salary range for this position is $63,000 to $91,000. Final base salary for this role will be based on the individual’s geographic location, as well as experience level, skill set, training, licenses and certifications.
In addition to base compensation, this role is eligible for an annual incentive plan. This role is not eligible for additional compensation such as an annual incentive bonus or sales commission plan.

This role is eligible to receive additional S&P Global benefits. For more information on the benefits we provide to our employees, please click here .

-----------------------------------------------------------

Equal Opportunity Employer
S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.

If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person.

US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law.

----------------------------------------------------------- 20 - Professional (EEO-2 Job Categories-United States of America), IFTECH203 - Entry Professional (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)

Job ID: 286434
Posted On: 2023-05-25
Location: Raleigh, North Carolina, United States",77000,10000+ Employees,Company - Public,Management & Consulting,Research & Development,1860,$10+ billion (USD),NC,163,data engineer,na,"['sql', 'nosql', 'python']",['aws'],[],['sql server'],[],[],bachelor,+10 years
BrightFarms Inc,3.8,"Irvington, NY",Data Engineer,"At BrightFarms, we’re on a mission to revolutionize the way leafy greens are grown. But we don’t just want to grow great tasting greens, we want them to do good as well: for the planet, for the health of people, and for the well-being of our employees.
Our passion for building the next generation of farming fuels our passion for nurturing our people. We give BrightFarmers the tools, training, support, and opportunities they need to do better for themselves and the world every day. Because when you do good for your people, they do good for the world.
BrightFarms. The Place to Grow.
Summary:
This is a new position within the existing Business Operations team and you will have the opportunity to establish and shape a data engineering competence within the organization. You will be working closely with our software and IT leads and their expanding teams, and partnering with all functions in the organization as we modernize our approach to data gathering and analysis.
You will be building a Postgres-backed data warehouse for employee and organizational analytics, using BrightFarms' existing data processing frameworks and creating new ones. You will work closely with other stakeholders, developers, data scientists, and analysts to write efficient pipelines aggregating data from various sources. This new position will play a ground-breaking role in driving more informed and objective decisions at BrightFarms by modernizing the way that we collect, manage and consume organizational information.
We’re looking for people who excel at working with others, challenge the status-quo, and are outstanding problem-solvers. We value clear communication, honest feedback, and empathy for the users of our services. This is an exciting time to join BrightFarms, as we embark on next-generation work.
Responsibilities:
Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making
Build ETL pipelines, data warehouses, and aggregate tables.
Interface with stakeholders and data analysts to understand data needs
Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources
Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts
Build data expertise and own data quality for allocated areas of ownership and supporting processes running in production
Choose, propose and use the appropriate technology or programming language for the task at hand.
Provide estimates or project ideas that will influence your team’s roadmap
Qualifications and Education:
A degree (preferably an advanced degree) in Computer Science, Engineering or a related field.
4-6 years of industry experience in architecting and developing data warehousing solutions, preferably in an operational business domain.
Familiar with traditional data warehousing methodologies and hands-on experience in extracting data from various data sources including unstructured excel models and building complex data models.
Experience building applications and managing infrastructure using one of the major cloud providers is preferred. (We use AWS and Azure here at Brightfarms).
Expertise in data ETL, schema design, and dimensional modeling.
Strong programming experience in production environments writing complex SQL queries to pull data into visualization tools and utilizing Python, (PHP as a bonus).
Experience working in an agile environment.
Additional Notes:
May require travel to farms as needed",110791,51 to 200 Employees,Company - Private,Agriculture,Crop Production,2011,$5 to $25 million (USD),NY,12,data engineer,na,"['sql', 'python']","['azure', 'aws']",['excel'],[],[],[],,
Acrisure Technology Group,3.9,"Austin, TX",Data Engineer,"Data Engineer
Hybrid Position (3 days per week average in Downtown Austin, TX or Grand Rapids, MI office)
Note: This is a full-time, in-house position. We do not offer C2C or C2H employment and are not able to sponsor visas for this position.
Acrisure Technology Group (ATG) is a fast-paced, AI-driven team building innovative software to disrupt the $6T+ insurance industry. Our mission is to help the world share its risk more intelligently to power a more vibrant economy. To do this, we are transforming insurance distribution and underwriting into a science.
At the core of our operating model is our technology: we're building the premier AI Factory in the world for risk and applying it at the center of Acrisure, a privately held company recognized as one of the world's top 10 insurance brokerages and the fastest growing insurance brokerage globally. By using the latest technology and advances in AI to push the boundaries of understanding risk, we are systematically converting data into predictions, insights, and choices, and we believe we can remove the constraints associated with scale, scope, and learning that have existed in the insurance industry for centuries.
We are a small team of extremely high-caliber engineers, technologists, and successful startup founders, with diverse backgrounds across industries and technologies. Our engineers have worked at large companies such as Google and Amazon, hedge funds such as Two Sigma and Jump Trading, and a variety of smaller startups that quickly grew such as Indeed, Bazaarvoice, RetailMeNot, and Vrbo.
The Role
The Business Intelligence team's mission is to unify data across the enterprise to optimize business decisions made at the strategic, tactical, and operational levels of the organization. We accomplish this by providing an enterprise data warehouse, data lake, reporting platform, and business processes that provide quality data, in a timely fashion, from any channel of the company and present them in such a manner as to maximize the value of that data for both internal and external customers.
The Data Engineer is responsible for designing and developing moderate to complex ETL processes required to populate a data lake and structured data warehouse which supply data for the machine learning, AI & BI teams. Responsibility includes working with a team of contracted developers as well as coaching and mentoring junior and mid-level developers. Ensuring high quality and best practices are maintained through the development cycle is key to this position.
You will interact with some of the top technologists on the planet. Our technology runs on Google Cloud and is configured with Kubernetes, leveraging various services in that environment. Our data storage layer includes BigQuery, BigTable, and Postgres. We code primarily in Kotlin, Python, Java, and JavaScript and make use of many frameworks, including Dataflow, Cloud AI Platform, KubeFlow, Spring, and React.
Here are some of the ways in which you'll achieve impact
Leverage established guidelines and custom designs to create complex ETL processes to meet the needs of the business
Develop from strategic and non-strategic data sources including data preparation/ETL and modeling for data visualizations in a self-service platform
Contribute to the definition and development of the overall reporting roadmap
Translate reporting requirements into reporting models, visualizations and reports by having a strong understanding of the enterprise architecture
Standardize reporting that helps generate efficiencies, optimization, and end user standards
Integrate dashboards and reports from a variety of sources, ensuring that they adhere to data quality, usability, and business rule standards
Independently determine methods and procedures for new or existing requirements and functionality
Work closely with analysts and data engineers to identify opportunities and assess improvements of our products and services
Contribute to workshops with the business user community to further their knowledge and use of the data ecosystem
Produce and maintain accurate project documentation
Collaborate with various data providers to resolve dashboard, reporting and data related issues
Perform Data Services reporting benchmarking, enhancements, optimizations, and platform analytics
Participate in the research, development, and adoption of trends in reporting and analytics
Mentor BI Developers and BI Analysts
Other projects as assigned in order to support necessary business goals across teams
You may be fit for this role if you have
Minimum 5 years required, particularly in an Azure environment with Azure Data Bricks, Azure Data Factory, Azure Data Lake
Minimum 5 years designing data warehouses, data modeling, and end-to-end ETL processes in a MS-SQL environment
Minimum 2 years developing machine learning models with Azure ML, ML Flow, BQML
Expert working knowledge of SQL, Python and Spark (and ideally PySpark) with a demonstrated ability to create ad-hoc SQL queries to analyze data, create prototypes, etc required.
Successfully delivered 2+ end to end projects – from Inception to Execution - in Data Engineering / Data Science / Data Integration as a Tech Senior/Principal
Ability to Analyze, summarize, and characterize large or small data sets with varying degrees of fidelity or quality, and identify and explain any insights or patterns within them.
Experience with multi-source data warehouses
Strong skills in in data analytics and reporting, particularly with Power BI
Experience with other cloud environments (GCS, AWS) a definite plus
Strong experience creating reports, dashboards, and/or summarizing large amounts of data into actionable intelligence to drive business decisions required
Strong understanding of core principles of data science and machine learning; experience developing solutions using related tools and libraries
Hands on experience building logical data models and physical data models and using tools like ER/Studio/Idera
Write SQL fluently, recognize and correct inefficient or error-prone SQL, and perform test-driven validation of SQL queries and their results
Proficient in writing Spark sql using complex syntax and logic like analytic functions etc.
Well versed in Data Lake & Delta Lake Concepts
Well versed in Databricks usage in dealing with Delta tables (external \ managed)
Well versed with Key Vault \ create & maintenance and usage of secrets in both Databricks & ADF
Should be knowledgeable in Stored procedures \ functions and be able to use them by ADF & Databricks as this is a widely used Practice internally
Familiar with DevOps process for Azure artifacts and database artifacts
Well versed with ADF concepts like chaining pipelines, passing parameters, using APIs for ADF & Databricks to perform various activities.
Experience creating and sharing standards, best practices, documentation, and reference examples for data warehouse, integration/ETL systems, and end user reporting
Apply disciplined approach to testing software and data, identifying data anomalies, and correcting both data errors and their root causes
Academics: Undergraduate degree preferred or equivalent experience along with a demonstrated desire for continuing education and improvement
Location: Austin, TX or Grand Rapids, MI
We are interested in every qualified candidate who is eligible to work in the United States. We are not able to sponsor visas for this position.",96659,5001 to 10000 Employees,Company - Private,Insurance,Insurance Agencies & Brokerages,2005,$1 to $5 billion (USD),TX,18,data engineer,na,"['sql', 'java', 'python']","['databricks', 'azure', 'aws', 'google cloud']",['power bi'],[],['spark'],[],,2-5 years
Slide Insurance LLC,5.0,"Tampa, FL",Data Engineer,"Calling all innovators ready to take a proactive approach to Insurance in the digital world!
Slide is a Tampa based insurtech company, recently named a
**2023 BEST PLACES TO WORK** by Tampa Bay Business Journal

Slide is looking for a Data Engineer who will assist in the designing, building, and maintaining large-scale data processing systems.
**This position is located in Tampa at our Corporate Headquarters and candidate must reside already in Tampa region**

We do not offer Sponsorship opportunities for any of our positions.
Duties & Responsibilities:
Design, build, and maintain scalable data pipelines and ETL processes.
Collaborate with cross-functional teams to integrate new data sources and optimize data flows.
Ensure the reliability and availability of our data infrastructure through monitoring and proactive maintenance.
Automate and streamline data processing workflows to increase efficiency and reduce manual effort.
Continuously evaluate and implement new technologies to improve the performance and scalability of our data infrastructure.
Manage data storage and retrieval using cloud-based technologies such as Amazon S3, Azure Blob Storage, or Google Cloud Storage.
Perform other duties as assigned.

Education, Experience & Licensing Requirements:
Bachelors degree or equivalent education and work experience required.
3+ year's experience in data warehouse development required.
3+ year's experience with SQL required.

Qualifications/Skills/Competencies:
Strong knowledge of ETL tools (SSIS, SSAS, SSRS) required.
Knowledge of SQL Code Performance Tuning and optimizing SQL Code.
Familiarity with code repositories such as Github, Gitlab, and Bitbucket.
Cloud Data Warehouse technologies (ADF, Azure SQL, Databricks) experience a plus
Experience with cloud-based data storage and processing technologies like AWS, Azure, or GCP.
Experience with data modeling and schema design.
Comfortable in a fast-paced agile process, embracing TDD and CI/CD practices.
Strong interpersonal skills.
Excellent verbal and written communication skills.
Ability to work independently and with a team and prioritize effectively.
Ability to think critically and objectively.
Desire to live Slide's Core Values.

What's in it for you?? A paycheck of course but really so much more!
*****
The Slide Vibe - An opportunity to be a part of a fun and innovation-driven culture fueled by Passion, Purpose and Technology!
*****
Benefits - We have extensive and cost-effective benefits that cover you and your family from every angle... Physical Health, Emotional Health, Financial Health, Social Health, and Professional Health",91541,51 to 200 Employees,Company - Private,Insurance,Insurance Carriers,2021,Unknown / Non-Applicable,FL,2,data engineer,na,['sql'],"['databricks', 'azure', 'aws', 'google cloud']",['ssis'],[],[],['gitlab'],,+10 years
DocuSign,3.7,"San Francisco, CA",Senior Data Engineer,"Company Overview

DocuSign helps organizations connect and automate how they agree. Our flagship product, eSignature, is the world’s #1 way to sign electronically on practically any device, from virtually anywhere, at any time. Today, more than a million customers and a billion users in over 180 countries use DocuSign to accelerate the process of doing business and simplify people’s lives.


What you'll do

DocuSign is seeking a talented and results-oriented Senior Data Engineer to focus on delivering trusted data to the business. The Senior Data Engineer delivers data for analytics using our Enterprise Data Warehouse, enabling the global DocuSign analytics community via curated, governed and cleansed data. As a member of the Global Data and Analytics team, the Data Engineer leverages a variety of technologies to accomplish this goal, ranging tools like Airflow, Matillion, dbt, Snowflake and Fivetran to languages like SQL and Python. The successful candidate will develop solutions with innovative cloud technologies, work on a variety of fast-paced assignments, and partner with world-class technical and business teams to maximize the value of data.

This position is an individual contributor role reporting to the Manager, Data Analytics.

Responsibility
Build data pipelines using Fivetran, dbt/Matillion, Snowflake and Airflow
Develop and maintain data documentation including ERD, data dictionaries, data lineage and metadata
Ensure data quality and integrity by implementing appropriate data validation and cleansing techniques
Monitor and troubleshoot data pipelines to identify and resolve issues in a timely manner
Build POCs to validate new concepts and new technologies
Collaborate with business, engineering, and data science teams to understand their data needs and design efficient solutions to support their requirements


Job Designation

Hybrid:
Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)


What you bring

Basic
Bachelor’s Degree in Computer Science, Data Analytics, Information Systems, relevant field or equivalent work experience
8+ years of relevant experience
Experience with database and data warehouse concepts such as facts and dimensions to design and develop data models that support enterprise reporting and analytics needs
5+ years of dimensional and relational data modeling experience
Experience with modern data integration and transformation tools such as Fivetran, Dbt, and Matillion
Experience with workflow orchestration tools such as Airflow
Experience with MPP databases like Snowflake, Redshift and BigQuery
Experience with cloud platforms like AWS, Azure and GCP
Experience with versioning tools like git
Experience working with tools like Jira and Confluence
Experience with SQL and Python
Experience with document and data debugging

Preferred
Ability to work independently with minimal supervision, as well as in a team environment
Excellent communication skills
Eye for detail, good data intuition, and a passion for data quality
Comfortable working in a rapidly changing environment with ambiguous requirements
Organizational and time management skills, with the ability to prioritize tasks and meet deadlines


Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

California: $130,500 - $208,050 base salary

Illinois and Colorado: $123,700 - $174,700 base salary

Washington and New York (including NYC metro area): $123,700 - $184,275 base salary

This role is also eligible for bonus, equity and
benefits.


Life at DocuSign

Working here
DocuSign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At DocuSign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation
DocuSign provides reasonable accommodations for qualified individuals with disabilities in job application procedures. If you need such an accommodation, including if you need accommodation to properly utilize our online system, you may contact us at accommodations@docusign.com.

If you experience any technical difficulties or issues during the application process, or with our interview tools, please reach out to us at taops@docusign.com for assistance.

Our global benefits
Paid time off
Take time to unwind with earned days off, plus paid company holidays based on your region.
Paid parental leave
Take up to six months off with your child after birth, adoption or foster care placement.
Full health benefits
Options for 100% employer-paid health plans from day one of employment.
Retirement plans
Select retirement and pension programs with potential for employer contributions.
Learning & development
Grow your career with coaching, online courses and education reimbursements.
Compassionate care leave
Paid time off following the loss of a loved one and other life-changing events.",149200,5001 to 10000 Employees,Company - Public,Information Technology,Information Technology Support Services,2003,$1 to $5 billion (USD),CA,20,data engineer,senior,"['sql', 'python']","['snowflake', 'aws', 'azure', 'redshift']",[],"['snowflake', 'dbt']",[],[],bachelor,+10 years
Long Finch Technologies,4.6,"Bellevue, WA",Data Engineer- AZURE with Strong SPARK,"Job Title: Data Engineer
Location: Bellevue, WA Onsite
Duration: Full Time
Job Description
Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Job Type: Full-time
Salary: $100,000.00 - $105,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Bellevue, WA 98004: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 5 years (Required)
Azure: 3 years (Required)
Spark: 3 years (Required)
Work Location: One location",102500,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$25 to $100 million (USD),WA,11,data engineer,na,['sql'],['azure'],['power bi'],['sql server'],['spark'],[],,2-5 years
HCA Healthcare,3.3,"Nashville, TN",Principal Data Engineer,"Introduction
Last year our HCA Healthcare colleagues invested over 156,000 hours volunteering in our communities. As a Principal Data Engineer with HCA Healthcare you can be a part of an organization that is devoted to giving back!
Benefits
HCA Healthcare, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn more about Employee Benefits
Note: Eligibility for benefits may vary by location.

Would you like to unlock your potential with a leading healthcare provider dedicated to the growth and development of our colleagues? Join the HCA Healthcare family! We will give you the tools and resources you need to succeed in our organization. We are looking for an enthusiastic Principal Data Engineer to help us reach our goals. Unlock your potential!
Job Summary and Qualifications
HCA Healthcare ITG
Job Summary:
The role requires working closely with others, frequently in a matrixed environment, and with little supervision. As a Principal Data Engineer/Architect level, the role requires 'self-starters' who are proficient in problem solving and capable of bringing clarity to complex situations. It requires contributing to strategic technical direction and system architecture approaches for individual projects and platform migrations. The culture of the organization places an emphasis on teamwork, so social and interpersonal skills are equally important as technical capability. Due to the emerging and fast-evolving nature of GCP/Big Data technology and practice, the position requires that one stay well-informed of technological advancements and be proficient at putting new innovations into effective practice.
Responsible for leading GCP development efforts, driving adoption and appropriate use of technology and consulting on internal and external development efforts to ensure code quality and sound architecture. This position that assumes the responsibility for project success and the upward development of team members technical skills. They are the development team's point of contact that must interface with business partners of varying roles ranging from technical staff to executive leadership. In addition, this candidate will have a history of increasing responsibility in a multi-role team. This position requires a candidate who can analyze business requirements, perform design tasks, construct, test, and implement cutting-edge technical data solutions with minimal supervision.
As a Principal Data Engineer/Architect, you will work closely with all team members to create a modular, scalable solution that addresses current needs, but will also serve as a foundation for future success. The position will be critical in building the team’s engineering practices in test driven development, continuous integration, and automated deployment and is a hands-on team member who actively coaches the team to solve complex problems. She / he will be responsible for the design, development, performance and support of the Cloud Platform components.
This candidate will have a record of accomplishment of participation in successful projects in a fast-paced, mixed team (consultant and employee) environment. In addition, the applicant must be willing to train and mentor other developers to prepare them for assuming the responsibilities.
General Responsibilities:
Responsible for building and supporting a GCP/Hadoop-based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data.
Bring new data sources into GCP/HDFS, transform and load to databases.
Lead projects in delivering the data and projects on-time
Closely collaborates with team members to successfully execute development initiatives using Agile practices and principles
Leads efforts to design, development, deploy, and support software systems
Experience with HL7, FHIR, and Whistle mapping.
Collaborates with business analysts, project lead, management and customers on requirements
Participates in large-scale development projects involving multiple areas outside of core team
Designs fit-for-purpose products to ensure products align to the customer's strategic plans and technology road maps
Demonstrates deep understanding and coaches’ value-based decision making and Agile principles across teams
Coaches team on clinical data, existing system structure, constraints and deficiencies with product
Shares knowledge and experience to contribute to growth of overall team capabilities
Participates in the deployment, change, configuration, management, administration and maintenance of deployment process and systems
Work closely with management, architects and other teams to develop and implement the projects.
Actively participate in technical group discussions and adopt any new technologies to improve the development and operations.
Focuses on customer satisfaction
Rapidly prototypes and delivers just-in-time solutions
Gather requirements, designs, constructs and delivers solutions with minimal team interaction
Works in an environment with rapidly changing business requirements and priorities
Demonstrates deep understanding and acts as a leader in the team’s continuous integration and continuous delivery automation pipeline
Work collaboratively with Data Scientists, business, and IT leaders throughout the company to understand Cloud/Big Data needs and use cases.
Education, Experience and Certifications:
Bachelor's Degree in computer science or related field – Required
Master's Degree in computer science or related field – Preferred
3+ years of experience in Data Engineer – Required
1+ year(s) of experience in Healthcare – Preferred
10+ years of experience in Information Technology – Required
CCDH (Cloudera Certified Developer for Apache Hadoop) – Preferred
GCP Cloud Professional Data Engineer – Preferred
Other Required Qualifications:
A successful candidate will have:

Strong understanding of best practices and standards for GCP/Hadoop application design and implementation.
Hands on experience with Big Data Technologies and experience with many of the following components:
Hadoop, MapReduce, Spark, Impala, Hive, Solr, YARN
Flume, Spark Streaming, Kafka
HBase or Cassandra
SQL, JSON, Avro, Parquet
Two Year of hands-on experience with GCP platform and experience with many of the following components:
GCS, Cloud Run, Cloud Functions
Bigtable, Cloud SQL
Kafka, Pub/Sub
Python, Spark, Scala or Java
BigQuery, Dataflow, Data Fusion
OpenShift, Docker
Experience with Unstructured Data
Understanding of Lambda Design Architectures and Real-Time Streaming
Ability to multitask and to balance competing priorities.
Requires strong practical experience in agile application development, file systems management, and DevOps discipline and practice using short-cycle iterations to deliver continuous business value.
Expertise in planning, implementing, supporting, and tuning Cloud/Hadoop ecosystem environments using a variety of tools and techniques.
Knowledge of all facets of Cloud/Hadoop ecosystem development including ideation, design, implementation, tuning, and operational support.
A successful candidate may have:
Experience in Healthcare Domain
Experience in Patient Data
Experience with Natural Language Processing (NLP)
Hardware/Operating Systems:
Linux, UNIX
GCP
Distributed, highly-scalable processing environments
Databases:
NoSQL, HBase, Cassandra, MongoDB, Cosmos, In-memory, Columnar, other emerging technologies
Other Languages – Java, Python, Scala, R
Build Systems – TFS, Github
Ability to integrate tools outside of the core Cloud/Hadoop ecosystem
Physical Demands/Working Conditions
Prolonged sitting or standing at computer workstation including use of mouse, keyboard, and monitor.
Requires ability to provide after-hours support.
Occasional Travel: The job may require travel from time- to-time, but not on a regular basis.
HCA Healthcare’s Information Technology Group (ITG) delivers healthcare IT products and services to HCA Healthcare's portfolio of business and partners, including Parallon, HealthTrust and Sarah Cannon.

For decades, ITG has been a pioneer in the industry, leading the transformation of healthcare into a new era of quality and connectivity. ITG relies on the breadth of the organization and depth of technical expertise to advance and enhance today’s healthcare and to enable our physicians and clinicians to provide world-class, innovative care for patients.

ITG employees rally around the noble cause of transforming healthcare through technology and find inspiration in the meaningful work they do—creating a culture that follows our mission statement which begins by saying “above all else we are committed to the care and improvement of human life.”

If you want a career in technology and have a heart for healthcare, apply your expertise to a mission that matters.
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""There is so much good to do in the world and so many different ways to do it.""- Dr. Thomas Frist, Sr.
HCA Healthcare Co-Founder
Be a part of an organization that invests in you! We are reviewing applications for our Principal Data Engineer opening. Qualified candidates will be contacted for interviews. Submit your application and help us raise the bar in patient care!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",116594,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1968,$10+ billion (USD),TN,55,data engineer,senior,"['sql', 'scala', 'java', 'nosql', 'r', 'python']",[],[],"['mongodb', 'hive']","['kafka', 'spark', 'hadoop']",['docker'],bachelor,+10 years
Truelogic Software,4.7,"San Francisco, CA",Sr Data Engineer (Java/ Python/ React) - Marketing (ID:1432),"Truelogic is a leading provider of nearshore staff augmentation services, located in New York. Our team of 500 tech talents is driving digital disruption from Latin America to the top projects in U.S. companies. Truelogic has been helping companies of all sizes to achieve their digital transformation goals.
Would you like to make innovation happen? Have you ever dreamed of building Products that impact millions of users? Nice! Then we have a seat for you on our team!

What are you going to do?
You will have the opportunity to work as a Data Engineer, and will need to have experience in Java, Python, AWS, Snowflake, Kafka, and Apache Flink real-time streaming applications, who is eager to design and deploy large-scale data applications and data governance solutions. The ideal candidate will have a strong background in software development and a passion for building scalable, high-performance systems.
Occupy a unique position in the market, you will enjoy the benefits of both worlds: the quick expansion and agility of a startup combined with the size and profitability of a huge international corporation.
Design and implement software solutions using Java, Python.
Build and maintain real-time streaming applications using Apache Flink and Kafka.
Contribute to the development of software architectures and designs.
Utilize AWS services to build and deploy scalable, reliable systems.
Review code and mentor junior team members.
Troubleshoot and debug software issues.

Why do we need your skills?
Education in the field of Computer Science or equivalent experience.
+5 years of hands-on Experience with software development using Python or Java.
Hands-on experience with SQL.
Knowledge of Data Governance and Role base access control.
Knowledge of Cloud databases like Snowflake.
Basic knowledge of design techniques involved in building object-oriented software applications.
Good verbal and written communication skills, problem-solving skills, and interpersonal skills.",129759,501 to 1000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2003,Unknown / Non-Applicable,CA,20,data engineer,senior,"['sql', 'java', 'python']","['snowflake', 'aws']",[],['snowflake'],"['kafka', 'flink']",[],,5-10 years
Dealer Tire,3.6,"Cleveland, OH",Data Engineer,"Who We Are
We’re Dealer Tire, a family-owned, international distributor of tires and parts established in 1918 in Cleveland, OH. We’re laser focused on helping the world’s largest and most trusted auto manufacturers grow their tire business—in fact, we’ve sold more than 60 million tires to date. We’re a thriving company, and we’re looking for driven individuals to join our team. That’s where you come in!
As a Data Engineer at Dealer Tire, you will be part of a highly skilled team of innovative data professionals who are responsible for designing and implementing our enterprise data lake, data model, and the ETL/ELT pipelines that feed our business and analytical systems. You and your teammates will collaborate with internal and external customers and our affiliate companies, empowering them to solve business problems and gain powerful insights using high-performance datasets, enterprise analytics systems, and self-service tools such as Alteryx, R, Python, and Power BI.
As a Data Engineer, your essential job functions will include the following
Collaborate with business and IT teams to implement a comprehensive and easily expandable Enterprise Data Model based on business need.
Be a thought leader on future data usage (Data Futurist) and a catalyst for converting data into a knowledge base.
Contribute, build, and participate in the design and enhancement of our enterprise data model, enterprise analytics systems, data marts, and data warehouse/data lake.
Build robust, scalable, and high-performing ETL/ELT solutions involving structured & unstructured data.
Leverage appropriate design patterns for the problem being solved, such as near-real-time/change data capture, batch processing, streaming data, etc.
Collaborate with internal and external customers, our affiliates, and other IT teams to define and implement a roadmap for the enterprise data model and the systems that support it, including the maintenance of consistent data entity and element definitions across multiple environments.
Participate in design reviews to foster team accountability and maintain a high standard of quality.
Evangelize and democratize our data assets and empower users with self-service tools and training to enable them to leverage those assets effectively.
Engage with multiple concurrent projects both within your team and with other teams.
Articulate ideas and architectural concepts clearly and concisely through verbal and written communication.
Monitor and measure system and process performance and make corrections and enhancements where needed as part of a focus on continual improvement.
Provide on-call/after-hours support for processes and systems owned by the team on a rotating basis.
Other Duties as Assigned
Position Requirements
3-5 years of experience in a Data Engineer or comparable role is required.
Strong skills in querying databases (t-SQL, PL-SQL, etc.), data modeling (including data warehouse design concepts such as star schema design, etc.), data engineering, and data reverse engineering skills are required.
Strong verbal and written communication skills are required.
Ability to work cross-functionally in a fast-paced, high growth environment and manage multiple concurrent workstreams and priorities is a must.
A bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field is a plus.
Experience with any of the following technologies is a plus: SAP Data Integrator, Syniti Data Replication, Microsoft SQL Server, SQL Server Integration Services (SSIS), Oracle Business Intelligence/Oracle Analytics Server, JD Edwards, DB2, Python, Power Shell, R, Scala, Go, Amazon AWS (EC2 instances, Redshift, lambda functions, S3, etc.), Google Analytics, Google Big Query, Snowflake, Alteryx, Power BI, and Windows and Linux servers.
Experience performing root cause analysis on datasets and processes to answer specific business questions, identify opportunities for improvement, or resolve system or process issues is a plus.
General expertise in enterprise IT architecture (databases, ERP, middleware, UI, networking, infrastructure) is a plus.
Experience implementing high availability (HA) and disaster recovery (DR) solutions is a plus.
Competencies Required
Results Orientation
Agility
Initiative
Influence
Customer Focus
Business Acumen
Strategic Thinking
Organizational Agility
Relationship Building
Physical Job Requirements
Continuous viewing from and inputting data to a computer screen
Sitting for long periods of time
Travel as necessary
Drug Policy:
Dealer Tire is a drug-free environment. All applicants being considered for employment must pass a pre-employment drug screen before beginning work.
Why Dealer Tire: An amazing opportunity to join a growing organization, built on the efforts of hard working, innovative, and team-oriented people. We offer a competitive salary + bonus, and a comprehensive benefit package including: paid time off, medical, dental, vision, and 401k matching (50% on the dollar up to 7% of employee contribution).
EOE Statement: Dealer Tire is an Equal Employment Opportunity (EEO) employer and does not discriminate on the basis of race, color, national origin, religion, gender, age, veteran status, political affiliation, sexual orientation, marital status or disability (in compliance with the Americans with Disabilities Act*), or any other legally protected status, with respect to employment opportunities.
ADA Disclosure: Any candidate who feels that they may need an accommodation to complete this application, or any portions of same, based on the impact of a disability should contact Dealer Tire’s Human Resources Department to discuss your specific needs. Please feel free to contact us at ADAAAccommodation@dealertire.com or via phone at 833-483-8232.",92386,501 to 1000 Employees,Company - Private,Retail & Wholesale,Wholesale,2001,$1 to $5 billion (USD),OH,22,data engineer,na,"['scala', 'shell', 'sql', 'r', 'python', 'go']","['snowflake', 'aws', 'redshift']","['sap', 'power bi', 'ssis']","['sql server', 'snowflake', 'oracle']",[],[],bachelor,
Decision Point Healthcare,5.0,"Boston, MA",Data Engineer,"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a data engineer, you will be responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, operation, and management of our client data hubs, including data intake, data quality assessment/evaluation and data curation and enrichment/preparation processes. Our client data hubs consist of various health plan data sources to support Decision Point services including our AI/ML platform, analytics platform and OPUS application. If this interests you, read on.
The position:
Design and develop scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Automate the processing of inbound client data feeds
Design and develop tools to support data profiling and data quality methodologies
Engage with our software engineering team to ensure precise data points per application specification
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering or applicable experience
3+ years of experience with ETL/ELT and data pipeline principles
3+ years of experience with Python, JavaScript, and/or PowerShell
3+ years of SQL experience; Microsoft SQL Server or PostgreSQL preferred
Knowledge of data manipulation methodologies
Excellent verbal and written communication
Strong data profiling skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW and other database design principles
Comfortable working with very large data sets and VLDB environment
Experience with version control tools: Git preferred
Experience with CI/CD such as Jenkins
Ability to write and interpret complex queries towards data-driven business solutions
Understanding of data science and machine learning concepts preferred
Experience working within hybrid cloud environment; AWS experience is a plus
Familiarity with data visualization tools such as Tableau or QuickSight is a plus
Experience with Python and PowerShell
Ability to write and interpret complex queries towards data-driven business solutions
Familiarity with healthcare data is a plus
Why Decision Point?:
We’re as passionate about our people as we are about making our mark on healthcare. Fostering a fun and challenging environment that’s centered around personal and professional growth has brought us to where we are today. We are constantly seeking out new ways to reinvest in our team members because let’s face it, we all do our best work when we feel valued.
Meaningful work
Remote friendly environment
We encourage outside the box ideas
Great healthcare coverage
Competitive compensation",98161,1 to 50 Employees,Company - Public,,,2013,Less than $1 million (USD),MA,10,data engineer,na,"['sql', 'python']",['aws'],['tableau'],"['sql server', 'postgresql']",[],[],,+10 years
Tetra Tech,3.9,"Arlington, VA",Data Engineer/Data Analyst,"Position Summary:
Segue Technologies, A Tetra Tech Company seeks a Data Engineer/Data Analyst to support our Federal Government clients. This position will leverage data to improve Business Intelligence (BI) in order to provide more value to end users.
This is a full-time remote position. Occasional travel expected to the Indian Affairs Office in Albuquerque, NM or nearest Indian Affairs office (less than 5%)

Job Duties and Responsibilities include but are not limited to:
Assess data quality, identifying suspect data via value distributions, cross field correlations, etc.
Formulate Extraction, Transformation, and Data Loading strategies to deal with anomalous data in a manner where the impact on the business domain is understood
Model, map and design data structures and data architecture to implement data mart or warehouse
Identify patterns and relationships within data that have explanatory power within a business domain, using appropriate methodologies to provide different types of information, such as Descriptive, Inferential, and Predictive
Design and develop data products that fulfill the business needs, such as reports, visualizations, dashboards, analyses, etc.
Perform and interpret data studies and product experiments concerning new data sources or new uses for existing data sources
Develop prototypes, proof of concepts, predictive models, and custom analysis
Design and build new data set processes for modeling and data mining
Search data for relevant patterns and significant relationships
Recognize non-uniformity of data and resolve inconsistencies
Devise tools to help business users gain new insights and improve overall business performance
Additional tasks as required

Required Skills:
Bachelor’s degree in Data Sciences, Statistics, Computer Science, IT Systems, or another technical field, or related major. Relevant technical experience may be substituted for the bachelor’s degree
Must have expert level Tableau server and cloud skillset and experience
Must have expert level proficiency in using Microsoft Excel with strong emphasis on data analysis
Experience developing data pipelines and ETL for dashboards, reports and for data warehousing
Ability to organize, interpret and present data
Experience identifying patterns and relationships within data
Relational database design, modeling (theory) experience
Data warehousing design, modeling (theory) experience
Data quality assessment and impact management experience
Experience communicating and working with stakeholders across the enterprise
Experience architecting and developing data pipeline and related processes
Experience working with data modeling/design tools such as SQL Developer Designer, etc.
Experience with SQL, SQL Server RDBMS, NoSQL databases, TOAD, and Data modeling tools
Excellent problem solving, debugging, and troubleshooting skills
Ability to prepare and interpret complex reports and dashboards
Ability to use independent judgment and to know when to escalate issues
Strong communication skills in multiple formats, such as writing, explaining, listening, asking, teaching, mentoring, etc.
Must hold or be able to pass a Federal Background investigation to obtain a T1 (also known as Public Trust or NACI)

Desired Skills:
Experience using AWS Cloud resources and data services
Experience working with IBM Maximo and Facilities related data
Segue Technologies is a wholly owned subsidiary of Tetra Tech, Inc. Segue is based out of Arlington, VA, with a presence in 14 states and DC. We support Federal and DoD organizations to develop and enhance mission-critical business systems. We provide custom software applications, solve data management problems, and support the evolution of the mobile workforce.

At Segue Technologies health and safety play a vital role in our success. Segue’s employees work together to comply with all applicable health & safety practices and protocols, including health orders and regulations related to COVID-19 that are mandated by local, state and federal authorities.

Our compensation package includes: Competitive Annual Salaries, Rewards and Recognition Program, Employee Stock Purchase Plan, Paid Time Off that Increases with Seniority, Paid Holidays, Life and Disability Insurance, 401K Retirement Plan with Employer Contribution, Dental, Vision, and Health Insurance, Flexible Spending Account, Tuition and Training Reimbursement.
Segue Technologies, A Tetra Tech Company is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law. Please visit our website, www.seguetech.com/careers, to submit an application.",85833,10000+ Employees,Company - Public,"Construction, Repair & Maintenance Services",Architectural & Engineering Services,1966,$1 to $5 billion (USD),VA,57,data engineer,na,"['sql', 'nosql']",['aws'],"['tableau', 'excel']",['sql server'],[],[],bachelor,
"BlueLabs Analytics, Inc.",4.2,"Washington, DC",Data Engineer II,"About BlueLabs
BlueLabs is a leading provider of analytics services and technology dedicated to helping our partners do the most good with their data. Our team of analysts, scientists, engineers, and strategists hail from diverse backgrounds yet share a passion for using data to solve the world's greatest social and analytical challenges. Since our inception we've worked with more than 400 organizations ranging from government agencies, advocacy groups, unions, political campaigns, and international groups. In addition, we service an ever-expanding portfolio of commercial clients in the automotive, travel, CPG, entertainment, healthcare, media, and telecom industries. Along the way, we've developed some of the most innovative tools available in analytics, media optimization, reporting, and influencer outreach.
About the team
The BlueLabs Civic Tech practice revolutionizes the way government agencies use data to reduce the friction between residents and the essential services they use. Our team develops deep expertise within the areas our clients care about most, then builds data programs that create impact at scale. We work closely with internal government innovation groups, and build on the analytics methodology pioneered in e-commerce, advocacy, politics, and consumer finance.
About the role:
As a Data Engineer, you will play a critical role in our database upgrade roadmap, in support of our work for a large federal healthcare program. The Data Engineer will work with complex and nuanced data pipelines and will be responsible for the continuous development, review, and documentation of data wrangling and pipeline solutions. You should have experience working in a rapid development team in the past in which you were responsible for significant contributions to client data pipeline solutions.
In this position you will:
Develop, test, and operationalize the data pipelines that power our analytics.
Provide visibility into data transformations by designing and implementing data tests throughout existing pipelines.
Extract business logic (ETL/ELT, metrics, metadata) from current data systems into portable cloud-agnostic layers.
Analyze, build, and deploy data models, including relational models for data warehousing.
Plan and maintain data architectures that are aligned with business requirements.
Produce scalable, replicable code and engineering solutions that help automate repetitive data management tasks.
Work closely with data analysts to understand, identify and effectively respond to their specific needs.
Partner with other engineers to deploy and troubleshoot pipeline-related tools, automations, and integrations.
Work as part of a team to maintain a well documented, consistent codebase.
What we are seeking:
3+ years of experience as a contributor to technical projects, such as working with complex data pipelines or software applications.
Experience working with production grade data warehouses using SQL queries and scripting languages (e.g., Python, R).
Experience with SQL transformation tools like dbt.
Strong background in database design and data modeling.
Experience with Snowflake, implementing a modern data stack, and contributing to large scale data migration efforts.
Effective communication skills when working with team members of varied backgrounds, roles, and functions.
Ability to manage your individual priorities and comfortably context-switch between active development, client discussion, and issue response.
Experience delivering on client priorities that operate on a regular deployment schedule.
Passion in applying your skills to our social mission to problem-solve and collaborate within a cross-functional, client-facing team environment.
The ability to successfully attain and maintain a Federal Public Trust background investigation that our government clients require; this includes a requirement that the individual has U.S. Citizenship or U.S. residency for three of the past five years.
What We Offer:
BlueLabs offers a friendly work environment and competitive compensation and benefits package including:
Salary range: $85,000 - $95,000 annually
Premier health insurance plan
401K matching
Unlimited vacation leave
Paid sick, personal, and volunteer leave
13 paid holidays
15 weeks paid parental leave
Professional development stipend & tuition reimbursement
Employee Assistance Program (EAP)
Supportive & collaborative culture
Flexible working hours
Remote friendly (within the U.S.)
And more!
The salary range for candidates who meet the minimum posted qualifications reflects the Company's good faith understanding and belief as to the wage range, and is accurate as of the date of this job posting.

To protect the health and safety of our workforce, as a company policy, BlueLabs strongly encourages all employees to be fully vaccinated against COVID-19 prior to beginning employment. BlueLabs adheres to all federal, state and local COVID-19 vaccination regulations. Except where prohibited by law, applicants who receive a conditional offer of employment will be required to produce proof of vaccination status prior to their first day of employment; if not the offer may be rescinded or employment terminated. BlueLabs will evaluate requests for reasonable accommodations for applicants unable to be vaccinated due to a religious belief, disability, pregnancy, or on an individualized basis in accordance with applicable laws.
At BlueLabs, we celebrate, support and thrive on differences. Not only do they benefit our services, products, and community, but most importantly, they are to the benefit of our team. Qualified people of all races, ethnicities, ages, sex, genders, sexual orientations, national origins, gender identities, marital status, religions, veterans statuses, disabilities and any other protected classes are strongly encouraged to apply. As an equal opportunity workplace and an affirmative action employer, BlueLabs is committed to creating an inclusive environment for all employees. BlueLabs endeavors to make reasonable accommodations to the known physical or mental limitations of qualified applicants with a disability unless the accommodation would impose an undue hardship on the operation of our business. If an applicant believes they require such assistance to complete the application or to participate in an interview, or has any questions or concerns, they should contact the Director, People Operations. BlueLabs participates in E-verify. EEO is the Law.
Collection of Personal Information Notice:
As you are likely aware, by submitting your job application, you are submitting personal information to our company. We collect various categories of personal information, including identifiers, protected classifications, professional or employment related information and sensitive personal information. We may retain and use this information for up to three years, in order to come to a decision on whether or not you are a good fit for our company. We may also retain or use some of this information to comply with any requirements under law, or for purposes of defending ourselves in any litigation. We do not use this information for any other purpose, or share it with third parties, unless you become an employee. To learn more, or to see our fully Notice to Job Applicants, please click here.",90000,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2013,Unknown / Non-Applicable,DC,10,data engineer,na,"['sql', 'r', 'python']",['snowflake'],[],"['snowflake', 'dbt']",[],[],,+10 years
Edelman Financial Engines,3.5,United States,Data Engineer,"Data Engineer (Remote)
At Edelman Financial Engines (EFE), we believe everyone deserves to move their financial life forward!
We know that assets have the power to fund goals. Those numbers represent individual lifetimes filled with hard work and dreams for our clients and generations to follow. Our clients trust us to guide them forward with empathy, integrity and invention. We uphold that same standard of respect and commitment for clients and colleagues alike.
Founded on the idea that financial education is a fundamental right for everyone, Edelman Financial Engines continues to grow and challenge the status quo. We’re moving forward, together. If our purpose-driven commitment inspires you, we invite you to consider joining our team.
This individual is a hardworking, and outcome focused data engineer with experience working with structured and unstructured data including data modeling and developing data pipelines primarily using Python and SQL on AWS cloud platform.
In addition to an earnest desire to help people, we are looking for the ideal candidate to complement the team’s existing talents. For this Data Engineer role, we are seeking a teammate with proven experience in data engineering, data governance and cloud technologies. If you are passionate about data, analytics and helping business leverage data as an asset, this may be an ideal match for you!
Responsibilities:
Continue to scale our cloud data analytics platform to meet the company’s vision and growth
Follow engineering and industry standards
Contribute to the development of data pipelines to publish high-quality data products for data analysts and data scientists
Flawless communication within internal teams and across the organization
Requirements:
2+ years of total experience with a bachelor's degree in computer science and equivalent experience
1-2+ years’ experience working as a developer in Data Engineering with data pipelines and data processing.
Practical hands-on experience with SQL and GitLab or other file control systems
Experience working with data formats such as Apache AVRO, Apache Parquet, and common methods in data transformation
Experience with AWS technology stack and AWS Certification is a strong plus
Dedicated, and ability to work with a geographically distributed diverse team
Excellent analytical, communication and organizational skills
Strong teamwork, coordination, planning and influencing skills
Experience with Agile framework is a plus
Must have desire to continue learning new technologies, ask questions, and improve skills
About Edelman Financial Engines
Since 1986, Edelman Financial Engines has been committed to always acting in the best interests of our clients. We were founded on the belief that all investors – not just the wealthy – deserve access to personal, comprehensive financial planning and investment advice. Today, we are America’s top independent financial planning and investment advisory firm, recognized by Barron’s,1 with 145+ offices2 across the country and entrusted by more than 1.3 million clients to manage more than $242 billion in assets.3 Our unique approach to serving clients combines our advanced methodology and proprietary technology with the attention of a dedicated personal financial planner. Every client’s situation and goals are unique, and the powerful fusion of high-tech and high-touch allows Edelman Financial Engines to deliver the personal plan and financial confidence that everyone deserves.
For more information, please visit EdelmanFinancialEngines.com.
© 2023 Edelman Financial Engines, LLC. Edelman Financial Engines® is a registered trademark of Edelman Financial Engines, LLC. All advisory services provided by Financial Engines Advisors L.L.C., a federally registered investment advisor. Results are not guaranteed. See EdelmanFinancialEngines.com/patent-information for patent information. AM2789819
For California residents, please see the link for the Privacy Notice for Candidates. California law requires that we provide you this notice about the collection and use of your personal information. Please read it carefully.
Edelman Financial Engines encourages success based on our individual merits and abilities without regard to race, color, religion, creed, sex, gender identity or expression, sexual orientation, pregnancy; marital, domestic partner or civil union status; national origin, citizenship, ancestry, ethnic heritage, genetic information, age, legally recognized disability, military service or veteran status.
Accommodations are modifications or adjustments to the hiring process that would enable you to fully participate in that process. If you need assistance to accommodate a disability, you may request one at any time by either contacting your recruiter or HRQ@EdelmanFinancialEngines.com.
1 The Barron’s 2022 Top 100 RIA Firms list, a seven-year ranking of independent advisory firms, is qualitative and quantitative, including assets managed by the firms, technology spending, staff diversity, succession planning and other metrics. Firms elect to participate but do not pay to be included in the ranking. Ranking awarded each September based on data within a 12-month period. Compensation is paid for use and distribution of the rating. Investor experience and returns are not considered. The 2018 ranking refers to Edelman Financial Services, LLC, which combined its advisory business in its entirety with Financial Engines Advisors L.L.C. (FEA) in November 2018. For the same survey, FEA received a precombination ranking of 12th.
2 Edelman Financial Engines data, as of Dec. 31, 2022.
3 Edelman Financial Engines data, as of Dec. 31, 2022.",112889,1001 to 5000 Employees,Company - Private,Financial Services,Investment & Asset Management,1987,Unknown / Non-Applicable,TX,36,data engineer,na,"['sql', 'python']",['aws'],[],[],[],['gitlab'],bachelor,+10 years
Blancas Sandoval & Associates. PA,4.0,"Miami, FL",Data Engineer,"Company Description

Cyncrocity is a startup that tracks global innovation ecosystems and aggregates all upcoming and past conferences, including post-event recordings. Our platform enables users to track and discover events, companies, and thought leaders.

Job Description

We are seeking a Data Engineer to join our team and help us design and implement systems to collect, store, and process large volumes of data scattered across different sources and formats. The ideal candidate should have a strong background in scraping, automation, data entry, and processes. We are looking for a candidate with a wide set of tools and skills who can also lead data entry and curation.

Qualifications

Background in Computer Science, Data Science, or related field

Proficiency in English

High attention to detail & creative problem solver

Additional Qualifications (preferred but not required):
Interest in mapping global innovation ecosystems.

Additional Information

Responsibilities:
Manual data entry to ensure completeness and accuracy of database.

Design and implement systems for large scale data collection from various sources.

Automate data collection and data processing, including scraping and cleaning.

Standardize, combine, and enhance different databases.

Toolbox Requirements:
Strong skills in web scraping tools and techniques

Familiarity with relational databases and Airtable

Strong skills in automation and familiarity with Make (Integromat) or AutoGPT

Google Sheets & Google Alerts,RSS & alternative website watchers

APIs & Integration with social media (Youtube, Twitter, Linkedin)

This is a full-time remote internship contract. To apply, please submit your resume or portfolio. We will contact you if we decide to move forward with your application. Thank you for your interest in Cyncrocity!",94721,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,FL,-1,data engineer,na,[],[],[],[],[],[],,
Tek Ninjas,4.0,"Carrollton, TX",CLOUD BIG DATA ENGINEER,"CLOUD BIG DATA ENGINEER

Looking for Cloud Big Data Engineer in Carrollton, TX area,
Candidate should work on Amazon Web Services (AWS) using EC2 for computing and S3 as storage.
ETL process jobs using Ab-Initio in dynamic, high-volume environment.
Design and deploy well-tuned Ab-Initio graphs for ODS and DSS instances using both Linux and UNIX environments.
Test all applications and transport data to target Warehouse tables, schedule and run extraction and load process by using Informatica Workflow Manager.
Should be familiar with Hadoop, Hive, Pig, Sqoop, HBase, Cassandra, Spark, Spark Streaming, Spark SQL, Oozie, Zookeeper, Kafka, Flume, MapReduce framework, Yarn, Scala and related technicalities.
Req: MS/BS in Computer Science/any engineering/closely related field. Ex: MS+1yr/BS+5yrs in MicroStrategy developer/any software-related experience.

Should be WILLING TO TRAVEL/RELOCATE TO CLIENT SITES THROUGHOUT THE US.

Email: hr@tekninjas.com, Tek Ninjas Solutions, LLC, 4425 Plano Pkwy, Ste#1402, Carrollton, TX 75010.",111153,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'scala']",['aws'],[],['hive'],"['kafka', 'spark', 'hadoop']",[],,
Optimal Inc.,3.6,"Dearborn, MI",Data Engineer-GCP/Alteryx,"Position Description:
Responsible for focusing on data security, data access and related activities to ensure data is efficiently utilized as an enterprise asset.
Define, implement, and maintain security policies for users, ensuring appropriate access based on data classifications and roles.
Provide feedback to enhance the enterprise data governance model as needed.
Partner with data stewards to ensure all information utilized adheres to compliance, access management and control polices.
Support the data security requirements of different functional areas like Ford Credit, MS&S, PD, etc., and all the regional KPI / Metrics initiatives.
Support Access Monitoring, Reviews and Audits
Comply with all Ford, OGC, ISP and ACR practices and procedures.
Skills Required:
Ability to troubleshoot and resolve access-related issues.
Big Data / Hadoop administration - include Ranger Policy development and maintenance.
Big Data / Hadoop development - Hive, MapReduce, Spark, etc.
Strong customer service skills with an ability to communicate complex solution concepts in simple terms while working collaboratively.
Ability to apply data management standards and data governance practices.
Skills Preferred:
GCP - Alteryx - Python
Experience Required:
2+ of experience in cloud platforms like AWS, Azure, and Google Cloud Platform (GCP)
2+ years of experience with automation using Python or similar language.
1+ years of performing Hadoop security administration - include Ranger Policy development and maintenance.
2+ years of experience in Customer Support role, such as call center, Application support or other administration.
Education Required:
Bachelor's degree in computer science, Computer Engineering, Data Analytics, or a closely related field of study",81597,1 to 50 Employees,Nonprofit Organization,Education,Education & Training Services,2004,Unknown / Non-Applicable,MI,19,data engineer,na,['python'],"['azure', 'aws', 'google cloud']",[],['hive'],"['spark', 'hadoop']",[],bachelor,+10 years
Gradient AI,4.8,"Boston, MA",Senior Data Engineer,"Gradient AI:
Gradient AI delivers state-of-the art artificial intelligence and Machine Learning solutions to the trillion-dollar insurance industry. AI has emerged as a disruptive force revolutionizing the way insurance professionals achieve their objectives, and Gradient AI is leading the charge. Our team is made up of Data Science and Insurance Technology experts with a history of building wildly successful technology companies. If you are passionate about making a difference for customers, and collaborating with passionate colleagues, then Gradient AI might be the right place for you.
Role:
If you want to work on cutting-edge technology with friendly, intelligent people in a highly collaborative environment, then Gradient AI is the right place for you. We are looking for a Data Engineer to join our team to design and implement data pipelines, optimize existing models, and create standardization as we move into new lines of business. The ideal candidate is a life-long learner who knows object-oriented coding and has experience with business intelligence reporting tools.
Responsibilities:
Analyze complex data schemas and relationships to implement information models to capture the data and data relationships.
Create ETL processing logic to manage large volume client data sets associated with various types of insurance lines including Worker's Compensation, Health, and Auto.
Integrate 3rd party commercial and public domain data sets to enhance and extend Gradient AI's ML data assets.
Design and implement data pipelines for processing data as part of the overall ML Ops strategy within Gradient AI's Cloud deployment.
Collaborate with data scientists to ""featurize"" the information models and construct Machine Learning models.
Assist software engineering team to design and create integration APIs and applications that sit on top of Gradient AI's ML data assets.
Qualifications:
5+ years of relevant working experience
Fluency in SQL and experience with relational and non-relational/alternative databases; Postgres is a desirable
Demonstrated ability to write effective, scalable Python code (or other similar object-oriented languages)
Experience with ETL / BPM. Exposure to tools like Databricks Amazon Glue, Apache Spark, Apache Airflow is desirable
Exposure working in a cloud-computing environment (e.g., AWS, GCP, Azure)
Comfortable with Linux, including developing shell scripts
Experience working with insurance data is desirable
What We Offer:
We are an equal opportunity employer that offers a number of benefits and perks to accommodate all types. Bring your authentic self to work in our supportive workplace where we offer:
A fun and fast-paced startup culture
A culture of employee engagement, diversity and inclusion
Full benefits package including medical, dental, vision, 401k, disability, life insurance, and more
Unlimited vacation days and ample holidays- we all work hard and take time for ourselves when we need it
Competitive salary and generous stock options - we all get to own a piece of what we're building
Ample opportunities to learn and take on new responsibilities",128983,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2018,Unknown / Non-Applicable,MA,5,data engineer,senior,"['sql', 'shell', 'python']","['databricks', 'azure', 'aws']",[],[],['spark'],[],,+10 years
Cisco Systems,4.4,"Portland, OR",Data Engineer,"Who we are
Cisco’s Workforce Experience organization is one of Cisco’s fastest growing teams, and the Chief Data and Analytics Office (CDAO) group is transforming how Cisco delivers value to our customers & partners via our product portfolio. The CDAO organization is the team that is in the epicenter of developing modern, cloud native technologies in support of the evolving CX Product portfolio. To lay hold of this exciting vision, we are looking for top talent to deliver high performing, distributed engineering teams that will redefine success for Cisco and our customers.



What you'll do
Software is a team sport. The best software is built by the highest functioning teams peopled with supremely skilled & fiercely collaborative engineers. You will throw your lot in with fellow engineers to build high quality, high scale & customer delighting SaaS products. We are looking for engineers with a growth mindset, eager to do their best work and build the best products possible. Simply put, you will deliver excellent software on a results-driven, high-trust team.



Primary Responsibilities include:
Build data pipelines in a scalable cloud environment
Partner closely with product management to identify, scope, estimate work for team
Lead teams in the delivery of data pipelines
Mentor junior engineers to produce their best work
Take ownership for large portions of the platform, across different engineering teams, and help us design, deliver and maintain that code going forward

Who you'll work with
Residing in the WFX organization, you will partner with teams within the WFX and CX organization to design, develop and deliver best in class SaaS solutions. You will work with peer teams in Product Management, Operations and UX Design to build the world’s best customer experience for our customers, partners and delivery engineers. Every day and every interaction will be unique, but you will likely engage with a diverse representation of engineers, product owners, product managers, business units, IT and WFX/CX leadership.



Who you are
Required Skills/Experience
8+ years designing, developing and deploying data pipelines
5+ years of shipping cloud native software in AWS
Experience taking difficult problems and translating them into solutions
Experience in creating and maintaining large scale distributed cloud native technology stacks that achieve over 99.98% uptime
Strong ability to collaborate, finding win-win solutions within Engineering and partnering with Product
Strong communication skills and ability to engage, interface and influence partners
Experience with Spark, Presto, AirFlow,Kinesis , Kafka streams
Experience with AWS Services, Linux, Docker, CloudFormation, Terraform, DynamoDB, Aurora
Experience with Programming languages such as GoLang, Python
Familiarity with AI/ML solutions

#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference powering an inclusive future for all.
We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (36 years strong) and only about hardware, but we’re also a software company. And a security company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!
But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)
Day to day, we focus on the give and take. We give our best, give our egos a break, and give of ourselves (because giving back is built into our DNA.) We take accountability, bold steps, and take difference to heart. Because without diversity of thought and a dedication to equality for all, there is no moving forward.
So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!

Message to applicants applying to work in the U.S.:

When available, the salary range posted for this position reflects the projected hiring range for new hire, full-time salaries in U.S. locations, not including equity or benefits. For non-sales roles the hiring ranges reflect base salary only; employees are also eligible to receive annual bonuses. Hiring ranges for sales positions include base and incentive compensation target. Individual pay is determined by the candidate's hiring location and additional factors, including but not limited to skillset, experience, and relevant education, certifications, or training. Applicants may not be eligible for the full salary range based on their U.S. hiring location. The recruiter can share more details about compensation for the role in your location during the hiring process.
U.S. employees have access to quality medical, dental and vision insurance, a 401(k) plan with a Cisco matching contribution, short and long-term disability coverage, basic life insurance and numerous wellbeing offerings. Employees receive up to twelve paid holidays per calendar year, which includes one floating holiday, plus a day off for their birthday. Employees accrue up to 20 days of Paid Time Off (PTO) each year and have access to paid time away to deal with critical or emergency issues without tapping into their PTO. We offer additional paid time to volunteer and give back to the community. Employees are also able to purchase company stock through our Employee Stock Purchase Program.
Employees on sales plans earn performance-based incentive pay on top of their base salary, which is split between quota and non-quota components. For quota-based incentive pay, Cisco pays at the standard rate of 1% of incentive target for each 1% revenue attainment against the quota up to 100%. Once performance exceeds 100% quota attainment, incentive rates may increase up to five times the standard rate with no cap on incentive compensation. For non-quota-based sales performance elements such as strategic sales objectives, Cisco may pay up to 125% of target. Cisco sales plans do not have a minimum threshold of performance for sales incentive compensation to be paid.",132088,10000+ Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,1984,$10+ billion (USD),OR,39,data engineer,na,['python'],['aws'],[],['dynamodb'],"['kafka', 'spark']","['terraform', 'docker']",,5-10 years
Location3,3.6,United States,Data Engineer,"This is a remote position, but we are currently only able to hire within the U.S.

About the Data Engineer position
We are looking for a Data Engineer who will drive the design, development, and future state of our enterprise data systems.
Location3 is committed to creating a diverse and inclusive company culture, and our team does not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under law. Additionally, Location3 is committed to diverse and equitable hiring practices. If you are a candidate that identifies as diverse and would like to self-identify, you can do so in the application. Providing this information is completely voluntary
Expected Salary Range - $95,000 to $135,000 annually depending on experience.
The Stack:
Front End: Angular, Typescript, RxJs, Nx, Akita, Jasmine, Visual Studio Code
Back End: C# Web API, .Net Core, MongoDb, xUnit, Visual Studio
Azure: Service Bus, App Services, Data Factory, Functions, and WebJobs
Data Warehouse: Azure Synapse Analytics (Synapse Studio, Dedicated SQL Pool, Data Factory, Data Lake Storage)
Architecture: Microservices with RESTFul APIs, event-driven, message-based
Hosted: 100% in Azure using Azure DevOps Build/Release pipelines
Data Engineer Responsibilities:
Responsible for the design, development, and deployment of the enterprise’s overall data strategy to include, but not limited to, data model designs, data collection, storage, transformation, distribution, governance, security, and usage
Review existing data architectures to determine overall effectiveness and compliance with original objectives, develop comprehensive strategies for improving or replacing underperforming areas and present these plans to executives
Designing and integrating new partner data (consumption of new data which shows up in a data lake/storage)
Research new technologies, data modeling methods, and management systems, to determine which ones should be incorporated into the overall company strategy, and develop implementation timelines and cost to deploy and maintain
Maintaining and Monitoring All SQL Databases (Azure Synapse SQL, Azure SQL, and NoSQL)
Convert business needs into data and system requirements, align business processes with IT systems, and manage the complex flow of data and information within the organization
Create & maintain existing ELT Pipelines & Data Flows in Azure Synapse Studio
Create & maintain existing Stored Procedures & Views serving Reporting features
Perform Power BI Shared Dataset Modeling for consumption by our Reporting Team
Provide Power BI technical guidance & best practices to our Reporting Team
Active participant of an Agile/Scrum team
Evangelize data strategies and directives upstream (Executive Leadership) and downstream (Engineering Team)
Report to senior management
Data Engineer Requirements:
MS/BS degree in Computer Science/Engineering or similar
5-10 years experience working as a Data Engineer
Strong knowledge of database structures, data mining, visualization, and machine learning skills
Ability to implement common data management and reporting technologies, data visualization, and predictive analytics
Experience working with structured and unstructured data
Excellent organizational and analytical abilities
Outstanding problem solver
Preferred Skills:
Applicable Industry Certifications
Digital marketing / Social Media experience
Experience working with private and sensitive enterprise information
Power BI, Power Apps, Power Automate, and Power Virtual Agents
In-depth knowledge of a wide range of established and emerging data technologies
Confident in decision making and the ability to explain processes
Interpersonal and customer service skills
Build & maintain Azure DevOps Release Pipelines
About US
Creative Thinkers, Data Geeks & Digital Enthusiasts - Location3 Media is a digital marketing company built to improve the findability and performance of consumer and retail brands through enterprise-level and local digital marketing solutions. Founded in 1999 and located in the heart of Denver, Location3 has a staff of 60+ full-time employees who service global, national and local brands. More than half of Location3's client base has worked with the agency for at least three years, as Location3 improves the findability and performance of every client they partner with.
Why Us?
Location3 is looking for passionate people with innovative thinking who want to work with a performance-driven team. We emphasize working hard to bring our clients the results they seek and celebrating those wins together in a positive and fun work environment. We offer the benefit of being a remote work organization, but we also strongly believe that collaboration is key to driving outcomes. Our company culture, our ongoing education and training programs, and our technology infrastructure all contribute to that goal. We also believe that bonding is equally important - whether it's in-person or over Zoom. We have dedicated annual company events like golf day, ski day, and monthly events like town halls, team happy hours, team trivia and team cooking demonstrations that help to create synergy among colleagues and teams. On top of working and playing hard together, we also offer a very competitive benefits package, complete with medical, dental, vision, matching 401K, a wellbeing stipend, summer and fall Fridays, remote work equipment, and more.",115000,51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1999,$5 to $25 million (USD),TX,24,data engineer,na,"['sql', 'nosql']",['azure'],['power bi'],['mongodb'],[],[],,0-2 years
Zodiac-Solutions,3.3,"Fort Mill, SC",Azure Solution Lead/Sr Data Engineer,"Azure Solution Lead/Sr Data Engineer:
Location: Fort Mill, SC and NJ (Hybrid 2 days onsite)
Experience – 15-20 years
Responsibilities:
Be part of strategic planning in defining the road maps and environments that supports our clients’ Azure data and advanced analytics initiatives.
Provide support in defining the scope and sizing of work.
Working closely with various enterprise architects, Information security teams, Data management team, to ensure the architected solution meets all the needs of a customer, from a functionality perspective and IT solution engineering perspective.
Translate business requirements into technology solutions.
Lead POCs and help our customers in choosing the right technologies to solve the business problems.
Lead designing of all aspects of our data solution including artifact creation such as diagrams, playbooks, and other technical documents.
Mentor and guide Jr. team members to deliver the solutions on-time.
Create various architecture blueprints and, be part of and work with the development team to deliver the vision.
Skills:
Over all 15-20 years of experience with Data Management, Big Data and Analytics.
At least 5 to 8 years of experience in architecting and implementing cloud native data solutions using Microsoft Azure.
Hands-on experience with data architecture, data management, designing and building highly scalable ELT processes.
At least 4 to 5 years of experience with big data using spark and java for data processing.
Apache Spark experience using Scala or PySpark or pre-packaged tools like Databrick is plus.
At least 3 years of experience in implementing the data solutions on MS Azure using Azure Data Factory, MS Synapse.
Good understanding of various file storage formats.
Experience in building and using ARM templates to provision and manage IaC.
Experience in implementing Big Data platform using event-based architecture.
One to two years of experience in Azure Synapse Analytics is plus.
At least one year experience with unified data governance solution using MS Purview.
Developing the CICD pipeline for Azure Infrastructure, version control strategy and Integrate source control (Azure repos)
Experience with java programming is a plus.
Extensive hands-on experience in designing and tuning ETL/ELT process development by using cloud native technologies.
In-depth understanding of various storage services offered by Azure.
Experience with implementation of data security, encryption, PII/PSI legislation, identity and access management across sources and environments.
Experience with data process Orchestration, end-to-end design and build process of Near-Real Time and Batch Data Pipelines.
Certification in Azure data engineering and solution architecture Azure is must.
Strong client-facing communication and facilitation skills.
Job Type: Full-time
Salary: $140,000.00 - $160,000.00 per year
Experience level:
11+ years
Schedule:
8 hour shift
Ability to commute/relocate:
Fort Mill, SC 29707: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Management: 9 years (Preferred)
cloud native: 5 years (Preferred)
Scala: 6 years (Preferred)
Work Location: One location",150000,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),SC,-1,data engineer,senior,"['java', 'scala']",['azure'],[],[],['spark'],[],,5-10 years
TIDAL,4.2,"New York, NY","Senior Data Engineer - Infrastructure, TIDAL","Company Description TIDAL is a global music and entertainment streaming platform committed to creating a deeper connection between artists and fans through its library of more than 80 million songs, over 350,000 high quality videos, and available in over 60 countries. In addition, TIDAL offers its subscribers exclusive access to high-profile music and music videos, original content series, podcasts, documentaries, livestream concerts, tickets, merchandise and live experiences. Together, TIDAL and Block will be music-obsessed and artist-focused while we explore new artist tools, listener experiences, and access to financial systems that help artists be more successful.
Job Description

Data is an integral part of TIDAL’s success, and the Business Intelligence team provides the ability for our company to deliver several key initiatives.
We are hiring a Senior Data Engineer who will lead the efforts to deliver data products that improve the decision-making capabilities in shaping the future of TIDAL. You will be responsible for collecting data about TIDAL’s performance, interpreting that data to identify problem areas and areas of improvement, then sharing that information with others. You will collaborate with several stakeholders, including executives across key disciplines such as Marketing, Product, and Finance & Strategy. The ideal candidate will have both hands on and leadership experience in data engineering.
Responsibilities:
You will lead the technical implementation of data projects within our Data Lakehouse, Business Intelligence reporting solution across TIDAL.You will review and verify customer data, direct the dissemination of data to the data lakehouse, and establish policies and protocols for the collection and examination of data. You will also manage and lead medium to large cross functional technology data projects such as introducing a new self-service model for stakeholders to access and use data.
You Will:
Provide technical leadership for TIDAL data projects
Continuously streamline and improve processes related to the service delivery area.
Collaborate with the Business, Marketing, Finance, and external stakeholders to identify, establish scope and implement solutions that support the company’s growth, scalability, and overall success.
Perform stakeholder management to gather and document data warehouse and reporting requirements to ensure development work meets business objectives
Be an expert in the business’ need for data - understand how it informs their strategy and use that knowledge to deliver new insights.
Provides services and data support, from data lakehouse to business insights, including the data pipelines and analytics systems
Partner with our data partners such as the decentralized analytics teams to help them discover new data solutions, support the development of data products, and promote data best practices.
Serves the needs of data producers by providing a seamless integration for event data and change data capture.
Assists data owners to publish and monitor data streams
Owns the data lakehouse and manage the unified data storage for all teams
Ensures that all data is compliant with GDPR, CCPA, and PII

Qualifications
5+ years of computer science, data science, or engineering experience
Experience in building large scale distributed systems in a product environment
Able to communicate and influence across disparate groups and levels in the organization
Knowledge of database and data warehousing cloud technologies (Redshift, Snowflake, AWS RDS, and S3)
Experience in the Big Data technologies (Hadoop, Hive, Spark, Kafka, Flink etc.)
Proven experience and knowledge of data warehousing and business intelligence, with emphasis on data architecture, sourcing, reporting and analytics through the entire project lifecycle
Demonstrable understanding of development processes and agile methodologies
Strong leadership skills – experience managing a team to drive high-impact work; a proven track record of leading, mentoring, hiring, and scaling Business Intelligence teams

Additional Information

Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.

Zone A: USD $167,300 - USD $204,500
Zone B: USD $158,900 - USD $194,300
Zone C: USD $150,600 - USD $184,000
Zone D: USD $142,200 - USD $173,800
To find a location’s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information.
Benefits include the following:
Healthcare coverage
Retirement Plans including company match
Employee Stock Purchase Program
Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance
Paid parental and caregiving leave
Paid time off
Learning and Development resources
Paid Life insurance, AD&D. and disability benefits
Perks such as WFH reimbursements and free access to caregiving, legal, and discounted resources
This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans.
US and Canada EEOC Statement
We’re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class.
We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible. Want to learn more about what we’re doing to build a workplace that is fair and square? Check out our I+D page.
Additionally, we consider qualified applicants with criminal histories for employment on our team, and always assess candidates on an individualized basis.

Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.",141532,5001 to 10000 Employees,Company - Public,Information Technology,Internet & Web Services,2009,$1 to $5 billion (USD),NY,14,data engineer,senior,['go'],"['snowflake', 'aws', 'redshift']",[],"['snowflake', 'hive']","['kafka', 'flink', 'spark', 'hadoop']",[],,+10 years
Tesla,3.6,"Palo Alto, CA","Data Engineer, Automation and Analytics","What to Expect
The electrical component team in Supply Chain is looking for a highly skilled and motivated Data Engineer to support our Supply Chain Organization. You will plan effective data storage, security, sharing and publishing within the organization, including our Global Supply Managers as well as Design Engineer on a regular basis. You will have the responsibility to maintain large supply chain datasets and formulate applicable data-driven solutions which can effectively utilized by the global supply management team. It is essential that you can think strategically, connecting the dots in the bigger picture, as well as being comfortable in the details of the deliverables. A candidate for this role needs to be a self-motivated for data analytics and process raw, unstructured data using batch and real time processing frameworks.
What You’ll Do
Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result
Drive problem solving and continuous improvement initiatives to improve supply chain operations processes
Ensure data quality and implement tools and frameworks for automating the identification of data quality issues
Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings
Think strategically and considering Tesla’s global supply chain structure
Build and maintain purchasing database on a regular basis
Make data and reporting updates as needed to ensure accuracy
Develop data-based solutions to complex purchasing challenges and be able to present relevant information in non-specialist forum
Identify process gaps and areas for optimization and automation
Own system requirements, and participate in the software development process from design to user acceptance testing
What You’ll Bring
Master’s degree in science or engineering or equivalent experience or evidence of exceptional ability
Strong experience with relational databases like SQL Server, MySQL and Vertica.
Proficiency with SQL, a good understanding of database systems, highly skilled in programming (R, Python). Apache Airflow is a plus
Structured, data-driven and quantitative problem-solving approach
Experience creating dashboards using Tableau (or similar) visualization tools
Experience with design, development, and implementation of highly scalable, high-volume source of truth systems for different business areas, developing and maintaining web services in an agile environment
Experience creating Full-Stack UI is a plus
Work experience in material planning, supply chain, purchasing, or manufacturing is a plus
Strong organizational skills and the ability to prioritize and manage multiple projects simultaneously with complex and demanding deadlines
Exceptional communications, strategic thinking, and interpersonal skills
Proven ability to set expectations, manage to deadlines and hold individuals and teams accountable to critical milestones
Collaborative, flexible, open working style and an ability to establish trust and credibility",112889,10000+ Employees,Company - Public,Manufacturing,Transportation Equipment Manufacturing,2003,$1 to $5 billion (USD),CA,20,data engineer,na,"['sql', 'r', 'python']",[],['tableau'],"['sql server', 'mysql']",[],[],master,
Harvard University,4.3,"Boston, MA","Big Data Engineer, Digital Transformation","Position Description
Be a pioneer in business, education, and global impact by joining the Harvard Business School Digital Transformation team - a “startup with assets,” where you will have the chance to deploy cutting-edge digital- and emerging-technology education solutions. Where else can you make a difference at the intersection of cutting-edge technology, world-class education, noble purpose, and timeless legacy?

As a Big Data Engineer, you will bring to life and implement architecture blueprints and help our team by mapping out solutions to some of their complex technical challenges. You'll provide technical expertise, mitigate risk and offer solutions tailored for their needs. From migrations of existing workloads to building advanced cloud solutions, you'll help shape and build to increase agility, improve security, reduce costs and meet utilization targets. You will work closely with the Data Engineering, Data Science, Infrastructure architecture & Platform teams and will focus on creating blueprints to nourish a culture of engineering excellence. You will report into the Sr. Managing Director Big Data Governance.

Duties and Responsibilities:
Support data engineering needs across HBS, including technical support and training in Big Data frameworks and ways of working, revision and integration of source code, to release and source code quality control
Designing and producing high performing stable end-to-end applications to perform complex processing of batch and streaming massive volumes of data in a multi-tenancy big data platform in the cloud, and output insights back to business systems according to their requirements.
Design and implement core platform capabilities, tools, processes, ways of working and conventions under agile development to support the integration of data sourcing and use cases implementation, towards reusability, to ease up delivery and ensure standardization across data deliverables in the platform.
Working with the Group architecture team to define the strategy for evolving the Big Data capability, including solution architectural decisions aligned with the platform architecture
Defining the technologies to be used on the Big Data Platform and investigating new technologies to identify where they can bring benefits
Collaborating with implementation efforts with teams of technical resources within HBS, Harvard, and vendors as needed
Performing other duties as assigned and/or as necessary
Basic Qualifications
Minimum of five years’ post-secondary education or relevant work experience
Additional Qualifications and Skills
Bachelor’s degree in Mathematics, Physics, Computer Science, Engineering, Statistics, or an equivalent technical discipline or 5 years of experience is required
3-5 years of experience in a hands-on technical role as an engineer and/or data engineering role working in large-scale big data environments is required
Experience with common build tools, unit, integration, functional and performance testing from automation perspective, and continuous delivery, under agile practices is necessary
Expert-level experience in designing, building and managing applications to process large amounts of data in a cloud ecosystem or other big data frameworks is a must
Additional Information
This role has the possibility of being a remote or hybrid position. You must reside in one of the following states: CA, CT, GA, IL, MA, MD, ME, NH, NJ, NY, RI, VA, VT or WA. There may be periodic visits to our Boston, MA based campus. In a hybrid role, you are required to be onsite at our Boston, MA based campus a determined number of days per month. Specific days and schedule will be determined between you and your manager.

We may conduct candidate interviews virtually (phone and/or via Zoom) and/or in-person for this role.

A cover letter is required to be considered for this opportunity.

Harvard Business School will not offer visa sponsorship for this opportunity.

Culture of Inclusion: The work and well-being of HBS is profoundly strengthened by the diversity of our network and our differences in background, culture, national origin, religion, sexual orientation, and life experiences. Explore more about HBS work culture here https://www.hbs.edu/employment .
Benefits
We invite you to visit Harvard’s Total Rewards website to learn more about our outstanding benefits package, which may include:
Paid Time Off: 3-4 weeks of accrued vacation time per year (3 weeks for support staff and 4 weeks for administrative/professional staff), 12 accrued sick days per year, 12.5 holidays plus a Winter Recess in December/January, 3 personal days per year (prorated based on date of hire), and up to 12 weeks of paid leave for new parents who are primary care givers.
Health and Welfare: Comprehensive medical, dental, and vision benefits, disability and life insurance programs, along with voluntary benefits. Most coverage begins as of your start date.
Work/Life and Wellness: Child and elder/adult care resources including on campus childcare centers, Employee Assistance Program, and wellness programs related to stress management, nutrition, meditation, and more.
Retirement: University-funded retirement plan with contributions from 5% to 15% of eligible compensation, based on age and earnings with full vesting after 3 years of service.
Tuition Assistance Program: Competitive program including $40 per class at the Harvard Extension School and reduced tuition through other participating Harvard graduate schools.
Tuition Reimbursement: Program that provides 75% to 90% reimbursement up to $5,250 per calendar year for eligible courses taken at other accredited institutions.
Professional Development: Programs and classes at little or no cost, including through the Harvard Center for Workplace Development and LinkedIn Learning.
Commuting and Transportation: Various commuter options handled through the Parking Office, including discounted parking, half-priced public transportation passes and pre-tax transit passes, biking benefits, and more.
Harvard Facilities Access, Discounts and Perks: Access to Harvard athletic and fitness facilities, libraries, campus events, credit union, and more, as well as discounts to various types of services (legal, financial, etc.) and cultural and leisure activities throughout metro-Boston.
Job Function
Information Technology
Department Office Location
USA - MA - Boston
Job Code
I1458P IT Rprting and Analyt Prof IV
Work Format
Hybrid (partially on-site, partially remote)
Department
Digital Transformation
Sub-Unit
-
Time Status
Full-time
Salary Grade
058
Union
00 - Non Union, Exempt or Temporary
Pre-Employment Screening
Criminal, Education, Identity
Commitment to Equity, Diversity, Inclusion, and Belonging
Harvard University views equity, diversity, inclusion, and belonging as the pathway to achieving inclusive excellence and fostering a campus culture where everyone can thrive. We strive to create a community that draws upon the widest possible pool of talent to unify excellence and diversity while fully embracing individuals from varied backgrounds, cultures, races, identities, life experiences, perspectives, beliefs, and values.
EEO Statement
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, gender identity, sexual orientation, pregnancy and pregnancy-related conditions, or any other characteristic protected by law.",112889,10000+ Employees,College / University,Education,Colleges & Universities,1636,$10+ billion (USD),MA,387,data engineer,na,[],[],[],[],[],[],bachelor,2-5 years
Fidelity Investments,4.3,"Durham, NC",Senior Data Engineer,"Job Description:
The Role
Do you enjoy the challenge of solving problems and working in a fast-paced environment where no two days are the same? Are you able to identify big picture ideas/concepts and translate those into well-defined and executable solutions? If so, the Senior Data Engineer role may be the right next step in your career!

The Team
Data Consulting is a group within Fidelity Workplace Investing built to serve and partner with Fidelity’s strategic clients on their data and technology needs. The Data Technology Team's dynamic and diverse skills are leveraged to complete sophisticated and exciting projects in partnership with both internal and external partners.

We design and build customized solutions to address unique scenarios on behalf of clients. We extract data from a variety of sources (Snowflake, SQLMI, client feeds/files, etc.), transform the data, then output the results based on requirements.

Our team engages across Workplace Consulting in data quality improvement efforts that span single and multiple product lines (i.e. Health and Welfare, Defined Benefit, Defined Contribution, Stock Plan Services).

The Expertise You Have and The Skills You Bring
5+ years in Technology with 3 years in developing and deploying data solutions
Strong understanding of data analysis concepts and associated tools such as SQL, Python, Stored Procedures, etc.
Extensive experience with SQL required; knowledge of MySQL, Snowflake (workspaces) preferred
Proficiency with ETL methods and tools – Informatica and Snap Logic preferred
Systems analysis background with an emphasis on legacy process decomposition and debugging/tuning/refactoring
Strong analytical and problem-solving skills, ability to research and find answers to technical challenges and learn new technologies as required
Prior exposure to data warehousing concepts and dimensional data models
Excellent written and verbal communication skills including experience writing documentation
Excellent collaboration skills to work with multiple teams in the organization
Ability to deal with ambiguity and work in a fast-paced environment
You like to take ownership, resolve issues and care about the quality of your work
Experience with WI Defined Benefit, Defined Contribution, Health & Welfare and/or Stock Plan Service business, data process flows and recordkeeping processes a plus
Knowledge of Fidelity backend product tables (WIDE, Snowflake, DBR1, etc.) a plus
Hands on experience with AWS or Azure cloud concepts is a plus
Project management experience is a plus
The Value You Deliver
Communicating with Business and Technical Consultants across WorkPlace Consulting practice areas on a wide variety of projects, you will analyze and maintain processes, identify data issues, perform root cause analysis, and help us to streamline existing processes.
Please see below for the salary range for work locations in Colorado only:
$64,000 - $100,000 per year
This position is eligible for incentive compensation or an annual bonus opportunity.
Please see below for the salary range for work locations in New York City, Westchester County, NY and Jersey City, NJ only:
N/A
Please see below for the salary range for work locations in California only:
N/A
Please see below for the salary range for work locations in Washington only:
N/A
Certifications:
Company Overview
Fidelity Investments is a privately held company with a mission to strengthen the financial well-being of our clients. We help people invest and plan for their future. We assist companies and non-profit organizations in delivering benefits to their employees. And we provide institutions and independent advisors with investment and technology solutions to help invest their own clients' money.

Join Us
At Fidelity, you'll find endless opportunities to build a meaningful career that positively impacts peoples' lives, including yours. You can take advantage of flexible benefits that support you through every stage of your career, empowering you to thrive at work and at home. Honored with a Glassdoor Employees' Choice Award, we have been recognized by our employees as a Best Place to Work in 2023. And you don't need a finance background to succeed at Fidelity—we offer a range of opportunities for learning so you can build the career you've always imagined.
At Fidelity, our goal is for most people to work flexibly in a way that balances both personal and business needs with time onsite and offsite through what we’re calling “Dynamic Working”. Most associates will have a hybrid schedule with a requirement to work onsite at a Fidelity work location for at least one week, 5 consecutive days, every four weeks. These requirements are subject to change.
We invite you to Find Your Fidelity at fidelitycareers.com.

Fidelity Investments is an equal opportunity employer. We believe that the most effective way to attract, develop and retain a diverse workforce is to build an enduring culture of inclusion and belonging.
Fidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation, contact the HR Accommodation Team by sending an email to accommodations @fmr.com, or by calling 800-835-5099, prompt 2, option 3.",82000,10000+ Employees,Company - Private,Financial Services,Investment & Asset Management,1946,$10+ billion (USD),NC,77,data engineer,senior,"['sql', 'python']","['snowflake', 'aws', 'azure']",[],"['snowflake', 'mysql']",[],[],,2-5 years
CVS Health,3.1,Texas,Data Engineer,"Assists in the development of large-scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs

Applies understanding of key business drivers to accomplish own work

Uses expertise, judgment and precedents to contribute to the resolution of moderately complex problems

Leads portions of initiatives of limited scope, with guidance and direction

Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing

Collaborates with client team to transform data and integrate algorithms and models into automated processes

Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines

Uses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems

Builds data marts and data models to support clients and other internal customers

Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards

Pay Range
The typical pay range for this role is:
Minimum: 70,000
Maximum: 140,000

This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.

In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company's 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (PTO) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.

For more detailed information on available benefits, please visit
jobs.CVSHealth.com/benefits

Required Qualifications
3+ years of progressively complex related experience

Experience with bash shell scripts, UNIX utilities & UNIX Commands

Preferred Qualifications
Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources

Ability to understand complex systems and solve challenging analytical problems

Strong problem-solving skills and critical thinking ability

Strong collaboration and communication skills within and across teams

Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar

Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment

Experience building data transformation and processing solutions

Has strong knowledge of large-scale search applications and building high volume data pipelines

Education
Bachelor's degree required

Master’s degree or PhD preferred

Business Overview
Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.",105000,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1963,$10+ billion (USD),Texas,60,data engineer,na,"['nosql', 'java', 'shell', 'python']",[],[],"['hive', 'mysql']",['hadoop'],['bash'],bachelor,+10 years
Cox powered by Atrium,4.0,United States,Data Engineer - Remote (2023-5940),"Minimum Qualifications:
A minimum of 3+ years related work experience
Working experience with data modeling, data access, schemas, and data storage techniques within Snowflake
Working experience with batch and real-time data processing frameworks
Working experience working with relational databases such as SQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Experience with ETL processes and tools
Preferred Qualifications:
Working experience with Snowflake
Working experience with Google Analytics BigQuery
Working Experience with Tableau Server
Working experience with SQL Server Integration Services
Pay Range:
$40-$49/hr
Requisition Disclaimer:
This job posting is for a temporary role as an employee of Atrium on assignment at Cox. The individual selected for this role will be offered the role as an employee of Atrium; compensation, medical benefits, fringe benefits and other terms and conditions of employment shall be presented by Atrium upon offer. The pay rate range provided is a reasonable estimate of the anticipated compensation range for this job at the time of posting. The actual pay rate will be based on a number of factors, including skills, competencies, experience, location and/or being pursued and other job-related factors permitted by law. In addition, this role will be eligible for overtime pay, in accordance with federal and state requirements

By applying for this position you agree to the Atrium Terms and Conditions. Agreeing to these terms, includes permission to use the email address and mobile phone number you provide during the application process or throughout the duration of your prospective or actual employees to notify you of job openings, profiles, articles, news, and other employment-related information, as well as to notify you of special promotions or additional products and services offered by us or our affiliates and partners (collectively, “Atrium Alerts”). Atrium Alerts may be sent by email, phone or text message. Your personal information will be safely stored in our database. Atrium does not sell your personal information to third parties. Text message and data rates may apply. To OPT OUT of text messaging or to modify your communication preferences for Atrium Alerts at any time, please contact us at privacyadministrator@atriumstaff.com.

If you do not agree with the Atrium Terms and Conditions, you can still complete your application for this position by emailing your resume to our team at coxrecruit@atriumworks.com. Please include the job title in the subject of your email.

As a woman-owned firm, Atrium values diversity. We are an equal opportunity employer and will consider all applications without regard to race, sex, age, color, religion, national origin, veteran status, disability, genetic information or any other characteristic protected by law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Posting: #zip",80100,,,,,-1,,TX,-1,data engineer,na,['sql'],['snowflake'],['tableau'],"['sql server', 'snowflake']",[],[],,
Kent State University,4.2,Ohio,Lead Data Engineer,"Participate in architectural design, optimization, implementation, and administration of our modern data warehousing environment. Work with analysts across the university to determine needs and develop data models to support those needs . Optimize data flow and collection from and between multiple complex cloud, on-premises, and hybrid systems. Reports to designated supervisor.

Additional Basic Function – if applicable:

The Data Management & Analytics team at Kent State University is looking for a Lead Data Engineer to join our team to help us with some exciting data adventures! We are currently working on building a modern analytics solution. Come join us while we build a data lake and data marts that will be used to create a university wide and system agnostic view our data to enable our decision makers to get the data they need and to provide new opportunities in support of predictive analytics and machine learning. Our team is also working with others in the Division of Information Technology to build a data pipeline to enable a consistent, governed way to provide data for application and system integrations.

Examples of Duties:

Duties/essential functions may include, but not be limited to, the following:
Participate in and/or facilitate data discovery sessions with business subject matter experts to translate business needs into enterprise dimensional and relational data models for reporting and analytics.
Design, create and maintain optimal data lake, data mart, data warehouse, and data integration architectures.
Assemble university-wide complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal data process improvements such as automating manual processes, optimizing data delivery, ensuring data integrity, re-designing infrastructure for greater scalability, etc.
Build the infrastructure and integrations required for optimal ETL/ELT of data from a wide variety of complex cloud, hybrid, and on-premises data sources.
Participate in development and implementation of data lifecycle of the modern data warehousing environment.
Lead and educate and mentor team members on best practices. Perform code and model design reviews.
Provide oversight / guidance related to data issues needing technical expertise.
Perform related duties as assigned.
JOB COMPETENCIES:
Communicate effectively with technical and non-technical users.
Maintain cooperative working relationships.
Manage time and effectively balance multiple evolving priorities.
Work effectively with very limited oversight.
Establish estimates and timelines for specific applications/projects and take direct accountability for results.
Establish focused, measurable goals for self and others.

Additional Examples of Duties – if applicable:

Minimum Qualifications:

Bachelor’s degree in computer science, information technology or a related field of study. Minimum of seven years progressive experience in the following:
Analyzing and translating business needs into long-term solution data models.
Experience data modeling principles/methods to design dimensional data models from the ground up.
Working with relational databases, as well as working familiarity with a variety of databases.
Advanced logical data model design, data warehouse design, and data integration.
Performing root cause analysis to perform troubleshooting on data flows through complex systems.
Analyzing internal and external data and processes for a business to build models to answer specific business questions.
Building processes for data transformation.
Developing and maintaining system documentation, metadata, data standards, and data quality metrics for data management systems.
Experience writing advanced SQL.
License/Certification:

Knowledge Of:
Advanced logical data design, data warehouse design, and data integration as well as the management of web content or other unstructured data
Common software application packages and tools for performance monitoring and issues tracking
Testing practices, application debugging, and troubleshooting procedures
Software development life cycle, structured programming, object-oriented design and development techniques, and change management
Managing the development and maintenance of system documentation, metadata, data standards, and data quality metrics for the data management system
Advanced working knowledge of SQL
Skill In:
Time management with the ability to set priorities to coordinate multiple assignments with fluctuating and time-sensitive deadlines
Written and interpersonal communication, with the ability to present complex technical information in a clear and concise manner to a variety of audiences
Ability To:
Foster positive and professional working relationships; effectively handle interpersonal interactions at all levels; and respond appropriately to conflicts and problems
Work with technical and non-technical staff to identify user needs and translate them into technology-based solutions
Keep abreast of industry trends

Preferred Qualifications – if applicable:
Experience working in a higher education institution.
Experience facilitating data discovery sessions to gather requirements needed to build data models.
Experience building solutions to support analytics, machine learning, and data science.
Experience with datalake and data warehouse technologies.
Experience with data analytics tools.
Experience developing solutions that are incorporated into all aspects of an organization’s data services to achieve data governance and data quality best practices.

Assessments:

Asterisk (*) indicates knowledge, skills, abilities which require assessments

Working Conditions / Physical Requirements:

Working Schedule:

Flexible hours and remote work options available

Additional Information:

Must pass a security check.

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.",96808,5001 to 10000 Employees,College / University,Education,Colleges & Universities,1910,$500 million to $1 billion (USD),Ohio,113,data engineer,senior,['sql'],[],[],[],[],[],bachelor,
Rokt,3.8,"New York, NY","Senior Software Engineer, Data Engineering - NYC","Company Description

Jobs for Humanity is dedicated to building an inclusive and just employment ecosystem. Therefore, we have dedicated this job posting to individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or hard of hearding, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. If you identify with any of the following communities do not hesitate to register.

Company Name: Rokt

Job Description

About Rokt
Rokt is the global leader in ecommerce technology, helping companies seize the full potential of every transaction moment to grow revenue and acquire new customers at scale. Live Nation, Groupon, Staples, Lands' End, Fanatics, UrbanStems, GoDaddy, Vistaprint and HelloFresh are among the more than 2,500 leading global businesses and advertisers that are using Rokt's solutions to drive more value through every transaction by offering highly relevant messages to their customers at the moment they are most likely to convert.
With our December 2021 Series E raise of USD$325M, Rokt is expanding rapidly and globally – operating in 19 countries across North America, Europe and the Asia-Pacific region with the largest office in NYC and a major R&D hub in Sydney. With annual revenues of more than US$200M and vibrant company culture, Rokt has been listed in ‘Great Places to Work’ in the US and Australia. Our award-winning culture is guided by our five core values: Smart with Humility, Own the Outcomes, Force for Good, Conquer New Frontiers, and Enjoy the Ride. These values help us attract, engage, and develop the right talent around the globe and ensure we have the right conditions to do our best work. Keen to join a fast-growing company and a vibrant culture? Learn more at rokt.com.
The Rokt engineering team builds best-in-class ecommerce technology that provides personalized and relevant experiences for customers globally and empowers marketers with sophisticated, AI-driven tooling to better understand consumers. Our bespoke platform handles millions of transactions per day and considers billions of data points which give engineers the opportunity to build technology at scale, collaborate across teams and gain exposure to a wide range of technology. We are expanding rapidly in our major R&D centers in NYC and Sydney. We are passionate about using intelligent systems to improve the transaction moment for retailers everywhere. Come join us and build the future!
Requirements
About the role
We’re working on building a platform for reporting and analytics that is able to handle huge amounts of data in a real-time fashion that would allow us to uncover new insights and help us make decisions.
Our goal is to unlock data and make it available to various users starting from other engineers to end business users and clients. We value pragmatic solutions and simplicity that help us build reliable and fast systems.
Outcomes & responsibilities
Build distributed, high-volume data pipelines and storage that power our reporting and analytics
Work on real-time distributed OLAP custom solutions
Do it with Spark, Kafka, Airflow, and other open-source technologies
Work all over the stack, moving fluidly between programming languages: Scala, Python, and more
You'll help define the processes and infrastructure to transform and make data readily available across the company
Join a tightly knit team solving hard problems the right way
Own meaningful parts of our service, have an impact, grow with the company
Take responsibility for system health, monitoring and alerting, and CI/CD pipelines
Support and mentor other engineers on best practices, architecture, and quality
Capabilities & requirements
You have built and operated data pipelines for real customers in production systems
You are fluent in several programming languages (JVM & otherwise)
You’ve worked with data stores and/or data warehouses, such as AWS Redshift, Snowflake, Clickhouse, or others
You have hands-on experience with BigData frameworks (Hadoop, Hive, Spark, etc.)
You’re able to explain advanced technical concepts in a simple manner and cater to your audience
You enjoy wrangling huge datasets and helping others unlock new insights
You’re concerned about resiliency, high-availability, data quality, and other aspects of a critical system
Benefits
Force for Good. We actively invest in the growth of our people and the strengthening of our communities. Our NYC office is 100% vaccinated to keep our employees and community safe and healthy. We require all Rokt’stars as well as anyone else who will be onsite at the Rokt NYC office – clients, contractors, vendors, and suppliers – to show proof of vaccination and their booster shot.
Work with the greatest talent in town. Our recruiting process is tough. We hold a high bar because we have a high-performing, high-velocity culture - we only want the brightest and the best.
Join a community. We believe the best things happen when we come together to solve complex problems and make meaningful connections with each other through interest groups, sports clubs, and social events.
Accelerate your career. Develop through our global training events, ‘Level Up’ investment, online training courses, and our fantastic people leaders. Take your career to Rokt’speed - Grow your career in our rapidly growing company.
Take a break. When you work hard, we know you also need to rest. We offer generous time off and parental leave policies, as well as mental health and wellness days for all employees. We also offer a paid Rokt’star Sabbatical for employees who have been with us for 3 years or more.
Stay happy and healthy. Enjoy catered lunch 3 times a week and healthy snacks in the office. Plus join the gym on us! In the US, access generous retirement plans like a 4% dollar-for-dollar 401K matching plan and get fully funded premium health insurance for your whole family. And our NYC office is dog-friendly!
Become a shareholder. All Rokt’stars have stock options. If we succeed, everyone enjoys the upside.
See the world! Along with our global all-staff events in amazing locations (Phuket, Thailand in January 2020, Hawaii in May 2022), we also offer generous relocation packages for those interested in moving to another Rokt office. We have cool offices in great cities - New York, Sydney, London, Singapore, Tokyo.
Get the best of both worlds with a hybrid workplace. We currently work 3 days a week in office, allowing you to enjoy the best of both worlds (please note: this is subject to change based on the needs of the business and some support roles still require a full time presence). One week per quarter, you also have the flexibility to work from anywhere.
We believe in equality. Rokt is an Equal Opportunity Employer and recognizes that a diverse workforce is crucial to our success as a business. We would love you to apply for one of our open roles - irrespective of socio-economic status or background, age, gender identity, race, religion, sexual orientation, color, pregnancy, carer/family responsibilities, national and social origin, political opinion, marital, veteran, or disability status.


Salary range: $120,000 - $200,000 / year
#LI-Hybrid",160000,201 to 500 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2012,Unknown / Non-Applicable,NY,11,data engineer,senior,"['python', 'r', 'scala']","['snowflake', 'aws', 'redshift']",[],"['snowflake', 'hive']","['kafka', 'hadoop', 'spark']",[],,2-5 years
Inovalon,3.1,United States,Senior Software Data Engineer,"Inovalon was founded in 1998 on the belief that technology, and data specifically, would empower the transformation of the entire healthcare ecosystem for the better, improving both outcomes and economics. At Inovalon, we believe that when our customers are successful in their missions, healthcare improves. Therefore, we focus on empowering them with data-driven solutions. And the momentum is building.
Together, as ONE Inovalon, we are a united force delivering solutions that address healthcare's greatest needs. Through our mission-based culture of inclusion and innovation, our organization brings value not just to our customers, but to the millions of patients and members they serve.
Overview: The Senior Data Engineer is responsible for contributing to data warehouse/lake, data operations organization. They will support existing data operations such as data pipeline, ETL, data lake / warehouse, data structure development, and troubleshooting of data issues. They will be part of the team which is building next generation data & reporting platform. This platform will cater internal business stakeholders and external customers to provide insights & forecasting to understand current state of business, improve decision-making for their tactical and strategic goals & KPIs. This position may require independent work, sharing information and assisting others with work request.
Duties and Responsibilities:
Work with the agile team to participate in agile ceremonies like grooming, planning, standup, retrospective, demos
Actively contribute to grooming, and standup, create & update tasks, estimate and status
Write complex queries, stored procedures, functions, SSIS Packages for various job execution
Work with data architects and business analysts to create a logical data model and create DDL scripts for physical model creation
Design and develop modern ETL framework utilizing tools like ADF (Azure Data Factory), MS-SSIS etc
Design and develop ETL pipelines, using SQL, Stored procedures/functions to extract data from various sources and load into warehouse
Design STAR or SNOWFLAKE database schema utilizing industry best practices to build Data warehouse, data marts, views, cubes and data sets/products
Design and code various data architecture component like data validation, cleansing, de-duping, Symantec layer etc
Design and develop data export frameworks to extract data from the warehouse, transform, pre-aggregate, perform calculations and load into various data marts for Analytics use
Design & develop configurable data export framework to extract data from Data warehouse and data marts to generate reports for internal and external customers in .csv, flat files and
Work on large data to ensure configurable ingestion of data, dynamic rule & validation of data, cleansing, transforming and loading into the data warehouse
Design and implement data validation and quality checks to ensure the accuracy and completeness of the data in the data warehouse
Build data marts which will be utilized by Analytics tools like PowerBI and work with PowerBI developer to optimize database schema and queries
Perform performance of queries and data processing, identify and resolve any issues looking at query plans, create appropriate indexes, resolve dead locks and create table hints
Participate in design discussions, data analysis, data model creation etc
Product quality design diagrams (using MS-Visio, Draw.io etc) and documentations (MS-Work, Excel etc)
Maintain compliance with Inovalon's policies, procedures and mission statement;
Adhere to all confidentiality and HIPAA requirements as outlined within Inovalon's Operating Policies and Procedures in all ways and at all times with respect to any aspect of the data handled or services rendered in the undertaking of the position; and
Fulfill those responsibilities and/or duties that may be reasonably provided by Inovalon for the purpose of achieving operational and financial success of the Company.
Job Requirements:
Minimum of 7 years industry experience working in data & reporting area, knowledge of healthcare data will be a plus
5+ years working on Datawarehouse, ETL process/pipeline, Data Workflow, Query Plans & Optimization
5+ experience in MS SQL, T-SQL, ETL Jobs
5+ experience in Microsoft tools like SSMS, SSIS, SQL Server
5+ years' experience in writing query plans, indexes etc
2+ years working with Analytics tools (like Tableau, PowerBI – Preferred)
2+ years' experience working on Azure Cloud is preferred utilizing ADF (Azure Data Factory), Azure Delta Lake, Azure SQL Server, Data Sync, Log Insights and Analytics
Experience working with Role based security at database, ETL jobs, data exports level
Strong understanding of database concepts and schema (like star, snowflake schema)
Experience with HIPPA and PHI will be a plus
Ability to effectively communicate with internal and external customers
Excellent verbal and written communication skills
Excellent computer proficiency (MS Office – Word, Excel and Outlook)
Must be able to work under pressure and meet deadlines; and
Ability to work independently and to carry out tasks to completion following standard accepted practices
Education:
BS degree in Computer Science or Computer Engineering, Business, or equivalent experience.
Physical Demands and Work Environment:
Sedentary work (i.e., sitting for long periods of time);
Exerting up to 10 pounds of force occasionally and/or negligible amount of force;
Frequently or constantly to lift, carry push, pull or otherwise move objects and repetitive motions;
Subject to inside environmental conditions; and
Some travel (less than 10%) may be required for this position, primarily for training and collaboration purposes.
Studies have shown that women and people of color are less likely to apply for jobs unless they believe they meet every one of the qualifications listed in a job description. If you don't meet every qualification listed but are excited about our mission and the work described, we encourage you to apply regardless. Inovalon is most interested in finding the best candidate for the job and you may be just the right person for this or other roles.
By embracing diversity, equity and inclusion we enhance our work environment and drive business success. Inovalon strives to reflect the diversity of the communities where we operate and of our clients and everyone whom we serve. We endeavor to create a culture of inclusion in which our associates feel empowered to bring their full, authentic selves to work and pursue their professional goals in an equitable setting. We understand that by fostering this type of culture, and welcoming different perspectives, we generate innovation and growth.
Inovalon is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirement.
The Company maintains a drug free work environment for all of its associates, which includes employees, contractors and vendors. It is unlawful for associates to manufacture, sell, distribute, dispense, possess or use any controlled substance or marijuana in the workplace and doing so will result in disciplinary action, up to and including termination of employment or the contracted relationship.",112889,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,1998,$500 million to $1 billion (USD),TX,25,data engineer,senior,['sql'],"['azure', 'snowflake']","['excel', 'tableau', 'ssis']","['sql server', 'snowflake']",[],[],,+10 years
Ghirardelli Chocolate Company,3.6,"Rogers, AR",Sr. Category Analyst - Data Engineer,"This Sr. Category Analyst - Data Engineer will join our Category Advisor Team and be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for the advisor team. The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up, guiding continuous business improvement and providing actionable insights to maximize Walmart's category volume and share.


The Category Advisor Team Leads the planning and development with Walmart in the creation of best-in-class chocolate category insights and planograms. Works directly with the Walmart buyer and Category Advisor Team to analyze, optimize and execute confection business with a focus on premium chocolate.
Requirements:
Category Analytics Management
Create data tools for analytics that assist in building and optimizing evaluation of Walmart's category strategies and 4P tactical solutions
Represent Premium data analytics in strategic category settings, operating in a firewall environment
Develops a deep understanding of Walmart's strategies, initiatives and performance
Analyzes data to help influence broader Advisor team and Buyer
Examples:
Planogram optimization & performance tracking
Assortment evaluation
Pricing & Promotional strategy
Display execution & performance tracking
Other Advisory Team Responsibilities
Attend store visits to assess planograms and store level conditions
Assist with custom research analytics and reporting
Assist with shelf resets in Walmart layout center and in Ghirardelli office – may include transporting product for resets",77235,1001 to 5000 Employees,Company - Private,Manufacturing,Food & Beverage Manufacturing,1852,$500 million to $1 billion (USD),AR,171,data engineer,senior,[],[],[],[],[],[],,
CVS Health,3.1,"Wellesley, MA",Data Engineer,"Caremark LLC, a CVS Health company is hiring for the following role in Wellesley, MA: Data Engineer to design, build and manage large scale data structures, pipelines and efficient Extract/Load/Transform (ETL) workflows to support business applications. Duties include: develop large scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs; write ETL (Extract/Transform/Load) processes, design database systems, and develop tools for real-time and offline analytic processing; collaborate with Data Science team to transform data and integrate algorithms and models into automated processes; leverage knowledge of Hadoop architecture, HDFS commands, and designing and optimizing queries to build data pipelines; utilize programming skills in Python, Java, or similar languages to build robust data pipelines and dynamic systems; build data marts and data models to support Data Science and other internal customers; integrate data from a variety of sources and ensure adherence to data quality and accessibility standards; analyze current information technology environments to identify and assess critical capabilities and recommend solutions; and experiment with available tools and advise on new tools to provide optimal solutions that meet the requirements dictated by the model/use case. Multiple positions.

Required Qualifications
Master’s degree (or foreign equivalent) in Computer Science, Data Science, Statistics, Mathematics, Analytics, or a related field and one (1) year of experience in the job offered or related occupation. Requires one (1) year of experience in each of the following: Analyzing large data sets from multiple data sources; Data analysis for retail and/or healthcare industries; Programming in Java, Python, or R; SAS or SQL programming languages; and Databases: Oracle, Teradata, or DB2.

Preferred Qualifications
See Required Qualifications.

Education
See Required Qualifications.

Business Overview
Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.",112889,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1963,$10+ billion (USD),MA,60,data engineer,na,"['sql', 'java', 'r', 'python']",[],[],['oracle'],['hadoop'],[],master,
Bloomerang,3.6,United States,Lead Data Engineer,"Bloomerang combines the best tools, resources, and people to provide a world-class experience for tens of thousands of nonprofits, allowing them to raise more money and do more good in the world. Our powerful software and stellar customer service have made us one of the highest rated fundraising/donor CRM on the market.
In addition to creating thriving nonprofits, we're also in the business of creating thriving employees. At Bloomerang, you'll be a part of a mission-driven culture built on the core values of Empathy, Unity, and Transparency. We know the key to our success is our people, and we're proud to be home to some of the most innovative and skilled employees in the workforce today.
The Role
We are seeking a skilled and motivated Data Engineer to join our fast-growing SaaS company. If you thrive on challenges and have a passion for scaling businesses, this is a rare opportunity to be part of an exciting sports startup. As a Data Engineer, you will collaborate with cross-functional teams to design and implement data solutions using Google BigQuery or Snowflake. Your responsibilities will include building data infrastructure, integrating databases and platforms, performing statistical analysis, and leveraging the right data technology to support our top-level business strategy. If you are driven, possess excellent analytical skills, and have experience in high-growth technology startups, we invite you to join our team and contribute to our data-driven success.
What You Will Do
Collaborate with other departments to design and implement data solutions that address business problems and support top-level business strategy.
Implement and support data tools such as Google BigQuery or Snowflake to enable efficient data processing and analysis.
Build data infrastructure to facilitate integrations with various databases (MySQL, PostgresQL), platforms, and APIs.
Utilize statistical analysis tools like SAS, R, or Python to perform data analysis and derive meaningful insights.
Pursue continuous improvement by identifying and implementing enhancements to data systems and processes.
Write documentation for data architecture and processes to ensure clarity and maintainability.
Apply analytical ability to collect, organize, model, analyze, and disseminate significant amounts of data.
What You Need to Succeed
4-5 years of experience working with Google BigQuery or Snowflake, demonstrating a solid understanding of their capabilities and best practices.
Familiarity with Agile Methodologies to facilitate iterative and efficient development cycles.
Experience with Google Cloud Machine Learning and Google Cloud AI Platform, enabling the utilization of advanced analytics and AI capabilities.
Knowledge and ability to create and administer data access policies and procedures, ensuring data security and governance.
Proficiency in creating dashboards and visualizations using tools like PowerBI, Tableau, or similar, to effectively communicate insights to stakeholders.
Experience with Azure SQL Managed Instances and SQL Elastic Pools, expanding your expertise in cloud-based data solutions.
Being high-energy with a hustle, motivation, and drive that's contagious, fostering a proactive and results-oriented work culture.
Outstanding communication and interpersonal skills, enabling effective collaboration and stakeholder management.
Strong problem-solving and prioritization skills, allowing you to address complex data challenges and meet project objectives.
Experience working in a high-growth early-stage technology startup company, demonstrating adaptability and a willingness to work in a dynamic environment.
Benefits
Health + Wellness
You'll have access to generous health, vision, and dental insurance options, as well as a free subscription to Bright, a wellness platform that offers live and on-demand fitness, meditation, mindfulness, and nutrition classes.

Time Off
You'll get a competitive PTO package that includes 20 PTO days, 3 flex days, 4 optional volunteer Days, 12 paid holidays, as well as paid parental leave.

401k
You'll receive a 401k match to help invest in your future.

Equipment
Everything you need to be successful, shipped right to your door.

Compensation
The salary range for this position is: $127,000 - $172,000. You may also be eligible for a discretionary bonus. Actual compensation within the range will be dependent on your skills, experience, qualifications, and location, as well as applicable employment laws.

Location
This is a permanent, full-time, fully remote position. Employees living in Indianapolis, IN are welcome to work from our company headquarters. We do not offer Visa sponsorship or relocation assistance at this time.

Accommodations
Applicants who require accommodations may contact careers@bloomerang.com to request an accommodation in completing an application.
Bloomerang is an Equal Opportunity Employer. Individuals seeking employment at Bloomerang are considered without regard to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, gender identity, or sexual orientation.",149500,201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,2012,$25 to $100 million (USD),TX,11,data engineer,senior,"['sql', 'r', 'python']","['snowflake', 'google cloud', 'azure']",['tableau'],"['postgresql', 'snowflake', 'mysql']",[],[],,5-10 years
Baylor Scott & White Health,3.9,"Dallas, TX",Snowflake DBT Data Engineer,"JOB SUMMARY
The EBI Developer II is accountable for delivery of modern business intelligence, analytics, and data warehousing solutions including design, development, testing, implementation, and maintenance of the enterprise business intelligence platforms. May specialize in BI/reporting, ETL, and/or database technologies. Works with other technical team members to deploy solutions using best practices for architecture, design, coding, and testing to help data movement/transformation, data testing, relational/multidimensional (OLAP) database objects, and data visualization. The Developer 2 assumes tasks of a moderate to high level of complexity and difficulty.
ESSENTIAL FUNCTIONS OF THE ROLE
Understands detailed end user specifications. Design and develop solutions to accommodate business requirements.
Tunes and optimizes process to improve the performance and works with DBAs on optimizing query response times.
Implements procedures to maintain, monitor, backup and recovery operations for the environment.
Helps production deployments and automation. Conduct troubleshooting production and performance issues.
Establishes security standards for projects, roles, users, privileges in different environments (DEV, QA, PROD etc.).
Is accountable for Installs, configures, and upgrades BI software. Own activities associated with maintaining repository and metadata information.
Performs code reviews and provide and Design and develop test plans for unit testing.
Helps EBI Developer 3 to maintain inventory of all the jobs and processes and their SLAs.
KEY SUCCESS FACTORS
Must have experience in development using ETL or BI tools.
Must have Snowflake experience.
Must have DBT experience.
Must have experience in Data Warehousing, with thorough knowledge of dimensional modeling.
Must have experience in database development, using SQL and PL/SQL, with in-depth knowledge of systematic functions.
Experience with SDLC a must, exposure to Agile/SCRUM practices is ideal.
Must possess excellent documentation and communication skills.
Experience with IBM Netezza, Microsoft SQL Server, ETL tools (Netezza, Informatica) as well as enterprise BI reporting tools (IBM Cognos, SAP Business Objects, Microsoft SSRS) preferred.
Experience in tools administration is preferred.
BENEFITS
Our competitive benefits package includes the following
Immediate eligibility for health and welfare benefits
401(k) savings plan with dollar-for-dollar match up to 5%
Tuition Reimbursement
PTO accrual beginning Day 1
Note: Benefits may vary based upon position type and/or level
QUALIFICATIONS
EDUCATION - Bachelor's or 4 years of work experience above the minimum qualification
EXPERIENCE - 2 Years of Experience",76462,10000+ Employees,Company - Private,Healthcare,Health Care Services & Hospitals,1903,$5 to $10 billion (USD),TX,120,data engineer,na,['sql'],['snowflake'],['sap'],"['sql server', 'snowflake', 'dbt']",[],[],bachelor,2-5 years
Humana,3.9,"Jersey City, NJ",Senior Data Engineer,"Humana is seeking a Senior Data Engineer to join our Fortune #40, Best Places to Work company and help us make a difference as we help our members achieve their best health!

Team cohesion and support amongst our team members for one another is of the utmost importance to us.

The Senior Data Engineer works in all data environments, which includes data design, database architecture, metadata, repository creation and most importantly Data Governance. The work assignments involve moderately complex to complex issues where the analysis of situations or data requires an in-depth evaluation of variable factors. This role will be helping build and architect systems mainly using Microsoft SQL Server and related technology.
Responsibilities
Responsibilities
The Sr. Data Engineer is responsible for developing blueprints for all data repositories, evaluating hardware and software platforms, and integrating systems. You will be joining a team with strong technical skills to assist in these duties.
The Sr. Data Engineer:
Translates business needs into long-term solutions.
Defines, designs and builds dimensional database schemas.
Evaluates reusability of current data for separate analyses.
Conducts data sheering to rid the system of old, unused or duplicate data.
Reviews object and data models and the metadata repository to structure the data for better management and quicker access.
Begins to influence department’s strategy.
Makes decisions on moderately complex to complex issues regarding technical approach for project components, and work is performed without direction.
Exercises considerable latitude in determining objectives and approaches to assignments.
Required Qualifications
Bachelor's Degree or equivalent relevant work experience
Advanced knowledge of SQL with the ability to write complex SQL queries and stored procedures
Ability to create Architectural documents E.g.. Data Flows, Entity Relationship, Dependencies, etc.
Deep understanding of SQL-based systems and experience with modern ETL tools
Experience with SSIS
Knowledge of relational and multi-dimensional database architectures
Preferred Qualifications
Healthcare industry experience
Experience in Cloud based data warehousing and code versioning tools
Hands-on experience in cloud technologies as it relates to migration, implementation and deployment.
Knowledge of the Agile methodologies
Experience with Power BI
Additional Information
Location/Work Style: Remote US
Why Humana?
At Humana, we know your well-being is important to you, and it’s important to us too. That’s why we’re committed to making resources available to you that will enable you to become happier, healthier, and more productive in all areas of your life. Just to name a few:
Work-Life Balance
Generous PTO package
Health benefits effective day 1
Annual Incentive Plan
401K - Excellent company match
Well-being program
Paid Volunteer Time Off
Student Loan Refinancing
If you share our passion for helping people, we likely have the right place for you at Humana.
Work at Home Guidance
To ensure Home or Hybrid Home/Office associates’ ability to work effectively, the self-provided internet service of Home or Hybrid Home/Office associates must meet the following criteria:
At minimum, a download speed of 25 Mbps and an upload speed of 10 Mbps is recommended; wireless, wired cable or DSL connection is suggested
Satellite, cellular and microwave connection can be used only if approved by leadership
Associates who live and work from Home in the state of California, Illinois, Montana, or South Dakota will be provided a bi-weekly payment for their internet expense.
Humana will provide Home or Hybrid Home/Office associates with telephone equipment appropriate to meet the business requirements for their position/job.
Work from a dedicated space lacking ongoing interruptions to protect member PHI / HIPAA information
#LI-Remote
This is a remote position
Scheduled Weekly Hours
40

Not Specified
0",112889,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1961,$10+ billion (USD),NJ,62,data engineer,senior,['sql'],[],"['power bi', 'ssis']",['sql server'],[],[],bachelor,
Match Group,3.7,"Dallas, TX",Sr. Data Engineer,"Know where you belong.

Match Group is a leading provider of dating products across the globe. Our portfolio includes Tinder, Match, Hinge, PlentyOfFish, The League, and others, each designed to spark meaningful connections for singles worldwide. Creating a sense of belonging doesn’t stop at our products - it’s the foundation of every team we hire.

As a Sr. Data Engineer, you will be implementing critical ETL pipelines and advancing best practices for the data engineering team, and the rest of the organization. You will work on delivering an actual big data architecture while concentrating on real-world problems such as privacy concerns.

This role is key to the success of Match Group. Not only will you help power the love lives of millions of people, but you will play a critical part in the functioning of every brand at Match Group (Match, Tinder, Hinge, Okcupid, PlentyofFish, BLK, and others), with stakeholders ranging from customer experience to marketing to leadership.

When it comes to dating, the connection starts online, but the real magic happens once you meet in real life (IRL). We think the same is true for creating the best platforms, so we work together IRL in our Dallas office 2 days/week.
How you'll make an impact:
Work with our Engineering teams to ensure data is flowing accurately through data creation to our presentation layers
Become an advocate for the Data Engineering team by developing and championing Data Engineering practices with the team and with the company at large
Improve our Data Engineering stack through containerization, data modeling, developing our ETL pipelines, and building scalable/reliable solutions
Work with stakeholders and translate their needs and expectations into action items and deliverables
Design and implement efficient data models to support business intelligence and data analytics
Develop and maintain data quality checks and data monitoring systems
Support existing on-prem infrastructure and help expand our processes into the cloud (AWS)
We could be a Match if you bring:
Expertise in SQL, Data Modeling, and Python
5+ years of professional/industry experience
Prior Airflow and/or Python experience is required
Used Redshift, Airflow, Spectrum and relational database like SQL Server
Capability to drive initiatives and articulate their value to Engineering and other stakeholders
Working knowledge of SSIS (SQL Server Integration Services), SSAS (SQL Server Analysis Services) and Amazon QuickSight is plus
What's the team like?
Our BI team is a service organization that delivers reporting solutions to the entire Match Group enterprise
The BI team is responsible for architecting and engineering new data systems and reporting to help facilitate business decision-making

#LI-CENTRAL
#MOGUL
#LI-CH1

Why Match Group?

Our mission is simple – to help people find love and happiness! We love our employees too and understand the importance of all life's milestones. Here are some of the benefits we are proud to offer:

Mind & Body – Medical, mental health, and wellness benefits to support your overall health and well-being
Financial Wellness – Competitive compensation, 100% employer match on 401k contributions up to 10% (cap at $10,000), as well as an employee stock purchase program to help you feel supported in your financial security
Unplug – Generous PTO and 15 paid holidays so you can unplug
Career – Annual training allowance for professional development and ERG membership opportunities and events so you feel connected and empowered in your work
Family – Families come in all shapes and sizes so we offer 20 weeks of 100% paid parental leave, fertility, adoption, and child care resources, as well as pet insurance and discounts
WFH Stipend – Hybrid/remote work allowance for full time employees to help you feel comfortable and efficient in your home office environment
Company Gatherings – We host fun happy hours and company events where our employees get to know each other and build a sense of connection and belonging!

We are proud to be an equal opportunity employer and we value the rich dynamics that diversity brings to our company. We do not discriminate on the basis of race, religion, color, creed, national origin, ancestry, disability, marital status, age, sexual orientation, sex (including pregnancy and sexual harassment), gender identity or expression, uniformed service or veteran status, genetic information, or any other legally protected characteristic. Period.",104956,1001 to 5000 Employees,Company - Public,Information Technology,Internet & Web Services,2015,$1 to $5 billion (USD),TX,8,data engineer,senior,"['sql', 'python']","['aws', 'redshift']",['ssis'],['sql server'],['spark'],[],,+10 years
Blue Cross Blue Shield of Massachusetts,4.0,"Boston, MA",Data Engineer,"Ready to help us transform healthcare? Bring your true colors to blue.

Job Description Summary:
At Blue Cross Blue Shield of Massachusetts, we have an exciting opportunity for a Data Engineer to join our Core Data Engineering team of analytics delivery and insights. Primary focus of this role is to work on multiple applications alongside with other engineers to design, build & deliver data solutions. Other element of the role is to work in conjunction with Senior Data Engineer to contribute towards the delivery of Data Platform. This is hands on position with 80% focus towards data engineering and 20% towards best practice, unit testing and reviewing code.

Responsibilities:
The Data Engineer will be responsible for the development, implementation, testing, documentation, and maintenance of analytics deliver and insights data solution applications.

Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.

Define relational tables, primary and foreign keys, and stored procedures to create a data model structure.

Partner directly with data owners and business SMEs to identify needed data sources, ensure the data is provided to project team in a reliable fashion, and prepare data in an optimal format for analysis

Develop ETL pipelines for optimal performance in collaboration with other data engineers.

Develop data integrations and data quality framework.

Analyze complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models

Perform initial data quality checks on extracted data.

Automate manual processes and optimize data delivery.

Test the code using the appropriate testing approach.

Required Technical Skills

Experience or exposure to AWS cloud services (AWS Glue, Lambda, EMR, Athena, Sage maker etc.)

Experience or exposure to OLAP architecture (Snowflake, Redshift etc.)

Experience or exposure to big data tools: Hadoop, Spark, Kafka, Hive, Presto, Impala etc.

Experience or exposure to relational SQL and NoSQL databases, including Postgres, DynamoDB.

Experience or exposure to data pipeline and workflow management tools: Airflow, Oozie etc.

Experience or exposure to DevSecOps pipeline is preferred (Jenkins, Veracode, Sonar cube etc.)

Experience building ETL solutions and familiarity with tools IICS/Talend is preferred

Experience or exposure to stream-processing systems: Storm, Spark-Streaming, etc.

Expert in one of the object-oriented/object function scripting languages: Python/Spark is preferred, Java and Scala.

Work with various project teams to deliver on commitments within time and scope

Experience or a good understanding of different types of data ingestion formats like JSON, XML, parquet file

Experience in developing scalable data engineering platform using cloud infrastructure.

Experience or a good understanding and willingness to include Security at design, development, and testing

Experience in performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

Experience in building processes for supporting data transformation, data structures, metadata, dependency, and workload management.

Understanding of servers, storage, and networking roles
Qualification

Bachelor’s degree in Computer Science, Math or Information Systems or related fields.

Strong verbal and written communication skills.

Ability to express complex technical concepts effectively, both verbally and in writing.

Ability to multi-task in a fast-paced, changing environment.

Demonstrate strong organization skills and detail oriented.

Experience or exposure to large-scale, complex data environments.

Ability to self-motivate and meet deadlines.

Intense desire to learn.

Exposure to an Enterprise Data Lake & Lakehouse concept.

Minimum Education Requirements:
High school degree or equivalent required unless otherwise noted above

Location Boston Time Type Full time

The job posting range is the lowest to highest salary we in good faith believe we would pay for this role at the time of this posting. We may ultimately pay more or less than the posted range, and the range may be modified in the future. An employee’s pay position within the salary range will be based on several factors including, but limited to, relevant education, qualifications, certifications, experience, skills, performance, shift, travel requirements, sales or revenue-based metrics, and business or organizational needs and affordability.

This job is also eligible for variable pay.

We offer comprehensive package of benefits including paid time off, medical/dental/vision insurance, 401(k), and a suite of well-being benefits to eligible employees.

Note: No amount of pay is considered to be wages or compensation until such amount is earned, vested, and determinable. The amount and availability of any bonus, commission, or any other form of compensation that are allocable to a particular employee remains in the Company's sole discretion unless and until paid and may be modified at the Company’s sole discretion, consistent with the law.

WHY Blue Cross Blue Shield of MA?
We understand that the confidence gap and imposter syndrome can prevent amazing candidates coming our way, so please don’t hesitate to apply. We’d love to hear from you. You might be just what we need for this role or possibly another one at Blue Cross Blue Shield of MA. The more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be brilliant. We encourage you to bring us your true colors, , your perspectives, and your experiences. It’s in our differences that we will remain relentless in our pursuit to transform healthcare for ALL.

As an employer, we are committed to investing in your development and providing the necessary resources to enable your success. Learn how we are dedicated to creating an inclusive and rewarding workplace that promotes excellence and provides opportunities for employees to forge their unique career path by visiting our Company Culture page. If this sounds like something you’d like to be a part of, we’d love to hear from you. You can also join our Talent Community to stay “in the know” on all things Blue.

At Blue Cross Blue Shield of Massachusetts, we believe in wellness and that work/life balance is a key part of associate wellbeing. We provide a flexible hybrid work model in which roles are designated as resident (on site 4-5 days/week), mobile (on site 1-3 days/week), or eworker (on site 0-3 days/month).",108737,1001 to 5000 Employees,Nonprofit Organization,Insurance,Insurance Carriers,1937,$100 to $500 million (USD),MA,86,data engineer,na,"['scala', 'java', 'nosql', 'sql', 'python']","['snowflake', 'aws', 'redshift']",[],"['dynamodb', 'snowflake', 'hive']","['kafka', 'spark', 'hadoop']",[],bachelor,
Blue Cross Blue Shield of Arizona,4.2,United States,Data Engineer - REMOTE-AZ,"Awarded a Healthiest Employer, Blue Cross Blue Shield of Arizona aims to fulfill its mission to inspire health and make it easy. BCBSAZ offers a variety of health insurance products and services to meet the diverse needs of individuals, families, and small and large businesses as well as providing information and tools to help individuals make better health decisions.
Designs and implements business intelligence and extract, transform, and load (ETL) solutions using programming, performance tuning, data modeling. Creates databases optimized for performance, implementing schema changes, and maintaining data architecture standards across all of the business’s databases. Serves as a liaison between the Database Administration department and development teams.
REQUIRED QUALIFICATIONS
Required Work Experience
4 years of experience in computer programming, query design, and databases
6 years of experience in computer programming, query design, and databases (Senior level)
Required Education
High-School Diploma or GED in general field of study
PREFERRED QUALIFICATIONS
Preferred Work Experience
4+ years of experience building and managing complex Data Integration solutions.
4+ years of experience in computer programming, query design, and databases
4+ years of experience Database administration with SQL Server
6+ years of experience building and managing complex Data Integration solutions (Senior)
6+ years of experience in computer programming, query design, and databases (Senior)
6+ years of experience Database administration with SQL Server (Senior)
Preferred Education
Bachelor’s Degree in Information Technology or related field preferred
Master’s Degree in Information Technology or related field (Senior)

ESSENTIAL JOB FUNCTIONS AND RESPONSIBILITIES

Data Engineer – Performs job functions with general supervision and peer review
Learn area’s direct flow; and how it affects surrounding systems and operational areas.
Architect, design, construct, test, tune, deploy, and support Data Integration solutions for various data management systems.
Contribute to the team’s knowledge base with useful information such as adopted standards, procedure documentation, problem resolution advice, etc.
Participate in the promotion of SQL Server best practices
Collaborate with other technology teams and architects to define and develop solutions.
Research and experiment with emerging Data Integration technologies and tools.
Work with the team to establish and reinforce disciplined software development, processes, standards, and error recovery procedures are deployed; ensuring a high degree of data quality.
Support Enterprise database clustering, mirroring, replication among other SQL Server technologies.
Develop, write and implement processing requirements and post implementation review
Facilitate and/or create new procedures and processes that support advancing technologies or capabilities
Design & Implement Extract, Transform, and Load (ETL) solutions utilizing SSIS
Apply data mining rules
Create logic, system, and program flows for complex systems, including interfaces and metadata
Write and execute unit test plans. Track and resolve any processing issues.
Implement and maintain operational and disaster-recovery procedures.
Participate in the review of code and/or systems for proper design standards, content and functionality.
Participate in all aspects of the Software Development Life Cycle
Analyze files and map data from one system to another
Adhere to established source control versioning policies and procedures
Meet timeliness and accuracy goals.
Communicate status of work assignments to stakeholders and management.
Responsible for technical and production support documentation in accordance with department standards and industry best practices.
Maintain current knowledge on new developments in technology-related industries
Participate in corporate quality and data governance programs
The position requires a full-time work schedule. Full-time is defined as working at least 40 hours per week, plus any additional hours as requested or as needed to meet business requirements.
Senior - Performs job functions with minimal supervision in a Lead capacity
Research, analyze, track and resolve complex production problems.
Build, support, and maintain complex processes, programs, and data.
Evaluate high-level project information and assess project components to forecast work effort required.
Provide peer-level review and mentoring to peers.
Participate and/or lead complex technical projects.
Act as a subject matter expert in two or more of areas as assigned:
BI Production Reporting
DAtabase Architecture
Database Architecture and Support
Training
Act as primary operational contact for internal and external customers when needed or in the absence of manager.
Ensure Service Level Agreements between department and operational or technical areas are met.
Lead, develop and mentor staff by providing opportunities for growth through delegation, training, and assignment to various project teams.
Inform manager of any issues impacting the efficient and effective performance of the department including system, resource, and informational barriers; Provide timely feedback to team member on performance.
Assist the manager in the day-to-day operations of the department.
ALL LEVELS
Each progressive level includes the ability to perform the essential functions of any lower levels and mentor employees in those levels.
Maintain current on new developments in technology-related industries.
Participate in corporate quality and data governance programs.
Participate in on-call rotation.
Perform all other duties as assigned.
The position requires a full-time work schedule. Full-time is defined as working at least 40 hours per week, plus any additional hours as requested or as needed to meet business requirements. Participate in on-call rotation.
competencies
REQUIRED COMPETENCIES
Required Job Skills (Applies to All Levels)
Intermediate skill in use of office equipment, including copiers, fax machines, scanner and telephones
Intermediate PC proficiency in spreadsheet, database and word processing software
Advanced knowledge of business intelligence, programming, and data analysis sotfware
Intermediate knowledge of Microsoft SQL databases and database administration
Intermediate proficiency in T-SQL, NZ-SQL, PostgreSQL, NoSQL, Hadoop, data tuning, enterprise data modeling and schema change management.
Working technical knowledge of current software protocols and Internet standards to the extent that they apply to database administration.
Excellent database troubleshooting skills
Working technical knowledge of PowerShell.
Strong object-oriented design and analysis skills
Experience consuming, organizing and analyzing JSON and XML messages as data.
Required Professional Competencies (Applies to All Levels)
Knowledge of agile development practices
Strong analytical skills to support independent and effective decisions
Ability to prioritize tasks and work with multiple priorities, sometimes under limited time constraints.
Perserverance in the face of resistance or setbacks.
Effective interpersonal skills and ability to maintain positive working relationship with others.
Verbal and written communication skills and the ability to interact professionally with a diverse group, executives, managers, and subject matter experts.
Systems research and analysis. Ability to write and present business intelligence documentation
Demonstrate the ability to stay current on global threats and vulnerabilities.
Maintain confidentiality and privacy
Required Leadership Experience and Competencies
Build synergy with a diverse team in an ever changing environment
Facilitate and resolve customer requests and inquiries for all levels of management within the Corporation (Senior only)
Participation in one or more groups that aids in setting/enforcing standards and/or providing educational opportunities across the IT organization. (Senior only)
PREFERRED COMPETENCIES
Preferred Job Skills (Applies to All Levels)
Knowledge of HIPAA regulations
Advanced proficiency in spreadsheet, SQL queries, database, flow charting, and word processing software
Advanced knowledge of data mapping techniques
Advanced knowledge of computer operating systems
Advanced knowledge of decision support systems
Advanced knowledge of programming, database systems, and data management.
Advanced knowledge of decision support systems
Advanced knowledge of Business Objects
Preferred Professional Competencies (Applies to All Levels)
Advanced systems research and analysis expertise
Impeccable project management skills
Solid technical ability and problem solving skills
Knowledge of internal departments and operations
Strong technical documentation skills and a strong ability to translate technical concepts so that they are easily understood by laypersons
Preferred Leadership Experience and Competencies (Applies to Senior level)
Ability to provide mentoring and peer review to junior team members
Ability to build lesson plans and deliver lessons to junior team members
REQUIRED QUALIFICATIONS
Required Work Experience
4 years of experience in computer programming, query design, and databases
6 years of experience in computer programming, query design, and databases (Senior level)
Required Education
High-School Diploma or GED in general field of study
Required Licenses
N/A
Required Certifications
N/A
PREFERRED QUALIFICATIONS
Preferred Work Experience
4+ years of experience building and managing complex Data Integration solutions.
4+ years of experience in computer programming, query design, and databases
4+ years of experience Database administration with SQL Server
6+ years of experience building and managing complex Data Integration solutions (Senior)
6+ years of experience in computer programming, query design, and databases (Senior)
6+ years of experience Database administration with SQL Server (Senior)
Preferred Education
Bachelor’s Degree in Information Technology or related field preferred
Master’s Degree in Information Technology or related field (Senior)
Preferred Licenses
N/A
Preferred Certifications
MS SQL Certification or other certification in current programming languages
ESSENTIAL job functions AND RESPONSIBILITIES

DAta Engineer – Performs job functions with general supervision and peer review
Learn area’s direct flow; and how it affects surrounding systems and operational areas.
Architect, design, construct, test, tune, deploy, and support Data Integration solutions for various data management systems.
Contribute to the team’s knowledge base with useful information such as adopted standards, procedure documentation, problem resolution advice, etc.
Participate in the promotion of SQL Server best practices
Collaborate with other technology teams and architects to define and develop solutions.
Research and experiment with emerging Data Integration technologies and tools.
Work with the team to establish and reinforce disciplined software development, processes, standards, and error recovery procedures are deployed; ensuring a high degree of data quality.
Support Enterprise database clustering, mirroring, replication among other SQL Server technologies.
Develop, write and implement processing requirements and post implementation review
Facilitate and/or create new procedures and processes that support advancing technologies or capabilities
Design & Implement Extract, Transform, and Load (ETL) solutions utilizing SSIS
Apply data mining rules
Create logic, system, and program flows for complex systems, including interfaces and metadata
Write and execute unit test plans. Track and resolve any processing issues.
Implement and maintain operational and disaster-recovery procedures.
Participate in the review of code and/or systems for proper design standards, content and functionality.
Participate in all aspects of the Software Development Life Cycle
Analyze files and map data from one system to another
Adhere to established source control versioning policies and procedures
Meet timeliness and accuracy goals.
Communicate status of work assignments to stakeholders and management.
Responsible for technical and production support documentation in accordance with department standards and industry best practices.
Maintain current knowledge on new developments in technology-related industries
Participate in corporate quality and data governance programs
The position requires a full-time work schedule. Full-time is defined as working at least 40 hours per week, plus any additional hours as requested or as needed to meet business requirements.
Senior - Performs job functions with minimal supervision in a Lead capacity
Research, analyze, track and resolve complex production problems.
Build, support, and maintain complex processes, programs, and data.
Evaluate high-level project information and assess project components to forecast work effort required.
Provide peer-level review and mentoring to peers.
Participate and/or lead complex technical projects.
Act as a subject matter expert in two or more of areas as assigned:
BI Production Reporting
DAtabase Architecture
Database Architecture and Support
Training
Act as primary operational contact for internal and external customers when needed or in the absence of manager.
Ensure Service Level Agreements between department and operational or technical areas are met.
Lead, develop and mentor staff by providing opportunities for growth through delegation, training, and assignment to various project teams.
Inform manager of any issues impacting the efficient and effective performance of the department including system, resource, and informational barriers; Provide timely feedback to team member on performance.
Assist the manager in the day-to-day operations of the department.
ALL LEVELS
Each progressive level includes the ability to perform the essential functions of any lower levels and mentor employees in those levels.
Maintain current on new developments in technology-related industries.
Participate in corporate quality and data governance programs.
Participate in on-call rotation.
Perform all other duties as assigned.
The position requires a full-time work schedule. Full-time is defined as working at least 40 hours per week, plus any additional hours as requested or as needed to meet business requirements. Participate in on-call rotation.
competencies
REQUIRED COMPETENCIES
Required Job Skills (Applies to All Levels)
Intermediate skill in use of office equipment, including copiers, fax machines, scanner and telephones
Intermediate PC proficiency in spreadsheet, database and word processing software
Advanced knowledge of business intelligence, programming, and data analysis sotfware
Intermediate knowledge of Microsoft SQL databases and database administration
Intermediate proficiency in T-SQL, NZ-SQL, PostgreSQL, NoSQL, Hadoop, data tuning, enterprise data modeling and schema change management.
Working technical knowledge of current software protocols and Internet standards to the extent that they apply to database administration.
Excellent database troubleshooting skills
Working technical knowledge of PowerShell.
Strong object-oriented design and analysis skills
Experience consuming, organizing and analyzing JSON and XML messages as data.
Required Professional Competencies (Applies to All Levels)
Knowledge of agile development practices
Strong analytical skills to support independent and effective decisions
Ability to prioritize tasks and work with multiple priorities, sometimes under limited time constraints.
Perserverance in the face of resistance or setbacks.
Effective interpersonal skills and ability to maintain positive working relationship with others.
Verbal and written communication skills and the ability to interact professionally with a diverse group, executives, managers, and subject matter experts.
Systems research and analysis. Ability to write and present business intelligence documentation
Demonstrate the ability to stay current on global threats and vulnerabilities.
Maintain confidentiality and privacy
Required Leadership Experience and Competencies
Build synergy with a diverse team in an ever changing environment
Facilitate and resolve customer requests and inquiries for all levels of management within the Corporation (Senior only)
Participation in one or more groups that aids in setting/enforcing standards and/or providing educational opportunities across the IT organization. (Senior only)
PREFERRED COMPETENCIES
Preferred Job Skills (Applies to All Levels)
Knowledge of HIPAA regulations
Advanced proficiency in spreadsheet, SQL queries, database, flow charting, and word processing software
Advanced knowledge of data mapping techniques
Advanced knowledge of computer operating systems
Advanced knowledge of decision support systems
Advanced knowledge of programming, database systems, and data management.
Advanced knowledge of decision support systems
Advanced knowledge of Business Objects
Preferred Professional Competencies (Applies to All Levels)
Advanced systems research and analysis expertise
Impeccable project management skills
Solid technical ability and problem solving skills
Knowledge of internal departments and operations
Strong technical documentation skills and a strong ability to translate technical concepts so that they are easily understood by laypersons
Preferred Leadership Experience and Competencies (Applies to Senior level)
Ability to provide mentoring and peer review to junior team members
Ability to build lesson plans and deliver lessons to junior team members
Our Commitment
BCBSAZ does not discriminate in hiring or employment on the basis of race, ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status or any other protected group.
Thank you for your interest in Blue Cross Blue Shield of Arizona. For more information on our company, see azblue.com. If interested in this position, please apply.",112889,1001 to 5000 Employees,Nonprofit Organization,Insurance,Insurance Carriers,1939,Unknown / Non-Applicable,TX,84,data engineer,na,"['sql', 'nosql']",[],['ssis'],"['sql server', 'postgresql']",['hadoop'],[],bachelor,+10 years
GEICO,3.0,"Chevy Chase, MD",Senior Data Engineer (Remote),"At GEICO, we are seeking a highly motivated Senior Data Engineer to join our vendor data product team. As a Senior Data Engineer, you will be responsible for creating products using data from both internal and external vendor data sources to help realize goals of attracting customers through more accurate pricing, greater customer retention, and an increase in underwriting profitability. The right candidate will develop well-designed, testable, and efficient data ingestion, data enrichment, and data transformation solutions using best software development practices. You should be an analytical thinker, a self-learner, and comfortable supporting the needs of multiple projects. This role is a part of the GEICO Data Movement team of Data, Security & Infrastructure (DSI) in the GEICO Technology Solutions organization.
In this role, you will:
Identify, design, and implement internal process improvements, automating manual processes, optimizing data delivery for greater scalability
Build the processes required for optimal extraction, transformation, and loading of data using a variety of languages and technologies such as Scala, Python, Kafka, Azure Data Factory, Fivetran/HVR, dbt, and Databricks
Collaborate with stakeholders including the Product, Data Engineering, and Agile Delivery teams in an agile environment to assist and resolve data-related issues and support data delivery needs
Work with other data platform or data domain teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Perform unit tests and conduct reviews with other team members to make sure code is rigorously designed, elegantly coded, and effectively tuned for performance
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Experience & Skills:
At least 2 years of experience with designing, developing, implementing, and maintaining solutions for Big Data or data warehouse system
At least 2 Years of experience working in a cloud environment such as Azure, AWS or other private or public cloud
Experience performing root cause analysis on internal and external data and processes to answer business questions and identify opportunities for improvement
Strong analytical skills related to working with unstructured datasets
Good experience with bringing data into a centralized data repository or manipulating the available data to build additional data sets for Analytics and Reporting purposes.
Experienced with maintaining data quality throughout the lifecycle of the data.
Experience with Data Modeling, source to target mapping, automated testing frameworks, CI/CD pipelines and task automation using scripting
Experienced with working in Agile environment and end to end automation
Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality) components
Strong working knowledge of SQL and the ability to write, debug and optimize SQL queries and ETL jobs to reduce the execution window or reduce resource utilization
Data Engineering experience focused on batch and real-time data pipelines development, Data processing/data transformation using ETL/ELT tools, SnowPipe, dbt, or Databricks
Experience with Cloud Data Warehouse solutions experience (Snowflake, Azure DW, Redshift or similar technology in other private or public clouds).
Complete software development lifecycle experience including design, documentation, implementation, testing, and deployment
Basic Qualifications:
Bachelor’s Degree in a computer-related field or equivalent professional experience required
At least 2 years of experience in data engineering using open-source technology stack along with cloud computing (AWS, Microsoft Azure, Google Cloud)
At least 2 years of experience with designing, developing, implementing, and maintaining solutions for data ingestion and transformation projects with dbt, SnowPipe, or DataBricks
At least 2 years of advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with cloud databases
Preferred Qualifications:
3+ years of experience with dbt, SnowPipe, or DataBricks
3+ years of experience working on real-time data and streaming applications (Spark Streaming or Kafka)
3+ years of experience working with Cloud Data Warehouse solutions (i.e., Snowflake, Synapse, Redshift)
3+ years of experience with Agile engineering practices
Benefits:
As a full time, associate, you’ll enjoy our
Total Rewards Program
to help secure your financial future and preserve your health and well-being, including:
Premier Medical, Dental and Vision Insurance with no waiting period**
Paid Vacation, Sick and Parental Leave
401(k) Plan with Profit Sharing
Tuition Assistance including Direct Billing and Reimbursement payment plan options
Paid Training, Licensures, and Certificates
Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
**Coverage begins with the pay period after hire date. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
GEICO is proud to be an equal opportunity employer. We are committed to cultivating an environment where equal employment opportunities are available to all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability, or genetic information, in compliance with applicable federal, state and local law. GEICO celebrates diversity and believes it is critical to our success. As such, we are committed to recruit, develop, and retain the most talented individuals to join our team
#LI-AP1
Annual Salary
$72,000.00 - $185,000.00
The above annual salary range is a general guideline. Multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. Factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations.",128500,10000+ Employees,Subsidiary or Business Segment,Insurance,Insurance Carriers,1936,$10+ billion (USD),MD,87,data engineer,senior,"['sql', 'scala', 'python']","['snowflake', 'google cloud', 'azure', 'aws', 'redshift', 'databricks']",[],"['snowflake', 'dbt']","['kafka', 'spark']",[],bachelor,+10 years
Zazmic,4.2,United States,Data Engineer (Looker),"Europe, Latin America
Senior
We are seeking a highly skilled and experienced Senior Data Engineer to join Zazmic team.
As a Senior Data Engineer, you will be responsible for designing, developing, and maintaining our data infrastructure, as well as leveraging Looker to analyze data and identify patterns across various tables.
If you have a strong background in scripting, Python, ETL pipeline development, and expertise in utilizing Looker and BigQuery, we encourage you to apply.
Key Responsibilities:
Utilize Looker to explore and analyze data from multiple tables, identifying patterns, trends, and insights
Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions
Optimize data workflows and processes to improve efficiency and reliability
Develop and implement data quality checks and ensure data accuracy and integrity
Perform data modeling and schema design to support reporting and analytics needs
Stay up-to-date with the latest industry trends and technologies related to data engineering and analytics
Minimum Requirements :
4+ years of professional experience as a Data Engineer or similar role.
Good proficiency in scripting languages, particularly Python.
Extensive experience in building and maintaining ETL pipelines
Proficiency in utilizing Looker for data exploration, visualization, and analysis
Solid understanding of relational databases and data warehousing concepts
Hands-on experience with BigQuery or other cloud-based data warehouses
Strong SQL skills and ability to write complex queries for data extraction and manipulation
Why join us:
Ability to work remotely from anywhere in the world
Close cooperation with the development team and client
Opportunity to influence product development
We cover English classes (with a native speaker)
Boost professional brand: you can participate in local conferences as a listener or as a speaker
Regular team buildings: have fun with teammates
Gifts for significant life events (marriage, childbirth)
Tech and non-tech Zazmic Communities: support and share experience with each other",112889,201 to 500 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2012,Unknown / Non-Applicable,TX,11,data engineer,na,"['sql', 'python']",[],['looker'],[],[],[],,+10 years
Highmark Health,3.6,Pennsylvania,Analytic Data Engineer,"Company :
Highmark Health
Job Description :
JOB SUMMARY
This job architects and engineers solutions associated with analytic data for the organization and, working closely with the IT teams, assists with the design, build, and upkeep for these solutions. This includes creating pathways for analysts to access operational, derived, and external data sets. The incumbent is responsible for the operation of Data Platforms as they are associated with analytic data discovery.
ESSENTIAL RESPONSIBILITIES
Receiving some direction, work closely with IT to architect and engineer solutions to provide views for the Analytic Data Warehouse. This would include working with the proper the teams, assisting with the design, building out the design, and providing upkeep for the solution.
Assemble, test, process, and maintain the Analytic Discovery Platform for the analytics organizations. This will include working to maintain pipelines with key analytic platforms throughout the organization.
Work with alternative analytic data systems to incorporate them into the operational data flow for the Analytics Teams. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by analytic teams.
Complete tasks associated with a project. Meet with customers as part of a team lead by Lead or Senior.
Other duties as assigned.
EDUCATION

Required
Bachelor's Degree in Computer Systems Analysis, Data Processing, Healthcare Informatics, Management Information Systems, or related field or relevant experience and/or education as determined by the company in lieu of bachelor's degree.

Preferred
None
EXPERIENCE
Required
3 - 5 years of Data Analytics Experience
Preferred
1 - 3 years of Data Warehousing Experience
1 -3 years of Database Administration Experience
1 - 3 years of Healthcare Industry Experience
Experience writing SAS and SQL ETL with the ability to dissect, analyze, and update large production datasets.
Experience in healthcare quality measurement and analytics related to HEDIS, CMS Stars, CHIP, QARR, or other state regulated quality programs.
LICENSES OR CERTIFICATIONS
Required
None
Preferred
None
SKILLS
Microsoft Office
SAS
Language Requirement (other than English)
None
Travel Required
0% - 25%
PHYSICAL, MENTAL DEMANDS and WORKING CONDITIONS
Position Type
Office-Based
Teaches / trains others regularly
Occasionally
Travel regularly from the office to various work sites or from site-to-site
Rarely
Works primarily out-of-the office selling products/services (sales employees)
Never
Physical work site required
Yes
Lifting: up to 10 pounds
Occasionally
Lifting: 10 to 25 pounds
Rarely
Lifting: 25 to 50 pounds
Never
Disclaimer: The job description has been designed to indicate the general nature and essential duties and responsibilities of work performed by employees within this job title. It may not contain a comprehensive inventory of all duties, responsibilities, and qualifications required of employees to do this job.
Compliance Requirement: This position adheres to the ethical and legal standards and behavioral expectations as set forth in the code of business conduct and company policies.
As a component of job responsibilities, employees may have access to covered information, cardholder data, or other confidential customer information that must be protected at all times. In connection with this, all employees must comply with both the Health Insurance Portability Accountability Act of 1996 (HIPAA) as described in the Notice of Privacy Practices and Privacy Policies and Procedures as well as all data security guidelines established within the Company’s Handbook of Privacy Policies and Practices and Information Security Policy. Furthermore, it is every employee’s responsibility to comply with the company’s Code of Business Conduct. This includes but is not limited to adherence to applicable federal and state laws, rules, and regulations as well as company policies and training requirements.
Pay Range Minimum:
$57,700.00
Pay Range Maximum:
$106,700.00
Base pay is determined by a variety of factors including a candidate’s qualifications, experience, and expected contributions, as well as internal peer equity, market, and business considerations. The displayed salary range does not reflect any geographic differential Highmark may apply for certain locations based upon comparative markets.
Highmark Health and its affiliates prohibit discrimination against qualified individuals based on their status as protected veterans or individuals with disabilities, and prohibit discrimination against all individuals based on their race, color, age, religion, sex, national origin, sexual orientation/gender identity or any other category protected by applicable federal, state or local law. Highmark Health and its affiliates take affirmative action to employ and advance in employment individuals without regard to race, color, age, religion, sex, national origin, sexual orientation/gender identity, protected veteran status or disability.
EEO is The Law
Equal Opportunity Employer Minorities/Women/Protected Veterans/Disabled/Sexual Orientation/Gender Identity ( https://www.eeoc.gov/sites/default/files/migrated_files/employers/poster_screen_reader_optimized.pdf )
We endeavor to make this site accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact number below.
For accommodation requests, please contact HR Services Online at HRServices@highmarkhealth.org
California Consumer Privacy Act Employees, Contractors, and Applicants Notice",106700,10000+ Employees,Nonprofit Organization,Healthcare,Health Care Services & Hospitals,-1,$10+ billion (USD),Pennsylvania,-1,data engineer,na,['sql'],[],[],[],[],[],bachelor,2-5 years
MetroStar,3.7,United States,Data Engineer (Mid),"As a Data Engineer (Mid), you'll bring creative architect solutions to end customers with the goal to make an impact across the federal government.
We know that you can't have great technology services without amazing people. At MetroStar, we are obsessed with our people and have led a two-decade legacy of building the best and brightest teams. Because we know our future relies on our deep understanding and relentless focus on our people, we live by our mission: A passion for our people. Value for our customers.
If you think you can see yourself delivering our mission and pursuing our goals with us, then check out the job description below!
What you'll do:
Work with AI team members to operationalize data pipelines and ML tasks.
Provide day-to-day support of deploying Python-native ML pipelines and perform data engineering tasks to enable AI/ML capabilities.
Present results to a diverse audience in presentation or report form.
Support architectural leadership, technical support, and advisement services to ensure identity management system technologies are integrated and meeting the appropriate security requirements.
Support leadership who engage with senior level executives at a public facing Federal agency and provide subject matter expertise in security architecture and other key domain areas.
What you'll need to succeed:
5+ years of experience in Data/ML engineering (if school experience is used, at most that would contribute to 2 years of actual experience).
Experience with ETL, Data Labeling and Data Prep.
Experience designing, implementing, and maintaining data architecture and services to be used for AI/ML. Additionally, operationalizing and maintaining AI/ML models in production.
The ability to perform data analytics on program related or system related activities. This will include assessing performance and manual processes implementing methods/algorithms to automate/optimize
A bachelor's degree in Computer Science, Information Technology Management or Engineering, or other comparable degree or experience and the ability to obtain and maintain DHS Suitability.

Like we said, we are obsessed with our people. That's why we offer a generous benefits package, professional growth, and valuable time to recharge. Learn more about our company culture code and benefits. Plus, check out our accolades.
Don't meet every single requirement?
Studies have shown that women, people of color and the LGBTQ+ community are less likely to apply to jobs unless they meet every single qualification. At MetroStar we are dedicated to building a diverse, inclusive, and authentic culture, so, if you're excited about this role, but your previous experience doesn't align perfectly with every qualification in the job description, we encourage you to go ahead and apply. We pride ourselves on making great matches, and you may be the perfect match for this role or another one we have. Best of luck! – The MetroStar People & Culture Team
What we want you to know:
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire.
MetroStar Systems is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The statements herein are intended to describe the general nature and level of work being performed by employees and are not to be construed as an exhaustive list of responsibilities, duties, and skills required of personnel so classified. Furthermore, they do not establish a contract for employment and are subject to change at the discretion of MetroStar Systems.
Not ready to apply now?
Sign up to join our newsletter here.",112889,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1999,$25 to $100 million (USD),TX,24,data engineer,na,"['go', 'python']",[],[],[],[],[],bachelor,2-5 years
Spartan Technologies,3.4,"Kansas City, MO",Data Engineer I,"Spartan Technologies, Inc. - Kansas City, MO
Applicants Need to Know
100% remote in East Coast or Central US only
No Sponsorship
Contract to Hire
Hourly Rate: $45 on W2
Data Engineer I
The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products for analytics. Brings together data from different sources into a common dataset with common KPIs and dimensions for stakeholders. Understands the analytics challenges that the marketing organization faces in their day-to-day work and partners with them to design viable data solutions.
Your Job
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
Understand the existing source system data models and perform end to end data validation to maintain accuracy.
Implements and optimizes data solutions in enterprise data warehouses and big data repositories.
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues.
Develop re-useable processes that can be leveraged and standardized for multiple instances
Works with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Recommends improvements to processes, technology, and interfaces that improve the effectiveness of the team and reduce technical debt.
Ensure performance by monitoring existing workflows to ensure they are working correctly and that they reconcile with the source.
Design and develop ETL workflows and datasets to be used in extracts by our BI reporting tool Tableau.
Will write complex SQL queries with multiple joins to automate/manipulate these reporting extracts
Works in partnership with marketing stakeholders concerning the accuracy of data and efficiency of processes
Qualifications
Working experience with data modelling, data access, schemas, and data storage techniques within Snowflake
Working experience with batch and real-time data processing frameworks
Working experience working with relational databases such as SQL
Working experience with business intelligence tools and platforms
Working experience with data quality tools
Experience with ETL processes and tools
Desirable:
Working experience with Snowflake
Working experience with Google Analytics BigQuery
Working Experience with Tableau Server
Working experience with SQL Server Integration Services",81000,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),MO,16,data engineer,na,['sql'],['snowflake'],['tableau'],"['sql server', 'snowflake']",[],[],,
Unilever,4.1,Remote,Health & Wellbeing Data Engineer,"Health & Well Data Engineer
Remote – USA; Los Angeles
Health & Wellness is a strategic Unilever global business unit established to capture the growth opportunity in the €140bn global consumer health segment defined by Vitamins, Minerals, and Supplements (VMS) product category. Our ambition is to build a €5bn business globally; and be a global top 3 player within this space.
As of today, our brands include Equilibra in Italy; OLLY, SmartyPants, Liquid IV, Onnit, Welly and Nutrafol in the US with ~€1.4bn in turnover. The H&W data and analytics team operates across all these brands in one common data function with the aim of helping these brands to grow through smarter, faster, and better data driven decision making. The data team within H&W was set up in May 2022 and is looking to expand. This is a fantastic opportunity to shape the future of data in a global business unit with the latest technologies in the cloud.
What you will do:
collaborate with other engineering and business teams within H&W and Unilever to solve complex challenges using data.
drive forward best practice by building out data frameworks and design patterns to be utilized across the H&W data infrastructure.
build production ready distributed ETL data pipelines from a wide range of different sources (APIs, flat files, databases, ERP systems etc.)
build performant and reliable data models to democratize the use of data across H&W
build integrations to facilitate the use of other technologies in H&W ecosystem (e.g Kinnaxis, Anaplan)
review and deploy code from other team members as part of a DevOps process, providing coaching and mentoring to junior developers.
manage access within the environments to ensure data security protocols are being met.
contribute to architectural and governance decision marking within the H&W data ecosystem.
Who you are:
Passionate about all things data
Entrepreneurial Self-starter with the ability to thrive in a fast paced start up environment.
Fast learner with the ability to pick up new technologies quickly.
Creative problem solver who thinks outside the box
What you will bring:
3-5 years’ Data engineering experience with strong pyspark and SQL skills
1+ year Distributed computing experience [databricks/Splunk]
1+ year experience working with data in cloud environments [GCP/Azure/AWS]
Bachelor’s degree required.
Experience with streaming workloads is a plus
Experience working within a delta lakehouse in databricks is a plus
Tech Stack:
Cloud agnostic: Databricks (pyspark, scala, SQL)
Azure: Azure Data Factory, Azure Logic Apps, Azure Data Lake Storage (ADLS), Azure Blob Storage, Azure Machine Learning, PBI (data modelling, DAX)
GCP: Google Big Query, Google Cloud Storage
AWS: S3
Pay: The pay range for this position is $83,200 - $124,700. Unilever takes into consideration a wide range of factors that are utilized in making compensation decisions including, but not limited to, skill sets, experience and training, licensure and certifications, qualifications and education, and other business and organizational needs. Bonus: This position is bonus eligible. Long-Term Incentive (LTI): This position is LTI eligible. Benefits: Unilever employees are eligible to participate in our benefits plan. Should the employee choose to participate, they can choose from a range of benefits to include, but is not limited to, health insurance (including prescription drug, dental, and vision coverage), retirement savings benefits, life insurance and disability benefits, parental leave, sick leave, paid vacation, and holidays, as well as access to numerous voluntary benefits. Any coverages for health insurance and retirement benefits will be in accordance with the terms and conditions of the applicable plans and associated governing plan documents.
-
Unilever is an organization committed to diversity and inclusion to drive our business results and create a better future every day for our diverse employees, global consumers, partners, and communities. We believe a diverse workforce allows us to match our growth ambitions and drive inclusion across the business. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

Employment is subject to verification of pre-screening tests, which may include drug screening, background check, credit check and DMV check.

If you are an individual with a disability in need of assistance at any time during our recruitment process, please contact us at
NA.Accommodations@unilever.com
. Please note: This email is reserved for individuals with disabilities in need of assistance and is not a means of inquiry about positions or application statuses.
#LI-Remote",103950,10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1872,$10+ billion (USD),Remote,151,data engineer,na,"['sql', 'scala']","['databricks', 'azure', 'aws', 'google cloud']",[],[],[],[],bachelor,+10 years
"iSpace, Inc.",3.8,"Houston, TX",Data Engineer- Databricks,"Company Description

iSpace is a global services company focused on outsourcing, consulting and staffing. Over the last decade, we have helped numerous corporations and institutions reach their business objectives and IT goals.
iSpace services are centered in three areas - IT Outsourcing, IT Staff Augmentation, and Business Process Outsourcing. Our team of seasoned professionals based in the United States and India focus on providing results, driving innovation and affecting outcomes. Our clients have come to rely on our expertise, our commitment to quality, customer service and our innovative approach to problem solving to help create sustainable value for their customers and shareholders.
Specializing in Healthcare, Entertainment, Automobile and Financial Services, we work with Fortune 1000 companies throughout the United States. Our commitment to customer satisfaction is reflected in the fact that over 90% of our client base have remained with us for over 5 years.

Job Description

Title: Data Engineer- Databricks
Location: Houston TX or Chicago IL
Duration: Full Time but open to contract to hire

REMOTE Work - Candidate must be based out of CST or EST time zones

Job Responsibilities
As a member of the Concert Business Systems development team the BI Data engineer performs a wide range of data modeling, engineering, architecture and data management activities.
Follow best practices in areas of data modeling, data interoperability, metadata management
Work to streamline existing ETL processes, migrate to new platforms and improve data processing while building functional data lake for the business
Apply best practices in data engineering processes, data architecture, data security, documentation. Deep understanding of agile methodologies is required
Gather, analyze and communicate requirements for the platform functionality based on needs off implementing ETL processes.
Actively engage and lead efforts of modernizing legacy systems, streamlining traditional ETL processes and documenting all systems and processes. Work with operations team to ensure that maintenance responsibilities are not part of data engineering day to day, including delivering easy to maintain, self-healing, scalable data pipelines
Actively participate in building a data driven culture, contributing to the data community and supporting data enablement team in efforts related to making data easily accessible, ready in time, discoverable, usable and easy to understand
Join continuous innovation efforts, bring and implement ideas to advance data engineering team into the center of excellence.

TECHNICAL SKILLS/COMPETENCIES
Demonstrated data modelling and engineering skills for scalability, data streaming, self-healing and scalable process design, data partitioning, distributed data processing, metadata management.
Advanced programming skills in Python, PySpark API, advanced Spark SQL.
Hands on experience with Databricks Delta Lake architectures.
Strong understanding of data warehousing techniques.
Deep understanding of query plans, ETL best practices for timeseries reporting, advanced dimensional modeling
Experience writing SQL queries for customer applications and troubleshooting.
Hands on experience using BI tools such as Tableau, Business objects, Cognos, Power BI etc.
Expertise in understanding complex business needs, analyzing, designing and developing solutions.
Experience with technical stack which includes
Data Lake technologies like S3, AWS Data Lake, Databricks Delta lake;
Data warehousing tech like Snowflake, Teradata, Oracle EDW;
Deep understanding of operationalizing ETL processes using Spark and modern streaming
ETL technologies like Spark Streaming, StreamSets, Kafka Connect

Candidates applying for this position should have a Bachelor's degree in computer science or related field plus six years demonstrated work experience. Ideal candidates are highly motivated, resourceful, and capable of coming up to speed quickly

Additional Information

All your information will be kept confidential according to EEO guidelines.",91811,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$5 to $25 million (USD),TX,23,data engineer,na,"['sql', 'python']","['databricks', 'snowflake', 'aws']","['tableau', 'power bi']","['snowflake', 'oracle']","['kafka', 'spark']",[],bachelor,
Nike,4.2,"Beaverton, OR",Senior Data Engineer (Remote Option*),"Become a Part of the NIKE, Inc. Team
NIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.
NIKE is a technology company. From our flagship website and five-star mobile apps to developing products, managing big data and providing leading edge engineering and systems support, our teams at NIKE Global Technology exist to revolutionize the future at the confluence of tech and sport. We invest and develop advances in technology and employ the most creative people in the world, and then give them the support to constantly innovate, iterate and serve consumers more directly and personally. Our teams are innovative, diverse, multidisciplinary and collaborative, taking technology into the future and bringing the world with it.

WHO WE ARE LOOKING FOR
We are looking for a senior data engineer who will be responsible for designing, building, and maintaining the infrastructure and systems necessary for processing and analyzing large volumes of data. They play a crucial role in ensuring the availability, reliability, and efficiency of data pipelines and workflows within an organization.

WHAT YOU WILL WORK ON
As a Senior Data Engineer, we are responsible for designing, implementing, and optimizing data architectures that enable efficient data extraction, transformation, and loading (ETL) processes. Your expertise will contribute to the seamless flow of data from various sources into our data storage and analytical systems. This role will report to the Manager of North America Global Operations and Logistics Analytics Engineering.

WHO YOU WILL WORK WITH
We work closely with other data engineers, software engineers, analysts, and stakeholders to develop and maintain robust data infrastructure and pipelines.
Collaborate with cross-functional teams to understand data requirements and translate them into scalable and efficient data solutions.
Design, build, and maintain data pipelines and ETL processes to ensure reliable and timely data ingestion, transformation, and delivery.
Develop and optimize data models, schemas, and databases to support data analysis and reporting needs.
Implement data quality and validation processes to ensure the accuracy, completeness, and consistency of data.
Monitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, performance, and reliability.
Implement data security and privacy measures to safeguard sensitive information.
Continuously evaluate and recommend improvements to data infrastructure, tools, and technologies to improve efficiency and scalability.
Collaborate with users and analysts to provide them with the necessary data sets and ensure smooth data access for their analysis and analytical needs.
Document data engineering processes, best practices, and technical specifications.

WHAT YOU BRING
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
5+ years of experience as a data engineer or in a similar role, with a strong understanding of data integration, ETL processes, and data modeling.
Proficiency in programming languages such as Python, SQL for data processing and automation
Experience with distributed computing frameworks like Apache Spark, or similar technologies.
Experience with relational databases, analytical databases, NoSQL, and query optimization techniques.
Experience with orchestration tools, such as Airflow.
Experience with data warehousing concepts and technologies, such as Snowflake, or Amazon Redshift.
Experience with cloud platforms like AWS, Azure, or GCP, and related services for data storage and processing.
Familiar with CI/CD, Infrastructure as Code, Terraform, or Cloud Formation
Understanding of data governance, data security, and privacy principles.
Remote Work Option – Open to remote work, except cannot work in South Dakota, Vermont, and West Virginia. These candidates will be required to relocate.

The annual base salary for this position ranges from $98,000 to $192,500. Actual salary will vary based on a candidate’s location, qualifications, skills, and experience.

Information about benefits
#LI-DS2
NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.
NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.",145250,10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1972,$10+ billion (USD),OR,51,data engineer,senior,"['sql', 'nosql', 'python']","['snowflake', 'aws', 'azure', 'redshift']",[],['snowflake'],['spark'],['terraform'],bachelor,
Caterpillar,4.0,"Peoria, IL","Data Engineer, Cat Digital","Career Area:
Digital
Job Description:
Cat Digital is the digital and technology arm of Caterpillar Inc., responsible for bringing world class digital capabilities to our products and services. With almost one million connected assets worldwide, we're focused on using IoT and other data, technology, advanced analytics and AI capabilities to help our customers build a better world.
This is position is in Connected Data Quality team in Cat Digital. The team is responsible for building tools, dashboards and processes to enable (E2E) telemetry data quality monitoring, finding source of quality issues and work with process partners to resolve the problems at source.

Job Duties: As a Data Engineer you will be responsible for building scalable, high performance infrastructure and data driven and predictive analytics applications that provide actionable insights across all Caterpillar businesses. The position will be part of Caterpillar’s fast-moving and engineering-driven digital organization with highly motivated engineers who tackle challenges and problems that are critical to realizing significant business outcomes. Data engineers work with data scientists, business analysts, and others as part of a team that assembles large, complex data sets that provide competitive advantage.
Build infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Design, develop, and maintain performant and scalable applications
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability
Perform debugging, troubleshooting, modifications and unit testing of integration solutions
Operationalize the developed jobs and processes and processes.
Create databases and infrastructure to processing data at scale
Create solutions and methods to monitor systems and solutions
Automate code testing and pipelines
Engage directly with business partners to participate in design and development of data integration/transformation solutions per functional requirements.
Work in a scaled Agile environment accountable to deliver results in sprints.
Engage and actively seek industry perspectives through external engagements such as hackathons, peer groups, etc.
Generate, prepare, and catalog APIs
Work with UI Designer to build user interfaces per design specifications
Employee is also responsible for performing other job duties as assigned by Caterpillar management from time to time.
Required Skills:
BS or MS degree in computer science or computer engineering
5+ years of software development experience or at least 3 year of experience with master’s degree in object-oriented/object function scripting languages: Python, Java, Javascript, C++, Scala, etc.
3+ years of Python coding experience
Understanding of data structures, algorithms, profiling & optimization.
Understanding of SQL, ETL design, and data modeling techniques
Top candidates will also have:
2+ years of experience developing, deploying, and maintaining software in AWS cloud and working with AWS services: S3, DynamoDB, RDS, SageMaker, ECS, EMR, Lambda, Athena, AWS Glue, CloudFormation
2+ years of experience in developing scripts, procedures in snowflake.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with using CI/CD tools such as Jenkins, GoCD, Azure Devops etc.
Experience with automated build automation tools (Maven, etc.).
Advanced level of experience with object oriented programming, data structures and algorithms.
Knowledge of enterprise data sources and uses
Working within an Agile framework (ideally Scrum)
#LI
#B
Relocation is available for this position.Visa sponsorship available for eligible applicants.
EEO/AA Employer. All qualified individuals - Including minorities, females, veterans and individuals with disabilities - are encouraged to apply.
Not ready to apply? Submit your information to our Talent Network here .",86643,10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,1925,$10+ billion (USD),IL,98,data engineer,na,"['sql', 'scala', 'java', 'nosql', 'python']","['snowflake', 'aws', 'azure']",[],"['dynamodb', 'snowflake']",[],[],master,+10 years
Regions,3.5,"Atlanta, GA",Data Engineer (REMOTE OPPORTUNITY),"Thank you for your interest in a career at Regions. At Regions, we believe associates deserve more than just a job. We believe in offering performance-driven individuals a place where they can build a career --- a place to expect more opportunities. If you are focused on results, dedicated to quality, strength and integrity, and possess the drive to succeed, then we are your employer of choice.

Regions is dedicated to taking appropriate steps to safeguard and protect private and personally identifiable information you submit. The information that you submit will be collected and reviewed by associates, consultants, and vendors of Regions in order to evaluate your qualifications and experience for job opportunities and will not be used for marketing purposes, sold, or shared outside of Regions unless required by law. Such information will be stored in accordance with regulatory requirements and in conjunction with Regions’ Retention Schedule for a minimum of three years. You may review, modify, or update your information by visiting and logging into the careers section of the system.

Job Description:
At Regions, the Data Engineer focuses on the evaluation, design, and execution of data structures, processes, and logic to deliver business value through operational and analytical data assets. The Data Engineer uses advanced data design and technical skills to work with business subject matter experts to create enterprise data assets utilizing state of the art data techniques and tools.

Primary Responsibilities
Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use cases
Builds data pipelines to collect and arrange data and manage data storage in Regions’ big data environment
Builds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark.
Coordinates design and development with Data Products Partners, Data Scientists, Data Management, Data Modelers, and other Technical partners to construct strategic and tactical data stores
Ensures data is prepared, arranged and ready for each defined business use case
Designs and deploys frameworks and micro services to serve data assets to data consumers
Collaborates and aligns with technical and non-technical stakeholders to translate customer needs into Data Design requirements, and work to deliver world-class visualizations, data stories while ensuring data quality and integrity
Provides consultation to all areas of the organization that plan to use data to make decisions
Supports any team members in the development of such information delivery and aid in the automation of data products
Acts as trusted adviser and partner to business leads- assisting in the identification of business needs & data opportunities, understanding key drivers of performance, interpreting business case data drivers, turning data into business value, and participating in the guidance of the overall data and analytics strategy

This position is exempt from timekeeping requirements under the Fair Labor Standards Act and is not eligible for overtime pay.

Requirements
Bachelor's degree and five (5) years of experience in a quantitative/analytical/STEM field or technical related field
Or Master’s degree and three (3) years of experience in a quantitative/analytical/STEM field or technical related field
Or Ph.D. and one (1) year of experience in a quantitative/analytical/STEM field
Three (3) years of working programming experience in Python/PySpark, Scala, SQL
Three (3) years of working experience in Big Data Technology in Hadoop, Hive, Impala, Spark, or Kafka

P references
Prior banking or financial Services experience
Experience developing solutions for the financial services industry
Background in Big Data Engineering and Advanced Data Analytics
Experience in Agile Software Development
Experience or exposure to cloud technologies and migrations

Skills and Competencies
Experience building data solutions at scale
Experience designing and building relational data structures in multiple environments
Experience with Airflow, Argo, Luigi, or similar orchestration tool
Experience with DevOps principals and CI/CD.
Experience with Docker and Kubernetes
Experience with No-SQL databases such as HBase, Cassandra, or MongoDB
Experience with streaming technologies such as Kafka, Flink, or Spark Streaming
Experience working with Hadoop ecosystem building Data Assets at an enterprise scale
Proven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data Assets
Significant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision making
Strong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworks
Strong background in synthesizing data and analytics in a large (fortune 500), complex, and highly regulated environment
Strong technical background including database and business intelligence skills
Strong communication skills through written and oral presentations

Additional Job Description
Candidates must have experience with DevOps principals and CI/CD.
Candidates must have experience with Docker and Kubernetes
Candidates must have experience with streaming technologies such as Kafka, Flink, or Spark Streaming.
Preferred experience in Snowflake, SQL and Python.
Preferred experience in developing API's.
Preferred experience with AWS Lambda functions.
This position may be filled at a higher level depending on the candidate's qualifications and relevant experience.

Position Type Full time

Compensation Details
Pay ranges are job specific and are provided as a point-of-market reference for compensation decisions. Other factors which directly impact pay for individual associates include: experience, skills, knowledge, contribution, job location and, most importantly, performance in the job role. As these factors vary by individuals, pay will also vary among individual associates within the same job.

The target information listed below is based on the national range and level of the position.

Job Range Target:
Minimum: $85,374.00 USD
Median: $122,800.00 USD
Incentive Pay Plans: This job is not incentive eligible.

Benefits Information
Regions offers a benefits package that is flexible, comprehensive and recognizes that ""one size does not fit all"" for associates. Listed below is a synopsis of the benefits offered by Regions for informational purposes, which is not intended to be a complete summary of plan terms and conditions.
Paid Vacation/Sick Time
401K with Company Match
Medical, Dental and Vision Benefits
Disability Benefits
Health Savings Account
Flexible Spending Account
Life Insurance
Parental Leave
Employee Assistance Program
Associate Volunteer Program

Please note, benefits and plans may be changed, amended, or terminated with respect to all or any class of associate at any time. To learn more about Regions’ benefits, please click or copy the link below to your browser.

https://www.regions.com/welcometour/benefits.rf

Location Details Regions Plaza Atlanta
Location: Atlanta, Georgia

Bring Your Whole Self to Work

We have a passion for creating an inclusive environment that promotes and values diversity of race, color, national origin, religion, age, sexual orientation, gender identity, disability, veteran status, genetic information, sex, pregnancy, and many other primary and secondary dimensions that make each of us unique as individuals and provide valuable perspective that makes us a better company and employer. More importantly, we recognize that creating a workplace where everyone, regardless of background, can do their best work is the right thing to do.

OFCCP Disclosure: Equal Opportunity Employer/Disabled/Veterans",63377,10000+ Employees,Company - Public,Financial Services,Banking & Lending,1971,$5 to $10 billion (USD),GA,52,data engineer,na,"['sql', 'scala', 'python']","['snowflake', 'aws']",[],"['mongodb', 'snowflake', 'hive']","['kafka', 'hadoop', 'flink', 'spark']",['docker'],bachelor,
Comcast,3.8,"West Chester, PA",Data Engineer 2,"Make your mark at Comcast - a Fortune 30 global media and technology company. From the connectivity and platforms we provide, to the content and experiences we create, we reach hundreds of millions of customers, viewers, and guests worldwide. Become part of our award-winning technology team that turns big ideas into cutting-edge products, platforms, and solutions that our customers love. We create space to innovate, and we recognize, reward, and invest in your ideas, while ensuring you can proudly bring your authentic self to the workplace. Join us. You’ll do the best work of your career right here at Comcast. (In most cases, Comcast prefers to have employees on-site collaborating unless the team has been designated as virtual due to the nature of their work. If a position is listed with both office locations and virtual offerings, Comcast may be willing to consider candidates who live greater than 100 miles from the office for the remote option.)

Job Summary
The Data Experience Team (dx) has the responsibility of data engineering and data governance for Comcast data platforms, focused on gathering, organizing, and making sense of Comcast data. Within the Big Data domain, this role is responsible for software development including planning, designing, developing, testing, implementation and management of big data applications focused on video usage and viewership, both on premise and in the cloud. Responsible for design to implementation, including new programs, enhancements, and modifications. Contribute to functional strategy development. The candidate should have experience operating in a DevSecOps team and will be expected to contribute to an internal DevSecOps culture encompassing end-to-end responsibility for development, deployment, production support, monitoring, data quality and automation of their applications.
Job Description
Core Responsibilities
Optimize data ingest, filtering and improvement
Knowledge of data structures, design patterns, and algorithms
Experience with OOO programming languages and SQL
An open mind and a passion for coding excellence
Building data products using AWS cloud and Spark processing technologies.
Experience building End to End Datalake data product ingestions and optimize the pipelines.
Great design and problem-solving skills, with a strong bias for architecting at scale
Strong troubleshooting and problem-solving skills, adaptable, proactive, and willing to take ownership
Analyzes and determines integration needs.
Evaluates and plans software designs, test results and technical manuals.
Reviews literature, patents and current practices relevant to the solution of assigned projects.
Programs new software, web applications and supports new applications under development and the customization of current applications.
Edits and reviews technical requirements documentation.
Works with Quality Assurance team to determine if applications fit specification and technical requirements.
Displays knowledge of engineering methodologies, concepts, skills and their application in the area of specified engineering specialty.
Displays knowledge of and ability to apply, process design and redesign skills.
Displays in-depth knowledge of and ability to apply, project management skills.
Consistent exercise of independent judgment and discretion in matters of significance.
Regular, consistent and punctual attendance. Must be able to work nights and weekends, variable schedule(s) and overtime as vital.
Other duties and responsibilities as assigned.
Required Technical Skills:
Programming languages like Scala, Python.
Databricks, Apache Spark, Spark Streaming, Pyspark, Spark Sql.
AWS Cloud Computing like Glue, Lambada, CloudWatch, Athena.
Big Data Architecture, Solutions & Technologies.
Relational databases like Hive, Oracle, and No-Sql Databases like Dynamodb.
CI/CD tools like GoCD, Jenkins, Concourse.
Employees at all levels are expected to:
Understand our Operating Principles; make them the guidelines for how you do your job.
Be responsible for the customer experience - think and act in ways that put our customers first, give them seamless digital options at every touchpoint, and make them promoters of our products and services.
Know your stuff - be hard-working learners, users and advocates of our groundbreaking technology, products and services, especially our digital tools and experiences.
Win as a team - make big things happen by working together and being open to new insights!
Be an active part of the Net Promoter System - a way of working that brings more employee and customer feedback into the company - by joining huddles, making call backs and helping us elevate opportunities to do better for our customers.
Get results and growth!
Respect and promote inclusion & diversity.
Do what's right for each other, our customers, investors and our communities.
Disclaimer:
This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.
This position is ineligible for visa sponsorship. To be considered for this role, you must be legally authorized to work in the United States and not require sponsorship for employment now or in the future.
Comcast is an EOE/Veterans/Disabled/LGBT employer.
Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law.

Education
Bachelor's Degree
While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.
Relevant Work Experience
2-5 Years

Salary:
Pay Range: This job can be performed in Denver Campus, with a Pay Range of $87,529.82 USD - $131,294.73 USD
Comcast intends to offer the selected candidate base pay within this range, dependent on job-related, non-discriminatory factors such as experience.

Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That’s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality – to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.",109412,10000+ Employees,Company - Public,Telecommunications,Telecommunications Services,1963,$10+ billion (USD),PA,60,data engineer,na,"['sql', 'scala', 'python']","['databricks', 'aws']",[],"['dynamodb', 'oracle', 'hive']",['spark'],[],bachelor,5-10 years
VedaInfo Inc,4.1,"Phoenix, AZ",Big Data Engineer,"Hi,
I hope this note finds you well
I am reaching out to you regarding your career opportunity with us for “Big Data Engineer” for a “6+ Months Contract” @Phoenix, AZ
Please find the below requirement details.
Job Title: Big Data Engineer
Location: Phoenix, AZ (Onsite)
Duration: 6+ months
Rate: $60/hr C2C
Responsibilities:
Design, implement, and maintain big data systems handling large volumes of data.
Utilize Hadoop, Hive, and Spark for efficient data processing and analysis.
Collaborate with cross-functional teams to understand data requirements.
Develop and optimize data ingestion, storage, and transformation processes.
Build scalable data pipelines for seamless data processing and analysis.
Monitor and troubleshoot data processing and performance issues.
Stay updated with emerging big data technologies.
Requirements:
Bachelor's/Master's degree in CS, Engineering, or related field
7+ years of experience as a Big Data Engineer
Strong proficiency in Hadoop, Hive, and Spark
Extensive experience with AWS or Azure
Solid understanding of data ingestion, storage, and transformation
Excellent problem-solving and communication skills
Proactive and self-motivated
This is a 6+ month onsite contract position in Phoenix, AZ, with the possibility of extension. Compensation is $60/hr on C2C basis.
Please submit your updated resume highlighting relevant experience.
Thank you for your interest!
Thanks & Regards
Mohammed ZAIN
Job Type: Contract
Pay: $50.00 - $60.00 per hour
Experience level:
7 years
Ability to commute/relocate:
Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
To improve your chances of getting Shortlisted, make sure to tailor your Profile to the Job Requirements, Highlight your Relevant Skills and Experience, and then upload your Resume.
Experience:
AWS (Amazon Web Services): 7 years (Required)
Azure: 7 years (Required)
Big data: 7 years (Required)
Hadoop: 7 years (Required)
Apache Hive: 7 years (Required)
Spark: 7 years (Required)
Work Location: One location",99000,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),AZ,-1,data engineer,na,[],"['azure', 'aws']",[],['hive'],"['spark', 'hadoop']",[],bachelor,5-10 years
Sconcept,4.0,Remote,Azure Data Engineer,"** W2 LONG TERM CONTRACT BASED....**(NO C2C)
JOB TYPE:- Long Term Contract(6 months)Extendable
JOB LOCATION:-USA
About Sconcept LLC:-Software Concepts Consulting was established in 2008. With our “hands-on” approach we are very successful in delivering high performance, secure IT solutions and IT services. With headquarters in Carrollton, Texas we are specialized in providing business solutions to the Fortune 500 clients. Our clients include Software, Banking, Financial, Healthcare, Retail, Consumer goods and Insurance sectors. We deliver candidates based on client requirements.
Required Skills:-
Must have worked on several (5+) projects building solutions for Data Pipelines, Data warehousing, Data Modelling on Azure Data Platform (AWS, Google Cloud Platform - good to have)
Expertise in design and development of Data pipelines and ETL using ADF, Databricks to move data from relational/structured/unstructured data from source to data lake to data warehouse, on Azure
Creating Pipelines and integrations of multiple data sources
Data processing and transformation using Databricks (Scala/Python)
Expertise in optimizing cost and performance of data pipelines
Experience working with Data Lake, Synapse on Azure
Designing Data Lake storage layer (Landing/staging, raw, trusted/curated zones)
Designing data storage DB schema, data models and processing on Azure Synapse DW
Experience with Synapse Spark Pool
Experience in Data Modelling
Building Data Models from structured and semi-structured data (Data Model, Common Data Model etc.)
Performance optimization of data model for high performance analytical and reporting workloads
Expertise with SQL Server and extensive experience with SQL programming
Programming and optimizing DB objects Views, Stored Procs, Functions
Good experience in defining data access policy and security for azure data services
Experience with Oracle EDW
Knowledge/Experience migrating from Oracle EDW to any cloud DW
Experience integrating AI skills/ML models with Data and reporting solutions
Experience in building API layer for downstream consumption
Job Type: Contract
Pay: $45.00 - $50.00 per hour
Benefits:
Health insurance
Compensation package:
Bonus pay
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'scala', 'python']","['databricks', 'azure', 'aws', 'google cloud']",[],"['sql server', 'oracle']",['spark'],[],,0-2 years
META FORCE IT LLC,4.0,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Type: Contract
Pay: $45.96 - $50.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Cincinnati, OH 45215: Reliably commute or planning to relocate before starting work (Required)
Experience:
data engineer: 10 years (Preferred)
Big data: 3 years (Preferred)
Work Location: Hybrid remote in Cincinnati, OH 45215",86364,,,,,-1,,OH,-1,data engineer,na,"['nosql', 'java']",[],[],[],[],[],bachelor,2-5 years
ProGrad,4.2,Remote,Data Engineer,"We are looking for data engineers that are excited about building and maintaining data pipelines for a variety of interests. The pipelines will ingest data from a variety of sources ranging from internal change data streams to external integrations with various SaaS platforms through webhooks and events. The ideal candidate is self driven and looking to grow as a data engineer while having an outsized impact on the future of data at the company. A love for database architecture and optimisation is a must as is the ability to work across diverse teams.
Requirements/Qualifications:
Bachelor's or Master's degree in Data Science (specialization)
Software development experience coding in a general-purpose programming language such as C, C++, Java, JavaScript, Golang, or Python
Experience working with data structures or algorithms (i. e., data structures /algorithms class, coursework/projects, research, internships, or other practical experience in/outside of school or work (including open source hobby coding)
Proficiency in SQL, and NoSQL Databases
Experience working with large datasets
Experience writing and debugging complex SQL queries
A strong understanding of cloud database best practices and performance optimisation
Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
. A strong understanding of cloud database best practices and performance optimisation
Good to have skills:
. Experience building and maintaining data pipelines, coupled with knowledge of good programming practices such as unit and integration testing
Job Types: Full-time, Permanent
Salary: Up to $1,200,000.00 per year
Experience level:
1 year
Under 1 year
Schedule:
Day shift
Work Location: Remote
Speak with the employer
+91 9057202434",112889,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'java', 'nosql', 'python']",[],[],[],[],[],bachelor,0-2 years
Enterprise Minds,4.0,Remote,Data Engineer,"Job Role: SQL Data Engineer
Location: Remote Opportunity - Need to work in PST hours
Visa: US Citizen Only
Duration: Long term
Roles and Responsibilities:
3+ years of experience with a complex build system in AWS using Starburst.
5+ years of experience with installation, maintenance, and administration of Oracle databases- PL/SQL and ANSI SQL
Oracle DBA + Oracle SQL to ANSI SQL conversion, performance/query tuning
3+ years of experience configuring, integrating, and securing multiple AWS database offerings (ex. RDS, Aurora, Casandra, and Dynamo)
2+ years of experience with supporting Linux systems engineering efforts in system design and evaluation, solution engineering, software development, or system administration.
Experience with Database Schema as Code tools
Ability to design and develop complex SQL queries and reports to retrieve required data.
Experience with database partitioning and microservices
Knowledge of Agile methodologies or the software development life cycle (SDLC)
US Citizenship is Mandatory.
Job Type: Full-time
Experience:
Data Engineer: 10 years (Preferred)
AWS using Starburst: 4 years (Preferred)
PL/SQL and ANSI SQL: 2 years (Preferred)
AWS RDS: 1 year (Preferred)
Work Location: Remote",112889,,,,,-1,,Remote,-1,data engineer,na,['sql'],['aws'],[],"['casandra', 'oracle']",[],[],,0-2 years
MARVEL TECHNOLOGIES INC,3.7,"Cincinnati, OH",Data Engineer,"Job Description
The data engineer designs and builds platforms tools and solutions that help the bank manage secure and generate value from its data. The person in this role creates scalable and reusable solutions for gathering collecting storing processing and serving data on both small and very large i.e. Big Data scales. These solutions can include on-premise and cloud-based data platforms and solutions in any of the following domains ETL business intelligence analytics persistence relational NoSQL data lakes search messaging data warehousing stream processing and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions elevating concerns and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite achieves results by consistently identifying assessing managing monitoring and reporting risks of all types.
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Responsible for design Development and Support of data solutions APIs tools and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams Enterprise architecture infrastructure information security and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design performance integration security etc.
Conduct research and Development based on current trends and technologies related to the banking industry data engineering and architecture data security and related topics.
Work with developers to Build CI/CD pipelines Self-service Build tools and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed including legacy system replacement Monitoring and analytics improvements tool Development and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
MINIMUM KNOWLEDGE SKILLS AND ABILITIES REQUIRED:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management business and IT sponsors and technical resources in language that is appropriate for each group.
Fundamental understanding of distributed computing principles
Knowledge of application and data security concepts best practices and common vulnerabilities.
Conceptual understanding of one or more of the following disciplines preferred big data technologies and distributions metadata management products commercial ETL tools Bi and reporting tools messaging systems data warehousing Java language and run time environment major version control systems continuous integration/delivery tools infrastructure automation and virtualization tools major cloud or rest API design and development.
Job Types: Full-time, Contract
Pay: $60.00 - $62.00 per hour
Compensation package:
Hourly pay
Yearly pay
Experience level:
9 years
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",109800,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,$5 to $25 million (USD),OH,-1,data engineer,na,"['nosql', 'java']",[],[],[],[],[],bachelor,5-10 years
DiamondPick,4.5,Remote,GCP Data Engineer,"Position: GCP Data Engineer (8+years)
Location: Remote
Type: Contract
Job Details:
· Relevant Industry Work Experience (6+ for Developer, preferred 9+ years)
· Experience extracting data from a variety of sources, and a desire to expand those skills (Excellent knowledge in SQL and Spark is mandatory)
· Strong knowledge of Google BigQuery and architecting data pipelines from on-prem to GCP.
· Experience building applications using Google Cloud Platform related frameworks such as DataProc and GCS at the minimum.
· Excellent Communication Skills to Understand and Pass on Requirements.
· Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark.
· Professional experience with a cloud hosting platform (GCP preferred)
· GCP certification is preferred.
Job Type: Contract
Salary: Up to $65.00 per hour
Experience level:
10 years
8 years
9 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Google BigQuery: 4 years (Preferred)
DataProc: 2 years (Preferred)
Google Cloud Platform: 9 years (Required)
Spark: 8 years (Required)
SQL: 8 years (Required)
License/Certification:
Google Cloud Platform(GCP) Certification (Required)
Work Location: Remote",117000,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,['sql'],['google cloud'],[],['hive'],"['hadoop', 'spark']",[],,5-10 years
Zllius Inc.,4.0,"Chicago, IL",Data Engineer,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the role Data Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Duration: Long-Term
Position Type: W2/ 1099
Visa: Any
Job Description:
Technical/Functional Skills
Successful candidates will have:
Hands on experience in Dremio and Scripting
Hands on Scripting experience in Python and Flask
Hands on experience in Integrating data from multiple source systems (PostgreSQL and CSV)
Experience with Apache Superset
Ability to write complex SQL queries
Experience in building data pipelines using PostgreSQL, Oracle database, Dremio, AWS Open Search
Experience with Docker Containerization of Code
Work as a part of a scrum team in Agile methodology
ETL, Data Pipelines, PostgreSQL, SQL Queries, Scripting
Job Types: Full-time, Contract
Salary: $111,076.11 - $133,769.08 per year
Schedule:
8 hour shift
Day shift
Monday to Friday
Work Location: On the road",122423,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,"['sql', 'python']",['aws'],[],"['postgresql', 'oracle']",[],['docker'],,
shreetek,4.0,"Charlotte, NC",Senior Data Engineer,"Job Title : Data Engineer
ONLY USC
On W2 only
Job Location : Newjersey OR Charlotte,NC
Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Degree in Data Engineering preferred.
Two Programmers –
Convert existing SAS code to python/pyspark code for model operation in the cloud.
Create and sustain policy analysis models in the cloud.
Troubleshoot user interfaces in the cloud.
Create and sustain intuitive user interfaces in the cloud.
A degree in Computer Science is preferred.
Overall across resources keep following stack as reference
Visualization : Tableau
Data Modeling/Science : Python / SAS
Engg : AWS cloud native services, security, data pipeline
Job Type: Contract
Salary: $50.00 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Charlotte, NC 28202: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",99000,,,,,-1,,NC,-1,data engineer,senior,['python'],['aws'],['tableau'],[],[],[],,
Amazee Global Ventures Inc,5.0,Remote,Data Engineer,"(W2 ONLY)
Job Title: Data Engineer
Duration : 12 months
Location: Remote
6-7 years of experience
Healthcare experience is must
Design, develop, and maintain Tableau dashboards and reports
Develop and maintain data models and ETL processes
Analyze data to identify trends and patterns
Create and maintain data dictionaries and data models
Develop and maintain data pipelines
Collaborate with stakeholders to understand data requirements
Develop and maintain data quality processes
Monitor data quality and performance
Troubleshoot data issues
Develop and maintain documentation
Requirements:
Bachelor’s degree in Computer Science, Information Systems, or related field
4+ years of experience in data engineering and analysis
Expertise in Tableau
Experience with SQL and data modeling
Knowledge of ETL processes
Knowledge of data warehousing
Excellent problem-solving and communication skills
Ability to work independently and in a team environment\
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Type: Full-time
Salary: $40.33 per hour
Work Location: Remote",72594,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),Remote,4,data engineer,na,['sql'],[],['tableau'],[],[],[],bachelor,+10 years
Dentsu Aegis Network,3.6,"New York, NY",Data Engineer - BI Developer,"Company Description

Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Some of our award-winning agencies include 360i, Carat, dentsumcgarrybowen, DEG, dentsuX, iProspect and Merkle. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
Part of Dentsu International, Dentsu Creative is a Global Creative Network that transforms brands and businesses through the power of Modern Creativity. Led by Global Chief Creative Officer Fred Levron, 9,000 experts across the globe work seamlessly together to deliver ideas that Create Culture, Shape Society and Invent the Future. Dentsu Creative was launched in June 2022 to address a client need for simplicity and will be Dentsu International’s sole creative network by the end of 2022.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description

The Data Engineer is a new key role within the Business Platforms Americas team. We are looking for a well-rounded Senior BI Developer expertise with strong knowledge of MS SQL, PowerBI, and data warehousing techniques. This is a unique opportunity to be involved in delivering leading-edge business analytics using the latest and greatest cutting-edge BI tools, such as cloud-based databases, self-service analytics and leading visualization tools enabling the company’s aim to become a fully digital organization.
Key Responsibilities
Collaborate with the BI Dev team members to evaluate, design, develop BI reports and dashboards according to functional specifications while maintaining data integrity and data quality
Deliver Technical Design Document capturing specific processes and data flows, data definitions and relevant business rules
Apply best practices of data integration for data quality and automation
Work with business analysts to understand business requirements and use cases to write and assign technical stories and tasks
Work independently within guidelines, responsible for initiating, planning, executing, controlling, and implementation of projects using a formal project management and agile methodology
Work collaboratively with key stakeholders to translate business information needs into well-defined data requirements to implement the BI solutions
Work with team to provide support for existing analytics and PowerBI reporting platforms
Coaching, mentoring, and providing technical direction and training to other IT personnel
Working with BI & Analytic teams to develop and establish BI road Map/Vision

Qualifications

Experience:
Excellent communication skills
Over 7-10 years of experience in Data warehousing and Business Intelligence
Over 5 years’ experience in a Business Intelligence Analyst or Developer roles
Over 4 years’ experience using ADF for data warehousing
Experience in designing and performance tuning data warehouses and data lakes.
2+ years experience in developing data models and dashboards using Power BI within an IT department
Being delivery-focused with a can-do attitude in a sometimes-challenging environment is essential.
Experience using Power BI to visualize data held in SQL Server
Experience working with finance data highly desirable
Other key Competencies:
Strong communications skills and ability to turn business requirements into technical solutions
Experience in developing data lakes and data warehouses using Microsoft Azure
Demonstrable experience designing high-quality dashboards using Power BI
Strong database design skills, including an understanding of both normalized form and dimensional form databases.
In-depth knowledge and experience of data-warehousing strategies and techniques e.g., Kimble Data warehousing
Experience in Cloud based data integration tools like Azure Data Factory
Experience in power bi data modelling and DAX is preferred
Experience in Azure Dev Ops or JIRA is a plus
Familiarity with agile development techniques and objectives

Additional Information

The anticipated salary range for this position is $94,000-146K. Salary is based on a wide range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit https://dentsubenefitsplus.com/.

Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.

#LI-AJ1
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.",120000,10000+ Employees,Company - Public,Media & Communication,Advertising & Public Relations,-1,Unknown / Non-Applicable,NY,-1,data engineer,na,['sql'],['azure'],['power bi'],['sql server'],[],[],,+10 years
NAVA TECH LLC,4.0,Remote,Azure Data Engineer,"Azure Data Engineer
Remote
Long Term
Note: Need 10+ years of experience
Sound skills and hands on experience with Azure Data Lake, Azure Data Factory, SQL Data Warehouse Azure Blob, Azure Storage Explorer
Proficient in creating Data Factory pipelines for on-cloud ETL processing; copy activity, custom Azure development etc.
Knowledge of Azure Data Catalog, Event Grid, Service Bus, SQL and Synapse
Experience using Azure Databricks platform.
Experience with Python programming.
Experience using pandas and numpy for data engineering and data cleansing;
Ability to troubleshoot the job scheduler and compute clusters.
Experience with databricks CLI and secrets module.
Experience with Azure Blob storage access configuration.
p l e a s e s h a r e r e s u m e s t o m o h a m m e d (a t) n a v a t e c h (d o t) u s O R 4 4 3 5 3 7 9 8 3 5
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",112500,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'r', 'python']","['databricks', 'azure']",[],[],[],[],,0-2 years
Fincons Group,4.0,Remote,Data Engineer – Databricks / Kafka / SQL,"Fincons Group is an IT business consulting company that has been designing the digital future of leading companies on international markets for 40 years. Fincons Group built its reputation on foreseeing and interpreting new business models and the rapid evolution of IT systems by building a complete range of services: from research and consulting to design and development, from system integration of leading vendor software solutions right up to application management, supporting clients step by step along their digital transformation journey. Fincons is a multinational with over 2,600 people and several offices worldwide (in Italy, Switzerland, Germany, France, the United Kingdom, Belgium and the USA), but above all a Family Company where the founders play a strategic role with commitment and passion, grounding the company in the same principles of a united and caring family.
We firmly believe in the value of cooperation and in the contribution that every idea and intuition can bring. We believe that everyone’s effort can make a difference. Transforming knowledge into a strategic resource is our company mission, and we pursue it with the help of our most strategic asset: our people.

Job Summary
Looking for an experienced data engineer with hands on experience in developing solutions based on Databricks. If you are a curious team-player with passion to learn and working for different clients to develop modern data streaming platforms, please reach out to us!

Work location: any US location might be considered
Employment type: freelance / contractor
Start date: about July 1th
Duration: about 6 months with possible renewal

Responsibilities and Duties
Design, develop and implement real time data ingestion pipelines from multiple sources using Databricks
Develop high-quality, scalable and re-usable frameworks for ingesting high volume and large data sets
Apply best practices to Integrate and ingest various big data formats like Protobuf, Parquet, AVRO and JSON
Work with event based / streaming technologies like Kafka, or Kinesis, to ingest and transform the data using KafkaStreams or KSQL
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Work to load test the pipelines for quality and better performance using JMeter, or Gatling etc

Qualifications and skills
Bachelor’s degree and first experience designing, developing, deploying and/or supporting data pipelines using Databricks
Experience in designing and implementing Medallion architecture is preferred
Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS
Hands on experience in performance tuning and optimizing code running in Databricks environment
Proficient in programming languages like Pyspark and Python
Good understanding of SQL, T-SQL and/or PL/SQL
Demonstrated analytical and problem-solving skills particularly those that apply to a big data environment
Experience with agile development methodologies.
Ability to work effectively both independently and in a group or team environment

Authorization to Work
Applicants for employment in the US must be authorized to work in the US. Fincons Group will not sponsor applicants for work visas.
All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, sexual orientation, gender identity, national origin, age, disability, or protected veteran status, or any other legally protected basis, in accordance with applicable law.
The data will be processed and stored exclusively for the purposes of this or future selections, guaranteeing the rights referred to in art. 13 Legislative Decree 196/03 and EU regulation 679/2016 (GDPR)
2yrKJdtT13",112889,201 to 500 Employees,Company - Private,Financial Services,Accounting & Tax,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'python']","['databricks', 'azure', 'aws']",[],[],['kafka'],[],bachelor,
E-Business International INC,3.6,"Alexandria, VA",AWS Data Engineer,"Role: AWS Data Engineers
Location: Alexandria, VA
Full-Time/Permanent
AWS Engineers
We are seeking TWO (2) AWS Engineers to join our growing team. The qualified applicants will become part of the Enterprise Data Analytics Services (EDAS) program for a large federal agency in Alexandria, VA. Hybrid work options are available.
The AWS Engineer is responsible for the development and support of cloud-based solutions. Solutions include moving assets from on-premise to cloud environment, evaluation, and optimization. AWS Engineer will collaborate with the team effectively to ensure the team's success.
Responsibilities/Duties:
Work on automating the migration process in AWS from development to production.
Create CFT deploying/updating cloud resources.
Advise the engineering and software engineering team as they migrate from on-premise to cloud infrastructure.
Optimize cloud workloads for cost, scalability, availability, governance, compliance, etc.
Guide and/or provide hands-on support to administer production, staging, and deployment environments.
Partner with multi-disciplinary teams to understand requirements and plan architecture and solutions.
Apply insight and expertise across AWS services.
Apply knowledge of scripting and automation using tools like PowerShell, Python, Bash, Ruby, Perl, etc.
Experience and Qualifications:
Good working knowledge of AWS Services, including Compute, Storage Networking, Database, Management tools, Security, Identity, and Compliance.
3 years’ experience in AWS (EC2, Glue, S3, Redshift, Python programming).
Experience implementing and debugging workloads in AWS.
Good knowledge of creating CFTs.
Experience building infrastructure inside AWS via code. Familiarity with tools such as Terraform or CloudFormation.
Ability to proactively communicate, interact, and solve critical customer problems.
Ability to think and act strategically and proactively.
3 years of experience in software development tools and methodologies.
3 years of experience in infrastructure architecture, database architecture, and networking.
3 years of experience architecting/deploying operating solutions built on AWS.
Education/Certifications:
Bachelor's Degree in Computer Science, Information Technology, Engineering, or associated discipline.
AWS certification is a plus!
Clearance Requirements:
Ability to attain and maintain a Public Trust.
Job Type: Full-time
Salary: From $100,000.00 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Alexandria, VA 20598: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Expected Salary Range (Must)?
Visa Status/ Work Authorization (Must)?
Experience:
AWS: 4 years (Required)
Work Location: One location",100000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1992,Unknown / Non-Applicable,VA,31,data engineer,na,['python'],"['aws', 'redshift']",[],[],[],"['terraform', 'bash']",bachelor,2-5 years
Dun & Bradstreet,3.9,Remote,Data Engineer I (R-14521),"Why We Work at Dun & Bradstreet
Dun & Bradstreet unlocks the power of data through analytics, creating a better tomorrow. Each day, we are finding new ways to strengthen our award-winning culture and accelerate creativity, innovation and growth. Our 6,000+ global team members are passionate about what we do. We are dedicated to helping clients turn uncertainty into confidence, risk into opportunity and potential into prosperity. Bold and diverse thinkers are always welcome. Come join us!

The Data Engineer I will work with the senior data engineers to build and maintain applications that are responsible for the ingestion of data into the Dun and Bradstreet Contact Pipeline.

The Data Engineer I will work on analyzing performance/throughput blockers of the applications and will work with other team members to remove the bottlenecks in the applications.

The Data Engineer I will also work with creating metrics from the ingestion applications to help determine the areas that need work in terms of performance and/or throughput.
Responsibilities:
Create and maintain applications in python.
Take ownership of existing applications for further development/improvements
Work closely with related groups to ensure business continuity
Perform analysis on code bases to increase performance.
Work as part of the team to code review and test other members’ code changes.
Work as a member of one or more agile teams, using lean principles and SCRUM methodology

Requirements:
Bachelor’s degree (preferable in computer science, mathematics, data science, or a related field)
Experience with Python for application development (2-5 years)
Experience with SQL for data analysis and querying (2-5 years)
Ability to work independently to deliver critical projects on time
Ability to work closely with others to problem solve
Experience with hosted environments, AWS, Azure, or other cloud service providers preferred
Benefits We Offer
Generous paid time off in your first year, increasing with tenure.
Up to 16 weeks 100% paid parental leave after one year of employment.
Paid sick time to care for yourself or family members.
Education assistance and extensive training resources.
Do Good Program: Paid volunteer days & donation matching.
Competitive 401k & Employee Stock Purchase Plan with company matching.
Health & wellness benefits, including discounted Gympass membership rates.
Medical, dental & vision insurance for you, spouse/partner & dependents.
Learn more about our benefits: http://bit.ly/41Yyc3d.

Pay Transparency
Dun & Bradstreet is an equal employment opportunity employer and believes in honesty and transparency in the employment hiring process, including pay transparency. Accordingly, listed on this posting is a good faith reasonable estimate of the salary range and other compensation in the job posting, as of the date of this posting. Actual compensation decisions for base salary and other compensation will be dependent upon a wide range of factors including but not limited to: an individual’s skill sets, experience, qualification, training, education, location, and any other legally permissible factors. Successful applicants will also be eligible for D&B’s generous benefit package, outlined above.

All Dun & Bradstreet job postings can be found at https://www.dnb.com/about-us/careers-and-people/joblistings.html. Official communication from Dun & Bradstreet will come from an email address ending in @dnb.com.

Equal Employment Opportunity (EEO): Dun & Bradstreet is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, age, national origin, citizenship status, disability status, sexual orientation, gender identity or expression, pregnancy, genetic information, protected military and veteran status, ancestry, marital status, medical condition (cancer and genetic characteristics) or any other characteristic protected by law. View the EEO is the Law poster here and its supplement here. View the pay transparency policy here.

Global Recruitment Privacy Notice",112889,5001 to 10000 Employees,Company - Public,Information Technology,Information Technology Support Services,1841,$1 to $5 million (USD),Remote,182,data engineer,na,"['sql', 'python']","['azure', 'aws']",[],[],[],[],bachelor,5-10 years
Vedainfo,4.1,"Peoria, IL",Data Engineer,"Job Description
· Create and develop optimal data pipeline which fetches data from raw and apply business transformation and load derived.
· Build the AWS Fargate components required for optimal extraction, transformation, and loading of data-to-data lake.
· Develop data pipeline setup between base and derive values based on Caterpillar specification.
· Implement quality validation rules and follow process defined by stakeholder.
· Develop business events as defined by the solution team to generate CDC events for consumption layer (Kinesis)
· Create re-usable and configurable components wherever applicable.
· Create and execute unit, integration, and regression tests
· Ensure the code is developed satisfying all business rules, perform unit testing where applicable.
· Data pipelines development
· Python Development
· AWS cloud services
· Data quality validation rules
· Java (preferred)
· Implement CDC
· Transformation rules creation
· Message broker tools like Kinesis
· Azure DevOps – CI/CD pipelines
· Automated and Manual Testing experience
Job Type: Contract
Pay: Up to $60.00 per hour
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Peoria, IL 61602: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
This position is remote initially and you will be asked to work any of the client location once offer is made. Please confirm your current location.
Experience:
Data Engineer: 1 year (Preferred)
Python Development: 1 year (Preferred)
AWS: 1 year (Preferred)
Message broker tools: 1 year (Preferred)
Work Location: In person",108000,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),IL,-1,data engineer,na,"['java', 'python']","['azure', 'aws']",[],[],[],[],,0-2 years
Deako,4.9,"Seattle, WA",Data Engineer II,"About Deako
Deako is delivering a revolutionary plug-n-play smart home platform with a laser focus on the untapped new home construction market. We make smart lighting so easy to upgrade that even those who would have never considered smart lighting are making it happen. We've built a company based on trust; where forming personal relationships is key to our success.
Software at Deako
The software team is a tight-knit team of smart, dedicated people. We are customer focused, we ship often, and are constantly asking “why.” We are passionate about the software we write. We work closely with other Deako teams. Developers at Deako are always looking for a better way and understand that no code is perfect.
Data at Deako
We believe that Data and Software go hand in hand. That's why the Data and Software teams work closely together to ensure we're collecting the right data, and getting it to the right people. Data is new at Deako, and we are working to make it a cornerstone of our company. We want every team at Deako to have a data-driven mindset.
Day to Day Expectations:
Participate in the code review process in our DBT repository
Build maintainable, testable SQL queries for various dashboards
Help maintain, contribute to and improve our data infrastructure and existing pipelines
Help maintain and improve our CI/CD pipelines
Responsible for obtaining, cleaning, and munging data and getting it into a form that our data analysts can access and analyze
Participate in meetings with stakeholders around data collection and cleaning

Must Have:
Demonstrable knowledge in SQL + Some Programming Language (Typescript, Python, Ect)
Demonstrates a high level autonomy and willingness to learn
1+ years of experience working as a Data Analyst/Engineer or Comparable field.
Experience with git, dbt
Nice to Have:
Experience in Snowflake, Fivetran, Hightouch
Experience with Gitlab CI
Experience with geospatial data
Stock Options
Hybrid work environment (Office + Remote)
Medical/Dental/Vision/Life/401K
Unlimited PTO
Free Snacks/Coffee/Drinks (Non-Alcoholic and Alcoholic)
Quarterly Company Parties",115000,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2015,$5 to $25 million (USD),WA,8,data engineer,na,"['sql', 'go', 'python']",['snowflake'],[],"['snowflake', 'dbt']",[],['gitlab'],,+10 years
Myticas Consulting,3.9,Arizona,BHJOB15656_20058 - Data Engineer,"Myticas's direct client based out of Phoenix, AZ is currently seeking a Data Engineer for a 100% remote contract position.
Pay Rate: Up to $34/hr

Job Description:
A Data Engineer can confidently understand, write, troubleshoot, optimize, and peer review SQL code. They understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs. They also understand relational database design, table structure, data types, and data models. They are experienced with and confident in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products. They can create reporting dashboards and products that inform the business. They easily grasp and retain complex data concepts and are able to explain them to others. Looking for engineers who are driven to innovate, use cutting edge technologies to solve problems and think outside the box.
What You’ll Do:
Digital Transformation into AWS
Data Warehouse related work (SQL, ETL, Replication, Databases)
Understand, write, troubleshoot, optimize, and peer review SQL code.
Understand business requirements and create code and reporting that exceeds specifications through anticipating underlying needs.
Understand relational database design, table structure, data types, and data models. 6.Experienced with and confidence in development in reporting visualizations in Excel, PowerBI, Tableau, QuickSight, SSRS, or other data products.
Data Warehouse related work (SQL, ETL, Replication, Databases
Perform other duties as assigned or apparent.
MINIMUM EDUCATION AND RELATED WORK EXPERIENCE:
Bachelor's degree in a Computer Science field and 2 years' work experience
2 years of experience in most phases of IT systems deployments in one or more of the following areas: design and deployment of cloud services, data migration to AWS cloud, Digital Transformation into AWS, and Data Warehouse related work
INDCHI
INDREM",61200,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,Arizona,-1,data engineer,na,['sql'],['aws'],"['tableau', 'excel']",[],[],[],bachelor,2-5 years
Locate Software,4.0,Remote,Cloud Data Engineer-DBA,"Role: Sr :Cloud Data Engineer with DBA Experience.
Location: Bellevue ,WA(Remote)
Experince:8+ (Senior Level)
Duration : 12+ Months
Looking for an experienced senior-level data engineer (DBA)with PROVEN and varied cloud experience who can hit the ground running as both contributor and strategist for our architects and engineers across multiple scrum teams. This is a role that must be able to deliver with results and excellence. Will expect to see well-rounded individuals with a deep understanding and proven diverse experience working with APIs, backend platforms and cloud (AWS,OpenSearch and Elastic search a must).
KEY EXPERIENCE & SKILLS
8+ years demonstrated expertise as data engineer and DBA responsibilities, diverse cloud experience with ability to be strategic and tactical.
Extensive experience working on both relational and SQL databases (Oracle, RDS, etc.) on security aspects like database provisioning configuration, DB identity management using Domain auth or secrets management tools like CyberArk or Vault, enforcing Role-based access controls following least privilege principle, data encryption and archival processes.
Understanding and enforcement of Security best practices for databases in support application development and operations.
Minimum 2-3 years diverse experience working with these Cloud technologies (AWS preferred) like Lambda, Amazon API Gateway, S3, DynamoDB, Cloudwatch.
Hands-on scripting experience using shell, python, Java to create DB automation utilities.
Proven security experience with role-based access control, access management and data replication
Experience working in an onshore, scrum Agile team and DevOps practices.
Experience with data integration principles within heterogeneous distributed database Platforms
Proven experience designing and building reliable, scalable data infrastructure with leading privacy and security techniques to safeguard data using AWS technologies.
Has built frameworks for data ingestion pipelines both real-time and batch using best practices in data modeling, ETL/ELT processes with seamless hand-off to data engineers
Demonstrated ability to work with architects, engineers and stakeholders in a variety of ways – tactical to strategic and consulting
Excellent collaborator and communicator
Experience presenting, communicating and partnering with leadership and executive stakeholders
Ability to drive technology direction and selection by making recommendations based on experience and research
Must Have:
Open-search and elastic search / AWS cloud experience.
Cloud (distributed with data).
NOT Big Data, Data transformation, data migration, SQL focused… this is an engineer / engineering consultant who will be HANDS ON with our engineers, architects, and principal engineers the language in this description chosen carefully to reflect the focus from a DBA/ Data perspective.
We are using data engineer instead of DBA on purpose – this is not a traditional DBA role, more of a data engineer role.
Entry level or lower mid-level will not work. We need someone who can hit the ground running with proven experience.
Able to communicate VERY well and collaborate in a fast-paced environment.
Job Types: Full-time, Contract
Pay: $117,796.00 - $127,676.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
Monday to Friday
Experience:
Data Engineer: 8 years (Required)
Database administration: 5 years (Required)
AWS: 6 years (Required)
Elasticsearch: 4 years (Required)
Lambda: 5 years (Required)
DBA: 4 years (Required)
Work Location: Remote",122736,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'java', 'shell', 'python']",['aws'],[],"['dynamodb', 'elasticsearch', 'oracle']",[],[],,2-5 years
Maven Workforce,4.1,"Alpharetta, GA",Data Engineer,"Must Have skills:
Building Data Pipeline exp is a must
ETL Tools – SSIS, Alteryx
Data modelling
SQL Server development
Microsoft SQL Stack
Responsibilities:
Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx. Data sources including but not limited to SQL Server, SAP, Teradata, Hadoop\Hive, PostgreSQL, Oracle and flat files.
Identifying ways to improve data reliability, efficiency and quality by various data solution techniques.
Expertise in Data project management with JIRA stories. Work as a liaison between business and IT to ensure successful and timely completion of the projects.
Assist in defining the data architecture framework, standards and principles, including modeling, metadata, security and reference data.
Create and optimize data models to support various business applications.
Reviewing modifications of existing data systems for cross-compatibility.
Automate and support workflows to ensure timely delivery.
Must Have:
5+ years of SQL Server development experience.
3+ years ALTERYX Admin /User management experience and advance workflow management.
5+ years data modeling experience.
5+ years of ETL experience.
5+ years of experience in working on more than one database technologies Microsoft SQL server, Teradata.
2+ years big data experience, Hadoop, Hive, Spark
Expert knowledge of data warehousing.
DESIRED SKILLS:
BI Lifecycle management
Working understanding of Microsoft VBA, HTML, Python
Data model development using ERWIN
Job Type: Contract
Salary: $50.00 per hour
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Building Data Pipeline: 8 years (Required)
ETL Tools – SSIS, Alteryx: 7 years (Required)
Data modelling: 6 years (Required)
SQL Server development: 8 years (Required)
Microsoft SQL Stack: 8 years (Required)
Work Location: One location
Speak with the employer
+91 7328456015",90000,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2008,Unknown / Non-Applicable,GA,15,data engineer,na,"['sql', 'python']",[],"['sap', 'ssis']","['sql server', 'postgresql', 'oracle', 'hive']","['spark', 'hadoop']",[],,5-10 years
TekValue IT Solutions,4.0,"Houston, TX",Data Engineer with Migrating,"Required Skills:
8-10 Required Experince on Data Engineer and Revalent Skills
Experience with NoSQL (MongoDB)
Experience with API'S
Good Knowledge on Data Migration like Oracle to Mongo db
Experience with Python Programming
Job Type: Contract
Salary: $70.00 - $75.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Houston, TX 77002: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 8 years (Required)
NoSQL: 6 years (Required)
MongoDB: 6 years (Required)
Migrating: 5 years (Required)
Work Location: One location
Speak with the employer
+91 7328323606",130500,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['nosql', 'python']",[],[],"['mongodb', 'oracle']",[],[],,5-10 years
Devcare Solutions,3.4,"Columbus, OH",Sr Data Engineer/data architect,"Role : Data Architect / Bigdata Architect / Hadoop / senior data engineer/ senior Bigdata
skills: oracle, SQL, Hadoop, bigdata , cloud era, Hive, data migration etc.
8+ years Data analysis/architecture experience in Waterfall and Agile Methodology in various domains (prefer Healthcare) in a data warehouse environment.
Good knowledge of relational database, Hadoop big data platform and tools, data vault and dimensional model design.
Strong SQL experience (prefer Oracle, Hive and Impala) in creating DDL’s and DML’s in Oracle, Hive and Impala (minimum of 8 years’ experience).
Experience in analysis, design, development, support and enhancements in data warehouse environment with Cloudera Bigdata Technologies (with a minimum of 8-9 years’ experience in Hadoop, MapReduce, Sqoop, PySpark, Spark, HDFS, Hive, Impala, Stream Sets, Kudu, Oozie, Hue, Kafka, Yarn, Python, Flume, Zookeeper, Sentry, Cloudera Navigator) along with Informatica.
Experience (minimum of 8 years) in working with Sqoop scripts, PySpark programs, HDFS commands, HDFS file formats (Parquet, Avro, ORC etc.), Stream Sets pipelines, jobs scheduling, hive/impala queries, Unix commands, scripting and shell scripting etc.
Experience in migrating data from relational database (prefer Oracle) to big data – Hadoop platform is a plus.
Experience eliciting, analyzing and documenting functional and non-functional requirements.
Ability to document business, functional and non-functional requirements, meeting minutes, and key decisions/actions.
Experience in identifying data anomalies.
Experience building data sets and familiarity with PHI and PII data.
Ability to establish priorities & follow through on projects, paying close attention to detail with minimal supervision.
Effective communication, presentation, & organizational skills.
Good experience in working with Visio, Excel, PowerPoint, Word, etc.
Effective team player in a fast paced and quick delivery environment.
Required Education: BS/BA degree or combination of education & experience.
DESIRED Skill Sets:
Demonstrate effective leadership, analytical and problem-solving skills
Required excellent written and oral communication skills with technical and business teams.
Ability to work independently, as well as part of a team
Stay abreast of current technologies in area of IT assigned
Establish facts and draw valid conclusions
Recognize patterns and opportunities for improvement throughout the entire organization
Ability to discern critical from minor problems and innovate new solutions
Skill
Data analysis/architecture experience in Waterfall and Agile Methodology in various domains (prefer Healthcare) in a data warehouse environment.
Good knowledge of relational database, Hadoop big data platform and tools, data vault and dimensional model design.
Strong SQL experience (prefer Oracle, Hive and Impala) in creating DDL’s and DML’s in Oracle, Hive and Impala
analysis, design, development, support and enhancements in data warehouse environment with Cloudera Bigdata Technologies, along with Informatica
Experience in migrating data from relational database (prefer Oracle) to big data – Hadoop platform is a plus.
Job Type: Contract
Salary: $70.00 - $90.00 per hour
Benefits:
Dental insurance
Health insurance
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Experience:
total: 10 years (Required)
oracle: 4 years (Required)
SQL: 3 years (Required)
cloud era: 3 years (Required)
hadoop / Bigdata: 3 years (Required)
data architect: 1 year (Required)
Data warehouse: 1 year (Required)
Willingness to travel:
100% (Required)
Work Location: On the road
Speak with the employer
+91 6148083833",144000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2005,Unknown / Non-Applicable,OH,18,data engineer,senior,"['sql', 'shell', 'python']",[],['excel'],"['hive', 'oracle']","['kafka', 'spark', 'hadoop']",[],,0-2 years
"Double Line, Inc.",4.1,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

Double Line understands the importance of creating a safe and comfortable work environment and encourages individualism and authenticity in every member of our team. We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",87121,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['sql', 'python']","['azure', 'aws']","['tableau', 'power bi']",[],[],[],,
Guardsman Group,3.0,United States,Data Engineer,"A Little About Us
The Guardsman Group stands on over 40 years of experience, unmatched technical capabilities and the unwavering belief in the right of safety for all. Each and every day, our matchless range of services puts Guardsman in the lives of people in every corner of Jamaica and throughout the Caribbean. We've pioneered technologies and perfected procedures to give our customers the best solutions for their homes and businesses. As we enter another decade, we continue to be the industry leader. Today, Guardsman consists of 13 companies and over seven thousand talented staffers who are proud to call themselves a Guardsman.
The Role
This role is within the Business Performance, Analytics and Intelligence (BPAI) Unit of the Guardsman Group and is directly responsible for the shaping, building and implementation of solutions that satisfy the business intelligence needs across the Group.
What You'll Be Doing
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytictools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Collaborate with other members of the BPAI unit to build and improve on the availability, integrity, accuracyand reliability of data pipelines
Work closely with all levels of management, IT department and other members of the BPAI Unit to achieve task objectives
A Little Bit About You
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing 'big data' data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Minimum Qualifications
Bachelor's Degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline",112889,5001 to 10000 Employees,Company - Private,Management & Consulting,Security & Protective,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,['sql'],[],[],[],[],[],bachelor,
Javara,3.1,Remote,Data Engineer II,"Summary:
Javara’s Data Engineer II is responsible for comprehensive technical subject matter expertise to maintain Javara’s enterprise data platform and business glossary driven data availability within Javara’s Technology department. To that end, the Data Engineer II builds and maintains pipelines that ingest, move, store, and prepare data based on conceptualized data frameworks and architecture for downstream analytics or operational use by data architects or business intelligence staff. With guiding principles of driven by integrity and empowered to make an impact, the senior data engineer is responsible for ensuring that the strategic data assets to support internal and external stakeholder success are available, trusted, and secure. Primary business use cases are varied across enterprise use cases— operational, clinical, and clinical research.
The Data Engineer II reports to the Vice President of Data and Analytics.
Essential Duties and Responsibilities:
Designing, developing, managing, and determining how Javara’s data will be stored and utilized, including management of the business data architecture to deliver maximal business value.
Assess internal and external data and design and maintain a blueprint to manage the available data.
Collaborating with business units to develop data solutions and models for their needs, as well as partner with enterprise architects to create data models in line with Javara’s business needs.
Provide technical subject matter expertise to internal and external teams for clinical trial data mapping, ingestion, and transformation best practices.
Create inventory of enterprise data and store data in an easily accessible format.
Design and develop complex database management systems and separate public data from private ones.
Implement a secured AWS-based disaster-recovery plan to cater for the data needs of the company in times of emergency or cyber-attack in close coordination with our Security Officer.
Collaborate with enterprise management needs to create data models in line with the organizations need.
Research to collate new data and update the company’s data warehouse from time to time.
Meld existing data architecture with new ones as new technology emerges.
Learn new techniques for data modeling and management of the data platform.
Design efficient and scalable data processing systems and pipelines on Databricks mounted on AWS, various AWS Services, Snowflake, Azure Active Directory IdP, Microsoft Power BI, and several third-party application technologies such as APIs, SFTPs, and EHRs/EMRs/clinical data warehouses.
Create technical solutions that solve business problems and are well engineered, operable, maintainable, and delivered.
Adhere to AWS based HIPAA data guidelines through all processes of the data lifecycle.
Develop scalable and reusable frameworks for ingestion and transformation of large datasets.
Ensure that data and metadata is accurate, complete, and across all platforms by designing and implementing tools to detect data anomalies.
Develop data models and mappings and build new data assets required by users.
Perform exploratory data analysis on existing products and datasets.
Provide technical guidance to help data users adopt new data pipelines and tools.
Understand trends and latest technologies and apply to evaluation of Javara’s requirements.
Identifying strategic data requirements and design models to make sure such data requirements align with the overall architecture for Javara.
Assessing the enterprise’s internal and external data and design blueprint to manage the available data.
Creating an inventory of Javara’s data and secure storage of the data in an easily accessible manner.
Protecting Javara’s data and ensuring data redundancy in times of emergency or cyber-attack.
Researching and discovering new data management models and techniques.
Meld existing data architecture with new ones as technology emerges.
Continuously improve the quality, consistency, accessibility, and security of Javara’s data activity across company needs.
Contribute to documentation and publications.
Qualifications:
Required
Fluent English.
Evidence of a successful track record of manipulating, processing and extracting value from large, disconnected datasets.
5 + years’ experience in data or systems engineering, ideally in Databricks.
3+ years’ experience in batch and streaming ETL using Spark, Python, Scala, or comparable language on Databricks.
Experience in delta sharing, JDBC connections, or other secure and scalable external data sharing services.
Familiarity with AWS Services not limited to Glue, Athena, Lambda, S3, and DynamoDB.
Demonstrated experience implementing data management life cycle, using data quality functions such as standardization, transformation, rationalization, linking and matching.
Demonstrated experience of data modeling techniques and database performance and cost optimization.
A Bachelor’s degree in information technology or equivalent experience.
Preferred
Experience in healthcare industry and/or clinical research.
Experience with FHIR, SDTM, HIPAA, 21 CFR Part 11.
Experience in predictive modeling.
Working knowledge of Microsoft services not limited to Azure Active Directory and Power BI.
Experience prepping structured and unstructured data for business glossary master data set and data visualization use cases.
Work Environment:
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job.
This job operates in a professional environment
The noise level in this work environment is usually light to moderate
Travel: This position may involve rare to minimal travel.
Pre-Employment Screening: Drug screen and background check required.
This job description covers the most essential functions of this position and is not designed to contain a comprehensive listing of activities, duties or responsibilities that are required of the employee in this job. Duties, responsibilities and activities may change at any time with or without notice.
Job Type: Full-time
Pay: $57,748.13 - $69,546.13 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
How many years of experience do you have in data or systems engineering?
How many years of experience do you have in Databricks?
How many years of experience experience do you have in batch and streaming ETL using Spark, Python, Scala, or comparable language on Databricks?
How many years of experience do you have in delta sharing, JDBC connections, or other secure and scalable external data sharing services?
How many years of experience do you have in AWS Services (Glue, Athena, Lambda, S3, DynamoDB)?
Do you have experience in the healthcare or life sciences industry?
What are your salary requirements?
Work Location: Remote",63647,201 to 500 Employees,Unknown,Healthcare,Medical Testing & Clinical Laboratories,2018,Unknown / Non-Applicable,Remote,5,data engineer,na,"['scala', 'python']","['databricks', 'snowflake', 'aws', 'azure']",['power bi'],"['dynamodb', 'snowflake']",['spark'],[],bachelor,+10 years
SecurePro,4.0,"Arlington, VA",Data Engineer,"Collect, manage, and convert raw data accurately and reliably
Organize data systems for subgroup access and analyses
Configure and sustain data cloud structures
Able to work with structured and unstructured data.
Validate outputs of data pipelines
Convert existing SAS code to python/pyspark code for model operation in the cloud
Create and sustain policy analysis models in the cloud
Troubleshoot user interfaces in the cloud
Create and sustain intuitive user interfaces in the cloud
Degree in Data Engineering preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $100.00 per hour
Experience level:
3 years
Schedule:
8 hour shift
Application Question(s):
Are you a US citizen?
Must have DOD secret Clearance?
Work Location: Remote",162000,,,,,-1,,VA,-1,data engineer,na,['python'],[],[],[],[],[],,2-5 years
TheRIIM LLC,4.0,"Houston, TX",Data Engineer,"Company Description

Client is in Banking & financial domain.

Job Description

Job Description Summary
Summary: As data engineer, you will join the Data, Analytics and Reporting team. Our goal is to help the business we support unlock the full potential of their data. In this role, you will design, implement and support data and analytics solution using a broad range of technologies including Hadoop, Spark, AWS and various NoSQL databases.

Description:
We are looking for individuals who are passionate about data and take pride in delivering high quality software.
As data engineer, you will join the Data, Analytics and Reporting team. Our goal is to help the business we support unlock the full potential of their data. In this role, you will design, implement and support data and analytics solution using a broad range of technologies including Hadoop, Spark, AWS and various NoSQL databases.
You’ll bring your in-depth knowledge of big data technologies best practice and a desire to work in a DevOps environment.

Qualifications

Your technical capabilities will include:
To excel in this role, you will be:
highly proficient in Hadoop and related technologies including HDFS, Spark, Impala and Hive
highly proficient in Java, Scala and/or Python
proficient in AWS including S3, Redshift, EKS, IAM and EC2
experienced with Linux
experienced with data modelling
experienced with CI/CD
familiar with AWS including S3, IAM and EC2
proactive and have great communication skills

Additional Information

Selection process includes Code exercise..",112889,,,,,-1,,TX,-1,data engineer,na,"['nosql', 'java', 'scala', 'python']","['aws', 'redshift']",['excel'],['hive'],"['spark', 'hadoop']",[],,
Kinertia,5.0,"Pittsburgh, PA",Data Engineer,"Do you like working with big data? Looking for a great company to work for with great clients, too? We are seeking a strong data professional with the experience and know-how to take charge of our largest client's needs while keeping things sane with strong organization of assets.
As a Data Engineer with a focus on GoogleSQL, you will be responsible for designing, developing, and maintaining the data infrastructure for the company. This includes building and managing data warehouses, data lakes, and data pipelines using GoogleSQL. You will also be responsible for developing and implementing data security and governance policies.
Responsibilities:
Design, develop, and maintain optimal data pipeline architecture
Build and manage data warehouses, data lakes, and data pipelines using GoogleSQL
Assemble large, complex data sets that meet functional / non-functional business requirements
Develop and implement data security and governance policies
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and other ‘big data’ technologies
Work with business stakeholders to understand their data needs and requirements
Stay up-to-date on the latest data technologies and trends
Qualifications:
Bachelor's degree in computer science, information technology, or a related field
5+ years of experience in data engineering, with a focus on GoogleSQL
Prefer 2-3 years of knowledge and experience in development and tuning of ETL processes
Experience with data warehousing, data lakes, and data pipelines using GoogleSQL
Experience with data security and governance using GoogleSQL
Experience with programming languages such as Python, Java, or Scala
Experience with big data technologies such as Hadoop, Spark, or Hive
Strong problem-solving and analytical skills
Excellent communication and teamwork skills
Results driven, ability to explain projects to both internal and external stakeholders
Benefits:
Very competitive salary and great benefits (medical, dental, vision), matching 401(k), company-paid life insurance and long-term disability
Great work-life balance, flexible work schedule
Opportunity to work on cutting-edge data projects
Collaborative and supportive work environment
Chance to make a real impact on the business
Job Type: Full-time
Pay: $120,000.00 - $160,000.00 per year
Benefits:
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Compensation package:
Performance bonus
Yearly pay
Experience level:
5 years
Schedule:
Day shift
Monday to Friday
Ability to commute/relocate:
Pittsburgh, PA: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
Big data: 5 years (Required)
SQL: 2 years (Preferred)
Data warehouse: 2 years (Preferred)
Work Location: In person",140000,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,PA,-1,data engineer,na,"['sql', 'java', 'scala', 'python']",[],[],['hive'],"['spark', 'hadoop']",[],bachelor,2-5 years
Hire Force Global,4.0,Remote,Lead Data Engineer,"We are hiring Lead Data Engineer with Min 6+ years experience with AWS Technologies.
Visa Type – GC/USC
Job Type-W2
Job Mode – Remote
Job Description:
Minimum 12+ years’ experience with designing, developing, delivering and maintaining large scalable Cloud systems.
Minimum 6+ year experience with AWS Technologies
Lead Data Engineer will be a key contributor to our Strategic Products Engineering team and will have the below responsibilities:
Work with product owners to understand existing application capabilities and build solutions that deliver the business value.
Apply technical background/understanding, business knowledge, system knowledge in the elicitation of Systems Requirements for projects
Leverage industry standard and design patterns to build technical approaches and to guide the team towards writing better code.
Take on development responsibility for key technical features.
Lead requirements and design for non-functional requirements like security, observability and performance as it pertains to AWS
Adhere to existing processes/standards including the project development lifecycle, business technology architecture, risk and production capacity guidelines and escalate issues as required
Experience working in an Agile environment with business and technical teams
Excellent time management skills with ability to prioritize and coordinate multiple tasks to ensure deadlines are met.
Experience with AWS Technologies like AWS S3, EC2, AWS Glue, Lamba Function, API Gateway and Python will be plus.
Ability to be forward-thinking and be able to analyse and anticipate project, technology, and team solutions to ensure successful project delivery
Ability to own and drive technological and team issues to resolution with minimal guidance.
Self-motivated, curious, eager to learn and able to thrive in a fast-paced, remote, or onsite environment
Strong communication (written, oral), documentation, and interpersonal skills.
Job Type: Contract
Pay: $50.00 - $55.00 per hour
Experience level:
10 years
6 years
Schedule:
8 hour shift
Application Question(s):
How many years of experience do you have in C#?
Experience:
Cloud development: 10 years (Required)
AWS: 6 years (Required)
Work Location: Remote",94500,,,,,-1,,Remote,-1,data engineer,senior,['python'],['aws'],[],[],[],[],,5-10 years
ArchsystemInc,4.1,Remote,Azure Data Engineer,"Databricks – Experience using Azure Databricks platform; Experience with Python programming; Experience using pandas and numpy for data engineering and data cleansing; Ability to troubleshoot the job scheduler and compute clusters; Experience with databricks CLI and secrets module; Experience with Azure Blob storage access configuration;
Job Type: Full-time
Salary: From $110,000.00 per year
Schedule:
Monday to Friday
Application Question(s):
Are you US Citizen or Green Care?
Do you have experience in Numpy or Pandas
Experience:
Azure Databricks: 2 years (Required)
Work Location: Remote",110000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,$5 to $25 million (USD),Remote,17,data engineer,na,['python'],"['databricks', 'azure']",[],[],[],[],,2-5 years
Veriforce LLC,3.7,"Spring, TX",Customer Success Data Engineer,"Who We Are
Veriforce® is a recognized leader in delivering supply chain risk management and compliance solutions that help bring workers home safely each day. The company’s SaaS-based contractor management and compliance software solutions, along with its standardized safety training programs and library of over 400 training courses, empower leading organizations to drive safety and compliance down to the worker level and more effectively mitigate supply chain and regulatory risk. With the industry’s largest safety and compliance network – comprised of 3200 hiring clients, 70,000 contractor companies, 9,000 accredited safety trainers and authorized evaluators, and 2.5 million individual workers – Veriforce is relied upon for innovative risk management solutions that help connect safety-conscious companies with a safe and qualified third-party workforce and make job sites safer, more productive, and more efficient.
The Role You’ll Play
The Data Engineer for Customer Success will be responsible for designing, building, and maintaining data processing architectures and solutions enabling the efficient conversion of structured and unstructured data to insights at enterprise scale. The CS Data Engineer work assignments are varied and frequently require interpretation and independent determination of the appropriate courses of action regarding data-driven decisions that lead to improved customer retention, advocacy, communication, and engagement outcomes. The Customer Success Team will depend on this person’s expertise and subject matter background to take initiative by digging and leaning into the data with heightened curiosity of trends and trajectories. A self-starter, this role will effectively translate data into addressable components that our customer-facing teams, leaders, and cross-org executives can act upon. This role will work closely with the entire Customer Success and Professional Services team to understand and then drive the strategic execution of reporting and system needs, then work to configure that vision into reality.
Core Responsibilities Will Be:
Ingesting data from internal and external sources utilizing cloud native platforms and software development best practices and patterns.
Develops software tools that leverage analytical and big-data techniques to cleanse, organize, and transform data into insights and actions that enables Veriforce To better serve our clients.
Understands department, segment, and organizational strategy and operating objectives, including their linkages to related areas.
Follow established guidelines/procedures.
Must be passionate about contributing to an organization focused on continuously improving consumer experiences
Arming the customer success organization with data driven insights by developing key dashboards for use by the CS and CS leadership team
Strategically collaborating with CS Leadership to enable the CS organization with the data they need to drive revenue growth and prevent churn
Developing key dashboards for use by the executive team for reflecting the activities and metrics within customer success
Creating the reporting frameworks and deliverables to support our Customer Success team in the ability to deliver peer benchmarking data, metrics, and other data points for Impact Assessments and Success Planning engagements.
Managing and updating a dashboard of key CS related reports for CSMs, Managers, Senior Leadership and the Chief Customer Officer
Identifying and gathering requirements from users, stakeholders, and CS leadership then proposing and building appropriate reports to support the identified needs.
Required Qualifications
Bachelor's degree or equivalent experience
5 years of technical experience
Experience with BI Tools such as PowerBI
Knowledge of SQL and relational database models
Documenting processes related to database design, configuration, and performance.
Experience designing, developing, and testing of software applications and/or infrastructure
Experience with APIs to expose large datasets to applications and data analytics solutions
Preferred Qualifications
Experience working with Customer Success Teams, reporting on customer churn, net dollar retention, gross dollar retention, etc.
Experience with Cloud-based solutions, Business Intelligence tools, and programming applications
Experience with Customer Success based systems, usage tracking, and CRM solutions such as Totango, Salesforce, and Pendo
Ability to create and manage data feeds to and from systems such as Salesforce, Totango, Pendo, Financial Systems, and others.
Ability to interpret data in order to help relay an accurate understanding of root causes that are primary drivers behind the data
Ability to collaborate with other analysts, engineers and data scientists in order to exploit data to drive the business forward
What Success Looks Like:
A successful candidate in this role will demonstrate:
The ability to quickly take action to mitigate any issues pertaining to data inaccuracies
Ability to quickly learn the company’s internal systems and the manner in which data is gathered.
A high focus on QA/QC efforts to ensure accuracy in the data
Enthusiasm toward continued personal/professional development
A desire to implement best practice technical solutions
Responsiveness to questions and system feedback
Here are just a few of the great reasons you should join our team!
We are mission-focused and mission-driven to help bring worker home safe every day. Our training products and compliance platform help keep workers safe.
Work with a global team! We have colleagues and customers across North America and overseas.
Veriforce is a great place to work! Our leaders and teams cite culture as one the top reasons this is a great place to work.
Veriforce provides
100% paid employee medical and dental insurance
Monthly contributions to Health Savings Accounts
A 401(k) match that is immediately fully vested
Outstanding time off benefits
Paid time off for volunteer activities
The successful candidate will have to undergo a criminal record check as condition of their employment.",94232,201 to 500 Employees,Company - Private,Information Technology,Internet & Web Services,2001,Unknown / Non-Applicable,TX,22,data engineer,na,['sql'],[],[],[],[],[],bachelor,5-10 years
Clairvoyant,4.1,"Hartford, CT",GCP Data Engineer,"Job type:C2C/W2
Location:Hartford, Connecticut or Remote with monthly visit to Client location.
Clairvoyant AI is looking for a highly motivated and experienced Data Engineer to join and help build our growing Cloud Engineering practice. The candidate would be responsible for working directly with customers to identify their Data Engineering needs, architecting any required solutions in Google cloud compute, working directly with our Onshore/Offshore team to facilitate task understanding and completion, maintaining the existing Data Pipelines, and also helping to implement solutions.
Must-Have
5+ Years of Experience in Data Engineering and building and maintaining large scale data pipelines
Experience with designing and implementing a large scale DataLake on Cloud Infrastructure
Strong technical expertise in Python and SQL
Extremely well-versed in Google Compute Platform including BigQuery, Cloud Storage, Cloud Composer, DataProc, Dataflow, Pub/Sub.
Experience with Big Data Tools such as Hadoop and Apache Spark (Pyspark)
Experience Developing DAGs in Apache Airflow 1.10.x or 2.x
Good Problem Solving Skills
Detail Oriented
Strong Analytical skills working with a large store of Databases and Tables
Ability to work with geographically diverse teams.
Nice to Have
Certification in GCP services
Experience with Kubernetes
Experience with Docker
Experience with CircleCI for Deployment
Experience with Great Expectations
Job Type: Full-time
Salary: $70.00 - $75.00 per hour
Schedule:
Monday to Friday
Experience:
Python: 5 years (Required)
SQL: 5 years (Required)
Hadoop: 5 years (Required)
Data lake: 5 years (Required)
GCP: 4 years (Required)
Work Location: Remote",130500,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,Unknown / Non-Applicable,CT,11,data engineer,na,"['sql', 'python']",['google cloud'],[],[],"['spark', 'hadoop']",['docker'],,2-5 years
spar information systems,3.5,Remote,Data Engineer,"Raja
Sr. Technical Recruiter
SPAR Information Systems
(a E-verify Company)
Phone: 469 – 750 – 0601
Fax : 1-214-291-2507
Email : Raja@sparinfosys.com
www.sparinfosys.com
Hello,
Hope you all doing great.
Kindly find the below JD and let me know if anyone interested for this below role on W2.
Sr. Data Engineer
Location: Remote
Duration: 3 Months Contract to Hire
Client: GEICO Insurance
No of positions: 30
Job Description:
Geico treats Data as Product and our Senior Engineer will be a key member of the engineering staff working across Business Services Engineering, Data Engineering, Platform Engineering, and Infrastructure Engineering to ensure that we provide a fiction-less experience to our customers, maintain the highest standards of protection and availability. Our team thrives and succeeds in supporting Data Driven company and delivering high quality technology products and services in a hyper-growth environment where priorities shift quickly. The ideal candidate has broad and deep technical knowledge in data, typically ranging from front-end UIs through back-end systems and all points in between.
Required:
3+ years of experience in data software development, programming languages and developing with big data technologies
2+ years of experience designing and building on existing and new data applications
2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework
1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express.
Additional Skills:
Experience in data software development, using data technologies such as Relational & NoSQL databases, open data formats, and programming languages such as Python, Scala, and/or other frameworks, building data pipelines (ETL and ELT) with batch or streaming ingestion, error handling, loading, and transforming data, and developing with big data technologies such as Spark, Hadoop, and MapReduce. Experience with analytics solutions.
Experience in development using Python or PySpark, Spark, Scala.
Advanced understanding of designing and building for data quality assurance, reliability, availability, and scalability, on existing and new data applications.
Advanced understanding of DevOps Concepts, Cloud Architecture, and Azure DevOps Operational Framework, Pipelines, Kubernetes.
Advanced understanding of designing and building solutions for data quality and observability, metadata management, data lineage, and data discovery.
Advanced understanding of building products of micro-services oriented architecture and extensible REST APIs.
Advanced understanding of open-source frameworks.
Experience with continuous delivery and infrastructure as code.
Experience in existing Monitoring Portals: Splunk or Application Insights.
Advanced understanding of Security Protocols & Products: Understanding of Active Directory, Windows Authentication, SAML, OAuth.
Advanced understanding of Azure Network (Subscription, Security zoning, etc) & tools like Genesis.
Advanced understanding of existing Operational Portals such as Azure Portal.
Knowledge of CS data structures and algorithms.
Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication).
Practical knowledge of working in an Agile environment (Scrum/Kanban/SAFe).
Strong problem-solving ability.
Ability to excel in a fast-paced, startup-like environment.
Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience
Thanks & Regards,
Raja
Sr. Technical Recruiter
SPAR Information Systems
(a E-verify Company)
Phone: 469 – 750 – 0601
Fax : 1-214-291-2507
Email : Raja@sparinfosys.com
Job Types: Full-time, Contract
Pay: $126,956.00 - $137,592.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Tuition reimbursement
Vision insurance
Compensation package:
1099 contract
Bonus pay
RSU
Stock options
Yearly pay
Experience level:
10 years
11+ years
Schedule:
8 hour shift
Application Question(s):
Must work on our W2 this is contract to hire role
Experience:
SQL (Required)
Azure (Required)
DevOps (Required)
10+ Years (Required)
Work Location: Remote",132274,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$25 to $100 million (USD),Remote,11,data engineer,na,"['sql', 'nosql', 'scala', 'python']","['azure', 'aws']",['excel'],[],"['hadoop', 'spark']",[],bachelor,+10 years
FanDuel,4.0,"Atlanta, GA",Data Engineer,"ABOUT FANDUEL GROUP
There are more ways to win, here at FanDuel. We're willing to bet on it.
THE ROSTER…
At FanDuel Group, we give fans a new and innovative way to interact with their favorite games, sports and teams. We're dedicated to building a winning team and we pride ourselves on being able to make every moment mean more, especially when it comes to your career. So, what does ""winning"" look like at FanDuel? It's recognition for your hard-earned results, a culture that brings out your best work—and a roster full of talented coworkers. Make no mistake, we are here to win, but we believe in winning right. That means we'll never compromise when it comes to looking out for our teammates. From creatives professionals to cutting edge technology innovators, FanDuel offers a wide range of career opportunities, best in class benefits, and the tools to explore and grow into your best selves. At FanDuel, our principle of ""We Are One Team"" runs through all our offices across the globe, and you can expect to be a part of an exciting company with many opportunities to grow and be successful.
WHO WE ARE…
FanDuel Group is an innovative sports-tech entertainment company that is changing the way consumers engage with their favorite sports, teams, and leagues. The premier gaming destination in the United States, FanDuel Group consists of a portfolio of leading brands across gaming, sports betting, daily fantasy sports, advance-deposit wagering, and TV/media.
FanDuel Group has a presence across all 50 states with approximately 17 million customers and nearly 30 retail locations. The company is based in New York with offices in California, New Jersey, Florida, Oregon, Georgia, Portugal, Romania and Scotland.
Its network FanDuel TV and FanDuel+ are broadly distributed on linear cable television and through its relationships with leading direct-to-consumer OTT platforms.
FanDuel Group is a subsidiary of Flutter Entertainment plc, the world's largest sports betting and gaming operator with a portfolio of globally recognized brands and a constituent of the FTSE 100 index of the London Stock Exchange.
THE POSITION
Our roster has an opening with your name on it
FanDuel Group is looking for an experienced Data Engineer with deep understanding of large-scale data handling and processing best practices in a cloud environment to help us build scalable systems. As our data is a key component of the business used by almost every facet of the company, including product development, marketing, operations, and finance. It is vital that we deliver robust solutions that ensure reliable access to data with a focus on quality and availability.
Our competitive edge comes from making decisions based on accurate and timely data and your work will provide access to that data across the whole company. Looking ahead to the next phase of our data platform we are keen to do more with real time data processing and working with our data scientists to create machine learning pipelines
THE GAME PLAN
Everyone on our team has a part to play
Creating and maintain optimal data pipeline architecture
Designing and implementing data pipelines required in the data warehouse and data lake in batch or real-time using data transformation technologies.
Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability
Designing and deploying data models and views with large datasets that meet functional / non-functional business requirements
Delivering timely after-action reporting to state regulatory groups
Delivering quality production-ready code in an agile environment
Delivering test plans, monitoring, debugging and technical documents as a part of development cycle
Creating data tools for analytics and working with stakeholders across all departments to assist with data-related technical issues and supporting their data infrastructure needs
THE STATS
What we're looking for in our next teammate
Experience writing Python scripts
Working SQL knowledge and experience working with relational databases
Build processes supporting data transformation, data structures, metadata, dependency, and workload management,
Show proficiency understanding complex ETL processes
Demonstrate the ability to optimize processes
Knowledge of data integrity and relational rules
Understanding of AWS and Google Cloud knowledge of DMS tasks and processes are nice to have.
Ability to quickly learn new technologies is critical
Proficiency with agile or lean development practices
Understanding of regulated systems and sensitive data
PLAYER CONTRACT
We treat our team right
From our many opportunities for professional development to our generous insurance and paid leave policies, we're committed to making sure our employees get as much out of FanDuel as we ask them to give. Competitive compensation is just the beginning. As part of our team, you can expect:
An exciting and fun environment committed to driving real growth
Opportunities to build really cool products that fans love
Mentorship and professional development resources to help you refine your game
Flexible vacation allowance to let you refuel
Hall of Fame benefit programs and platforms
FanDuel Group is an equal opportunities employer and we believe, as one of our principal states, ""We Are One Team!"" We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, and Veteran status. We believe FanDuel is strongest and best able to compete if all employees feel valued, respected, and included. We want our team to include diverse individuals because diversity of thought, diversity of perspectives, and diversity of experiences leads to better performance. Having a diverse and inclusive workforce is a core value that we believe makes our company stronger and more competitive as One Team!
#LI-Hybrid",97160,501 to 1000 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2009,$100 to $500 million (USD),GA,14,data engineer,na,"['sql', 'python']","['aws', 'google cloud']",[],[],[],[],,
"Akira Technologies, Inc",3.9,"Washington, DC",Senior Data Engineer,"Akira Technologies, Inc. is looking for a Senior Data Engineer to add their skills and experience to our team developing, deploying, and maintaining enterprise wide data services supporting the US federal agencies. This is an exciting and dynamic job opportunity where you will help modernize the applications and workloads used by a dynamic operational customer, and advance the government agencies' analytic and operational capabilities.
As a member of the data analytics team, you will work on developing innovative strategies to identify, process, and develop automation and integration methodologies for large datasets used by analysts to inform policy makers and operational planners.

Your Responsibilities:
Drive technology solutions in the data analytics and emerging technologies space
Help clients modernize, build and support reporting, data integration and data science environments in a cloud or hybrid environments
Create data quality dashboards and reporting to improve observability
Design and build well architected and managed data streaming services
Develop, transform, and model data to improve decision making
Create workflows and predictive models for end-users using system tools
Manage the analytics request process by capturing requirements
Provide mentorship and guidance to other data analytics colleagues within the company
You may also be responsible for data extraction and manipulation of data sets, the creation and maintenance of surveys and deciphering survey analytics, and implementation of AI/ML strategies.
The ideal person for this position is detail oriented, has expertise as a data engineer, and brings good working knowledge of the leading data platforms, analysis tools, and relevant cloud technologies. He/she will work closely with product owners across various functional areas to understand the objectives of the agency / program and prioritize any requirements.
Required Qualifications & Experience:
Bachelor's degree in Computer Science, Mathematics, Information Management, Data Science, Data Analytics, or a related field
5 or more years of experience developing data streaming and data management services and solutions
Strong, hands-on experience with one of more of Python/Jupyter, R, SAS, Apache Spark, Kafka, AWS Kinesis.
Strong Azure Cloud experience, particularly migrating R/Python workloads to Azure Cloud.
Strong data pipeline experience in Azure Cloud.
Ability to understand business needs and relay into easy to understand, non-technical language
High-level written and verbal communication skills
Optional: Experience with unified data analytics platforms like Databricks and Snowflake

About Akira Technologies:
Akira strives to meet and exceed the mission and objectives of US federal agencies. As a leading small business cloud modernization and data analytics services provider, we deliver trusted and highly differentiated solutions and technologies that serve the needs of our customers and citizens. Akira serves as a valued partner to essential government agencies across the intelligence, cyber, defense, civilian, and health markets. Every day, our employees deliver transformational outcomes, solving the most daunting challenges facing our customers.
Akira is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.

GcBLU1hIwe",120113,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,Unknown / Non-Applicable,DC,20,data engineer,senior,"['r', 'python']","['databricks', 'azure', 'aws', 'snowflake']",[],['snowflake'],"['kafka', 'spark']",[],bachelor,
Fresh Consulting,3.9,"Menlo Park, CA",Data Engineer,"Fresh Consulting is a design-led, software development and hardware engineering company, offering end-to-end digital services to help companies innovate. We bring together amazing UX designers, sophisticated developers, digital strategists, and top-notch engineers to help companies create fresh experiences that connect humans, systems, and machines. We’ve been growing fast and need someone to help us continue to manage the delivery of high-quality work in a fast-paced environment.
See more at freshconsulting.com Visit freshconsulting.com/portfolio to see our project work across several industries.
View and apply to all jobs - https://freshconsulting.applytojob.com/apply/ or visit freshconsulting.com/careers
Title: Data Engineer
Duration: 6 months with possible extension
Location: Onsite Menlo Park, CA
Benefits: Employee benefits at 100% including Medical, PTO, Holiday Pay, 401K Plan, and much more!
Hours: Minimum 40 Hours/Week
Role:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate, and maintain large-scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Skills:
5+ years of work experience as a Data Engineer.
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers, and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Process certification, such as Six Sigma, CBPP, BPM, ISO 20000, ITIL, and CMMI.
Education: BSCSE or related.
FRESH-
Work on engineering and research assignments with F500 companies and startups.
The relationships that we have created with our clients are one of a kind.
We help solve problems in many technologies focusing on R&D, product development, and manufacturing.
We work with the most cutting-edge and latest technologies from AR/VR to Autonomous technologies.
Closely working with our clients, we believe that long-term investments are extremely important to maintain the culture we together have created.
We’re a handpicked team of Engineers, digital strategists, designers, and developers united together in creating a fresh experience. Whether we are strategizing, designing, developing, or analyzing, our integrated team works as an extension of yours to improve your impact, your usability, and your customer conversion. In the process, we collaborate with you to get to know your business, understand your industry, and incorporate your big ideas into memorable experiences that keep your customers coming back for more.
Equal employment opportunity: All qualified persons will be considered for employment without regard to race, color, religion, sex, national origin, age, marital status, familial status, gender identity, sexual orientation, disability for which a reasonable accommodation can be made or any other status protected by law. Assistance will be gladly provided upon request for any applicant with sensory or non-sensory disabilities.
Fresh Consulting is a participating E-Verify company.
freshconsulting.com
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Range $70/hr - $80/hr.
ylM5WIC1Wr",135000,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,2007,$25 to $100 million (USD),CA,16,data engineer,na,['r'],[],[],[],[],[],,+10 years
Central City Concern,3.4,"Portland, OR",Data Engineer I,"Central City Concern (CCC) is an innovative nonprofit agency providing comprehensive services to single adults and families impacted by homelessness, poverty, and addictions in the Portland metro area. We hire skilled and passionate people to meet our mission to end homelessness through innovative outcome-based strategies that support personal and community transformation.
Data Engineer I will assist with designing, implementing, and supporting a data platform while interfacing with business partners across CCC, including Health Services, Housing, Employment, Finance, HR, and other shared service functions. The Data Engineer I will assist with Extract, Transformation, and Load (ETL) strategies, perform data modeling to meet customers’ data needs, and continually improve ongoing reporting and analysis processes while automating or simplifying self-service support for datasets. Data Engineer I will work closely with the Decision Support team to deliver timely reports. This position will look to the Data Engineer II and Data Engineer Senior for coaching/mentoring and join them on projects and development tasks.
Schedule: Monday - Friday 8.00 am-5.00 pm
RESPONSIBILITIES:
Assist in maintaining/developing the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud technologies
Assist with designing, optimizing, testing, and maintaining architectures for our client databases, data pipelines, and processing systems, as well as optimizing data flow and collection for cross-functional teams.
Assist in serving relevant data to all stakeholders, internally and externally, and maintain the infrastructure so the decision support team can present their work to the organization.
Assist in maintaining/developing a scalable data warehouse by connecting disparate data housed across numerous organizational systems and business lines.
Assist in collecting and documenting user requirements, development of user stories, and time estimates.
Supports key meetings and events (governance user groups, discovery events, etc.).
Partners with other Data Engineers to facilitate technical review meetings.
Generates runbook documentation of supported systems.
Actively audits and monitors system performance.
Supports during system and unit testing events.
Assists during code and system upgrades.
Develops and implements new technology with review by DE2 or DE Senior.
Escalates issues to DE2 and/or DE Senior, as needed.
Adhere to all state and federal privacy regulations, including HIPAA and 42 CFR Part 2, and CCC policies and agreements regarding confidentiality, privacy, and security. Support compliance with all privacy and security requirements pursuant to community partners' and outside providers’ patient confidentiality agreements, including privacy and security requirements for EMR access. This includes immediately reporting any breach of protected health information or personal identification information of any person receiving CCC services by CCC or an outside provider to the CCC Compliance Department, as well as to your supervisor or their designee.
Provide the highest standard of customer service to internal and external stakeholders, including CCC clients, CCC staff, and community members.
Attend all mandatory CCC training promptly.
Other duties as assigned.
QUALIFICATIONS:
Bachelor’s degree in related field required and 1+ years of recent data engineer experience that includes Python, Java and/or other object-oriented script language and experience using SQL and relational databases, including query authoring
OR Associate’s degree or trade school certificate in an IT related field with 2+ years of recent data engineer experience that includes experience with Python, Java and/or other object-oriented script language and experience using SQL and relational databases, including query authoring
OR 3+ years of recent data engineer experience that includes experience with Python, Java, and/or other object-oriented script language experience using SQL and relational databases, including query authoring
Familiarity with a variety of databases
Experience with manipulating, processing, and extracting value from large, disconnected datasets
Familiar with gathering data through multiple sources through API calls and scripting languages
Understanding reporting tools, like Power Bl, SQL Server Analysis Services, and SQL Server Report Services.
Understanding of the Agile principles and methodologies
Must be able to work within an integrated, multidisciplinary setting
Adhering to Central City Concern’s drug-free workplace encourages a safe, healthy, and productive work environment and strictly complies with the Drug-Free Work Place Act of 1988. An employee shall not, in the workplace, unlawfully manufacture, distribute, dispense, possess, or use a controlled substance or alcohol.
Must pass a pre-employment drug screen, TB Test, and background check.
Must adhere to the agency’s non-discrimination policies.
Ability to effectively interact with co-workers and clients with diverse ethnic backgrounds, religious views, cultural backgrounds, lifestyles, and sexual orientations and treat each individual with respect and dignity.
BENEFITS:
Central City Concern offers an incredible benefits package to our employees!
Generous paid time off plan beginning at 4 weeks per year at the time of hire. Accrual increases with longevity.
Amazing 403(b) Retirement Savings plan with an employer match of 4.25% in your 1styear, 6% in the 2nd year, and 8% in your 3rd year!
11 paid Holidays PLUS 2 Personal Holidays to be used at the employee’s discretion.
Comprehensive Medical, Vision, and Dental insurance coverage.
Employer Paid Life, Short-Term Disability, AND Long-Term Disability Insurance!
Sabbatical Program is offers extended time off in years 7, 14, and 21.
This description is intended to provide a snapshot of the work performed. It is not designed to contain a comprehensive inventory of all duties, responsibilities, and qualifications required for the position.
Central City Concern is a second-chance employer and complies with applicable laws regarding considering criminal background for employment purposes. Government regulations, contractual requirements, or the duties of this particular job may require CCC to conduct a background check and take appropriate action to address prior criminal convictions.
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)",77601,Unknown,Nonprofit Organization,Nonprofit & NGO,Civic & Social Services,1979,Unknown / Non-Applicable,OR,44,data engineer,na,"['sql', 'java', 'python']",[],[],['sql server'],[],[],bachelor,+10 years
BigLynx Computer Software,4.9,"Redmond, WA",Databricks Data Engineer,"BigLynx, Inc is an American multinational technology corporation headquartered in Seattle, Washington, with operations in the United States, Canada, and India. The company began in 2016, as a product development company specializing in AI/ML Data Engineering in the Retail vertical space with its products warehouse & fast. Post Pandemic in 2022, BigLynx added a business division of boutique technology consulting, specializing inData Engineering, Full Stack , and Microsoft Dynamics helping clients build the next generation data platform and big data pipelines.
Data pipeline development: Design, develop, and maintain scalable and efficient data pipelines using Databricks to ingest, transform, and load data from various sources. This includes data extraction, data cleansing, data transformation, and data loading processes.
Data modeling and schema design: Design and implement data models, database schemas, and data structures on Databricks. Optimize data models for performance, scalability, and ease of use.
ETL processes: Develop and maintain ETL (Extract, Transform, Load) processes using Databricks to transform and cleanse data. Implement efficient data integration and transformation logic using languages such as Python, SQL, or Scala.
Data integration: Integrate data from multiple systems and sources, ensuring data consistency, accuracy, and quality. Develop and maintain data connectors, APIs, and data ingestion processes.
Performance optimization: Identify and address performance bottlenecks in data pipelines and data models. Optimize query performance, data loading, and data processing capabilities on Databricks.
Data governance and security: Implement data governance practices, data privacy measures, and security controls on Databricks. Ensure compliance with data governance policies and regulations.
Monitoring and troubleshooting: Monitor the health and performance of Databricks data infrastructure, data pipelines, and data processing jobs. Troubleshoot issues and provide timely resolutions.
Collaboration and teamwork: Collaborate with cross-functional teams, including data scientists, data analysts, and business stakeholders, to understand data requirements, provide data engineering expertise, and support their data-related needs.
Qualifications:
Databricks expertise: Strong knowledge and hands-on experience with the Databricks platform, including Databricks notebooks, Databricks runtime, and Databricks clusters.
Data engineering skills: Proficiency in data engineering principles, ETL processes, data modeling, and data integration techniques. Experience with programming languages such as Python, SQL, or Scala.
Big data technologies: Experience with big data technologies, such as Apache Spark, Apache Hadoop, or related frameworks. Familiarity with distributed computing and data processing concepts.
Cloud platforms: Experience working with cloud platforms, preferably Azure Databricks, AWS Databricks, or Google Cloud Databricks. Knowledge of cloud storage, compute, and networking services.
Database and data warehouse concepts: Understanding of relational databases, data warehousing concepts, and SQL. Familiarity with data warehousing best practices and dimensional modeling.
Performance optimization: Strong skills in optimizing Spark jobs and queries on Databricks. Ability to identify and resolve performance bottlenecks.
Problem-solving skills: Strong analytical and problem-solving abilities to tackle complex data engineering challenges and troubleshoot issues.
Collaboration and communication: Excellent collaboration and communication skills to work effectively with cross-functional teams and stakeholders, translating business requirements into technical solutions and providing technical guidance.
Education: A bachelor's or master's degree in computer science, data engineering, or a related field is typically required. Relevant certifications, such as Databricks Certified Developer or similar, are h
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Work Location: In person",135000,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,WA,-1,data engineer,na,"['sql', 'scala', 'python']","['databricks', 'azure', 'aws', 'google cloud']",[],[],"['hadoop', 'spark']",[],bachelor,5-10 years
"PRIMUS Global Services, Inc",4.1,"Austin, TX","Data Engineer – Snowflake, SQL – REMOTE WORK 43357","We have an immediate long-term opportunity with one of our key clients for a position of Senior Data Engineer, to work on a remote basis.

The ideal applicant must have extensive experience with Snowflake, SQL and Multiple ETL tools.

**ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**

For immediate consideration, please contact:

Tanya Khatri
PRIMUS Global Services
Direct: 972-200-4514
Phone No: 972-753-6500 Ext: 258
Email: jobs@primusglobal.com",89729,501 to 1000 Employees,Private Practice / Firm,Information Technology,Information Technology Support Services,2002,$5 to $25 million (USD),TX,21,data engineer,na,['sql'],['snowflake'],[],['snowflake'],[],[],,
Smartbridge,3.3,"Houston, TX",Data Engineer / Consultant,"Hybrid in Houston Texas
Salary range 105K to 150K Annual Salary (see note below)
All the benefits and perks you need:
100% Paid Healthcare Insurance, including First Primary Care Wellness Program
Start accruing Paid Days Off from Day One
Career Development
Fitness Reimbursement
Matching 401K Retirement Plan
Mentoring Program
Team Events
And more….
Smartbridge
We simplify business transformation, applying thought leadership and innovation to create digitally connected enterprises. From Strategy to Implementation, we bring our clients’ digital agenda to life. One of the keys to our success as a company is finding and hiring exemplary employees. We believe that each member of our team contributes directly to Smartbridge’s growth and success, and we take pride and celebrate with our employees as they continue to grow and succeed with their career paths as well.
We are seeking an experienced Senior Data Engineer who will work with clients and members of the consulting team on the architecture, design, and development of highly scalable data integration and data engineering processes. The Senior Consultant must have a strong understanding and experience with data & analytics solution architecture, including data warehousing, data lakes, ETL/ELT workload patterns, and related BI & analytics systems.
Additional responsibilities of this role will include the following:
Deliver consulting projects/work on-time, on-budget, and in a way that accomplishes client goals
Develop and implement technical best practices for data ingestion, data quality, data cleansing, and other data integration/ETL/Engineering-related activities
Understand and experience maintaining a multi-terabyte enterprise data warehouse with accompanying incremental data pipelines
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and modern cloud technologies.
Conduct or participate in meetings with owners of key system components to fully understand current data and systems environments
Resolve source data issues and refine transformation rules
Analyze source system data to assess transformation logic and data quality through data profiling
Leverage data quality processes to assist with data cleansing requirements
Work with technical and business representatives to determine strategies for handling data anomalies that are identified
Design ETL processes and develop source-to-target data mappings, integration workflows, and load processes
Develop, test, integrate, and deploy data pipelines using a variety of tools and external programming/scripting languages as necessary
Provide technical documentation and other artifacts for data pipelines, ingestion, integration or other data solutions
Identify problems, develop ideas and propose solutions within differing situations requiring analytical, evaluative or constructive thinking in daily work
Apply creative thinking to identify possible reporting solution alternatives
Other duties assigned as needed
Requirements and Qualifications
3+ years hands-on experience with one or more of these data integration/ETL tools:
Azure Data Factory
Databricks/Spark
Experience building on-prem data warehousing solutions
Experience with designing and developing ETL's, Data Marts, Star Schema's
Experience with building data warehousing solutions in Azure
Moving data from on-prem to cloud
Designing a data warehouse solution using Synapse or Azure SQL DB
Experience building pipelines using Synapse or Azure Data Factory to ingest data from various sources
Understanding of integration run times available in Azure
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Knowledge of scripting languages like Python, Scala.
Microsoft Azure Cloud platform certifications (nice to have)
Must be able to travel to client locations based on project needs
Please note that the compensation information that follows is a good faith estimate for this position only and is provided pursuant to the Equal Pay Transparency Laws in numerous states we operate. It takes into consideration a candidate’s education, training, and experience, as well as the position’s work location, expected quality and quantity of work, required travel (if any), external market and internal value, including seniority and merit systems, and internal pay alignment when determining the salary level for potential new employees.
7ttzIjxalD",127500,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2003,$5 to $25 million (USD),TX,20,data engineer,na,"['sql', 'scala', 'python']","['databricks', 'azure']",[],[],['spark'],[],,+10 years
Koantek,4.0,Remote,Adobe CDP Data engineer,"As an Adobe Real-Time CDP Data Engineer, you will be responsible for performing
data integration and configuration of schemas, datasets, and profiles against a
variety of sources and microservices. You will be working closely with clients and
stakeholders to understand their data needs, and design and implement the
necessary data solutions to meet those needs. This role requires expertise in Adobe
Real-Time CDP, as well as experience in data integration and configuration.
Responsibilities:
Design and implement data solutions using Adobe Real-Time CDP
Perform data integration and configuration of schemas, datasets, and profiles
against a variety of sources and microservices
Work with clients and stakeholders to understand their data needs and design
appropriate data solutions
Develop, test, and maintain data pipelines and workflows
Troubleshoot data issues and provide solutions
Work with data analysts to ensure data quality and accuracy
Stay up-to-date with the latest Adobe Real-Time CDP features and technologies
Collaborate with cross-functional teams including product, engineering, and data
science to ensure successful project delivery
Provide guidance and mentorship to junior data engineers
Participate in code reviews and ensure compliance with coding standards and
best practices
Requirements:
Bachelor's degree in computer science, data science, or a related field
Strong experience in Adobe Real-Time CDP, including data integration and
configuration of schemas, datasets, and profiles against a variety of sources and
microservices
Experience with data integration tools and platforms
Strong understanding of data modeling and database design principles
Experience with SQL, NoSQL, and other data storage technologies
Experience with data warehousing, ETL, and data transformation
Knowledge of data governance and data security best practices
Strong problem-solving and analytical skills
Excellent communication and collaboration skills
Ability to work independently and as part of a team
Job Type: Contract
Contract length: 3 months
Salary: Up to $36,000.00 per month
Experience:
total work: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91 9411943957",112889,,,,,-1,,Remote,-1,data engineer,na,"['sql', 'nosql']",[],[],[],[],[],bachelor,0-2 years
Expression Networks,4.4,"Washington, DC",Junior Data Engineer,"We are looking to hire a Jr. Data Engineer to add to the continued growth we are seeing with our Data Science division. This position will work in a team led by a Sr. Data Engineer on tasks related to designing and delivering high-impact data architecture and engineering solutions to our customers across a breadth of domains and use cases. You will be working on developing, testing, and documenting software code for data extraction, ingestion, transformation, cleaning, correlation, and analytics. You will be using various databases and tools. You will have the opportunity to strengthen your skill set while learning new technologies utilized across the Expression Networks' breadth and depth of projects.
About Expression Networks
Founded in 1997 and headquartered in Washington DC, Expression Networks provides data fusion, data analytics, software engineering, information technology, and electromagnetic spectrum management solutions to the U.S. Department of Defense, Department of State, and national security community. Expression's “Perpetual Innovation” culture focuses on creating immediate and sustainable value for our clients via agile delivery of tailored solutions built through constant engagement with our clients. Expression Networks was ranked #1 on the Washington Technology 2018's Fast 50 list of fastest-growing small business Government contractors and a Top 20 Big Data Solutions Provider by CIO Review.
We make sure to provide everyone with the tools and opportunities to grow while working on some of the newest technologies in the industry. With Covid-19 being a major theme over the last two years having a growing collaborative culture has been one of the key focuses of our C-suite and upper management since our humble beginnings. We get excited about celebrating our professionals' milestones, accomplishments, promotions, overcoming challenges, and many other aspects that make an engaging collaborative environment.
Expression Networks HQ is conveniently located northeast of Union Station. We are a 5-minute walk from the Gallaudet - Noma DC Metro station. The U-line building has a parking garage attached to the building. Whether you are commuting via train or car we have a commuter assistance program that covers VRE, MARC, Metro, and parking expenses!
Security Clearance:
US Citizenship is required.
Ability to obtain and maintain a Secret or higher clearance.
Location:
Hybrid in the DMV area, with the ability to attend periodic events and deployments at our client site (Annapolis and Northern Virginia)
Required Skills:
Bachelor's degree or higher in engineering, Computer Science, Computer Engineering, Machine Learning, Mathematics, Physics, or related field and 1+ years of industry experience working on projects such as real-time SLAM and 3D reconstruction, sensor fusion, and active depth sensing, object and body tracking and pose estimation, and/or image processing
Proficient with state-of-the-art object detection algorithms
Deployment of vision algorithms in AWS
Experience with programming languages (e.g., JavaScript, TypeScript, React) and strong knowledge of programming techniques, especially for parallel architectures
Familiarity with neural network frameworks such as Theano, Torch, or Caffe
Occasional travel to conferences and customer visits may be required
Experience building or maintaining databases
Experience with Caffe, TensorFlow, or other deep learning frameworks
Programming experience with computer vision and 3D geometry libraries
Must be hands-on and work well within a team of algorithm, software, and hardware engineers
Preferred Skills
Master's or Advance Degree in Deep Learning, Computer Vision, Robotics, or Artificial Intelligence
Up-to-date knowledge and understanding of recent advances in machine learning, particularly deep learning
Certification in AWS, SQS, SNS, S3, SDL, SWLC, ISO26262, MISRA, RDS, CDK
Benefits:
Expression Networks offers competitive salaries and benefits, such as:
401k matching
PPO and HDHP medical/dental/vision insurance
Education reimbursement
Complimentary life insurance
Generous PTO and holiday leave
Onsite office gym access
Commuter Benefits Plan
Equal Opportunity Employer/Veterans/Disabled",89499,51 to 200 Employees,Contract,Information Technology,Enterprise Software & Network Solutions,1997,Unknown / Non-Applicable,DC,26,data engineer,na,[],['aws'],[],[],[],[],bachelor,
Bluesky,2.0,"San Francisco, CA",Founding Data Engineer,"Company website: http://getbluesky.io/
Company/Founders’ Location: Menlo Park, California
Local to SF/Bayarea, California preferred
We are a stealth mode early-stage startup with the mission to build a new generation of data infra on the cloud. Today, users suffer from unexpected incidents, slowness, and huge bills. We are big data domain experts with 15+ years of experience solving similar problems across Google, Hadapt, Vertica, Uber, Dropbox, Facebook, and Yahoo, and thought leaders in the big data industry. We are privileged to have received funding from top-tier VCs, angels and big data thought leaders such as founders of Cloudera and Qubole. We are building a world-class team to create systems that will have a profound impact on how people use big data.

We are searching for a Founding Data Engineer with strong interests in big data to help design and build a next-generation data cloud. As a key early team member, you will be responsible for critical architectural decisions that will shape the technology stack for years to come. There is also the opportunity to make an immediate impact to the big data stack at multiple well-known silicon valley startups (our design partners) simultaneously. Last but not least, you will receive generous equity compensation with huge growth potentials.
What You’ll Do
Development - Design and build highly-efficient and scalable ETL pipelines with Snowflake, Airbyte, dbt, and Prefect.
Management - Configure, monitor, and manage Snowflake, Airbyte, dbt, and Prefect software in production.
Modeling - Apply your expertise to help model structured data with emphasis on Snowflake’s ACCOUNT_USAGE schema. Own these models at a high level, and provide consulting help for others to model applications on top of your data models.
Performance Tuning - Analyze and improve the performance and efficiency of the ETL pipelines by identifying bottlenecks, finding out root causes, applying best practices like incremental computation and parallel computation, and dogfooding Bluesky product.
Stewardship - Own or support the data definitions and lineage across our entire data warehouse.
Mentoring - Help teach other team members about data architecture, and also be a consultant for developers who need help with data.
Learning - Stay current on technical knowledge and tooling to help the team utilize new technologies.
What We Require
BS level technical degree or equivalent
4+ years of professional experience in data engineering or data scientists
1+ year of experience with Snowflake
1+ year of experience with dbt
Preferred Qualifications
Experience with more than one big data computation framework: Spark, Tez, MapReduce, Hive, Pig, Presto
Experience with cloud deployments and containers
Why Join Bluesky
We raised $8.8M to solve the biggest challenges of big data cloud adoption: the cost management and workload optimization. We are backed by Greylock venture capitalists. Our founder, Mingsheng Hong has decades of experience in the big data industry, and is behind the names of well-known big data technology like Apache Hive and Vertica. Bluesky has also officially joined the Snowflake Partner Network. We achieved Select Tier status less than a month after coming out of stealth. Please see techcrunch and data engineering podcast for details.
View all job listings here https://jobs.ashbyhq.com/Bluesky",114866,Unknown,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,[],['snowflake'],[],"['snowflake', 'hive', 'dbt']",['spark'],[],,+10 years
"Contact Government Services, LLC",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
1YGuQTHTUE",103515,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['sql', 'r', 'python']",[],[],['postgresql'],[],[],,+10 years
Oran Inc,3.3,Remote,Big Data Engineer,"Job Title: Senior Big Data Technical Lead
Company: Oran Inc.
Location: [Location]
Job Type: Full-Time, Permanent
Job Summary: Oran Inc. is seeking a highly skilled and experienced Senior Big Data Technical Lead to join our team. In this role, you will be responsible for the development, integration, operations, and sustainment of various operational and in-development systems. You will play a crucial role in designing and developing integration solutions using AWS platforms, collaborating with solution architects, and ensuring alignment with the strategic vision. Your expertise in data ingestion, processing, and cloud technologies will be essential in delivering advanced production and dissemination capabilities.
Responsibilities:
Drive hands-on development of solutions in an Agile/Scrum environment, ensuring timely delivery and high-quality code.
Collaborate with solution architects to understand business and technical requirements and deliver solutions accordingly.
Analyze requirements and guide the implementation from the conceptual phase to the implementation phase.
Design and build data ingestion pipelines using Kafka, Glue ETL, Lambda, and NIFI frameworks.
Develop and guide developers in coding solutions using Spark framework in Scala, Java, or Python.
Design and develop solutions in AWS cloud services, including AWS MSK, AWS Elastic, AWS EMR, AWS SageMaker, S3 Archival, and AWS Kinesis.
Have a thorough understanding of container-based application architectures, including ECS and EKS on AWS.
Implement and ensure compliance with cyber security best practices, including code security using tools like HP Fortify and SonarQube.
Implement AWS security controls such as SAML, Kerberos authentication, RBAC, TLS, and data encryption controls.
Collaborate closely with Scrum team members to deliver high-quality products.
Provide daily support to internal clients with a sense of urgency.
Stay updated with new technologies and concepts and apply them to improve existing systems.
Contribute positively in a team environment and make technical contributions to drive business impact.
Think creatively, challenge existing thinking, and stimulate new ideas.
Qualifications:
Bachelor's degree in a relevant field with 6 years of prior experience, or Master's degree with 4 years of prior experience. Relevant experience may be considered in lieu of a degree.
U.S. citizenship is required.
Minimum of 5 years of strong server-side knowledge in back-end programming.
Minimum of 3 years of experience in creating Rest API Micro-services using ReactJS or AngularJS.
Minimum of 1 year of experience working in an Agile delivery environment.
Minimum of 3 years of experience working on a Cloud platform (AWS, Azure, GCP, or similar).
Strong experience in developing data ingestion pipelines using Kafka, Glue ETL, Lambda, and NIFI frameworks.
Proficiency in coding solutions using Spark framework in Scala, Java, or Python.
In-depth knowledge of AWS cloud services such as AWS MSK, AWS Elastic, AWS EMR, AWS SageMaker, S3 Archival, and AWS Kinesis.
Familiarity with container-based application architectures, including ECS and EKS on AWS.
Understanding of cyber security best practices, including code security using tools like HP Fortify and SonarQube.
Experience implementing AWS security controls such as SAML, Kerberos authentication, RBAC, TLS, and data encryption controls.
Strong problem-solving skills and ability to deliver high-quality code within deadlines.
Excellent communication and collaboration skills, with a positive and can-do attitude.",150000,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,2004,$1 to $5 million (USD),Remote,19,data engineer,na,"['java', 'scala', 'python']","['azure', 'aws']",[],[],"['kafka', 'spark']",[],bachelor,2-5 years
Serenity Healthcare,2.6,"Lehi, UT",Junior Data Engineer,"Junior Data Engineer
Serenity Healthcare is hiring a Junior Data Engineer for our Lehi, UT headquarters. (Remote availability for residents of Utah or Colorado) While previous ETL experience is preferred, we are open to exceptional entry-level talent for this role.
We intend to provide on the job training in data-skills: SQL, BI (PowerBI), ETL (SSIS), Warehousing (SQL Stored Procedures), Exploratory Data Analysis, etc. It’s our intention to train you in Microsoft’s new tool: PowerApps.
Desired skill sets:
Must be a quick learner
SSIS experience strongly preferred
Skills used in the role:
SQL 20%
SSIS 40%
PowerApps 40%
Day-to-day work description:
The Junior Data Engineer will be responsible for keeping the data flowing, building new data pipelines, and creating business applications using MS-PowerApps. You’ll need to be comfortable with SQL, SQL Server, and SSIS. You’ll be reading API documentation to establish new ETL flows, as well as automating report delivery.
Job Fit:
Capable of “Deep Work”
Problem Solver
Reliable and consistent
Attention to detail
What We Offer to You:
Competitive pay (DOE), including additional target compensation
Opportunity to work and grow your career in a fast-paced environment
Medical, Dental, Vision Insurance (90% coverage for you and codependents)
Life Insurance
Flexible spending account
Paid time off
Vision insurance
401k
Open and friendly, professional office environment
Who We Are:
We have helped thousands of patients take back their lives from mental illness with specialized clinical expertise and the foremost cutting-edge technology available in mental health today. Serenity’s approach to treating mental illnesses is to offer holistic options and treat the whole person by providing an atmosphere of positivity, support, and healing in an outpatient setting.
We believe people should live their best lives, and mental health is a substantial segment of total well-being. We bring the same passion we have for improving our patient’s lives to providing a work experience that will help you do your best work, enjoy the time you invest at work, and succeed in life outside of work. We take our people and culture seriously and make it a priority to invest in both.
Serenity Mental Health Centers is an equal opportunity employer. This position is contingent on successfully completing a criminal background check and drug screen upon hire.",85462,201 to 500 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2017,Unknown / Non-Applicable,UT,6,data engineer,na,['sql'],[],['ssis'],['sql server'],[],[],,
Lightcast,4.4,"Moscow, ID",Senior Data Engineer,"A Senior Data Engineer will architect big data analytical frameworks, and translate complex functional and technical requirements into detailed architecture design and high-performing software. A Senior Data Engineer will select data solution software and define hardware requirements. The position will be responsible for developing and implementing standards and processes for integration projects and initiatives.

Major Responsibilities:
Architect software applications, and test and build automated tools.
Select data solution software and define hardware requirements
Develop standards and processes for integration projects and initiatives.
Lead the design/development of software applications, testing, and building tools
Ensure database changes are reviewed and approved to standards
Lead and communicate to leadership on solution design
Provide technical assistance to junior members and to colleagues across the company
Skills & Abilities:
Analytical mind, personality and aptitude for working with data
Ability to look at the numbers, trends, and data to derive conclusions based on findings
Work closely with management to prioritize business and information needs
Proven ability to work effectively to meet goals and deadlines with minimal supervision
Highly creative problem-solving skills and an ability to tailor efforts based on the importance of the issue being addressed.
A hands-on, detail-oriented mindset with the ability to look beyond the box
Expert knowledge of object-oriented design, data structures, and algorithms
Demonstrated experience with agile or other rapid application development methods, object-oriented design, coding, testing patterns with a variety of languages
Significant knowledge of data structures, algorithms, data modeling and disaster recovery of data systems
Education & Experience:
Requires 5 years of data/software engineering/science experience
Bachelor's degree in computer science or other technical disciplines (masters preferred)
About Lightcast:
Lightcast is a global leader in labor market insights with headquarters in Moscow (ID) and Boston (MA) and offices in the United Kingdom, Europe, and India. We work with partners across six continents to help drive economic prosperity and mobility by providing the insights needed to build and develop our people, our institutions and companies, and our communities.

Lightcast is proud to be an equal opportunity workplace and is committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. Lightcast has always been, and always will be, committed to diversity, equity and inclusion. We seek dynamic professionals from all backgrounds to join our teams, and we encourage our employees to bring their authentic, original, and best selves to work.

#LI-SS1",118643,1 to 50 Employees,Company - Private,Government & Public Administration,State & Regional Agencies,2011,Unknown / Non-Applicable,ID,12,data engineer,senior,[],[],[],[],[],[],bachelor,5-10 years
Geo Owl,4.6,"Fort Gordon, GA",Senior Data Engineer,"Geo Owl is currently looking for a motivated and qualified Senior Data Engineer to support our Department of Defense contract opportunity. To be qualified, you need knowledge of Army structure and defense level intelligence, intelligence collection, fusion, analysis, production, and dissemination for intelligence databases and products, and meet the requirements listed below. If interested, apply now, or contact one of our recruiters.
Location: Fort Gordon, GA
Clearance: TS/SCI
Requirements: Must meet all the requirements listed below.
Excellent written & oral communication, research, and analytic skills
Expert ability to manage personnel, requirements, and coordination of projects
Expert capabilities to research, create, develop, and deliver professional briefings, multimedia presentations, and written reports
Experience utilizing programming languages such as SAS, R, Java, C, MATLAB, ScaLa, or Python; experience accelerating large data transactions across industry-leading GPU architectures to answer analytic questions
Experience with assessments, enterprise data integration, governance, and metrics, including the application of metadata management techniques and ability to interrogate databases efficiently using SQL
Experience with tradecraft and publication; ability to coordinate and support cross-community meetings and working groups; assimilate large volumes of information, and independently produce reports using data science focused libraries such as Pandas, Scikit, TensorFlow and Gensim to answer analytical questions
Desired Requirements:
Knowledge of Army structure and defense level intelligence operations: intelligence collection, fusion,
analysis, production, and dissemination for intelligence databases and products
Knowledge and experience with intelligence operations and in assisting with drafting expert assessments
across operations priorities on behalf of the stakeholder
Specialized training from any intelligence collection and analysis school or certification to include GEOINT Professional Certification (GPC-F, GPC_IA-II, GPC_GA-II, GPC_IS-II, etc)
Knowledge and understanding of the National System for GEOINT (NSG) and Intelligence Community;
knowledge of private sector data science/analytics, machine learning, and data visualization communities
Education Requirements:
MA or MS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 2 years CURRENT
Intelligence Analysis experience; OR
BA or BS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 5 years CURRENT
Intelligence Analysis experience; OR
Undergraduate degree with graduate/professional certificate in Data Science, Data Analytics, Informatics,
Statistics, or related field AND at least 10 years of Intelligence Analysis experience

Benefits:
Health Insurance (Geo Owl pays 80%+ of the premium).
401k matching.
Dental, Vision, and other supplemental insurance plans available.
Company-paid short-term and long-term disability and life insurance.
Peer-to-Peer spot bonuses.
120 hours of PTO per year plus federal holidays.
Joining the Geo Owl Team | What to Expect
At Geo Owl, we highly value our team members. We offer challenging but rewarding opportunities for those who want to work hard to provide a great experience for the customer and strive to reach their professional goals. As a member of the Geo Owl family, you will be working alongside people who share this work ethic and are aiming to be the best partner for our customer. We are all proud to be a part of this company and we want you to be too.
Our Mission
Provide high quality solutions to our mission partners in the United States through our expert analysts.
Be recognized as the best at what we do by our customers.
Be a team our team members are proud and excited to be a part of.
Continually strive for excellence and seek to tackle the most difficult challenges our industry has to offer.
About Us
Geo Owl is a premiere provider of Full-Motion Video (FMV), Geospatial, ISR, Intelligence and IT services to the Department of Defense and Intelligence Community. We are vitalized by our engaged team of professionals that truly value each other and the important missions we support.
Equal Opportunities
Geo Owl is an equal opportunity employer and does not discriminate on the basis of race, color, religion, creed, sex, age, sexual orientation, national origin, disability, marital status, military status, genetic predisposition, or any other basis protected by law.
To stay up to date about new career opportunities:
Follow us on Twitter
Follow us on Instagram
Follow us on LinkedIn

I8pR3c9K0u",126295,51 to 200 Employees,Company - Private,Aerospace & Defense,Aerospace & Defense,2013,Unknown / Non-Applicable,GA,10,data engineer,senior,"['scala', 'java', 'sql', 'r', 'python']",[],[],[],[],[],,0-2 years
"Titan America, LLC.",3.9,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Job Type: Full-time
Pay: $75,000.00 - $87,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Day shift
Monday to Friday
On call
Ability to commute/relocate:
Roanoke, VA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 5 years (Preferred)
SQL: 5 years (Preferred)
Data warehouse: 5 years (Preferred)
Work Location: In person",81000,1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable,VA,121,data engineer,na,"['sql', 'r', 'python']","['snowflake', 'aws', 'google cloud', 'azure']",[],['snowflake'],[],[],bachelor,5-10 years
"JPMorgan Chase Bank, N.A.",3.8,"Wilmington, DE",Data Engineer III,"As a Data Engineer III under Consumer and Community Banking within the ADE Data Governance team at JPMorgan Chase, you serve as a seasoned member of an agile team to design and deliver trusted data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. You are responsible for developing, testing, and maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Supports review of controls to ensure sufficient protection of enterprise data
Responsible for advising and making custom configuration changes in one to two tools to generate a product at the business or customer request
Updates logical or physical data models based on new use cases
Frequently uses SQL and understands NoSQL databases and their niche in the marketplace
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Experience across the data lifecycle
Advanced at SQL (e.g., joins and aggregations)
Working understanding of NoSQL databases
Proficiency with JSON/Java, Erwin, Oracle, Snowflake, AWS
Significant experience with statistical data analysis and ability to determine appropriate tools and data patterns to perform analysis, including data modeling
Experience customizing changes in a tool to generate product
Executes data architecture solutions and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down problems
Produces data architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of data frameworks, applications, and systems that proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture

Preferred qualifications, capabilities, and skills
Familiarity with modern technologies
Exposure to cloud technologies
Chase is a leading financial services firm, helping nearly half of America's households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.
We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $97,850.00 - $150,000.00 / year",123925,10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD),DE,224,data engineer,na,"['sql', 'java', 'nosql']","['snowflake', 'aws']",[],"['snowflake', 'oracle']",[],[],,
SEDAA,3.4,"Oakland, CA",Sr. Data Engineer,"Description:
FTE/ Client's DIRECT HIRE
Job title: Data Engineer
Job Category: Information Technology

Work Type: Hybrid

Job Location: Oakland
Team Overview
The Decision Products team strives to utilize best in class modeling techniques and industry leading data science to drive client’s transition to the sustainable energy network of the future through data driven decision making. This work moves beyond descriptive reporting and is focused on pushing the business forward through applied statistics, predictive and prescriptive analytics, and insightful tool design. The cornerstone of these high value analytics is one of the largest smart meter usage databases in the industry, that when combined with billing, program engagement, customer demographic, grid, and other data sources has unprecedented potential.
Current and past projects include:
Deployment of computer vision algorithms in tools that accelerate and automate asset inspections processes
Predicting electric distribution equipment failure before it occurs allowing for proactive maintenance
Optimizing renewable resource portfolios, including location and resource adequacy considerations
Supporting asset strategy decision making including, where should PG&E underground electrical assets
Supervised and unsupervised machine learning models using Python and Spark, trained on AWS, deployed on Palantir Foundry
Position Summary

We are looking for a savvy and driven Data Engineer to join our growing team of analytics experts. In this role you will work as part of cross functional teams, including data scientists, other data engineers, technology experts, and subject matter experts to develop data driven solutions. Successful candidates will be responsible for building, expanding, and optimizing our data, data storage, and data pipeline. This individual will support team members (data scientists, software developers, etc.) and decision products to ensure that data delivery is reliable and optimized. They will be supporting the data needs of multiple teams, systems, and products. This role will help the team continue its history of success. Qualified candidates will have a unique opportunity to be at the forefront of the utility industry and gain a comprehensive view of the nation’s most advanced smart grid. It is the perfect role for someone who would like to continue to build upon their professional experience and help advance
client’s sustainability goals.
client is providing the salary range that the company in good faith believes it might pay for this position at the time of the job posting. This compensation range is specific to the locality of the job. The actual salary paid to an individual will be based on multiple factors, including, but not limited to, specific skills, education, licenses or certifications, experience, market value, geographic location, and internal equity. We would not anticipate that the individual hired into this role would land at or near the top half of the range described below, but the decision will be dependent on the facts and circumstances of each case.

A reasonable salary range is:
ACTUAL BAY AREA SALARY RANGE: $98,000.00 to $122,000.00

ACTUAL CALIFORNIA SALARY RANGE: $93,000.00 to $116,000.00
This position is hybrid, working from your remote office and your assigned work location based on business need.

Responsibilities
Enhance and maintain our current data pipelines and associated infrastructure

Assemble large, moderately complex data sets that meet functional / non-functional business requirements.
Engage with different stakeholder teams to troubleshoot various database systems
Build and maintain tools that monitor data and system health
Identify, design, and implement internal process improvements to optimize production of results and enable cost savings.
Performance tune and optimize data pipeline on Spark
Create and maintain documentation describing data catalog and data objects

Minimum Requirements

Bachelor’s degree in computer science, an engineering field, or equivalent work experience in an engineering field
3 years of experience with data engineering/ETL ecosystem, such as Palantir Foundry, Spark, Informatica, SAP BODS, OBIEE
Required Skills

Experience with data engineering/ETL ecosystem, such as Palantir Foundry, Spark, Informatica, SAP BODS, OBIEE
Database design fundamentals
Experience with Python, Pandas and APIs
Knowledge of Time Series data set development.
Demonstrated commitment to teamwork and enabling others
Proven ability to translate business desires into technical requirements
Ability to communicate with various stakeholders and leadership
Ability to breakdown an ambiguous problems
Desired Skills

Experience with Scikit Learn, PySpark or equivalent big data processing framework, CI/CD tool
Experience with an infrastructure as code tool, writing production-level code, writing health checks, unit tests, integration tests, schema validations
Familiarity with cloud computing security fundamentals
Experience with the Palantir Foundry platform
Experience working with data scientists and machine learning engineers
Familiarity with model deployment
Front end tools: PowerBi, Tableau",110000,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2000,Unknown / Non-Applicable,CA,23,data engineer,senior,['python'],['aws'],"['sap', 'tableau']",[],['spark'],[],bachelor,2-5 years
Health Plan One,3.2,"Trumbull, CT",Data Engineer,"The Data Engineer will be responsible for designing, developing, and maintaining our data infrastructure and pipelines, as well as ensuring the quality and reliability of our data. The successful candidate will work closely with our data scientists, analysts, and other stakeholders to ensure that our data is accurate, accessible, and secure.
Duties/Responsibilities:
Design and implement data pipelines and infrastructure to ensure efficient data processing and storage.
Ensure the quality and reliability of data, including identifying and resolving data inconsistencies, errors, and anomalies.
Develop and maintain ETL processes and data integration workflows.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Collaborate with data scientists, analysts, and other stakeholders to understand data needs and requirements.
Monitor and optimize data performance, including troubleshooting and resolving issues as they arise.
Keep up to date with new technologies and approaches to data engineering and recommend improvements to our data infrastructure.
Performs other related duties as required.
Required Skills/Abilities:
Bachelor's degree in Computer Science, Data Science, or a related field.
Minimum of 3 years of experience in data engineering or a related field.
2+ years in data modeling, data governance, and or data architecture
Strong experience with ETL processes, data integration, and data pipelines.
Proficiency with SQL, Python or similar.
Experience with data warehousing and data modeling concepts.
Knowledge of cloud computing and storage solutions, such as Azure or AWS.
Strong analytical and problem-solving skills.
Excellent communication and collaboration skills.
Preferred Skills/Abilities:
Master's degree in Computer Science, Data Science, or a related field.
Experience with data visualization tools such as Domo, Tableau or Power BI.
Experience with data integration through APIs, web services, SOAP, and/or REST services
Physical Requirements:
Prolonged periods of sitting at a desk and working on a computer, typically in an office or cubicle environment (constant noise, fluorescent overhead lighting)
Our centers are consistent with CDC guidelines and align with local government orders pertaining to all Company physical locations in relation to COVID-19.
Equal Employment Opportunity (EEO) is a fundamental principle at HPOne, where employment is based upon personal capabilities and qualifications. HPOne does not discriminate because of actual or perceived sex, sexual orientation or preference, gender identity, gender, transgender, race, color, religion, national origin, creed, citizenship status, ancestry, age, marital status, pregnancy, childbirth or related medical conditions, medical conditions including genetic characteristics, mental or physical disability, military and veteran status, or any other protected characteristic as established by law. HPOne requires the necessary drug testing and background checks as part of our pre-employment practices.
Job Type: Full-time
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Day shift
Monday to Friday
Work Location: Hybrid remote in Trumbull, CT 06611",94155,1001 to 5000 Employees,Company - Private,Insurance,Insurance Carriers,2006,$25 to $100 million (USD),CT,17,data engineer,na,"['sql', 'python']","['azure', 'aws']","['tableau', 'power bi']",[],[],[],bachelor,+10 years
Northeastern University,4.3,"Boston, MA",Data Engineer,"About the Opportunity
Do you love learning the shapes of datasets, and molding them into something new? The Digital Scholarship Group (DSG) in the Northeastern University Library is excited to open a search for a Data Engineer. Working within a warm and collaborative environment dedicated to social justice, the Data Engineer gathers, organizes, manipulates, transforms, and documents a variety of humanities research data. The Data Engineer works with colleagues across the university to create sustainable platforms and data for community-led digital scholarship.
The Data Engineer position is situated within the DSG, which is part of the Northeastern University Library. The Library is a vital partner in learning, teaching, and community-engaged research for a diverse R1 university. Northeastern is committed to intensive research and experiential learning for students at all levels.
The Digital Scholarship Group is based on Northeastern’s Boston campus. This position is eligible for a hybrid work arrangement. Specific arrangements can be negotiated at the time of hire.
Responsibilities
The Data Engineer has responsibility for helping DSG work with data in a wide range of formats, across multiple projects and often in unforeseen contexts. The Data Engineer develops data dictionaries, mappings between data standards, transformation routines, and other curatorial systems. This position also manages projects and engages in high-level needs analysis and project planning. The DSG is committed to digital approaches that consider the pedagogical, research, social, and ethical implications of data and its design and use.
Working closely with other DSG and library staff, faculty collaborators, and students, the Data Engineer contributes to grant-funded and internal projects including the Boston Research Center; the Civil Rights and Restorative Justice project; the Digital Archive of Indigenous Language Persistence; the TEI Archiving, Publishing, and Access Service; Digital Humanities Quarterly; and the Women Writers Project. They will also take the lead on building DSG’s policies and practices in working with external data platforms such as Wikidata and partner project APIs. To support this work, expertise with tools like regular expressions and OpenRefine, and facility with data including RDF, JSON, XML, various API responses, and other formats will be important.
We warmly invite people with various skills and levels of expertise to apply to this position. Candidates who meet some, but not all, of the qualifications listed below are strongly encouraged to apply. We seek colleagues who are committed to building an inclusive and diverse working environment and who have been and remain underrepresented or marginalized in the field of librarianship – including but not limited to people of color, LGBTQ+ people, individuals with disabilities and applicants from lower-income and first-generation library or academic backgrounds. We expect this position to be an ongoing learning experience and are committed to supporting professional development.
Qualifications
We realize that this is a lengthy list of activities and qualifications. There are multiple paths toward success in this position, and each may look somewhat different depending on the successful candidate’s interests and experience.
Bachelor’s degree required; Master’s degree or similar training in data science, information science, information design, or other relevant discipline preferred
Minimum of 2 years of experience working or studying in a data-intensive environment, preferably in an academic or non-profit research setting
Experience working with quantitative and qualitative datasets, especially with historical and cultural heritage data
Experience working with structured data formats (for instance, XML, RDF, JSON, CSV, relational databases) and with data conversion, data enhancement, and data analysis
Ability to write code to assist in carrying out these kinds of data-related work (for instance, using R, Python, SQL, SPARQL, XSLT, Perl, and/or regular expressions)
Ability to work on multiple concurrent projects and adapt to the evolving landscape of digital humanities
Collaborative problem-solving skills, and the ability to research and recommend solutions as part of a participatory design process
Commitment to thoughtful, adaptive engagement with the needs of community collaborators
Strong oral and written skills, ability to communicate across expertise levels and prepare project documentation
Desire and aptitude to grow skills (especially in technical areas) and learn new things
The following skills are desirable but are not all essential for applicants to possess at the outset; we can provide training:
Knowledge of metadata standards relevant to research data, such as the Data Documentation Initiative
Experience creating, manipulating, and querying linked open data
Experience in open-source development practices and workflows, preferably within an academic or non-profit environment
Experience working with databases, data management systems, and APIs
Experience with developing and leading workshops
Experience communicating complex ideas about data and how it is used to many audiences
Salary Range:
$82,725 - $93,000
About the Digital Scholarship Group
A recognized leader in the field, the Digital Scholarship Group supports digital modes of research, publication, and collaboration through applied research, systems and tools development, and consultative services. The DSG offers a friendly and closely collaborative work environment, and actively fosters the professional and intellectual development of all of our colleagues and collaborators, including training opportunities and mentorship.
Our team engages with faculty in the digital humanities and quantitative social sciences from across the university to develop digital research and teaching projects, organize events, plan grant-funded initiatives and provide training and mentorship. We also work in close partnership with Northeastern’s Archives and Special Collections, the NULab for Maps, Texts, and Networks, and with cultural heritage partners in Boston including the Massachusetts Historical Society and the Boston Public Library.
We develop tools and platforms for working with digital artifacts and data, for querying and publishing them. We also provide workshops, mentorship opportunities, and pedagogical frameworks to the Northeastern community. Some of our major projects include the Boston Research Center, the Civil Rights and Restorative Justice Project, and the Digital Archive of Indigenous Language Persistence, as well as a number of digital archiving projects from the Library’s Archives and Special Collections. In all of our projects, we are attentive to inclusive and anti-racist approaches to data modeling, platform development, and collaborative working processes.
About the Library
The Northeastern University Library supports the mission of the University by working in partnership with the University community to develop and disseminate new scholarship. The Library fosters intellectual and professional growth, enriches the research, teaching, and learning environment, and promotes the effective use of knowledge by managing and delivering information resources and services to library users.
Northeastern University is an equal opportunity employer, seeking to recruit and support a broadly diverse community of faculty and staff. Northeastern values and celebrates diversity in all its forms and strives to foster an inclusive culture built on respect that affirms inter-group relations and builds cohesion.
All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, religion, color, national origin, age, sex, sexual orientation, disability status, or any other characteristic protected by applicable law.
To learn more about Northeastern University’s commitment and support of diversity and inclusion, please see www.northeastern.edu/diversity.
About Northeastern
Founded in 1898, Northeastern is a global research university and the recognized leader in experience-driven lifelong learning. Our world-renowned experiential approach empowers our students, faculty, alumni, and partners to create impact far beyond the confines of discipline, degree, and campus.
Our locations—in Boston; Charlotte, North Carolina; London; Portland, Maine; San Francisco Bay area; Seattle; Silicon Valley; Toronto; Vancouver; and the Massachusetts communities of Burlington and Nahant—are nodes in our growing global university system. Through this network, we expand opportunities for flexible, student-centered learning and collaborative, solutions-focused research.
Northeastern’s comprehensive array of undergraduate and graduate programs— in a variety of on-campus and online formats—lead to degrees through the doctorate in nine colleges and schools. Among these, we offer more than 195 multi-discipline majors and degrees designed to prepare students for purposeful lives and careers.
The position will remain open until filled but application review will begin after June 16.
Position Type
Information Technology
Additional Information
Northeastern University considers factors such as candidate work experience, education and skills when extending an offer.
Northeastern has a comprehensive benefits package for benefit eligible employees. This includes medical, vision, dental, paid time off, tuition assistance, wellness & life, retirement- as well as commuting & transportation. Visit
https://hr.northeastern.edu/benefits/
for more information.
Northeastern University is an equal opportunity employer, seeking to recruit and support a broadly diverse community of faculty and staff. Northeastern values and celebrates diversity in all its forms and strives to foster an inclusive culture built on respect that affirms inter-group relations and builds cohesion.
All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, religion, color, national origin, age, sex, sexual orientation, disability status, or any other characteristic protected by applicable law.
To learn more about Northeastern University’s commitment and support of diversity and inclusion, please see
www.northeastern.edu/diversity
.",87863,1001 to 5000 Employees,College / University,Education,Colleges & Universities,1898,$100 to $500 million (USD),MA,125,data engineer,na,"['sql', 'r', 'python']",[],[],[],[],[],bachelor,2-5 years
Fuge Technologies Inc,4.7,"Palmyra, NJ",Senior Data Engineer,"Job Description:
RESPONSIBILITIES
Basic Qualifications for consideration:
5+ Overall industry experience
3+ years' experience with building large scale big data applications development
Bachelor’s in computer science or related field
Provide technical leadership in developing data solutions and building frameworks
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Hands-on experience with Springboot framework, MVC architecture, Hibernate, API creation, Restful services, SOAP using Java
Java experience with OOPS concepts, multithreading
Experience deploying code on containers
Hands on experience with leveraging CI/CD to rapidly build & test application code
Conduct code reviews and strive for improvement in software engineering quality
Review data architecture and develop detailed implementation design
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance
Qualifications
Bachelor’s degree in computer science or computer Engineering is required
Skills Set Mandate :
Java, Spark, and Azure cloud
Job Type: Contract
Salary: $55.00 - $60.00 per hour
Ability to commute/relocate:
Palmyra, NJ 08065: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",103500,1 to 50 Employees,Contract,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,senior,"['sql', 'nosql', 'java']","['azure', 'aws']",[],['dynamodb'],"['kafka', 'spark']",[],bachelor,0-2 years
EnergyHub,3.6,"Brooklyn, NY",Staff Data Engineer,"EnergyHub empowers utilities and their customers to create a clean, distributed energy future. We help consumers turn their smart thermostats, EVs, batteries, and other products into virtual power plants that keep the grid stable and enable higher penetration of solar and wind power.
Data is core to the function and operation of EnergyHub. We spend most of our time on customer facing systems, which means that your work on this team directly improves a product that impacts climate change while improving grid stability at the same time. If you'd like to make a difference for current and future generations using your data skills, you are in the right place.
We operate on a fivetran/DMS/snowflake/airflow stack, and we are in the middle of transitioning our transformation tooling to DBT from a mix of custom python batch processing in aws batch and snowpark wrappers.
Main Responsibilities
Design data models in partnership with product managers, data analysts, data scientists, and occasionally a backend engineer on another team.
Port old code into DBT, possibly spotting uncaught bugs along the way
Write new pipelines, in DBT, optimizing first for correctness, second for maintainability, and third for performance
Set the bar for engineering best practices in an analytics engineering context
Occasionally write or patch a data integration (written in python)
find, evaluate, and adopt great tools and vendors
give and accept feedback readily and with an open mind
Participate in and contribute to architecture reviews in data engineering and engineering more broadly
Educate, mentor and guide members of the Data team to improve their skill sets
Key Skills and Experience
We recognize that years of experience is an imperfect proxy for ability. If you have deep or intense experiences with any skill listed which do not meet the years listed and you achieved success with those skills, be sure to mention those experiences in your resume!
6 years of cumulative experience in roles for which the primary responsibility is programmatic data processing using at least one of the following languages/frameworks or something substantially similar: python, R, SQL, Snowflake, Redshift, BigQuery, Vertica, ClickHouse, Julia, Scala, Spark, Flink, Kafka
4+ years of experience using DBT
5+ years of experience using SQL
4+ years of experience with python/R/julia or a similar scripting language, of which at least one year is with pandas
2+ years of experience writing api integration
Preferred Skills and Experience
Mastery of SQL: you have not found a data transformation task that you cannot solve relatively cleanly in SQL (or at least SQL + Jinja/DBT). You can read, explain, and, refactor a large and complex sql statement into a collection of smaller ones, without access to someone who can answer questions about it.
DBT expertise: You are comfortable implementing custom materialization types. You have implemented multiple project structures and can neutrally articulate pros and cons of different approaches. You are fluent in model selection syntax.
Mastery of Snowflake: You can articulate the query profile that you are expecting before running a query, given the clustering keys of tables involved. You might have used MATCH_RECOGNIZE for event analysis. You have 1+ years of experience navigating database permissions, or a willingness to read a lot of documentation. You have a working knowledge of future grants and RBAC.
The salary range for this position is $150,000-$190,000. Base pay offered may vary based on location, job-related knowledge, skills and experience.
Why work for EnergyHub?
Collaborate with outstanding people: Our employees work hard, do great work, and enjoy collaborating and learning from each other.
Make an immediate impact: New employees can expect to be given real responsibility for bringing new technologies to the marketplace. You are empowered to perform as soon as you join the team!
Gain well rounded experience: EnergyHub offers a diverse and dynamic environment where you will get the chance to work directly with executives and develop expertise across multiple areas of the business.
Work with the latest technologies: You'll gain exposure to a broad spectrum of IoT, SaaS and machine learning obstacles, including distributed fault-tolerance, device control optimization, and process modeling to support scalable interaction with disparate downstream APIs.
Be part of something important: Help create the future of how energy is produced and consumed. Make a positive impact on our climate.
Focus on fun: EnergyHub places high value on our team culture. Happy hours and holiday parties are important to us, but what's also important is how our employees feel every single day.
Company Information
EnergyHub is a growing enterprise software company that works with the most forward-thinking companies in smart energy. Our platform lets consumers turn their smart thermostats, electric cars, water heaters, and other products into virtual power plants that keep the grid stable and enable higher penetration of solar and wind power. We work on technology that already provides energy and cost savings to millions of people through partnerships with the most innovative companies in the Internet of Things.
Company Benefits
EnergyHub offers a generous benefits package including 100% paid medical for employees and a 401(k) with employer match. We offer a casual environment, the flexibility to set your own schedule, a fully stocked fridge and pantry, free Citi Bike membership, secure bike rack, gym subsidy, paid parental leave, and an education assistance program.
EnergyHub is an Equal Opportunity Employer
In connection with your application, we collect information that identifies, reasonably relates to or describes you (""Personal Information""). The categories of Personal Information that we may collect include your name, government-issued identification number(s), email address, mailing address, other contact information, emergency contact information, employment history, educational history, and demographic information. We collect and use those categories of Personal Information about you for human resources and other business management purposes, including identifying and evaluating you as a candidate for potential or future employment or future positions, recordkeeping in relation to recruiting and hiring, conducting analytics, and ensuring compliance with applicable legal requirements and Company policies.",170000,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Computer Hardware Development,2007,$5 to $25 million (USD),NY,16,data engineer,na,"['sql', 'r', 'python', 'scala']","['snowflake', 'aws', 'redshift']",[],"['snowflake', 'dbt']","['kafka', 'flink', 'spark']",[],,+10 years
"Second Wave Delivery Systems, LLC",4.0,Remote,Data Engineer (Looker/LookML),"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build Looker LookML for new datasets
Maintain and improve Looker LookML data models
Create Looker looks and dashboards, including both standalone and embedded dashboards
Optimize existing dashboards as well as Looker instance to improve response time and user experience
Promote best practices with development of Looker dashboards and embedded visualizations
Work closely with the Business Intelligence and Client management teams to implement and track reporting deliverables for clients
Implement operational processes in BigQuery and Looker
Maintain tools used in data analysis and reporting
Train teams on using and creating reports on an ad-hoc basis

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
Experience with data modeling using LookML
Experience with big data analysis tools such as BigQuery
Experience utilizing SQL and other tools to manipulate and analyze complex data sets
Strong knowledge and understanding of data integrity and data quality
Experience implementing best practices for data governance related to standard naming conventions and data definitions
Practical knowledge of Information Security concepts and Data Loss Prevention
Experience communicating to various stakeholders utilizing visualization tools
Excellent verbal and written communication skills

Preferred Qualifications
Experience extracting and correlating healthcare data analytics",112889,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020,Unknown / Non-Applicable,Remote,3,data engineer,na,['sql'],[],['looker'],[],[],[],,
"Stark Dev, LLC",4.0,"Plano, TX",Data Engineer/Analyst / W2 /USC or GC (GC EAD) or H4 holders,"This position is open for United States Citizens or Green Card holders (GC EAD)/H4 EAD ONLY.
This is an on-site position in Plano/Dallas
CONTRACT W2
Top Skills Details
1) Experience working on a data migration project as a Data Analyst.
- This person will be working on their Permitting, Planning, and Inspection System Project. They are moving from a legacy permitting system, TRAKIT, to a completely new Salesforce application, Clariti. (TRAKIT and Clariti experience not required)
2) Experience doing data discovery, classification, verifying data, mapping rules
- On this project this team will be moving all of the historical data from the old application, TRAKIT, to the new application Clariti.
3) Proficient with SQL and writing SQL queries
\* Interpret data, analyze results using statistical techniques and provide ongoing reports
\* Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality
\* Acquire data from primary or secondary data sources and maintain databases/data systems
\* Identify, analyze, and interpret trends or patterns in complex data sets
\* Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems
\* Work with management to prioritize business and information needs
\* Locate and define new process improvement opportunities
Drug Test Required
false
Go To Work
false
Workplace Type
On-site
Experience Level
Expert Level
Job Type: Contract
Pay: $55.00 - $60.00 per hour
Schedule:
Monday to Friday
Application Question(s):
What is your visa status?
Work Location: In person",103500,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['sql', 'go']",[],[],[],[],[],,
2ULaundry,3.8,"Charlotte, NC",Data Engineer,"2ULaundry and LaundroLab are on a mission to create time for the world to focus on the things that matter most - and that probably isn't laundry! We're a rapidly growing startup that is rewriting the rules around this time-consuming task by building two complementary brands. With our convenient pick-up and drop-off service and modern, inviting laundromats, we provide people with an effortless and convenient solution to laundry and dry cleaning to give them back time because as we all know, time is valuable.
The Data Engineer Role:
We are looking for a Data Engineer who will be responsible for managing and processing large amounts of data. This includes integrating data from various sources, such as internal systems and external APIs, and ensuring its quality and integrity. You would design and maintain data warehousing solutions for efficient storage and retrieval, while also optimizing data processing workflows for analysis and reporting purposes. Database management tasks, including performance tuning and security measures, would be part of your responsibilities. Additionally, you would contribute to data governance practices, documentation, and compliance. Collaboration with cross-functional teams and effective communication with stakeholders are crucial aspects of the role. Overall, your focus would be on leveraging data to drive operational efficiency and enhance customer services in the logistics and consumer services domain.
This role directly reports to our Director of Engineering.
What are the day-to-day responsibilities of the Data Engineer role?
Partner with company leaders, product managers, engineering and other data analysts to support our service delivery with diverse data and analytics initiatives.
Develop, test, and maintain robust, scalable data structures and pipelines.
Ensure systems meet business requirements and industry best practices, with an emphasis on platform scalability to support our rapid growth.
Design and implement high-performance algorithms, predictive models, and proof of concepts to improve service delivery and customer experience.
Identify opportunities for data acquisition and leverage existing data for additional business value.
Foster a data-driven culture by educating the team on data analytics best practices and data literacy.
Integrate emerging data management technologies and software engineering tools into existing structures to enhance system performance.
Build and extend our BI tools to include robust models that enable better self-service capabilities for non-technical stakeholders.
What are the requirements for the Data Engineer role?
Minimum 3 years of experience in data engineering, database architecture, and data warehousing
Proficiency in SQL and other programming languages
Experience with data processing tools and cloud platforms (Google Cloud preferred.
Strong understanding of databases, data processing, data storage, and data privacy, particularly in the context of service delivery.
Excellent analytical, problem-solving, and communication skills.
What are the perks of being the Data Engineer
Ability to get in on the ground floor and have a significant impact at one of Charlotte's fastest growing franchise companies
Opportunities to grow in the marketing field
Direct and regular access to thought leaders in the startup, franchising, and laundry industries
Benefits package that includes medical, dental, and vision insurance, a 401k, and an Employee Assistance Program
Unlimited PTO policy that our employees actually use!
Working with a supportive, driven team working to build our startup into a nationally recognized brand
Free laundry!
2ULaundry and LaundroLab are equal opportunity employers. We value diversity and strive to create an inclusive environment representative of a variety of backgrounds and experiences. Employment is decided solely on the basis of qualifications, merit, and business need.",90174,51 to 200 Employees,Company - Private,Personal Consumer Services,Laundry & Dry Cleaning,2015,Unknown / Non-Applicable,NC,8,data engineer,na,['sql'],['google cloud'],[],[],[],[],,2-5 years
Octaura LL TradingCo LLC,5.0,"New York, NY",Data Engineer,"We’re on a mission
At Octaura, we continually evolve markets to unleash value for clients. It’s in our DNA to make a difference and do things differently.

Existing workflows within our markets are painful for clients: they are outdated, overcomplicated, and time-consuming. We want to change that. Octaura fundamentally rebuilds and redefines the markets by streamlining workflows, digitizing platforms, and bringing transactions, data and analytics together for the first time.

Join our inclusive culture
At Octaura, everyone belongs.

It’s so important to us that all Octaurians are confident in knowing they have the space to use their voice and talents. We love the diversity we see in the world and we actively want our team to reflect this. We’re a values-driven company and by engaging, solving and evolving together, we create a culture that is collaborative, switched-on, and fun to be part of.

The role in a nutshell
We are looking for a Data Engineer who can steer the organization's data architecture and management strategy, playing a crucial role in consolidating disparate data sources into a single, easily accessible data pool. The right candidate can navigate the complexity of data systems, advocate for data governance, and have an inherent knack for translating raw data into actionable business intelligence.
This is an exciting time for Octaura as we are rapidly growing and are looking for energetic, collaborative and driven thinkers to join us! Please note this role is based out of our NYC office and our current structure is 4 days in the office, 1 day from home.
Core responsibilities
Design, build, and maintain scalable data pipelines, leveraging and integrating structured and unstructured data from diverse sources.
Adopt, implement, and maintain a Cloud Data Infrastructure working with services such as Snowflake, Databricks, or similar.
Normalize data and ensure that it is secure, reliable, and easily accessible for consumption and sale.
Implement strategies to manage a centralized data pool, facilitating its use in BI and other reporting tools.
Uphold data integrity and enforce data security policies in alignment with compliance and privacy standards.
Collaborate with all Product, Engineering, Sales, Marketing and Executive teams to deliver customized data reports for internal and external stakeholders.
Feed processed data back into the trading platform to enhance our product and provide valuable insights for our customers' investment decisions.
Stay informed about emerging trends and technologies in the field of Data Engineering, suggesting and implementing new tools and methodologies as appropriate.
Desired qualifications
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Minimum of 5 years of experience as a Data Engineer or in a similar role, preferably in financial services or FinTech.
Proficiency in NoSQL databases, MongoDB, and other data warehousing technologies.
Strong programming skills in Python, with demonstrated ability to manipulate data and build data solutions.
Extensive experience with ETL tools and processes, data modelling, and data architecture.
Advanced proficiency with BI tools like Tableau, with the ability to generate complex, large-scale data reports.
Knowledge of cloud-based data solutions (AWS) and big data technologies (Hadoop, Spark) is a plus.
Strong understanding of data privacy and security principles, along with experience in relevant data protection regulations.
Exceptional problem-solving skills and the ability to work both independently and collaboratively.
Excellent communication skills, with the ability to translate complex data insights into understandable and actionable information.
Experience building and adopting best practices in a start-up environment
The base pay range for this position in New York is $140,000 - $170,000 annually. Pay may vary depending on job-related knowledge, skills, and experience. Equity and year-end bonus may be provided as part of the compensation package, in addition to a full range of medical, financial, and other benefits, dependent on the position offered. Applicants should apply via Octaura's internal or external careers site.

Octaura Work Perks
At Octaura, our people are our most valuable asset, and we are pleased to offer the following benefits to all full-time employees:
Competitive compensation and equity
Unlimited Paid Time Off
Competitive Parental Leave
Daily breakfast, coffee and snacks in the office
90% company-paid healthcare
Onsite gym & discounted membership

We’re committed to equal opportunity employment
Octaura is committed to a diverse and inclusive workplace. We are an equal opportunity employer and do not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.",155000,1 to 50 Employees,Company - Private,Financial Services,Financial Transaction Processing,2022,Unknown / Non-Applicable,NY,1,data engineer,na,"['nosql', 'python']","['databricks', 'snowflake', 'aws']",['tableau'],"['mongodb', 'snowflake']","['spark', 'hadoop']",[],bachelor,
Zillion Technologies,3.8,"Saint Louis, MO",Data Engineer III – Hybrid,"Job Location: Data Engineer III – Hybrid
Location: St. Louis, MO
Position Description:
· Create and maintain optimal data pipeline architecture,
· Assemble large, complex data sets that meet functional / non-functional business requirements.
· Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
· Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies. Knowledge on Teradata and/or Databricks must.
· Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
· Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
· Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
· Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
· Work with data and analytics experts to strive for greater functionality in our data systems.
Key skill set:
· Teradata SQL, Fast Export and other Teradata native tools. Teradata Vantage knowledge is plus.
· Informatica Power Center and Informatica Cloud Data Integration
· Databricks
· AWS cloud basic knowledge, S3, Lamba etc.
· Linux
· Jenkins (CI/CD)
· GitHub
QUALIFICATIONS (education, experience special skills):
· Bachelor of Science degree in a business or technical field from an accredited college/university required.
· 5+ years of direct experience working with Data Modeling and Data Solution Development.
· Experience developing processes to manage data model development, principles, and standards.
· Experience working on Enterprise Data Warehouse initiatives.
· Proficient in the use of collaboration tools such as Jira, Confluence, Excel, and SharePoint.
· Experience in utility/energy Corporation preferred.
· Successful candidate will demonstrate: good analytical, communication, leadership and human relations skills required.
Job Types: Full-time, Contract
Pay: $70.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
St. Louis, MO 63103: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Work Authorization?
Experience:
Data Modeling and Data Solution Development: 5 years (Required)
Teradata, Informatica, and AWS: 5 years (Required)
Enterprise Data warehouse: 5 years (Required)
Work Location: In person",130500,201 to 500 Employees,Company - Private,Management & Consulting,Business Consulting,-1,$5 to $25 million (USD),MO,-1,data engineer,na,['sql'],"['databricks', 'aws']",['excel'],[],[],[],bachelor,5-10 years
GrowthLoop,4.0,"New York, NY",Senior Data Engineer,"GrowthLoop
GrowthLoop is a high-growth technology startup offering a customer segmentation platform on Snowflake and BigQuery that is changing the way businesses acquire, retain, and win back their customers. We are a completely bootstrapped and profitable startup with clients like Indeed, Google, and Uber.
Our Mission
Unlock the value of data in the cloud data warehouse to make it drive massive, real-world impact.
How we work
A collaborative team that strongly believes in taking the learner's mindset
We encourage exploration of new technologies
We believe in empowering every person on our team to do their best work
Dedication to building a product people love
We build slowly but surely for the long term. We are transparent about the challenges of building a great company. We are humble in facing those challenges. But, we know that if we keep improving every day the GrowthLoop spins
About You
We are looking for an outstanding Senior Data Engineer. This is a unique role to work directly with our customers, along with GrowthLoop Platform Engineering, Data Science, and DevOps teams to help us architect and build both new product features along with tools to enable more efficient data modeling and engineering workflows. You have experience with various open source and third-party tools (DBT, Matillion, SQLAlchemy) and have a deep understanding of tool-independent principles for good data modeling. Our customers are some of the most advanced companies in the world, their first-party data lives in Google BigQuery, Snowflake, and Amazon Redshift. You will help build new tools and services to support the data modeling needs of our customers and the data needs of new product features. High-energy self-starter with experience and passion for data and big data scale problems.
Responsibilities
Architect and design scalable data platforms that work hand in hand with the product
Support and Develop SQL data pipelines using tools like Airflow, DBT, and SQLAlchemy
Snowflake, BigQuery, Redshift experience
Optimize queries and incremental SQL pipelines for cost efficiency and speed
Work hand in hand with customers to understand their needs, architect and design thoughtful and scalable data models, ready for BI and machine learning use cases
Work on data services like Identity Resolution and Campaign performance Evaluation
Support a cross-functional team, including analytics, engineering, and product in developing all of the above in complex hybrid cloud environments
Experience with 3rd party and open-source data import tools like Singer, Matillion, Fivetran
Qualifications
3+ years work experience in Data Modeling, Data Engineering, or equivalent
Expertise in SQL, scalable data pipeline development, column-oriented databases (e.g. BigQuery and Redshift), and analytical data modeling
Hands-on experience with all aspects of design, development, and maintenance of analytical data warehouses/lakes using both distributed file systems and cloud-scale data processing systems
Experience building data integration frameworks to accelerate time to value on common integration challenges
Strong analytical and problem-solving skills
Insatiable intellectual curiosity to learn new (cloud) technologies with an ability to self-manage (we don't micromanage here)
We are a collaborative team that strongly believes in taking the learner's mindset to everything we do. This role will have the opportunity to learn how to apply machine learning / artificial intelligence models to some of the most important problems businesses face.
By joining our team, we hope you will change the trajectory of your professional career and that of our business.
The estimated base salary for this role is $110,000-$130,000, with the option for additional variable based compensation. Final base salary decisions will be based on a variety of non-discriminatory factors, such as the individual's performance, experience and qualifications.
Benefits
Meritocracy
Spot bonuses for major milestones
Grow into leadership roles as we scale this rocket-ship
Startup equity for star performers
Generous Time-off
Take as much vacation as you like!
Flexible remote work policies
Platinum Benefits ‍
Free Platinum Health Insurance with Aetna
401(k) Program with Generous Company Match
Flexible PTO and WFH Policies
Learn and Grow ✍️
Education Stipend towards your professional development
Work directly with Founders (ex-Googlers)
Learners' mindset culture of 'Friendly Geniuses'",120000,,,,,-1,,NY,-1,data engineer,senior,['sql'],"['snowflake', 'redshift']",[],"['snowflake', 'dbt']",[],[],,+10 years
"Evergreen Technologies, LLC.",5.0,"Madison, NJ",Data Engineer,"Position: Data Engineer
Location: Madison, NJ
Duration: 9 Months (possible extension)
Responsibilities:
The Data Platform team in our Companys Animal Health IT (MAHI-IT) designs and implements end to end data solutions to support customer facing applications in animal traceability, monitoring, well-being, and more.
We seek a data engineer to help the team setting up, maintaining, optimizing, and scaling data pipelines from multiple sources and across different functional teams in a cloud environment.
Assist in developing best practices for deploying, monitoring, and scaling data pipelines in the cloud Identify requirements for ingestion, transformation, and storage of data Design and implement optimal and scalable data pipelines
Use cloud tools to integrate data from multiple data sources into the data lake and design and implement ways to expose it
Identify opportunities for automation and optimization of data pipelines and re-design of data architecture and infrastructure for great scalability and optimal delivery
Implement cloud/ data infrastructure required to extract, transform, and load data from multiple sources
Identify required security and governance procedures to keep the data safe in a cloud environment
Assist in developing and executing testing plans to help with QA efforts.
Qualifications Requirements/ Qualification:
Bachelors degree in Data Engineering, Computer Science, or related field.
Experience designing and implementing data engineering pipelines.
Advanced knowledge in Python and PySpark.
Working knowledge of one or more SQL languages.
3+ years of hands-on experience with developing data warehouse solutions and data products.
1+ year of hands-on experience developing a distributed data processing platform with Hadoop, Hive, Spark, Airflow, Kafka, etc.
3+ years of hands-on experience in modeling and designing data schemas.
Advanced experience with programming languages: Python, Pyspark, Scala, etc. Knowledge of scripting languages: Perl, Shell, etc.
Practice working with, processing, and leading large data sets.
Experience with cloud tools for ingesting and processing data.
Preferred Experience And Skills:
Experience with AWS tools big data platforms S3, EMR, EKS, Lambda, etc.
Experience with data ingestion and transformation tools like Streamsets and Databricks
Experience working with DevOps teams
Experience with container technologies such as docker and Kubernetes
Experience with data warehousing tools like Snowflake and Redshift",96690,201 to 500 Employees,Company - Public,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['shell', 'sql', 'scala', 'python']","['databricks', 'snowflake', 'aws', 'redshift']",[],"['snowflake', 'hive']","['kafka', 'spark', 'hadoop']",['docker'],,+10 years
i-Link Solutions Inc,4.0,"Boston, MA",Data Engineer,"i-Link Solutions is seeking a Data Engineer to join our team in Hanscom, MA. The Data Engineer will be responsible for designing, developing, and maintaining the data infrastructure that supports the Kessel Run program. The ideal candidate will have experience with AWS, open-source tools, and data engineering best practices
Responsibilities
Design, develop, and maintain the data infrastructure that supports the Kessel Run program
Work with stakeholders to understand business requirements and translate them into technical solutions
Build and maintain data pipelines that ingest, transform, and load data into data warehouses and data lakes
Develop and deploy data models and algorithms to support data-driven decision making
Work with data scientists and analysts to develop and deploy data products
Ensure the security and compliance of data assets
Qualifications
Bachelor's degree in Computer Science, Information Technology, or a related field
10+ years of experience in data engineering
Experience with AWS, open-source tools, and data engineering best practices
Strong programming skills in Python, Java, Scala, or SQL
Experience with data modeling and data warehousing
Experience with data visualization and reporting
Strong analytical and problem-solving skills
Excellent communication and teamwork skills
US Citizenship
Secret Clearance
Kessel Run Experience Desirable
Job Types: Full-time, Permanent
Pay: $89,571.07 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Boston, MA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineering: 10 years (Required)
AWS: 3 years (Required)
Data warehouse: 4 years (Required)
Work Location: In person",114786,,,,,-1,,MA,-1,data engineer,na,"['sql', 'java', 'scala', 'python']",['aws'],[],[],[],[],bachelor,2-5 years
Division of Information Technology - NYC Department of Health and Mental Hygiene,3.2,"Long Island City, NY",Data Engineer,"The New York City Department of Health and Mental Hygiene (DOHMH) is the nation's leading public health agency protecting and promoting health of all New Yorkers. Our 7,000-plus team members bring an extraordinary array of languages, cultures, and experiences to bear on the work of public health. Our diversity fuels creativity because all perspectives are heard and valued. DOHMH aims to improve the health outcomes of all New Yorkers by centering persistent racial inequities and promotion of social justice at the core of its work.
The Division of Information Technology aims to align technology solutions with the DOHMH mission by prioritizing resource use and deploying innovations that facilitate the agency’s day-to-day activities and enhance staff productivity and efficiency. Our goal is to provide users with a reliable, stable, and safe computing environment, through the collaboration of the Bureau of Technology Strategy & Project Management provides business analysis and IT project management services to define and deliver IT solutions that meet all program needs.
**This a grant-funded W-2 position with full employment benefits that expires 6/30/2025 (possibility for extension) hired through Public Health Solutions and will be assigned to NYC DOHMH. Only those with authorization to work in the U.S. without sponsorship should apply. Remote or hybrid option available. Professional references are required. **
The agency is seeking a Data Engineer (Azure, SQL, Python) to help in the infrastructure, process, and procedures to create “data lakes” in the cloud for large amounts of public health data including but not limited to syndromic surveillance, electronic laboratory and case reporting, notifiable diseases, vital records, mental health, maternal mortality, chronic diseases, as well as agency operational data such as finance, human resources, emergency operations, and information technology data. This data will be made available to analysts across DOHMH to consume and process in analytic platforms such as R, Python, SAS and visualize them in tools like Tableau, Power BI, Data Wrapper. In addition, the data can be processed with machine learning algorithms to help make more informed policy and operational decisions.
Job Duties:
Engineer data pipelines using a variety of technologies including Azure Synapse, SQL, SSIS, Python to extract, transform and load data.
Load and transform data from external sources and partners in a variety of different structured and unstructured file formats as well as internal on-premises data and applications primarily in SQL databases and CSV file formats.
Process public health data requests following applicable procedures and document changes in an electronic repository.
Apply technical knowledge to architect solutions that meet business needs, create Data Platform, AA/AI roadmaps, and ensure long term technical viability of new deployments, infusing key analytics and AI technologies where appropriate (e.g., Azure ML, ML Server, BOT framework, Cognitive Services, Big Data, Data Lake, Azure Databricks, etc.).
Ensure that solution exhibits high levels of performance, security, scalability, maintainability, appropriate reusability, and reliability upon deployment
Develop deep relationships with key customer IT decision makers, who drive long-term cloud adoption within their company to enable them to be cloud advocates.
Assess the Customers' knowledge of Azure platform and overall cloud readiness to support customers through a structured learning plan and ensure its delivery through partners.
For senior data engineers, supervisor or mentor junior data engineer and analysts.
Qualifications and Requirements:
5+years of experience with Azure data platform including Azure SQL, Azure Data Factory, Azure Databricks, Azure SQL Data Warehouse (Synapse Analytics), Azure Data Lake Storage, Azure Cosmos DB, SQL Server, SSIS, SSRS, etc.
Strong Object relational mapping experience with UML modeling and OO modeling.
Experience with T-SQL, SSIS, Power BI and Azure Analysis Services
Experience with developing data pipelines using python.
Strong problem solving and analytical skills.
Passion for public health and solving problems and creating new insights through data.
Experience working with Healthcare data and proven track of implementation experience in EDI data formats.
Experience in job roles involving metadata management, relational dimensional modeling and big data solution approaches with native Azure Data Platform tools or 1st party services.
Excellent communication, presentation skills with the right attitude towards problem solving in diverse teams.
Strong aptitude and technical skills with conceptual strength in logical solutions driven towards balanced optimal considerations.
Solid knowledge in SQL, security standards, BI tools, ETL tools and Microsoft Azure specific technologies.
Certifications in Azure, data modeling and metadata management.
Ability to use BI tools like Power BI to represent insights and other presentation techniques.
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
Health insurance
Schedule:
8 hour shift
Work Location: In person",115000,Unknown,Government,Government & Public Administration,State & Regional Agencies,-1,Unknown / Non-Applicable,NY,-1,data engineer,na,"['sql', 'r', 'python']","['databricks', 'azure']","['ssis', 'tableau', 'power bi']",['sql server'],[],[],,+10 years
Dupaco Community Credit Union,4.6,"Dubuque, IA",Data Engineer,"Are you looking to use your technical expertise to uncover the value in data? Shift your career over to the Data Engineer role at Dupaco! As a Data Engineer, you'll collaborate with various departments to accomplish strategic goals by creating automated processes and pipelines by extracting data from source systems and migrating it to our enterprise data warehouse or other end points!
As an employee at Dupaco, you’ll be part of an inclusive team that believes that by working together we allow our members to build a life worth loving. Pair that with the training opportunities, career growth potential, attractive compensation, amazing benefit package, and the ability to have a voice in the success of the organization, you’ll have a career worth loving!
You’ll be:
Extracting data from source systems, translating the raw data into analysis-ready datasets then migrating it to our enterprise data warehouse or other end points
Developing automated data pipelines that take raw data from disparate sources and model it into formats that are useful for operations and reporting
Utilizing a variety of programing languages and tools (i.e. Java, SQL, Talend, Python) to marry systems together
Researching opportunities for data acquisition and new uses for existing data
Implement data processing flows that adhere to data security requirements
Recommending ways to improve data reliability, efficiency and quality
Constructing, installing, testing and maintaining highly scalable data management systems
Collaborating with Data Team to accomplish various data sprint projects
Explaining complex technical concepts to other stakeholders in a way that is easy for them to understand
Providing quality service to members, potential members and coworkers
Demonstrating teamwork and professionalism in all interactions with coworkers
Performing other duties as assigned
You’ll need:
An excitement for working with large volumes of data
Bachelor’s degree in Computer Science or Computer Information Systems or equivalent experience
Proficient experience with programming languages (i.e. Java, Python, C#, etc.)
Familiarity with ETL (Extract, Transform, Load) tools (i.e. Talend, Informatica, SSIS, or DataStage), strongly preferred
Experience working with SQL queries
Ability to demonstrate complex problem solving and strong decision-making skills
Strong interpersonal and communication (verbal & written) skills
To be self-motivated, resourceful and well organized with the ability to prioritize work assignments efficiently
Ability to accurately handle large volumes of details and effectively manage multiple projects simultaneously
Ability to promote a professional image of the credit union at all times
This role does not allow for remote work outside of Dupaco's geographic branch network (Iowa, Wisconsin, and Illinois) and the selected candidate must reside or relocate to this area.
Qualifications
Skills
Behaviors
:
Motivations
Required
Ability to Make an Impact: Inspired to perform well by the ability to contribute to the success of a project or the organization
:
Education
Required
Bachelors or better in Computer Science or related field.
Experience
Required
Experience working with SQL queries
Proficient experience working with programming languages (i.e. Java, Python, C#, etc.)
Preferred
Familiarity with ETL (Extract, Transform, Load) tools (i.e. Talend, Informatica, SSIS, or DataStage), strongly preferred
Licenses & Certifications
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)",87014,501 to 1000 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1948,$1 to $5 billion (USD),IA,75,data engineer,na,"['sql', 'java', 'python']",[],['ssis'],[],[],[],bachelor,
"Intelliswift Software, Inc.",4.2,"Menlo Park, CA",Data Engineer III,"Job Title: Data Engineer III
Duration: Longterm Contract
Location: Menlo Park, CA
Pay Range: $80-$90/hr
Intelliswift Software Inc. conceptualizes, builds, and supports the world's most amazing technology products and solutions. Our team of rich experts from diverse backgrounds contributes to making Intelliswift one of the most reliable partners in IT and Talent solutions. We specialize in delivering world-class Digital Product Engineering, Data Management and Analytics, and Staffing Solutions services to Fortune companies, SMBs, ISVs, and fast-growing startups.

Job Description: Onsite at Menlo Park location only.
Summary:
The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization’s data assets.

Job Responsibilities:
Design, construct, install, test and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate and maintain large scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models and proof of concepts.

Skills:
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.

Education/Experience:
Bachelor's degree in a technical field such as computer science, computer engineering or related field required.

Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI.",153000,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2001,$100 to $500 million (USD),CA,22,data engineer,na,[],[],[],[],[],[],bachelor,
LTIMindtree,3.8,"Cincinnati, OH",GCP Data Engineer - Contractor,"Job Details:

GCP Data Architecture and designing streaming & Batch pipelines.
Implementing Big Data solutions leveraging GCP cloud data technologies with extensive experience in ingestion, processing, and transformation.
Proficient in Python, PySpark, relational and NoSQL databases and GCP data technologies such BigQuery, Dataproc, Dataflow, Data Fusion, Cloud Composer, PubSub, DataPrep, Dataplex
Cloud databases: Spanner, Cloud SQL, Memory store, BigQuery.
Looker studio & Operations suite (Cloud monitoring and Logging).
Excellent relationship management skills and leadership skills.

Good to have:
Experience on open-source or commercial Modern Data Stack tools such as Airbyte, Fivetran, dbt, Monte Carlo, CDAP, etc. is an ad added advantage
Flask FastAPI Development
Google certified Data Engineer
Role Description:
Hands-on experience architecting, designing and implementing Big Data solutions leveraging GCP cloud data technologies; extensive experience in the area of ingestion, processing and transformation
Proficient in Python, PySpark, relational and NoSQL databases and GCP data technologies such BigQuery, Dataproc, Dataflow, Data Fusion, Cloud Composer, PubSub, DataPrep, Dataplex
Experience on open-source or commercial Modern Data Stack tools such as Airbyte, Fivetran, dbt, Monte Carlo, CDAP, etc. is an ad added advantage.
Good Analytics skill is needed on issue identification and resolution.
Experience in distributed data processing, performance tuning
Experience in maintenance & enhancement projects.
Complete ownership of the tasks and deliverable
Good communication skills.
Flexibility to support customer across the Geos and time zones.
Ensures consistency of process and usage, and champions best practices in data management Oversees data accuracy processes, goals and assessment.
Ensures resolution of data conflicts between systems and within systems’ data universe.
Works with internal stakeholders to develop strategies for leveraging data to gain a deeper insight into IIE’s business, impact on international education, and IIE’s story.
Ensures rigorous adherence to IIE policies regarding PII protection and data security. Oversees data expiry practices and establishes and implements processes for data archival and expiry.
Manages and develops the staffing of the Data & Reporting unit; champions team collaboration; monitors and develops career paths within the unit. Make recommendations concerning employment, termination, performance evaluations, salary actions, and other personnel actions.
Responsible for user experience with data and reporting tools.
Member of the Knowledge Management Cabinet.
Excellent relationship management skills
Familiarity working and collaborating with teams from various geographies culture and time zones",92426,10000+ Employees,Company - Private,Information Technology,Information Technology Support Services,1997,Unknown / Non-Applicable,OH,26,data engineer,na,"['sql', 'nosql', 'python']",[],['looker'],['dbt'],[],[],,
Zllius Inc.,4.0,"North Chicago, IL",Data Engineer,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the role Data Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Duration: Long-Term
Position Type: W2/ 1099
Visa: Any
Job Description:
Technical/Functional Skills
Successful candidates will have:
Hands on experience in Dremio and Scripting
Hands on Scripting experience in Python and Flask
Hands on experience in Integrating data from multiple source systems (PostgreSQL and CSV)
Experience with Apache Superset
Ability to write complex SQL queries
Experience in building data pipelines using PostgreSQL, Oracle database, Dremio, AWS Open Search
Experience with Docker Containerization of Code
Work as a part of a scrum team in Agile methodology
ETL, Data Pipelines, PostgreSQL, SQL Queries, Scripting
Thanks & Regards
Zllius Inc.
Job Types: Full-time, Contract
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
North Chicago, IL 60064: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",100218,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,"['sql', 'python']",['aws'],[],"['postgresql', 'oracle']",[],['docker'],,0-2 years
Optimal Inc.,3.6,"Dearborn, MI",Data Engineer - Google cloud platform,"Position Description:
As a Data Engineer you will be responsible for providing data support for enterprise data management tasks, standardization, enrichment, mastering and assembly of data products for downstream applications in Google Cloud Platform. Provide visibility to Data Quality issues and work with the business owners to fix the issues. Implement an Enterprise Data Governance model and actively promote the concept of data standardization, integration, fusion, and quality. Support the data requirements of the different functional teams like MS&S, PD, Quality, etc. and all the regional KPI / Metrics initiatives. Continuously increase Data Coverage by working closely with stakeholders and Data Scientists, understanding, and evaluating their data requirements to create meaningful, organized, and structured ""information""
Skills Required:
Candidates should have experience with using data analysis tools such as: Hive, PySpark, SQL, Python and ETL tools.
Experience of working within a complex business environment, including at least a year in a single function, with deep understanding of the information constructs of that business.
Demonstrated experience and expertise in conceptual thinking of how to apply information solutions to a business challenge.
Experience of applying problem solving capabilities. Proven capability to robustly examine large data sets and highlight patterns, anomalies, relationships, and trends.
Self-starter, demonstrating high levels of data integrity. Ability to manage deliverables according to a robust project plan.
Experience in Google Cloud Platform or other cloud platform is a plus.
Experience Required:
Minimum of a year experience in Google Cloud Platform Minimum of a year of experience in a Data Engineering role creating data products, writing codes/queries/scripts, and building data visualizations.
Minimum of a year of experience in data design, data architecture and data modeling (both transactional and analytic).
Experience in Google Cloud Platform or other cloud platform is a plus.
Education Required:
Bachelor's degree in computer science or related field from an accredited college or university
Prefer Master's degree in computer science or related field from an accredited college or university",103365,1 to 50 Employees,Nonprofit Organization,Education,Education & Training Services,2004,Unknown / Non-Applicable,MI,19,data engineer,na,"['sql', 'python']",['google cloud'],[],['hive'],[],[],bachelor,
grow.com,4.1,"Albany, NY",Azure Data Engineer,"General information
Office (s)
, Albany, NY, Atlanta, GA, Austin, TX, Cincinnati, OH, Cleveland / Akron, OH, Minneapolis, MN, Philadelphia, PA, Salt Lake City, UT, San Diego, CA, San Francisco Bay Area, CA
Date Published
Friday, June 2, 2023
Country
United States
Job ID
22303
Function
Finance
Salary Range
70,000 - 180,000
Description & Requirements
Azure Data Engineer
Job Description
As a member of the Epicor Cloud Data team, you are joining a team that invests in your success by providing comprehensive learning and mentorship programs. You will be the Principal engineer driving innovation and success of our next-generation Microsoft Azure-based Data Lake/DataMart and Reporting platforms. You will be innovating and mentoring the rest of the engineering org as we grow our teams.
What You'll Do
Create and maintain data pipeline architecture on the Azure platform. Ensure that system designs adhere to solution architecture design and are traceable to functional and non-functional requirements.
Leverage Azure Data Factory and Databricks to assemble large, complex data sets. Design new solutions and services to improve overall user experience.
Identify, design, and implement internal process improvements such as automating manual processes and optimizing data delivery. Define system design standards to improve and sustain standardization.
Build the infrastructure required for optimal data extraction, transformation, and loading from a wide variety of data sources.
Design relational and non-relational data stores on Azure.
Assist Manager-Data Administration in leading coordination with other staff to ensure data handling meets organizational objectives for data quality, business process management, and risk management.
Provide technical mentoring to team members
Perform all duties and maintain all standards in accordance with company policies, procedures, and internal controls such as SOX.
What You Bring:
4 years of experience working as part of an IT Data team using Azure-based technologies to solve business problems and build solutions.
Azure PaaS, Data analytics, Data warehousing, and Data science.
Azure Synapse Analytics, SQL Pool
Azure Spark, ADF, T-SQL Scripting, Stored Procs, Data bricks, Python
Microsoft related certifications
Experience with visualization tools such as Tableau (preferred)
The Team:
We put a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. Most of our engineers have been with us for 5+ years growing in their respective roles. We believe striking the right balance between your personal and professional life is critical to life-long happiness and fulfillment while offering flexibility in working hours and encouraging you to find your own balance.
Why Epicor?

At Epicor, we know that success comes from working together. Everyone has a role to play, and it’s the essential partnerships across our company that are crucial to our customer’s success and our growth as a business.

We’re truly a team. Working in close partnership, we bring wide-ranging talents together in powerful collaborations. We think innovatively, share our knowledge generously, and constantly learn from our colleagues. We’re proud of the success we achieve every day, but we never stop challenging ourselves and encouraging each other. Together, we go further and imagine an even brighter future.

Whatever your career journey, we’ll help you find the right path. Through our training courses, mentorship, and continuous support, you’ll get everything you need to thrive. At Epicor, your success is our success. And that success really matters, because we’re the essential partners for the world’s most essential businesses—the hardworking companies who make, move, and sell the things the world needs.
Equal Opportunities and Accommodations Statement
At Epicor, we strive to create a welcoming, inclusive, and diverse workplace every day. Bring the whole and real you—that’s who we’re interested in. If you’re interested in this role but your experience and current skillset don’t match every qualification of the job description, that’s okay! We encourage you to apply anyway. Learning and sharing our knowledge keeps us all moving forward. You just might be the right fit and gain new skills new along the way.
#LI-MB2 #LI-Remote",125000,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,2014,Unknown / Non-Applicable,NY,9,data engineer,na,"['sql', 'go', 'python']","['databricks', 'azure']",['tableau'],[],['spark'],[],,+10 years
Acuity Brands,3.6,"Conyers, GA",Data Engineer Senior,"We Light the Way!

Acuity Brands, Inc. (NYSE: AYI) is a market-leading industrial technology company. We use technology to solve problems in spaces and light. Through our two business segments, Acuity Brands Lighting and Lighting Controls (“ABL”) and the Intelligent Spaces Group (“ISG”), we design, manufacture, and bring to market products and services that make the world more brilliant, productive, and connected. We achieve growth through the development of innovative new products and services, including lighting, lighting controls, building management systems, and location-aware applications.
Job Summary
Acuity’s Business Intelligence team is comprised of people who are passionate about data. We believe accurate, timely and understandable data is vital to a data driven culture. We are devoted to aligning Acuity’s data to serve the needs of the enterprise and its customers. You will be joining a team of seasoned Business Intelligence professionals well versed in architecting bespoke BI (Business Intelligence) applications and implementing Microsoft Azure BI products.
We are seeking a talented and enthusiastic individual to be a Senor Data Engineer on our Business Intelligence Team as we transform Acuity Brands analytics and migrate our BI platform to Azure and Power BI. This position will work closely with BI Architects to bring to deliver end to end BI solutions that are innovative, scalable, and responsive.
Key Tasks & Responsibilities (Essential Functions)
Research, architect, drive, and deploy scalable, resilient cloud agnostic BI solutions to address Acuity current and future business needs and obligations
Partner with Data Architect, Solution Architect, and BI Product Managers to drive technology transformation on the BI platform to ensure the BI platform remains current and responsive
Mentor and guide Senior BI Developers to ensure adherence to BI standards and procedures
Partner with Data Architect to deliver integrated end to end data engineering solutions
Write application and cloud-based data processing code to transform inbound data to meet business requirements
Design and develop data models leveraging advanced modeling techniques to handle large and or complex data
Modify and optimize data engineering processes to handle ever-growing, complex, diverse data formats, sources, and pipelines
Work with infrastructure partners to tune and optimize code and BI resources such as but not limited to (index tuning, partitioning, caching, buffer tuning, and data archiving strategies.
Proactively estimate and plan development work and track performance to deliver work on schedule
Create and maintain current documentation in GIT (c4 and Plant UML)
Education (minimum education required)
Bachelor of Science
Preferred Education (i.e. type of degree)
Bachelor of Science in Computer Science or Information Systems
Experience (minimum experience required)
Bachelor’s Degree in Computer Science, MIS (Management Information System), or other technical/analytical field (or equivalent experience)
2 Azure Certifications
Working knowledge of data warehousing principles (Kimball, Inmon, Hybrid)
4-7 years or more database programming experience (SQL (preferred), Oracle, DB2)
4-7 years or more of BI experience (Power BI, Tableau, Qlik Sense/View, D3.js, SAP Business Objects, IBM Cognos)
4-7 years or more of working with Microsoft BI stack (SSIS, SSAS, T-SQL)
4-7 years or more of developing and enhancing ETL packages
4-7 years or more of working with and or constructing API (Push, Get, Post)
4-7 years or more of advanced experience identifying and optimizing database objects
2 years or more of application development (C# or .NET)
3 years or more experience working in Python or Scala
We invite you to apply today to join us as We Light the Way to a Brilliant, Productive, and Connected World!

We value diversity and are an equal opportunity employer. All qualified applicants will be considered for employment without regards to race, color, age, gender, sexual orientation, gender identity and expression, ethnicity or national origin, disability, pregnancy, religion, covered veteran status, protected genetic information, or any other characteristic protected by law.
Please click here and here for more information.

Accommodation for Applicants with Disabilities: As an equal opportunity employer, Acuity Brands is committed to providing reasonable accommodations in its application process for qualified individuals with disabilities and disabled veterans. If you have difficulty using our online system due to a disability and need an accommodation, you may contact us at (770) 922-9000. Please clearly indicate what type of accommodation you are requesting and for what requisition.

Any unsolicited resumes sent to Acuity Brands from a third party, such as an Agency recruiter, including unsolicited resumes sent to an Acuity Brands mailing address, fax machine or email address, directly to Acuity Brands employees, or to Acuity Brands resume database will be considered Acuity Brands property. Acuity Brands will NOT pay a fee for any placement resulting from the receipt of an unsolicited resume.

Acuity Brands will consider any candidate for whom an Agency has submitted an unsolicited resume to have been referred by the Agency free of any charges or fees. This includes any Agency that is an approved/engaged vendor, but does not have the appropriate approvals to be engaged on a search.

E-Verify Participation Poster
e-verify.gov
eeoc.gov",92338,10000+ Employees,Company - Public,Manufacturing,Electronics Manufacturing,2001,$1 to $5 billion (USD),GA,22,data engineer,senior,"['sql', 'scala', 'python']",['azure'],"['qlik', 'tableau', 'ssis', 'sap', 'power bi']",['oracle'],[],[],bachelor,2-5 years
Iron Systems,3.4,"Menlo Park, CA",Data Engineer III,"Date Posted:
6/1/2023

Job Function:
Software Development

Location:
Menlo Park CA - USA

Offered Salary:
USD 68 Hourly


Iron Systems is an innovative, customer-focused provider of custom-built computing infrastructure platforms such as network servers, storage, OEM/ODM appliances & embedded systems. For more than 15 years, customers have trusted us for our innovative problem-solving combined with holistic design, engineering, manufacturing, logistic, and global support services.

Job Title: Data Engineer III
Location: US - CA - Menlo Park

Summary:
The main function of the Data Engineer is to develop, evaluate, test, and maintain architectures and data solutions within our organization.
The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization’s data assets.
Job Responsibilities:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate, and maintain large-scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models, and proof of concepts.
Skills:
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers, and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint.
Education/Experience:
Bachelor's degree in a technical field such as computer science, computer engineering, or related field required.
Process certification, such as Six Sigma, CBPP, BPM, ISO 20000, ITIL, and CMMI.",122400,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1987,$25 to $100 million (USD),CA,36,data engineer,na,[],[],[],[],[],[],bachelor,
"Calhoun International, LLC",3.7,"Washington, DC",Data Engineer (Senior),"About Us:
Calhoun International is a Professional Services company providing innovative solutions to our clients. Our expertise ranges from strategic intelligence analysis, expert instruction on intelligence analysis and sensors, cyberspace operations, information systems training, and knowledge management services among others. Calhoun International is located in Tampa, FL with employees in Florida, Virginia, Maryland, Washington, D.C. and overseas. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.
Responsibilities:
Executes and on occasion, leads research, analysis, and evaluation efforts as well as studies, analyses, assessments, and technical reports, which may include the use of existing documents, databases, models, architectures and simulations. Deliverables produced shall be in a variety of formats in response to a wide range of requirements and delivery schedules. Provides mid to high-level analytical assessments and advice on complex issues, which require extensive knowledge of the subject matter. May attend various types of symposia and meetings at the ARSTAF and DOD level.
Requirements:
Minimum Education: bachelor’s degree; advanced intelligence discipline training; or other equivalent DoD or service Intelligence experience.
Minimum Experience: Fourteen (14) years of experience as an Army Intelligence analyst with experience from tactical to strategic. Experience shall have been within Two (2) years of starting on this contract.
Has served as a staff action officer at the HQDA (DCS, G-2 preferred) or Joint or a closely related DOD organization/agency.
Demonstrated SME level of knowledge of intelligence fusion systems, capabilities / employment, training, associated R&D efforts and program budget processes.
Demonstrated knowledge of Joint and Army processes –JCIDS, TAA, PED, JUONS/ONS, and CONOPS.
Experience in preparation and presentation of briefings on projects, studies and analysis to senior leaders and other ARSTAF action officers.
Desired:
Graduate from the Command and General Staff College or similar Senior Staff College
Security Clearance:
Active Top Secret with SCI eligibility required.",127546,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,$1 to $5 million (USD),DC,-1,data engineer,senior,['r'],[],[],[],[],[],bachelor,
Gridiron IT,4.5,Remote,Sr. Data Engineer,"GridironIT is seeking a Data Engineer.
Responsibilities:
We are looking for seasoned Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for a more than just a ""Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving.
Lead and architect migration of data environments with performance and reliability.
Assess and understand the ETL jobs, workflows, BI tools, and reports
Address technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data products
Experience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).
Key must have skill sets – Python, AWS
Support an Agile software development lifecycle
You will contribute to the growth of our Data Exploitation Practice!
Qualifications:
US Citizen Only
Ability to hold a position of public trust with the US government.
8+ years industry experience coding commercial software and a passion for solving complex problems.
8+ years direct experience in Data Engineering with experience in tools such as:Big data tools: Hadoop, Spark, Kafka, etc.
Relational SQL and NoSQL databases, including Postgres and Cassandra
Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
AWS cloud services: EC2, EMR, RDS, Redshift
Data streaming systems: Storm, Spark-Streaming, etc.
Search tools: Solr, Lucene, Elasticsearch
Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Amazon S3, Athena, Redshift Spectrum, AWS Glue, AWS Glue Catalog, AWS Functions, and Amazon EC2 with SQL Server Developer
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.
Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience manipulating, processing, and extracting value from large, disconnected datasets.
Experience manipulating structured and unstructured data for analysis
Experience constructing complex queries to analyze results using databases or in a data processing development environment
Experience with data modeling tools and process
Experience architecting data systems (transactional and warehouses)
Experience aggregating results and/or compiling information for reporting from multiple datasets
Experience working in an Agile environment
Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Experience with Informix and Data Stage
Job Type: Full-time
Pay: $84,436.60 - $150,799.40 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Monday to Friday
Experience:
AWS Glue: 5 years (Required)
SQL: 8 years (Required)
Data warehouse: 5 years (Required)
Work Location: Remote",117618,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2017,Unknown / Non-Applicable,Remote,6,data engineer,senior,"['python', 'scala', 'java', 'sql', 'nosql']","['azure', 'aws', 'redshift']",[],"['sql server', 'elasticsearch']","['kafka', 'spark', 'hadoop']",[],,5-10 years
Procore Technologies,4.5,Oregon,Staff Data Engineer,"Job Description

What if you could use your technology skills to develop a product that impacts the way communities' hospitals, homes, sports stadiums, and schools across the world are built? Construction impacts the lives of nearly everyone in the world, and yet it's also one of the world's least digitized industries, not to mention one of the most dangerous. That's why we're looking for a talented Staff Data Engineer to join Procore's journey to revolutionize a historically underserved industry.
As a Staff Data Engineer, you'll design and develop data products for Procore Data Platform data management area. You'll be part of the high-performance team of Data Engineers and will collaborate with platform engineers and product leaders.
This position will report to our Senior Manager of Data Engineering, and can be based remotely from any US location. We're looking for someone to join our team immediately.
What you'll do:
Lead the design and development of big data predictive analytics using object-oriented analysis, design and programming skills, and design patterns
Implement ETL workflows for data matching, data cleansing, data integration, and management
Maintain existing data pipelines and develop new data pipelines using big data technologies
Develop and maintain tables and data models in SQL, abstracting multiple sources and historical data across varied schemas to a format suitable for further analysis
Responsible for leading the effort to continuously improve the reliability, scalability, and stability of the enterprise data platform
Contribute to and lead the continuous improvement of the software development framework and processes by collaborating with Quality Assurance engineers
Deliver observable, reliable, and secure software, embracing the ""you build it, you run it"" mentality, focusing on automation and GitOps
Participate in daily standups, team meetings, sprint planning, and demo/retrospectives while working cross-functionality with other teams to drive the innovation of our products
Apply data governance framework, including the management of data, data compliance operating model, data policies, and standards
What we're looking for:
BS degree in Computer Science, a similar technical field of study, or equivalent practical experience; MS or Ph.D. degree in Computer Science or a related field is preferred
5+ years of experience in a Data Engineering position
Strong expertise with 3+ years of experience building enterprise techniques for large-scale distributed system design and data processing, including:
Building data pipelines with Databricks as the source
Building and maintaining data warehouses in support of BI tools (Snowflake, dbt, Tableau)
Building data pipeline framework for data workflow to process large data sets and Real-Time & Batch Data Pipeline development
Experience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metrics providers ranging from advertising, web analytics, and consumer devices
Desire to be actively hands-on with code, using Java, Python (80%), and SQL, along with willingness and passion for mentoring junior engineers and performing code reviews
Possess familiarity with AWS-managed services for data (Glue, Athena, Data Pipeline, Flink, Spark) and Snowflake

Additional Information

Base Pay Range $147,200-$202,400. Eligible for Bonus Incentive Compensation. Eligible for Equity Compensation. Procore is committed to offering competitive, fair, and commensurate compensation, and has provided an estimated pay range for this role. Actual compensation will be based on a candidate’s job-related skills, experience, education or training, and location.
Perks & Benefits
At Procore, we invest in our employees and provide a full range of benefits and perks to help you grow and thrive. From generous paid time off and healthcare coverage to career enrichment and development programs, learn more details about what we offer and how we empower you to be your best.
About Us
Procore Technologies is building the software that builds the world. We provide cloud-based construction management software that helps clients more efficiently build skyscrapers, hospitals, retail centers, airports, housing complexes, and more. At Procore, we have worked hard to create and maintain a culture where you can own your work and are encouraged and given resources to try new ideas. Check us out on Glassdoor to see what others are saying about working at Procore.
We are an equal-opportunity employer and welcome builders of all backgrounds. We thrive in a diverse, dynamic, and inclusive environment. We do not tolerate discrimination against employees on the basis of age, color, disability, gender, gender identity or expression, marital status, national origin, political affiliation, race, religion, sexual orientation, veteran status, or any other classification protected by law.
If you'd like to stay in touch and be the first to hear about new roles at Procore, join our Talent Community.",174800,1001 to 5000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2002,Unknown / Non-Applicable,Oregon,21,data engineer,na,"['sql', 'java', 'python']","['databricks', 'snowflake', 'aws']",['tableau'],"['snowflake', 'dbt']","['flink', 'spark']",[],,+10 years
"JPMorgan Chase Bank, N.A.",3.8,"Plano, TX",Senior Lead Data Engineer,"Embrace this pivotal role as an essential member of a high performing team dedicated to reaching new heights in data engineering. Your contributions will be instrumental in shaping the future of one of the world's largest and most influential companies.

As a Senior Lead Data Engineer at JPMorgan Chase- Global Technology Strategy team, you are an integral part of an agile team that works to enhance, build, and deliver data collection, storage, access, and analytics in a secure, stable, and scalable way. Leverage your deep technical expertise and problem solving capabilities to drive significant business impact and tackle a diverse array of challenges that span multiple data pipelines, data architectures, and other data consumers.

Job Responsibilities
Provides recommendationsandinsight on data managementandgovernance proceduresandintricacies applicable totheacquisition,maintenance,validation, andutilization of data
Designs and delivers trusted data collection, storage, access, and analytics data platform solutions in a secure, stable, and scalable way.
Defines database back-up,recovery, and archivingstrategy
Generates advanced data models for one or more teams using firmwide tooling, linear algebra, statistical and geometrical algorithms
Approves data analysis toolsandprocesses and creates functional andtechnicaldocumentationsupporting bestpractices
Leads and mentors'datavisualization analysts ininformationpresentation anddelivery and adds to team culture of diversity, equity, inclusion, and respect
Evaluates and reports onaccesscontrol processes todetermineeffectiveness of dataasset security

Required Qualifications, Skills, and Capabilities
Working experience with bothrelational and NoSQL databases
Advanced understanding of database back-up, recovery, and archiving strategy
Advanced knowledge of linear algebra, statistical and geometrical algorithms
Experience presenting and delivering visual data
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Seattle,WA $156,750.00 - $190,000.00 / year",173375,10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD),TX,224,data engineer,senior,['nosql'],[],[],[],[],[],,0-2 years
MeridianLink,3.8,United States,Data Engineer - 1483,"JOB SUMMARY
We are looking for an accomplished Data Engineer to join our quickly growing Analytics team. The hire will be responsible for expanding and improving our data and data pipeline architecture, as well as optimizing data flow and MDM for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data pipelines to support our next generation of products and data initiatives.
RESPONSIBILITIES
Design, develop, and operate large scale data pipelines to support internal and external consumers
Improve and automate internal processes
Integrate data sources to meet business requirements
Write robust, maintainable, well documented code
QUALIFICATIONS
2-4 years professional Data Engineering and Data warehousing experience
Extremely strong implementation experience in
Python, Spark, Azure Databricks, Delta Lake, and Databricks Data Warehouse.
SQL development knowledge – Stored procedures, triggers, jobs, indexes, partitioning etc.
Be able to write/debug complex SQL queries
Azure Data factory or Azure Synapse Analytics
ETL/ELT and Data-warehousing techniques and best practices
Experience with MS-SQL server and Databricks Data warehouse.
Experience building, maintaining, and scaling ETL/ELT processes and infrastructure
Knowledge of being able to work with a variety of Ingestion patterns such as API/SQL servers etc.
Experience with cloud infrastructure (Azure strongly preferred)
Knowledge of Master Data Management
Implementation experience with various data modelling techniques
Implementation experience working with a BI visualization tool (Sisense is a plus)
Experience with CI/CD tools (Preferred Gitlab, Jenkins)
Pluses for experience with
oUI development frameworks such as java script, Django, REACT etc.
Experience working in a fast-paced product environment, with an attitude of getting the job done with the least amount of tech debt
Prior Financial industry experience a plus.
Be able to navigate ambiguity and pivot based on business priorities with ease.
Strong communication, negotiating and estimating skills.
Be a team player and should be able to collaborate well.
Enables entrepreneurs and consumers to achieve the American dream by creating technological solutions that fuel the engine for financial growth. Our top-notch solutions create the premier customer experience every time. We believe in the principles of empowerment, collaboration, individual achievement, and innovation.
At MeridianLink, we work together to design, implement, test, and deliver state-of-the-art web applications for the financial services industry, using the latest technologies including cloud computing, mobile development, responsive design, ASP.NET, JavaScript, C#, VB.NET, and SQL Server.
OUR CULTURE
Our low turnover is a testament to our wonderful culture where people value the work they do and appreciate each other for their contributions. MeridianLink develops our employees so they can grow professionally by preferring to promote from within. We have an open door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments.
MeridianLink is an Equal Opportunity Employer. We do not discriminate on the basis of race, religion, color, sex, age, national origin, disability or any other characteristic protected by applicable law.
MeridianLink runs a comprehensive background check, credit check and drug test as part of our offer process.

MeridianLink has a wonderful culture where people value the work they do and appreciate each other for their contributions. We develop our employees so they can grow professionally by preferring to promote from within. We have an open-door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments.
MeridianLink is an Equal Opportunity Employer. We do not discriminate based on race, religion, color, sex, age, national origin, disability, or any other characteristic protected by applicable law.
MeridianLink runs a comprehensive background check, credit check, and drug test as part of our offer process.
Salary range of $94,500-$133,400. [It is not typical for offers to be made at or near the top of the range.] The actual salary will be determined based on experience and other job-related factors permitted by law.
Meridianlink offers:

Potential For Equity-Based Awards
Insurance coverage (medical, dental, vision, life, and disability)
Flexible paid time off
Paid holidays
401(k) plan with company match
Remote work
All compensation and benefits are subject to the terms and conditions of the underlying plans or programs, as applicable and as may be amended, terminated, or superseded from time to time.
#LI-REMOTE",113950,501 to 1000 Employees,Company - Public,Information Technology,Computer Hardware Development,1998,Unknown / Non-Applicable,TX,25,data engineer,na,"['sql', 'java', 'python']","['databricks', 'azure']",[],['sql server'],['spark'],['gitlab'],master,2-5 years
Dropbox,4.6,Remote,"Senior Data Engineer, New Initiatives","Company Description

Dropbox is a special place where we are all seeking to fulfill our mission to design a more enlightened way of working. We’re looking for innovative talent to join us on our journey. The words shared by our founders at the start of Dropbox still ring true today.

Wouldn’t it be great if our working environment—and the tools we use—were designed with people’s actual needs in mind? Imagine if every minute at work were well spent—if we could focus and spend our time on the things that matter. This is possible, and Dropbox is connecting the dots.

The nearly 3,000 Dropboxers around the world have helped make Dropbox a living workspace - the place where people come together and their ideas come to life. Our 700+ million global users have been some of our best salespeople, and they have helped us acquire customers with incredible efficiency. As a result, we reached a billion dollar revenue run rate faster than any software-as-a-service company in history.

Dropbox is making the dream of a fulfilling and seamless work life a reality. We hope you’ll join us on the journey.

Team Description

Our Engineering team is working to simplify the way people work together. They’re building a family of products that handle over a billion files a day for people around the world. With our broad mission and massive scale, there are countless opportunities to make an impact.

Role Description

In this role you will build very large, scalable platforms using cutting edge data technologies. This is not a “maintain existing platform” or “make minor tweaks to current code base” kind of role. We are effectively building from the ground up and plan to leverage the most recent Big Data technologies. If you enjoy building new things without being constrained by technical debt, this is the job for you!
Responsibilities

Help define company data assets (data model), spark, sparkSQL and hiveSQL jobs to populate data models
Help define and design data integrations, data quality frameworks and design and evaluate open source/vendor tools for data lineage
Work closely with Dropbox business units and engineering teams to develop strategy for long term Data Platform architecture to be efficient, reliable and scalable
Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems
Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve
Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way
Define and manage SLA for all data sets in allocated areas of ownership
Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership
Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains
Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources
Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts
Requirements

Startup mentality with strong ownership to solve 0-1 problems with minimal guidance and being comfortable with ambiguities
Excellent product strategic thinking and communications to influence product and cross-functional teams by identifying the data opportunities to drive impact
BS degree in Computer Science or related technical field involving coding (e.g., physics or mathematics), or equivalent technical experience
5+ years of Python or Java, C++, Scale development experience
7+ years of SQL experience (No-SQL experience is a plus)
5+ years of experience with schema design and dimensional data modeling
Proven ability in regards to managing and communicating data warehouse plans to internal clients.
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or a MPP system on any size/scale
High tech experiences are preferred

Total Rewards

Our Engineering Career Framework is viewable by anyone outside the company and describes what’s expected for our engineers at each of our career levels. Check out our blog post on this topic and more here .

For candidates hired in San Francisco metro, New York City metro, or Seattle metro, the expected salary/On-Target Earnings (OTE) range for the role is currently $178,500 - $210,000 - $241,500.

For candidates hired in the following locations: Austin (TX) metro, Chicago metro, California (outside SF metro), Colorado, Connecticut (outside NYC metro), Delaware, Massachusetts, New Hampshire, New York (outside NYC metro), Oregon, Pennsylvania (outside NYC or DC metro), Washington (outside Seattle metro) and Washington DC metro, the expected salary/On-Target Earnings (OTE) range for the role is currently $160,700 - $189,000 - $217,400.

For candidates hired in all other US locations, the expected salary/On-Target Earnings (OTE) range for this role is currently $142,800 - $168,000 - $193,200.

Range(s) is subject to change. Dropbox takes a number of factors into account when determining individual starting pay, including job and level they are hired into, location/metropolitan area, skillset, and peer compensation. Dropbox uses the zip code of an employee’s remote work location to determine which metropolitan pay range we use.

Salary/OTE is just one component of Dropbox’s total rewards package. All regular employees are also eligible for the corporate bonus program or a sales incentive (target included in OTE) as well as stock in the form of Restricted Stock Units (RSUs).

Dropbox is committed to investing in the holistic health and wellbeing of all Dropboxers and their families. Our benefits and perks programs include, but are not limited to:

Competitive medical, dental and vision coverage

(US Only) Competitive 401(k) Plan with a generous company match and immediate vesting

Flexible Time Off/Paid Time Off, paid holidays, 11 Company-wide PTO days, Volunteer time off and more

Protection Plans including; Life Insurance, Disability Insurance and Travel benefit plans

Perks Allowance to be used on what matters most to you, whether that’s wellness, learning and development, food & groceries, and much more

Parental benefits including; Parental Leave, Child and Adult Care, Day Care FSA (US Only), Fertility Benefits (US Only), Adoption and Surrogacy support and Lactation Support

Mental Health and Wellness benefits Free Dropbox space for your friends and family

Additional benefits details are available upon request.

Benefits

Dropbox is committed to investing in the holistic health and wellbeing of all Dropboxers and their families. Our benefits and perks programs include, but are not limited to:

Competitive medical, dental and vision coverage*

Retirement Savings through a defined contribution pension or savings plan**

Dropbox provides a Flexible PTO Policy in addition to your statutory holidays allowing you to unplug, unwind, and refresh

Dropbox also provides exclusive additional paid time off for all FTE employees across the Globe, in addition to any relevant statutory holidays

Protection Plans including Life and Disability Insurance*

A Perks Allowance to be used on what matters most to you, whether that’s wellness, learning and development, food & groceries, and much more

Parental benefits including; Parental Leave, Fertility Benefits, Adoptions and Surrogacy support, and Lactation support

Additional benefits details are available upon request.

Where group plans are not available, allowances are provided
**Benefit, amount, and type are dependent on geographical location, based upon applicable law or company policy

Dropbox is an equal opportunity employer. We are a welcoming place for everyone, and we do our best to make sure all people feel supported and connected at work. A big part of that effort is our support for members and allies of internal groups like Asians at Dropbox, BlackDropboxers, Latinx, Pridebox (LGBTQ), Vets at Dropbox, Women at Dropbox, ATX Diversity (based in Austin, Texas) and the Dropbox Empowerment Network (based in Dublin, Ireland).",112889,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,2007,Unknown / Non-Applicable,Remote,16,data engineer,senior,"['sql', 'java', 'python']",[],[],[],['spark'],[],,+10 years
Suprha Svc LLC,4.0,"Horsham, PA",Lead Data Engineer,"Job Title : Lead Data Engineer
Job Location: Horsham, PA/Hybrid
Job Description
This role will be responsible for transforming extensive and complex data into consumable business capabilities.
Create system architecture, design, and specification using in-depth engineering skills and knowledge to solve complex development problems and achieve engineering goals.
Determine and source appropriate data for a given analysis.
Work with data modelers/analysts to understand the business problems they are trying to solve, then create or augment data assets to feed their analysis.
Acts as a resource and mentor for colleagues with less experience.
Lead a team of data engineers through a modernization project of moving on-premise big data implementation to cloud.
Skillset Required:
MUST have Lead experience, Data Analysis and Architecture, Spark, PySpark, Python, Sqoop, Hive, Azure, Google Dataproc, Databricks, No SQL Data Stores, Object Store, Design and Development of APIs, Kafka, Agile skills.
Must have Databricks, Snowflake, Azure/Google Dataproc experience.
Your expertise is deep and broad; you’re hands-on, producing both detailed technical work and high-level architectural designs.
8+ years of recent hands-on in an object-oriented language (Java, Scala, Python).
8+ years of experience designing and building data pipelines and data-intensive applications.
Experience using Big Data frameworks (e.g., Hadoop, Spark), databases for complex data assembly and transformation.
Experience designing and implementing complex big data implementation using Databricks, Snowflake, Azure, Google Dataproc.
Experience working with Healthcare data would be preferabble.
Job Type: Full-time
Pay: $110,000.00 - $120,000.00 per year
Experience level:
8 years
Schedule:
Monday to Friday
Ability to commute/relocate:
Horsham, PA 19044: Reliably commute or planning to relocate before starting work (Required)
Experience:
Hadoop: 4 years (Required)
Adobe Spark: 4 years (Required)
Databricks: 4 years (Preferred)
Snowflake: 4 years (Required)
Python: 5 years (Required)
Work Location: One location",115000,,,,,-1,,PA,-1,data engineer,senior,"['sql', 'java', 'scala', 'python']","['databricks', 'azure', 'snowflake']",[],"['snowflake', 'hive']","['kafka', 'hadoop', 'spark']",[],,5-10 years
MyFitnessPal,4.2,United States,Data Engineer 2,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals.
We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments.
What you'll be doing:
Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives
Integrate with services and systems across the MyFitnessPal engineering teams
Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently
Support resolution of production issues across the Data Platform stack
Engage in code-reviews, working with teammates to learn and grow
Live our core values in all you do:
Be Kind and Care
Live Good Health
Be Data-Inspired
Champion Change
Leave it Better than You Found It
Make It Happen
Qualifications to be successful in this role:
2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions
Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.)
Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.)
Experience with development languages (e.g. Python, SQL, Scala, etc.)
Understanding of data modeling for analysis by business intelligence or data science teams
Experience with a variety of API design patterns, such as REST
Experience developing validation and data integrity frameworks
Triaging and debugging production data issues
Experience building high volume data pipelines for downstream analysis supporting operational indicators
Experience with data at scale
Worked alongside client teams to support integration efforts
Familiarity with AWS and/or other cloud computing platforms

Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Full Time Employee Perks, Benefits, and Culture:
Remote equal philosophy enabling you to work from any state in which we have operations in the continental U.S.
Want to work in an office? We also have a physical office in Austin, TX
Annual, in-person company retreats to work, bond, and enjoy team-building activities
Opportunities for team members to meet and connect in person for company paid lunches or working sessions
Flexible time-off policy + flexible working hours (Unlimited PTO Plan)
Competitive medical, dental, and vision benefits
Safe Harbor 401K program
Paid maternity and parental leave
Monthly Wellness Allowance to assist team members to focus on their own physical and mental wellbeing and select wellness initiatives of their own choice
Reward & recognition platform enabling peers to recognize and reward their peers for all the great work they do
MyFitnessPal Premium
Modern Virtual Learning and Development Library
DEI Committee dedicated to ongoing efforts to foster a diverse and inclusive workplace by setting actionable goals and evaluating progress
Diversity training for employees
A dynamic, motivating, and fun work environment
At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, color, religion, military or veteran status, sex, gender, gender identity or expression, sexual orientation, national origin, age, disability or genetic information. These are our guiding ideologies and apply across all aspects of employment.
MyFitnessPal participates in E-Verify.",112889,51 to 200 Employees,Company - Private,Information Technology,Internet & Web Services,2005,$100 to $500 million (USD),TX,18,data engineer,na,"['sql', 'scala', 'python']","['snowflake', 'aws']",[],"['dynamodb', 'mongodb', 'snowflake', 'mysql']",[],[],,
HCA Healthcare,3.3,"Nashville, TN",Senior Data Engineer,"Introduction
Last year our HCA Healthcare colleagues invested over 156,000 hours volunteering in our communities. As a Sr Database Admin with HCA Healthcare you can be a part of an organization that is devoted to giving back!
Benefits
HCA Healthcare offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn more about Employee Benefits
Note: Eligibility for benefits may vary by location.

Would you like to unlock your potential with a leading healthcare provider dedicated to the growth and development of our colleagues? Join the HCA Healthcare family! We will give you the tools and resources you need to succeed in our organization. We are looking for an enthusiastic Sr Database Admin to help us reach our goals. Unlock your potential!
Job Summary and Qualifications
As a Sr. Data Engineer you will have the opportunity to grow and leverage your technical skills in MPP systems like that of Teradata’s to perform physical database design, SQL tuning and support our Teradata Data Warehouse platform on-prem and in the public cloud. You will be performing various operational tasks, including, but not limited to, system monitoring, object deployment, backup and recovery, user provisioning, query optimization and system maintenance. The job requires communicating with data architecture, integration and reporting teams for adherence to best practices and process standardization.
The role requires self-starters who are proficient in problem solving and communicating complicated, technical issues with clarity to the other technology teams and stakeholders. The culture of our organization places an emphasis on teamwork, so social and interpersonal skills are equally important as technical capability.

Assist in developing automation of various operational tasks performed by a DBA.
Work with cross-functional teams to build the physical databases and provide technical guidance during all phases of the development process.
Work with team in researching, evaluating and implementing new technologies as needed.
Provide regular, clear, and consistent communication (written and oral) on the status of projects, issues, and deliverables to team leadership.
Work with vendor technical support to facilitate analysis of and resolution to technical issues.
Provide planning input to leadership – operational and tactical – in order to drive success for team and company goals.
Participate in planned system maintenance tasks.
Provide rotational on-call support
What qualifications you will need:
Bachelors Degree preferred
Five or more years of relevant work experience
Other/Special Qualifications
Teradata development and/or administration (5+ years).
Teradata certification(s).
Shell Scripting and/or python or any development experience using C/C++/Java
Exposure to Cloud technologies
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""There is so much good to do in the world and so many different ways to do it.""- Dr. Thomas Frist, Sr.
HCA Healthcare Co-Founder
Be a part of an organization that invests in you! We are reviewing applications for our Sr Database Admin opening. Qualified candidates will be contacted for interviews. Submit your application and help us raise the bar in patient care!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",115561,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1968,$10+ billion (USD),TN,55,data engineer,senior,"['sql', 'java', 'shell', 'python']",[],[],[],[],[],,+10 years
Spectrum Communications & Consulting Inc.,4.3,"Chicago, IL",Data Engineer,"What do incubators and Spectrum have in common? Well, they’re great for growth, and even better for stability. As an innovative software development and digital marketing company pioneering the field of artificial intelligence, we can offer our newest Data Engineer the best of both words – a high energy, forward-thinking start-up culture, inside of a well-established, profitable, and stable structure . if you’re interested in getting your hands dirty and inciting change into a larger organization with a vision to change the world uses data today, then please read on.
Responsibilities
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Responsibilities
Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at Spectrum. Here are just a few different responsibilities you can expect off the bat:
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with Spectrum’s and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities
Some Characteristics That Define You
We understand that as a Data Engineer for Spectrum, you have many different professional goals and personal interests. As such here are just a few different things that typically define our team members on the Data Science team:
Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.
Required Skills and Experience
On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at Spectrum. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:
A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression
Benefits
As a Data Engineer at Spectrum there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the Spectrum family:
Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects
About Us
Our mission at Spectrum has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.
INDSPCI
Hours
40",92696,51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1992,$5 to $25 million (USD),IL,31,data engineer,na,"['sql', 'python']",['azure'],[],['sql server'],[],[],bachelor,2-5 years
CNA Insurance,3.9,"Chicago, IL",Senior Data Engineer,"You have a clear vision of where your career can go. And we have the leadership to help you get there. At CNA, we strive to create a culture in which people know they matter and are part of something important, ensuring the abilities of all employees are used to their fullest potential.
CNA seeks to offer a comprehensive and competitive benefits package to our employees that helps them — and their family members — achieve their physical, financial, emotional and social wellbeing goals.

For a detailed look at CNA’s benefits, check out our
Candidate’s Guide
.
JOB DESCRIPTION:
Essential Duties & Responsibilities
Performs a combination of duties in accordance with departmental guidelines:
Lead the design and build data solutions and applications that enable reporting, analytics, data science, and data management.
Design, develop and implement data integration projects using Informatica and SSIS.
Provide Azure application insights and Cloud-based integration using Python and PowerShell Scripts.
Create analytical Business Intelligence (BI) reports using Google Analytics for web applications.
Lead the design, implementation and automation of data pipelines, including sourcing data from internal and external systems and transforming the data for the optimal needs of various systems and business requirements.
Lead robust unit testing to ensure deliverables match the design and provide expertise to support subsequent release testing.
Apply machine learning concepts to development work.
Adhere to and establish quality and reliability standards, and ensure team adheres to the same quality and standards working in an Agile development environment.
Design complex physical data models, projects and cloud-based data lake constructs including SQL/NoSQL database systems.
Research, identify and implement process improvements that address complex technology gaps and build strong knowledge of technology enablers.
Maintain professional and technical knowledge by attending educational workshops, reviewing professional publications, establishing personal networks, and participating in professional societies.
Reporting Relationship
Typically Manager or above
Education & Experience
Bachelor’s degree in computer science, information technology or related and 5 (five) years of experience in data analytics or application development.
Must have work experience with each of the following:
Design, develop and implement data integration projects using Informatica and SSIS;
Provide Azure application insights and Cloud-based integration using Python and PowerShell Scripts;
Create analytical Business Intelligence (BI) reports using Google Analytics for web applications.
Primary Location United States – Illinois – Chicago
Organization – IT
Mon-Fri., 8:30am – 4:45pm, 37.5 hours/week, $140,046 to $148,800 per year, overtime exempt. This position qualifies for CNA’s employee referral policy program.
Apply: Submit cover letter and resume at
www.cna.com
.
CNA is committed to providing reasonable accommodations to qualified individuals with disabilities in the recruitment process. To request an accommodation, please contact
leaveadministration@cna.com
.",144423,1001 to 5000 Employees,Company - Public,Insurance,Insurance Carriers,1897,$10+ billion (USD),IL,126,data engineer,senior,"['nosql', 'sql', 'go', 'python']",['azure'],['ssis'],[],[],[],bachelor,
HP,4.2,"Vancouver, WA",Data Engineer,"Applies basic foundation of a function's principles, theories and concepts to assignments of limited scope. Uses professional concepts and theoretical knowledge acquired through specialized training, education or previous experience. Develops expertise and practical knowledge of applications within business environment. Acts as team member by providing information, analysis and recommendations in support of team efforts. Exercises independent judgment within defined parameters.
Responsibilities
Codes limited enhancements, updates, and programming changes for portions and subsystems of data pipelines, repositories or models for structured/unstructured data.
Analyzes design and determines coding, programming, and integration activities required based on objectives and guidance from more senior project team members.
Executes established portions of testing plans, protocols, and documentation for assigned portion of application; identifies and debugs issues with code and suggests changes or improvements.
Participates as a member of a project team of other data science professionals to develop reliable, cost effective and high-quality solutions for assigned data system, model, or component.
Knowledge & Skills
Using data engineering tools, languages, frameworks to cleanse, mine and explore data.
Basic understanding of SQL and NoSQL & relational based systems along with complex, distributed and massively parallel systems.
Ability to apply analytical and problem-solving skills.
Ability to understand complex data structures.
Understanding of database technologies and management systems.
Understanding of database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong written and verbal communication skills; mastery in English and local language.
Scope & Impact
Collaborates with peers, senior engineers, data scientists and project team.
Typically partners with more senior Individual Contributors.
Supports projects requiring data engineering solutions expertise.
Education & Experience
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering, or equivalent.

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.
So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.
From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!",112889,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1939,Unknown / Non-Applicable,WA,84,data engineer,na,"['sql', 'nosql']",[],[],[],[],[],bachelor,
Latitude Inc,4.3,"Arlington, VA",AWS Data Engineer (US Citizen),"What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5 years of experience in application development including Python, SQL, Scala, or Java
2 years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3 years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2 year experience working on real-time data and streaming applications
2 years of experience with NoSQL implementation (Mongo, Cassandra)
2 years of data warehousing experience (Redshift or Snowflake)
3 years of experience with UNIX/Linux including basic commands and shell scripting
2 years of experience with Agile engineering practices
Job Type: Full-time
Pay: $70,000.00 - $100,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Arlington, VA 22202: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
AWS: 1 year (Preferred)
SQL: 1 year (Preferred)
Python: 1 year (Preferred)
Security clearance:
Confidential (Preferred)
Work Location: Hybrid remote in Arlington, VA 22202",85000,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,$1 to $5 million (USD),VA,-1,data engineer,na,"['python', 'scala', 'shell', 'java', 'sql', 'nosql']","['snowflake', 'google cloud', 'azure', 'aws', 'redshift']",[],"['snowflake', 'hive', 'mysql']","['kafka', 'spark', 'hadoop']",[],bachelor,0-2 years
Par Government Systems Corporation,4.4,"Bethesda, MD",Senior Data Engineer,"PAR Government is excited to welcome a Senior Data Engineer to the our Intelligence and Readiness Operations. As a Senior Data Engineer you will support a large-scale intelligence processing system that is focused on digital document exploitation.

Establish a data engineering processes to support the understanding of information needs to maximize the use and value of data and information assets by consumers. You will manage information consistently across the enterprise and align data management efforts and technologies with business needs. Key task areas on the program include forensic image processing, machine learning model production, knowledge graph construction and reasoning, agile development, system security, technology transition, and system operations.

Responsibilities and Duties:
Analyze, design, build, test, deploy, operate, and maintain solutions, capabilities, and services to meet data needs.
Plan and lead major technology assignments, evaluate performance results, and recommend major changes affecting short-term project growth and success.
Conduct requirements analysis.
Conduct requirements design.
Implementation solutions
Maintenance databases of related solution components


Required Skills/Experience:
Must have an active or in scope US Top Secret Clearance with SCI Eligibility
Expertise in analyzing. designing, building, testing, deploying. operating, and maintaining solutions, capabilities, and services to meet data needs, including requirements analysis, and design, implementation, and maintenance of databases' related solution components.
Bachelors degree or equivalent with a minimum of 10 years of experience as a Data Engineer
Experience with defining and recording data requirements and delivering data requirements specifications.
Experience with developing and maintaining conceptual data models and delivering conceptual data model diagrams, logical data models, physical data models, and physical databases .
Experience with managing data model versions and integrating and delivering data model libraries.
Expertise in designing data integration services and delivering source-to-target maps, data extract-transform- load (ETL) design specifications, and data conversion designs.
Expertise in writing software code and scripts to distributed the processing of information extraction tasks to identify entities, events, and relationships from large corpus of structured and unstructured data and multimedia stored in a distributed file system ox object store.
Experience with applying data cleansing, transformation, and augmentation methods to measure and improve data quality.
Experience building, testing, and delivering data integration services
Experience with establishing Golden Records and delivering reliable reference and master data.
Experience defining, delivering, and maintaining hierarchies and affiliations that define the meaning of data within the context of its interrelationships with other data
Experience with importing and exporting data between an external RDBMS and a Hadoop cluster, including the ability to import specific subsets, change the delimiter and file format of imported data during ingest, and alter the data access patten or privileges.
Experience ingesting real-time and near-real time (NRT) Streaming data into the Hadoop File System (HDFS), including the ability to distribute to multiple data sources and convert data on ingest from one format to another.
Expertise in loading data into and out of the Hadoop File System (HDFS) using the HDFS command line interface; converting sets of data values in a given format at stored in Hadoop File System (HDFS) into new data values and/or a new data format and writing them into HDFS or Hive/HCatalog.
Expertise in filtering, sorting, joining, aggregating, and transforming one or more data sets in a given format (e.g., Parquet, Avro, JSON, delimited text, and natural language text) stored in the Hadoop Distributed Filesystem (HDFS)

It is the policy of PAR to prohibit all forms of discrimination and to affirmatively implement equal opportunity to all qualified employees and applicants for employment without regard to race, color, creed, religion, sex, age, veteran status, national origin, disability, marital status, predisposing genetic characteristics, sexual orientation, gender identity, or other legally protected status and positive action shall be taken to insure the fulfillment of this policy.
If you require reasonable accommodation in the application process, call Human Resources at 315.356.2260. All other applications must be submitted online.***

Required Skills

Required Experience",122700,201 to 500 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,senior,[],[],[],['hive'],['hadoop'],[],master,0-2 years
Tremco Incorporated,3.7,United States,Data Engineer,"GENERAL PURPOSE OF THE JOB:
*100% REMOTE / TELEWORK*
Division - Tremco Roofing & Building Maintenance
We are seeking an experienced and skilled Data Engineer to join our team! We are looking for a candidate that thrives in a collaborative environment, is a self-starter, and is passionate about data science. Our data science team is the foundation for data-driven business decisions and is leading the way for continued growth in innovative markets within the construction industry.
On the Data Science team, the Data Engineer’s purpose is to design, develop, and maintain the company's data infrastructure, pipelines, and workflows. They are responsible for merging predictive and prescriptive modeling to ensure it stays consistent with data flowing across the organization. They work closely with data scientists, analysts, and other stakeholders to ensure the data is properly collected, stored, processed, and analyzed to drive informed business decisions.
If you are passionate about data science and want to work with a dynamic team of professionals, please apply today!
ESSENTIAL DUTIES AND RESPONSIBILITIES:
Design, develop, build, and maintain the company's data infrastructure, pipelines, and workflows and all associated engineering tasks.
Develop and maintain ETL (Extract, Transform, Load) processes to collect and integrate data from various sources.
Build and maintain data APIs to enable data access across the organization.
Develop and implement scalable data solutions to optimize data processing, storage, and retrieval.
Develop and maintain documentation for data pipelines, including data dictionaries, standard operating procedures, and data flow diagrams.
Work with unstructured data and develop data models to enable data analysis and insights.
Identify any hidden patterns or data inconsistencies and work along with similar ad-hoc analysis
Ensure data quality, consistency, and accuracy and is properly structured and formatted to support analyses.
Ensure data security, integrity, and compliance with data privacy regulations.
Troubleshoot and resolve data-related issues, including data quality, integrity, and performance.
Continuously monitor, maintain, and optimize the health and performance of the data infrastructure, pipelines, and workflows
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements.
Stay up to date with the latest advancements in data engineering and recommend new technologies, tools, and processes to improve efficiency and productivity.
EDUCATION:
Bachelor's or Master's degree in Information Technology, Computer Science, or a related field
EXPERIENCE:
3+ years of experience in a data science or related role
CERTIFICATES, LICENSES, REGISTRATIONS:
Not Required but beneficial:
Certified SQL
Certified SQL, Advanced Queries
Python for Data Science & Machine Learning
R for Data Science & Machine Learning
Databricks Lakehouse Fundamentals
OTHER SKILLS AND ABILITIES:
Proficiency in programming languages such as Python, R, and SQL
Strong understanding of database technologies and SQL queries
Strong experience with ETL processes, data integration, and data modeling
Experience with cloud-based data storage and computing services, specifically Azure
Excellent problem-solving and analytical skills
Experience with data visualization tools such as Tableau or Power BI
Experience with data lakehouse tools such as Synapse (data lake) or databricks
Excellent communication and collaboration skills
Ability to work independently and prioritize tasks in a fast-paced & dynamic environment

Qualified applicants will receive consideration for employment without regard to their race, color, religion, national origin, sex, sexual orientation, gender identity, protected veteran status or disability.",112889,1001 to 5000 Employees,Subsidiary or Business Segment,Manufacturing,Chemical Manufacturing,1997,$500 million to $1 billion (USD),TX,26,data engineer,na,"['sql', 'r', 'python']","['databricks', 'azure']","['tableau', 'power bi']",[],[],[],bachelor,+10 years
Ocean Health Initiatives,2.2,"Neptune City, NJ",Data Engineer,"Company Intro: Ocean Health Initiatives, Inc. (OHI) is a Federally Qualified Health Center (FQHC) dedicated to providing quality, accessible and comprehensive primary health care to the residents of Ocean and Monmouth County; regardless of economic status.

Our Health Center Locations: Brick, Freehold, Lakewood, Little Egg Harbor, Manahawkin, Manchester and Toms River; with our school-based Wellness Programs located within the Clifton Ave Grade School, Lakewood; and Lakewood High School.

OHI services include family and internal medicine, pediatrics, behavioral health, nutrition, OB/GYN women’s health, dental, family planning, specialty care, pharmacies, PrEP, and a STI clinic. Hours: Full-time hours are M-F 8:00am- 4:30pm with rotating evening/Saturday shifts (subject to seasonal changes and business/site needs)
Position Summary
Reporting to the Director of Informatics, the Data Engineer will be integrated within the Informatics Team. This individual will play a key role in managing and optimizing Ocean Health Initiatives’ data systems, particularly relating to our Athenahealth Electronic Medical Records (EMR) system and Microsoft Azure platform, to support data-driven decision-making and enhance the quality of healthcare services for our communities. The Data Engineer will collaborate with other members of the IT Department to include: IT Systems Analyst, IT Helpdesk Specialist, Data Analyst, as well as other Informatics team members.
Responsibilities
Collaborates with the Informatics team to design, construct, install, test and maintain highly scalable data management systems.
Works with Athenahealth EMR and Microsoft Azure platforms to enhance data collection procedures, ensure data integrity, and optimize data delivery for each project.
Extracts and integrates data from various sources, including the Athenahealth EMR system and Microsoft Azure, while ensuring data privacy and adherence to HIPAA guidelines.
Develops, tests, and maintains architectures, including databases and large-scale processing systems, on Microsoft Azure.
Employ a variety of languages and tools to marry systems together or to create data interfaces.
Recommends and implements ways to improve data reliability, efficiency, and quality.
Collaborates with Informatics teams to strive for greater functionality within OHI’s data systems.
Stays informed about industry trends and emerging technologies in the fields of data engineering, healthcare informatics, and information technology.
Is highly accountable for one's own productivity. In this era of transparency, all projects must be represented clearly on a project board in Monday.com. Responsible for ensuring the board is always updated and accurate and that information is accessible to those that need it.
Maintains a passing monthly scorecard threshold at 80%.
Adheres to Corporate Compliance policies.
Other duties as assigned.
Education/Experience/Licensure
Bachelor degree in Computer Science, Information Systems, or a related field is required. Advanced degrees and certifications in Microsoft Azure or Athenahealth are a plus.
Two to three years of experience in a data engineering role, with specific experience in Athenahealth EMR and Microsoft Azure environments is required.
Proficiency in understanding and optimizing complex data systems, including healthcare EMR systems and cloud platforms like Microsoft Azure is required.
Strong understanding of data warehousing concepts and ETL (Extract, Transform, Load) tools and processes is required.
Familiarity with healthcare data standards, such as HL7, FHIR, and ICD-10 is required.
Knowledge of data security and privacy regulations, such as HIPAA is required.
Excellent problem-solving, analytical, and communication skills, with the ability to translate complex data concepts for non-technical stakeholders is required.
The ability to work both independently and collaboratively in a fast-paced, dynamic environment is required.
Proficiency in Microsoft Office 365 is required.
Benefits
Paid Time Off (PTO)
Holidays (8)
Health Insurance
Dental Benefits
401(k) + match
Group Term Life Insurance
Flexible Spending Account
Pre-Employment Requirements
Physical
Criminal Background Checks
Drug Screening
Tuberculosis Screening
Ocean Health Initiatives is firmly committed to creating a diverse workplace and is proud to provide equal employment opportunities to all applicants. Therefore, Ocean Health does not discriminate on the basis of creed, color, national origin, sex, gender identity, sexual orientation, age, religion, marital or parental status, alienage, disability, political affiliation or belief, military or military discharge status.
In accordance with New Jersey Executive order (COVID-19 No.283) in conjunction with the Federal CDC guidelines, COVID vaccinations are a requirement for Ocean Health Initiatives as well as many other Healthcare Organizations. Proof of full vaccination and the booster shot is required prior to the beginning date of employment. If you have a medical or religious contraindication, please inform Human Resources when the offer is extended",91048,201 to 500 Employees,Nonprofit Organization,Healthcare,Health Care Services & Hospitals,2003,Unknown / Non-Applicable,NJ,20,data engineer,na,[],['azure'],[],[],[],[],bachelor,
Royalty Staffing,3.8,"Moline, IL",Data Engineer,"Senior software engineers specializing in Data Engineering and Databricks to work on projects for one of its top clients (John Deere).
Required Skills (MUST HAVE):
Python, AWS, Apache Spark
3 years of Data Engineering work experience building Pipelines and ETL processes.
Client project experience in Databricks implementation
Client project experience working with Big Data
5 years or more experience in building backend systems in AWS / JAVA J2EE in the design, development, testing, and integration of highly complex backend
Experience with CI/CD build processes and configuration",82819,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,"['java', 'python']","['databricks', 'aws']",[],[],['spark'],[],,5-10 years
Iron Service Global Inc,3.4,"Menlo Park, CA","Data Engineer (Python, SQL)","Data Analytics & Engineering - Data Engineer III
Job Description: Onsite at Menlo Park location only.
Summary:
The main function of the Data Engineer is to develop, evaluate, test, and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization’s data assets.
Job Responsibilities:
Design, construct, install, test, and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate and maintain large scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models and proof of concepts.Skills:
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers and IT team members on project goals.
Strong PC skills including knowledge of Microsoft SharePoint
Strong Python/SQL experience
Education/Experience:
Bachelor's degree in a technical field such as computer science, computer engineering or related field required.
Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI.
Job Types: Full-time, Contract
Salary: $110,000.00 - $135,000.00 per year
Benefits:
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
ETL: 5 years (Preferred)
Data management: 5 years (Preferred)
Enterprise data ETL processes: 5 years (Preferred)
Business process modeling: 5 years (Preferred)
License/Certification:
Six Sigma (Preferred)
ISO 20000 (Preferred)
ITIL (Preferred)
Work Location: One location",122500,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1987,$25 to $100 million (USD),CA,36,data engineer,na,"['sql', 'python']",[],[],[],[],[],bachelor,5-10 years
CHS Inc,3.9,"Inver Grove Heights, MN",Senior Big Data Engineer,"CHS Inc. is a leading global agribusiness owned by farmers, ranchers and cooperatives across the United States that provides grain, food and energy resources to businesses and consumers around the world. We serve agriculture customers and consumers across the United States and around the world. Most of our 10,000 employees are in the United States, but today we have employees in 19 countries. At CHS, we are creating connections to empower agriculture.
CHS Inc.
Senior Big Data Engineer
Location: Inver Grove Heights, MN

Job Description
Analyze, Model, Develop the ELT framework for ingestion, transformation and distribution of data streams to Snowflake, AWS and PowerBI Reports. Implement various data modelling techniques using DBT tool. Design and Develop python lambda to load various API data into Snowflake to perform analytics for business requirements. Use tidal to automate the job schedule. Perform continuous integrations and deployment using Azure DevOps, Git, Octopus and Terraform. Collaborate with team to troubleshoot and develop standards and best practices. Integration testing to validate data between Cloudera and Snowflake. Implemented Change Data Capture (CDC) technology in HVR to load the deltas to Snowflake. Experience with Snowflake and AWS S3 bucket for integrating data from multiple sources with nested JSON formatted data into Snowflake table. Position allows working from home within commuting distance of worksite location.

Job Requirements
The qualified candidate must have at least a Bachelor’s degree or foreign equivalent degree in Computer Science, Information Technology, Management Information Systems (MIS), Business Intelligence, or closely related technical field. The qualified candidate must have at least 4 years (48 months) of experience with all the following: (a) Data Integration, Data Modeling, and ETL/ELT and SQL Development; (b) developing solutions related to Big Data, and Data Sciences from end-to-end (data ingestion to consumption); (c) developing and maintaining scalable data pipelines that will ingest, transform, and distribute data streams and batches; and (d) utilizing all the following tools/technologies: Java, C++ and C#, Object Oriented Design, Python 2.7 and 3, NoSQL Databases (Hive, Spark), AWS Native Tools (Glue, DMS, S3, Athena), Snowflake, Software Development Lifecycle (SDLC) Cloudera CDP and Databricks. All experience may be gained concurrently. Position allows working from home within commuting distance of worksite location.",113398,10000+ Employees,Company - Public,Agriculture,Farm Support,1929,$10+ billion (USD),MN,94,data engineer,senior,"['sql', 'java', 'nosql', 'python']","['databricks', 'snowflake', 'aws', 'azure']",[],"['snowflake', 'hive', 'dbt']",['spark'],['terraform'],bachelor,2-5 years
Synopsys,4.1,"Mountain View, CA",Data Engineer Leader,"44867BR
USA - California - Mountain View/Sunnyvale
Job Description and Requirements
The Synopsys Central Engineering team is tasked to digitize Synopsys product development activities. We will achieve it by identifying areas of improvement and correlating various aspects of the product development, and providing all levels of management visibility for action. The domain areas we are focused on relate to Quality, Productivity, and Operational Efficiency,

We are looking for an experienced Data Engineer who will contribute to building the next-generation Data Platform. As a Data Engineer, you will be working on modern, large-scale big data technologies to build data platforms on Snowflake and toolset. The goal is to create an effective and efficient data pipeline to facilitate data exchange between various applications.

In this role, you will be partnering with data providers to enable them to make available volumes of data and integrate in a common data platform. You will also interact with Data analysts to transform data into information and insights driving data-based strategic outcomes. This is a hands-on role, and you are joining the team near the beginning of our journey where you can help shape our way to manage big data.

Responsibilities:
Drive strategic goal of data consolidation for the whole of engineering to enable cross-domain analytics.
Own and establish Center of Excellence for Data Engineering practices by defining architecture, rules, and setting guardrails for data processing capabilities. This includes data Ingestion, quality control, transformation, and high availability.
Incorporate state-of-the-art practices in Data Engineering to scale the value we deliver in transforming data into insights.
Identify, design and implement internal process improvements, including data infrastructure, for scalability, optimizing data delivery, and automating manual processes.
Leverage your experience and proficiency in all aspects of data management, data cataloging, analytics solution architecture & design, and implementation roadmap.
Build data cataloging infrastructure and metadata platform to enable data discovery, data observability, and federated governance.
Drive cross-team projects to integrate data from numerous separate sources into a unified data environment.

Expertise & Skills:
Must be proficient in ELT (Extract-Load-Transform) process with hands-on experience.
Must be detailed-oriented with a passion for data accuracy and reliable solution development.
Subject Matter Resource in designing and building high performance data pipelines to move and process data using modern tools.
Experience in Data Engineering Architecture and Design.
Subject Matter Resource in SQL (advanced).
Experience in at least one prominent programming language, such as Python (preferred), or Java.
The ability to work with other team members, drive projects to completion, and work autonomously.
Excellent written and verbal communication, work autonomously, and have proven organizational and planning skills.
We also value:
Prior experience in leading Data or Analytics teams.
Experience in database design and management, such as MS SQL Server, Oracle Database, MySQL Database, Cassandra, MongoDB, etc
Familiarity and experience in Snowflake toolset is a proven asset for this position.
Experience with Power BI and/or Tableau or other visualization tools.
Experience with HVR and Fivetran a plus.
Experience with dbt Cloud a plus.

Requirements:
Bachelor's or master's Degree in a quantitative field
> 5-10 years of relevant experience
5+ years’ experience engineering and operationalizing data pipelines with large and complex datasets
3+ years’ experience working with Cloud technologies such as Snowflake

At Synopsys, we’re at the heart of the innovations that change the way we work and play. Self-driving cars. Artificial Intelligence. The cloud. 5G. The Internet of Things. These breakthroughs are ushering in the Era of Smart Everything. And we’re powering it all with the world’s most advanced technologies for chip design and software security. If you share our passion for innovation, we want to meet you.

Stay Connected: Join our Talent Community

The hourly range across the U.S. for this role is between $117,000 - $204,000. In addition, this role may be eligible for an annual bonus, equity, and other discretionary bonuses. Synopsys offers comprehensive health, wellness, and financial benefits as part of a comparative total rewards package. The actual compensation offered will be based on a number of job-related factors, including location, skills, experience, and education. Your recruiter can provide more specific details on the total rewards package upon request.

Inclusion and Diversity are important to us. Synopsys considers all applicants for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, military veteran status, or disability.

Job Category
Engineering
Country
United States
Job Subcategory
R&D Engineering
Hire Type
Employee
Base Salary Range
$117,000 - $204,000",160500,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,-1,$1 to $5 billion (USD),CA,-1,data engineer,senior,"['sql', 'java', 'r', 'python']",['snowflake'],"['tableau', 'power bi']","['snowflake', 'oracle', 'mysql', 'sql server', 'mongodb', 'dbt']",[],[],bachelor,+10 years
BigLynx Computer Software,4.9,"Redmond, WA",AWS Data Engineer(Tech Lead),"BigLynx, Inc is an American multinational technology corporation headquartered in Seattle, Washington, with operations in the United States, Canada, and India. The company began in 2016, as a product development company specializing in AI/ML Data Engineering in the Retail vertical space with its products warehouse & fast. Post Pandemic in 2022, BigLynx added a business division of boutique technology consulting, specializing inData Engineering, Full Stack , and Microsoft Dynamics helping clients build the next generation data platform and big data pipelines.
Design and architect data solutions: Work closely with stakeholders to understand business requirements and design scalable and efficient data solutions on AWS. Create data architectures, data models, and data flow diagrams.
Data pipeline development: Develop, implement, and manage data pipelines to extract, transform, and load (ETL) data from various sources into AWS. Utilize AWS services such as AWS Glue, AWS Data Pipeline, or Apache Airflow for data integration and orchestration.
Data transformation and processing: Transform raw data into structured and usable formats for analytics and reporting purposes. Apply data manipulation techniques and develop data transformation workflows using AWS services like AWS Glue, AWS Lambda, or Apache Spark.
Data storage and management: Design and implement scalable data storage solutions on AWS, such as Amazon S3, Amazon Redshift, or Amazon DynamoDB. Optimize data storage and retrieval for performance and cost efficiency.
Data quality and governance: Ensure data quality, consistency, and accuracy through data cleansing, validation, and standardization processes. Implement data governance practices and adhere to data privacy and security standards.
Performance optimization: Identify and resolve performance bottlenecks in data pipelines and data processing workflows. Optimize query performance and data processing capabilities using AWS tools and techniques.
Team leadership and collaboration: Lead a team of data engineers, providing technical guidance, mentoring, and driving best practices. Collaborate with cross-functional teams, including data scientists, analysts, and stakeholders, to understand data requirements and deliver high-quality data solutions.
Cloud infrastructure management: Configure and manage AWS infrastructure components related to data engineering, such as EC2 instances, VPCs, IAM roles, and security groups. Monitor and troubleshoot issues related to infrastructure and data services on AWS.
Documentation and knowledge sharing: Create and maintain technical documentation, including design documents, architecture diagrams, and standard operating procedures. Share knowledge and provide training to team members and stakeholders.
Qualifications:
Extensive experience in data engineering: Minimum of [X] years of experience in data engineering roles, with a focus on AWS cloud-based solutions.
AWS expertise: In-depth knowledge and hands-on experience with AWS services related to data engineering, including AWS Glue, AWS Data Pipeline, Amazon S3, Amazon Redshift, or Amazon DynamoDB.
ETL and data integration: Strong proficiency in ETL processes, data integration techniques, and data transformation workflows. Familiarity with tools like Apache Spark, Apache Airflow, or AWS Glue for data manipulation and processing.
Database and data warehouse technologies: Solid understanding of relational databases, data warehousing concepts, and SQL. Experience with Amazon Redshift or other data warehousing solutions is preferred.
Programming and scripting: Proficiency in Python, SQL, and shell scripting for data engineering tasks. Knowledge of other programming languages such as Java or Scala is a plus.
Data modeling and schema design: Experience in designing and implementing data models, database schemas, and dimensional modeling concepts. Familiarity with schema design optimization for analytical workloads.
Cloud infrastructure and security: Strong understanding of AWS cloud infrastructure components, security controls, and best practices. Experience in managing AWS resources and implementing security measures for data.
Leadership and teamwork: Proven exper
Job Types: Full-time, Contract
Pay: $70.00 - $80.00 per hour
Experience level:
8 years
Schedule:
Monday to Friday
Work Location: In person",135000,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,WA,-1,data engineer,senior,"['scala', 'shell', 'java', 'sql', 'python']","['aws', 'redshift']",[],['dynamodb'],['spark'],[],,5-10 years
"JPMorgan Chase Bank, N.A.",3.8,"Jersey City, NJ",Software Engineer III - Data Engineering,"We have an exciting and rewarding opportunity for you to take your software engineering career to the next level.

As a Software Engineer III at JPMorgan Chase, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Executes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems
Creates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems
Produces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems
Proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture
Contributes to software engineering communities of practice and events that explore new and emerging technologies
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Hands-on practical experience in system design, application development, testing, and operational stability using Scala, Core Java, and/or Python
Solid understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security
Experience with big data technologies such as Spark (ideally with Scala), Hadoop, Databricks or related technologies
Knowledge of Unix shell and SQL as well as NoSQL DBs is required.
Experience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages
Hands-on practical experience in system design, application development, testing, and operational stability
Proficient in coding in one or more languages

Preferred qualifications, capabilities, and skills
Experience performance tuning and applying modelling concepts with data (SQL and/or no-SQL)
Exposure to cloud technologies (AWS EMR, EC2, Snowflake)
Liquidity/Capital Markets/Prime Brokerage Risk domain knowledge/skills
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $123,500.00 - $180,000.00 / year",151750,10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD),NJ,224,data engineer,na,"['scala', 'shell', 'java', 'nosql', 'sql', 'python']","['databricks', 'snowflake', 'aws']",[],['snowflake'],"['hadoop', 'spark']",[],,0-2 years
Zllius Inc.,4.0,"Canton, MI",Data Engineer AUG76,"Hi,
Greetings from Zllius Inc!
We have an urgent opening for the roleData Engineer. If interested, revert me back with your Updated resume, along with contact details, work authorization.
Job Role: Data Engineer
Location: Hartford, CT ( Initially Remote )
Duration: Long-term
Position: W2/1099
Visa: Any (Except H1B)
Job Ref: AUG76
Job Description:
Good experience on designing and developing data pipelines for data ingestion and transformation using Spark.
Distributed computing experience using Pyspark.
Good understanding of spark framework and spark architecture.
Experience working in Cloud based big data infrastructure.
Excellent in trouble shooting the performance and data skew issues.
Must have good understanding of spark run time metrics and tune applications based on metrics.
Deep knowledge in partitioning, bucketing concepts of data ingestion.
Good understanding of AWS services like Glue, Athena, S3, Lambda, Cloud formation.
Preferred working knowledge on the implementation of datalake ETL using AWS glue, Databricks etc.
Experience with data modelling techniques for cloud data stores and on prem databases like Teradata, Teradata Vantage (TDV) etc.
Preferred working experience in ETL development in Teradata vantage and data migration from on prem to Teradata vantage.
Proficiency in SQL, relational and non-relational databases, query optimization and data modelling.
Experience with source code control systems like Gitlab.
Experience with large scale distributed relational and NoSQL database systems.
Experience : 9+years
Thanks & Regards:
Zllius Inc.
844 495 5487
Job Types: Full-time, Contract
Schedule:
8 hour shift
Work Location: In person",86440,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,MI,-1,data engineer,na,"['sql', 'nosql']","['databricks', 'aws']",[],[],['spark'],['gitlab'],,+10 years
"JPMorgan Chase Bank, N.A.",3.8,"Jersey City, NJ",Software Engineer II - Data Engineering,"You're ready to gain the skills and experience needed to grow within your role and advance your career - and we have the perfect software engineering opportunity for you.

As a Software Engineer at JPMorgan Chase, within the Corporate Sector, Finance Technology team, you are part of an agile that works to enhance, design, and deliver the software components of the firm's state-of-the-art technology products in a secure, stable, and scalable way. As an emerging member of a software engineering team, you execute software solutions through the design, development, and technical troubleshooting of multiple components within a technical product, application, or system, while gaining the skills and experience needed to grow within your role.
Job responsibilities

Executes standard software solutions, design, development, and technical troubleshooting
Writes secure and high-quality code using the syntax of at least one programming language with limited guidance
Designs, develops, codes, and troubleshoots with consideration of upstream and downstream systems and technical implications
Applies knowledge of tools within the Software Development Life Cycle toolchain to improve the value realized by automation
Applies technical troubleshooting to break down solutions and solve technical problems of basic complexity
Gathers, analyzes, and draws conclusions from large, diverse data sets to identify problems and contribute to decision-making in service of secure, stable application development
Learns and applies system processes, methodologies, and skills for the development of secure, stable code and systems
Adds to team culture of diversity, equity, inclusion, and respect
Required qualifications, capabilities, and skills

Looking for top-notch lead engineering talent who will be able to thrive in an entrepreneurial environment that demands quick turnaround for mission critical technical solutions, have extremely high standards with a low tolerance for low quality output.
Strong background in computer science concepts.
Minimum of 3+ years of server side development using Java, Scala and/or Python.
Excellent oral and written, and problem-solving skills are required.
Candidate should be comfortable working in a fast-paced environment and can help build APIs, Calculators, on new cutting edge cloud and big data technologies such as AWS EMR, EC2, Scala Spark, Scala, Snowflake
Ideal candidates should have strong analytical skills and a penchant for tackling complex problems and designs of scale.
Knowledge of Unix shell and SQL as well as NoSQL DBs is required.
Preferred qualifications, capabilities, and skills

Proactively improve support services by building upon best practice and tools
Cloud certification.
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Jersey City,NJ $109,250.00 - $145,000.00 / year",127125,10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD),NJ,224,data engineer,na,"['scala', 'shell', 'java', 'nosql', 'sql', 'python']","['snowflake', 'aws']",[],['snowflake'],['spark'],[],,0-2 years
Oak Street Health,3.7,"Chicago, IL","Sr. Engineer I, Data Engineering","Company: Oak Street Health
Title: Sr. Engineer I, Data Engineering
Oak Street Health is a rapidly growing public company of primary care centers for adults on Medicare in medically-underserved communities where there is little to no quality healthcare. Oak Street’s care is based on a unique model that is focused on value for its patients, not on volume of services. The company is accountable for its patients’ health, spending more than twice as long with its patients and taking on the risks and costs of their care. For more information, visit http://www.oakstreethealth.com.
For more information, visit www.oakstreethealth.com.

Role Description:
The Data Engineer will be responsible for delivering high quality modern data solutions through collaboration with our engineering, analysts, and product teams in a fast-paced, agile environment leveraging cutting-edge technology to reimagine how Healthcare is provided. You will be instrumental in designing, integrating, and implementing solutions as well supporting migrations of existing workloads to Azure cloud. The Data Engineer is expected to have extensive knowledge of modern programming languages, designing and developing data solutions

Core Responsibilities:
Develop and automate solutions to consume data from multiple data sources including external API
Programming and modifying code in languages like Java, Json, and Python to support and implement Data Warehouse solutions
Design and deploy enterprise-scale cloud infrastructure solutions
Research, analyze, recommend and select technical approaches for solving difficult and meaningful development and integration problems
Work closely with the Data and Engineering teams to design best in class Azure implementations
Participate in efforts to develop and execute testing, training, and documentation across applications
Design, develop and deliver customized ETL and Database solutions
What are we looking for?
3+ years of relevant working experience with Azure
3+ years of experience working with SQL
3+ years Hands-on experience with cloud orchestration and automation tools, CI/CD pipeline creation
3+ Experience in provisioning, configuring, and developing solutions in Azure Data Lake, Azure Data Factory, Azure SQL Data Warehouse, Azure Synapse and Cosmos DB
Hands-on experience working with PaaS/ IaaS/ SaaS products and solutions
Hands-on experience with Python, Javascript or PySpark
Understanding of Distributed Data Processing of big data batch or streaming pipelines
A desire to work within a fast-paced, collaborative, and team-based support environment
Ability to work independently as well as function as part of a team
Willingness to identify and implement process improvements, and best practices as well as ability to take ownership
Familiarity with healthcare data and healthcare insurance feeds is a plus
Excellent oral and written communication skills
US work authorization

What does being “Oaky” look like?
Radiating positive energy
Assuming good intentions
Creating an unmatched patient experience
Driving clinical excellence
Taking ownership and delivering results
Being scrappy

Why Oak Street?

Oak Street Health offers our coworkers the opportunity to be at the forefront of a revolution in healthcare, as well as:
Collaborative and energetic culture
Fast-paced and innovative environment
Competitive benefits including paid vacation and sick time, generous 401K match with immediate vesting, and health benefits
Oak Street Health is an equal opportunity employer. We embrace diversity and encourage all interested readers to apply to oakstreethealth.com/careers.",112274,1001 to 5000 Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['sql', 'java', 'python']",['azure'],[],[],[],[],,+10 years
"Labviva, Inc.",2.8,"Boston, MA",Senior Data Engineer,"Senior Data Engineer
Location: Boston, MA
Category: Data/Analytics
Type: Full-time, Hybrid Boston Office

About the Role
As a Senior Data Engineer at Labviva, you will be charting the blueprint of data infrastructure for a leading marketplace in life science. You will be the first hire in the data engineering function and will join our growing team of analytics and data professionals, reporting to the VP Analytics and Data Science. You would be a great fit for the role if you are a data engineering expert knowledgeable of data architecture in Big Data. You have worked in a scaled e-commerce platform, preferably having experienced its growth stages. You are a self-starter who thrives in an ambiguous, fast-paced startup environment. Success in this position will mature Labviva's approach to data to meet standards that are high quality (accurate, complete, consistent), comprehensive to meaningful business activity, easy to use properly and difficult to use incorrectly to drive the business forward.

How You Will Contribute
Work with internal/external resources to design and build a centralized business intelligence environment
Develop and implement data pipelines to ETL/ELT data from multiple sources into a data warehouse or data lake and monitor for data accuracy
Maintain data infrastructure in production and development environments
Provide data engineering support throughout product/solution lifecycle from research, development, testing to production
Improve operational efficiency by automating and streamlining data processes
Collaborate with stakeholders to develop and implement data governance procedures to ensure data security, privacy and compliance

What You Bring to the Team
Bachelor's degree in computer science, engineering or a related field
5+ years of experience with database design, data modeling, data pipelines, data warehousing/data lake
Retail/ecommerce data management expertise; e-Marketplace, supply chain management and/or data platform experience is a plus
Data governance and compliance experience in regulated industries are preferred
Public cloud experience; AWS preferred
Event driven architecture and microservices
Big Data technologies such as Kafka, Spark, Hadoop
Excellent working knowledge of SQL; proven ability to debug and optimize queries & procedures
PostgreSQL, MySQL, Microsoft SQL Server
NoSQL DBs such as MongoDB
Python, Java
Exposure to scaled enterprises andp mentality
Strong intellectual curiosity and self-motivation

About the Company
Labviva is on a mission to accelerate the pace of life science research. We connect researchers with suppliers of reagents, chemicals and instrumentation in an intuitive user-friendly platform that supports the priorities of scientists while staying compliant with purchasing rules. We are a startup that acknowledges the unique contributions of each team member drive our success. We commit to creating a diverse and inclusive workspace where people can make a positive impact. Labviva does not discriminate based on race, religion, national origin, sexual orientation, gender identity or expression, age, disability, marital, veteran status and classifications protected by discrimination laws.",118922,Unknown,Company - Private,Pharmaceutical & Biotechnology,Biotechnology,-1,Unknown / Non-Applicable,MA,-1,data engineer,senior,"['java', 'sql', 'python', 'nosql']",['aws'],[],"['sql server', 'postgresql', 'mongodb', 'mysql']","['kafka', 'hadoop', 'spark']",[],bachelor,+10 years
Inspire Medical Systems I,4.4,"Golden Valley, MN",Azure Data Engineer,"Inspire Medical Systems has developed the only FDA-approved neurostimulation technology that transforms the lives of people with moderate to severe sleep apnea. We are a ground-breaking, fast-growing company where the patient’s outcome is first and foremost our top priority.
If you want to become part of a purpose-driven company and directly help to transform lives, this is the perfect career opportunity for you!
Position Summary:
The Azure Data Engineer should be experienced building in the Big Data space, using traditional, new, and emerging technologies. A good understanding of data modeling and SQL coding best practices is expected.
We are looking for a highly energetic and collaborative Azure Data Engineer with experience leading enterprise data modeling projects with Business and IT operations using Azure Analytics, Azure Data Warehousing and Azure Big Data products.

MAIN DUTIES/RESPONSIBILITIES:

You will produce high-quality, secure, and maintainable code in an agile environment
Learn and understand business processes with limited guidance
Work collaboratively as a member of the development team to build best-in-class software solutions in an agile environment
Support and research issues across all application layers and database
You will identify areas to improve and scale our Azure architecture and application design
Ensure code can be deployed using Azure DevOps
Design and query database tables, views, functions, stored procedures and batch processes
Develop, implement, and support interfaces that connect our websites, back-end systems, and various 3rd party cloud solutions
Required Qualifications:
Bachelor’s degree from an accredited college or University
Experience with private and public cloud architectures, pros/cons, and migration considerations
Minimum of 5 years of RDBMS experience
Experience with JSON, JSON-LD, XML data structures
Experience implementing data pipelines using latest technologies and techniques
Experience with SDLC products (JIRA, Confluence, Github, etc) or similar agile project management tools
5+ years of hands-on experience in programming languages such as Java 8, c#, node.js, python, SQL, Unix shell/Perl scripting etc.
At least 5 years of consulting or client service delivery experience on Azure
Bachelor’s or higher degree in Computer Science or related discipline
Experience handling structured and unstructured datasets
Expert in USQL, Java, Python, Hive SQL, Spark SQL, DataBricks or Snowflake
Strong t-SQL skills with experience in Azure SQL DW
Experience in Data Modeling and Advanced SQL techniques
Cloud migration methodologies and processes including tools like Azure Data Factory, Azure Synapse, Event Hub, etc.
Excellent problem solving, analytical, and critical thinking skills
Preferred Qualifications:
Master’s degree from an accredited college or University
Experience ingesting data from MS Dynamics CRM a big plus
Microsoft Azure certifications are a plus

Inspire Medical Systems provides equal employment opportunity (EEO) to all employees and applicants without regard to race, color, religion, creed, sex, national origin, age, disability, marital status, familial status, sexual orientation, status with regard to public assistance, membership or activity in a local commission, military or veteran status, genetic information, or any other status protected by applicable federal, state and local laws. This policy applies to all aspects of the employment relationship, including recruitment, hiring, compensation, promotion, transfer, disciplinary action, layoff, return from layoff, training and social and recreational programs. Inspire Medical Systems complies with applicable laws governing non-discrimination in employment in every location in which Inspire Medical Systems has facilities. All such employment decisions will be made without unlawfully discriminating on any prohibited basis.

Inspire Medical Systems is an equal opportunity employer with recruitment efforts focused on ensuring a diverse workforce. Applicants with a disability that are in need of accommodations to complete the Inspire Medical Systems application process should contact Human Resources at 844-672-4357 or email careers@inspiresleep dot com.

Inspire Medical Systems participates in E-Verify.",108551,51 to 200 Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2007,$5 to $25 million (USD),MN,16,data engineer,na,"['sql', 'java', 'shell', 'python']","['databricks', 'azure', 'snowflake']",[],"['snowflake', 'hive']",['spark'],[],bachelor,5-10 years
International Sports Sciences Association,4.1,"Phoenix, AZ",Sr. Data Engineer,"ISSA is looking for Sr Data Engineer!
REMOTE, but must reside in an ISSA-eligible state (AZ, UT, NV, ID, OR, TX, IL, IN, MN, OH, FL, SC, GA, TN)
About ISSA: ISSA is an organization that operates as an education and certification company for fitness trainers, personal trainers, strength and conditioning coaches, nutritionists, nutrition coaches, aerobic instructors, and medical professionals. For more than 35 years, ISSA has created a personal fitness training program to merge gym experience with practical and applied sciences, we have certified nearly 500,000 trainers in 174 countries. With over 200 plus employees nationwide. We have been voted a Best Place to Work by The Phoenix Business Journal in 2022!
We are looking to add Sr. Data Engineer to our effective relationship-builders with a laser focus on providing exceptional service. We are tenacious and resilient in our pursuit of personal and professional success, while we incorporate fun into every day!
Job Summary:
We are seeking an experienced Senior Data Engineer with expertise in Azure SQL, Data Factory, and the Azure Cloud Environment to join our dynamic team. As a Senior Data Engineer, you will play a crucial role in designing, implementing, and maintaining our data infrastructure to support our organization's data-driven initiatives. You will be responsible for building and optimizing data pipelines, ensuring data quality and integrity, and collaborating with cross-functional teams to deliver scalable and efficient data solutions.
Responsibilities and duties:
Design, develop, and maintain data pipelines and workflows using Azure Data Factory to extract, transform, and load data from various sources into Azure SQL databases.
Collaborate with data architects, data scientists, and business analysts to understand data requirements and translate them into technical specifications.
Develop and optimize SQL queries, stored procedures, and functions to ensure efficient data retrieval and processing.
Monitor and troubleshoot performance issues, data quality problems, and other data-related anomalies in Azure SQL databases.
Implement data security and access controls in compliance with industry best practices and regulatory requirements.
Develop and maintain data governance policies, standards, and procedures to ensure data integrity, privacy, and compliance.
Collaborate with Azure Cloud Administrators to provision and manage Azure SQL resources, including server administration, backups, and disaster recovery strategies.
Leverage general cloud experience to design scalable, reliable, and cost-effective data solutions within the Azure Cloud Environment.
Provide guidance and mentorship to junior data engineers, promoting best practices and ensuring high-quality deliverables following industry standards.
Stay up to date with the latest advancements in Azure SQL and related technologies and assess their potential impact on our data engineering processes.
What is ISSA looking for:
Minimum of 5+ years of experience as a Data Engineer, with a strong focus on Azure SQL and related technologies.
Proficiency in designing and implementing data pipelines using Azure Data Factory.
Extensive experience in Azure SQL Database, Azure SQL Managed Instance, and Azure SQL Data Warehouse.
Strong knowledge of SQL programming, query optimization, and performance tuning.
Familiarity with Azure Cloud environment and associated services, including Azure Storage, Azure Data Lake, and Azure Key Vault.
Experience with general cloud concepts, such as virtual machines, networking, and identity management.
Experience with data modeling, data warehousing concepts, and dimensional data modeling techniques.
Knowledge of ETL (Extract, Transform, Load) processes and data integration patterns.
Understanding of data governance, data security, and privacy practices.
Excellent problem-solving skills and the ability to work effectively in a fast-paced, collaborative environment.
Strong communication and interpersonal skills, with the ability to explain complex technical concepts to non-technical stakeholders.
Computer Science or Information Systems Degree Preferred
Bonus Skills:
Microsoft Azure Certifications, such as Azure Data Engineer or Azure Administrator.
Experience with other Azure services like Azure Databricks, Azure Synapse Analytics, or Azure Analysis Services.
Familiarity with big data technologies, such as Apache Spark or Hadoop.
Knowledge of programming languages such as Power shell, Python or Scala.
Experience with version control systems, such as Git and GitHub.
Anything else? Absolutely! Benefits and Perks:
ISSA is a place where every day we are inspired by our teammates, encouraging each other to be our best. The environment is relaxed, friendly, and upbeat! And we feel it’s important to reward our team with competitive pay and benefits. Here are some of the highlights:
Free certifications: Who wants to be their best self? WE DO! As an ISSA team member, you have access to all 25 certifications…at no expense! Additionally, you'll receive exclusive discounts on leading products in the fitness industry. Cool, right?
Work-Life Synergy: Our Core Team members have 4 weeks of PTO. If you join ISSA as a People Leader you get 5 weeks of PTO and our Directors and Above have Discretionary Time Off! All PTO starts accruing on day one! And everyone at ISSA gets 8 paid national holidays every year! Work hard, play hard!
Parental Time Off: We support our team members as they grow their families! Once you qualify for FMLA, you are eligible to receive up to 6 weeks of paid paternity time to bond with a new child.
Well-rounded support: We are proud to offer Maven, a global platform that provides on-demand care! Maven provides an option for inclusive, comprehensive support for the whole employee and their family journey!
Medical, Dental, and Vision Insurance: A key component to living a healthy lifestyle is having access to the care we (and our families) need. We offer 2 medical insurance plans accompanied by Health Saving Account (HSA) and Flexible Savings Account (FSA) options, as well Dental and Vision coverage. And you don’t have to wait forever! Eligibility begins the 1st of the month after you start.
Team Building: We may be remote, but we don’t have to be alone! We host a monthly schedule of virtual team activities, town halls, weekly gratitude and motivational sessions, and other special events…so many fun ways for us to connect and support one another.
Growth opportunities: When our people grow…we grow! We offer leadership training, development journeys for each role and career path, and many coaching/mentoring opportunities. Constant growth and development are inherent in our culture.
Tools to do the job: We ensure you are hooked up with the tools, equipment, and systems you need. We begin the process prior to your start date, so you are ready to rock ‘n roll on your first day. We also provide a monthly work-from-home allowance!
What else? We have a 401(k) company match and 100% company-paid life/AD&D insurance/short-term disability. We also offer an employee assistance program to assist with work-life concerns (legal, financial, and mental health). Our ultimate goal is to support you and your overall wellness.
Transform your career at ISSA!",92457,51 to 200 Employees,Company - Private,Education,Education & Training Services,1988,$25 to $100 million (USD),AZ,35,data engineer,senior,"['sql', 'shell', 'python', 'scala']","['databricks', 'azure']",[],[],"['hadoop', 'spark']",[],,+10 years
The Hartford,3.9,"Hartford, CT",Sr. Data Engineer,"You are a driven and motivated problem solver ready to pursue meaningful work. You strive to make an impact every day & not only at work, but in your personal life and community too. If that sounds like you, then you've landed in the right place.
Strong candidate has been identified
The Property & Casualty Support team within the Actuarial Information Services department is seeking a talented professional Senior Data Engineer to join our team. We are looking for a talented professional with a proven track record of data engineering and operationalizing a next generation analytics solution suite using Oracle, Cloud (Snowflake) and Big Data Technologies.
Our ideal candidate will leverage deep technical expertise and strong communication skills to deliver both invest and maintenance projects within the Actuarial portfolio. Responsibilities include but are not limited to:
Responsibilities:
Architect innovative solutions that continue to advance our actuarial reserving capabilities while maintaining focus on delivering what is truly needed to our business partners in a timely fashion
Provides highly technical consulting and leadership in identifying and implementing new uses of information technologies that assist the functional business units in meeting their strategic objectives
Identify opportunities to optimize processes by creatively harnessing available data/tools, reusing strategic components, and reducing system complexities where appropriate
Solve problems in an efficient and structured manner; understand the desired outcome, assess a root cause, produce a viable and realistic solution for our business partners
When appropriate, leverage Agile methodology to work iteratively pairing closely with our business partners
Possesses functional knowledge and skills reflective of a competent practitioner with the ability to deliver on work of highest technical complexity
Formulates logical statements of business problems and devises, tests and implements efficient application program solutions (e.g., codes and/or reuses existing code using program development software alternatives and/or integrates purchased solutions)
Use Toad for Oracle to connect to multiple Oracle databases and access needed datasets
Maintain documentation and other data design artifacts that define business data requirements and handling rules.
Perform data or statistical analysis across multiple business units and processes.
Experience & Skills Qualifications:
Candidates must have the technical skills to transform, manipulate and store data, the analytical skills to relate the data to the business processes that generates it, and the communication skills to document & disseminate information regarding the availability, quality, and other characteristics of the data to a diverse audience.
These varied skills may be demonstrated through the following:
Bachelor’s degree with at least 7 or more years of applicable work experience with respect to Data Analysis, manipulation, and technical development
Experience accessing and retrieving data from large data sources, by creating and tuning SQL queries. Understanding of data modeling concepts, data warehousing tools and databases (e.g., Oracle, AWS, Snowflake and R)
Demonstrated ability to create and deliver high quality PL/SQL code using software engineering best practices. Experience with object-oriented programming and software development a plus.
Ability to analyze data sources and provide technical solutions
Determine business recommendations and translate into actionable steps
Critical thinking skills that lead to findings, solutions, and positive business outcomes
Self-starter with curiosity and a willingness to become a data expert
Forward-looking and continuous improvement mindset
Detail and results driven with commitment to managing multiple deadlines in a fast-paced environment.
Strong relationship building / interpersonal and influencing skills
Results oriented with the ability to multi-task and adjust priorities when necessary
Ability to work both independently and in a team environment with internal customers
Ability to effectively communicate through written and verbal means
Experience in Reserving is an added advantage
Compensation
The listed annualized base pay range is primarily based on analysis of similar positions in the external market. Actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. The base pay is just one component of The Hartford’s total compensation package for employees. Other rewards may include short-term or annual bonuses, long-term incentives, and on-the-spot recognition. The annualized base pay range for this role is:
$110,560 - $165,840
Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age
About Us
|
Culture & Employee Insights
|
Diversity, Equity and Inclusion
|
Benefits
Sr Data Engineer - GE07BE",112889,10000+ Employees,Company - Public,Insurance,Insurance Carriers,1810,$10+ billion (USD),CT,213,data engineer,senior,"['sql', 'r']","['snowflake', 'aws']",[],"['snowflake', 'oracle']",[],[],bachelor,
D.A. Davidson Companies,3.8,Remote,Data Engineer,"Job Description:

DA Davidson is looking for an energetic, creative Data Engineer to join our expanding team of analytics professionals. This role will be responsible for expanding and optimizing our data and data pipeline architecture, as well as monitoring, troubleshooting and performance tuning data flows used in the collection, integration and provisioning of data. Candidates should have experience building data pipelines and enjoy optimizing entire data systems as well as building them from the ground up. The Data Engineer will work alongside our software development teams, database architects, data analysts and end users on data initiatives, and will ensure our data delivery architecture is consistent with best practices. Candidates must be comfortable working in an Agile team in support of multiple teams, across various systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our growing portfolio of data initiatives.
D.A. Davidson Companies is an independent, employee-owned company with a rich history spanning more than 80 years. We are dedicated to conducting our business in accordance with the highest standards of integrity and ethics, and delivering outstanding service to our clients and each other. We support a friendly, open and adaptive culture, and encourage candid communication and productive collaboration across our firm. Just as we work to improve our clients and employees’ well-being, we also work to strengthen local communities-and giving back is one of our core values.
Experience and Skills:
Employee Growth, Job Qualifications, and Responsibilities
Growth
D.A. Davidson places significant importance on career growth and employee development
Position includes opportunities to work with cloud strategies, branding, standards, and emerging technologies
Numerous employee training programs that include tuition credit, conferences, technical certifications, field training, and broker-dealer series licensing
This position includes working with a global workforce and subject matter experts across many IT and business domains
Position offers employee ownership that includes profit sharing and access to ESOP programs
Professional culture with healthy work / life balance, along with options and technology to support working remotely
D.A. Davidson maintains an Agile IT culture that with communities of practice and centers of excellence to broadly share knowledge and insights

Qualifications
We are looking for a candidate with 3+ years of experience in a Data Engineering role, who has attained a degree in Computer Science, Information Systems, Business Intelligence or comparable work history
Prior experience in the Financial Services industry is beneficial but not required
A flexible, growth-based mindset focused on teamwork, collective project management and knowledge-forward collaboration
Advanced working TSQL knowledge and experience working with relational databases, query authoring, stored procedure development, end-to-end data pipeline development
Experience performing operational analysis on internal and external data and processes to uncover failure points and identify opportunities for improvement
Strong analytic skills related to working with complex, multi-layered data pipelines and dependent datasets
Holistic awareness of processes supporting data preparation, metadata, dependencies, lineages, compliance and workload management
Understanding of logical data modeling and data normalization
Understanding of meta-model and metadata management requirements
Specific skills include:
o Experience with relational and dimensional database modeling
o Experience with relational database platforms: preferably MS SQLServer
o Experience with data pipeline tools such as: SSIS
o Experience using orchestration tools such as: ActiveBatch
o Experience with web services and API integrations
o Experience with scripting languages such as: Python and Powershell
o Experience with Agile, DevOps and DataOps methods and technologies: Source Code Control, Versioning, Test Automation, Continuous Integration, Automated Deployment, Recoverability, Telemetry, etc.
o Familiarity with cloud data services such as: Azure, AWS, Snowflake
o Familiarity with data visualization technologies such as: Tableau, PowerBI
Job Responsibilities
Design, implement and maintain a robust, efficient and compliant data pipeline architecture
Assemble data sets that meet functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal acquisition, transformation, and provisioning of data from a variety of data sources
Build and maintain a service-based architecture for the publication of consumable data and information objects
Work with data architects and data analysts to build analytics products that utilize the data pipelines to provide business-relevant information
Work with Business and IT stakeholders to assist with data-related technical issues and support their data and information requirements
Contribute to the development of frameworks to ensure data structures and integration processes are accurate, compliant and secure
Facilitate the development and implementation of data quality standards, data protection standards and adoption requirements across the enterprise
Define indicators of performance and quality metrics and ensure compliance with data related policies, standards, roles and responsibilities, and adoption requirements
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
From: D.A. Davidson Companies",112889,1001 to 5000 Employees,Company - Private,Financial Services,Investment & Asset Management,1935,$100 to $500 million (USD),Remote,88,data engineer,na,['python'],"['azure', 'aws', 'snowflake']","['tableau', 'ssis']",['snowflake'],[],[],,+10 years
Booz Allen Hamilton,4.2,"Norfolk, VA","Tech Excellence Data Engineer, Junior","The Opportunity:
Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there are more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it’s gathered from disparate sources. We need a data professional like you to help our clients find answers in their big data to impact important missions—from fraud detection to cancer research to national intelligence.
As a big data engineer at Booz Allen, you’ll use your skills to implement data engineering activities on some of the most mission-driven projects in the industry. You’ll help develop and deploy the pipelines and platforms that organize and make disparate data meaningful.
Here, you’ll learn from a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, Agile environment. You’ll grow your skills in analytical exploration and data examination while you support the assessment, design, building, and maintenance of scalable platforms for your clients.
Work with us to use big data for good.
Join us. The world can’t wait.
You Have:
Experience with querying or analyzing data to answer questions and solve problems
Experience with visualizing data to identify or communicate key insights
Knowledge of basic concepts in mathematics and statistics
Knowledge of Cloud environments
Ability to obtain a security clearance
Bachelor’s degree
Nice If You Have:
Experience with systems engineering or systems administration
Experience with Amazon Web Services
Experience with Azure
Experience with Docker, Kubernetes, and Ansible
Experience with Python or GitHub in Data Science
Ability to learn a programming language
Bachelor’s degree in Data Science, Mathematics, Engineering, Physics, Statistics, or CS
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
Create Your Career:
Grow With Us
Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.
A Place Where You Belong
Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll develop your community in no time.
Support Your Well-Being
Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.
Your Candidate Journey
At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.
Compensation
At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.
Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $45,300.00 to $93,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees.
Work Model
Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.
If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.
If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.
EEO Commitment
We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",69150,10000+ Employees,Company - Public,Management & Consulting,Business Consulting,1914,$5 to $10 billion (USD),VA,109,data engineer,na,['python'],['azure'],[],[],[],"['ansible', 'docker']",bachelor,
McLane Company,3.2,"Temple, TX",BI Data Engineer,"JOB SUMMARY / GENERAL DESCRIPTION:

Cleans, prepares, and optimizes data for further analysis and modelling. Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to Data Pipeline (ie ELT) principles and business goals.

ESSENTIAL JOB FUNCTIONS / PRINCIPAL ACCOUNTABILITIES:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to delivers insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineer effective features for modelling in close collaboration with data scientists and businesses
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering to improve productivity and quality.
Partners with machine learning engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coach other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Learns about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics as necessary to carry out role effectively.

MINIMUM SKILLS AND QUALIFICATION REQUIREMENTS:
Bachelor's degree in computer science, statistics, engineering, or a related field
5-10 years of experience required.
Experience with designing and maintaining data warehouses and/or data lakes with big data technologies such as Spark/Databricks, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience in building data pipelines and deploying/maintaining them following modern DE best practices (e.g., DBT, Airflow, Spark, Python OSS Data Ecosystem)
Knowledge of Software Engineering fundamentals and software development tooling (e.g., Git, CI/CD, JIRA) and familiarity with the Linux operating system and the Bash/Z shell
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Basic familiarity with BI tools (e.g., Alteryx, Tableau, Power BI, Looker)
Expertise in ELT and data analysis, SQL primarily
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data

WORKING CONDITIONS:
Office Environment

Pay and Benefits:

The pay range for this position is $90k to $110k annually based on qualifications and experience. This role is also eligible to participate in the annual incentive plan with a target incentive of 15% of your base annual salary. Full-time employees are offered benefits including health/RX, dental and vision insurance; flexible and health spending accounts (FSA/HSA); short and long-term disability coverage, supplemental life insurance; 401(k); paid time off and holiday pay for Company designated holidays",100000,10000+ Employees,Company - Public,Retail & Wholesale,Wholesale,1894,$10+ billion (USD),TX,129,data engineer,na,"['sql', 'shell', 'python']","['databricks', 'snowflake', 'azure', 'redshift']","['looker', 'tableau', 'power bi']","['snowflake', 'dbt']",['spark'],['bash'],bachelor,
National Grid,3.9,"Waltham, MA",Senior Data Engineer,"Senior Data Engineer
Location: Waltham, MA, US, 02451
Division: Global Head IT Service Delivery
Job Type:
Requisition Number: 44337
Department:
Job Function: Information Technology
About us

Come be a part of driving National Grid’s digital transformation! We are digital creators, continuous learners and daring innovators. We leverage digital innovative ways to create products and catalyze the transformation of National Grid's business units into more agile and digitally native organizations in our shared purpose of bringing energy to life. We need you
Job Purpose

This is an exciting opportunity to design and develop highly scalable and extensible data pipelines that enable collection, storage and distribution, modeling and analysis of large data sets from a variety of channels.
What you'll do

Build out new data integrations including APIs to support continuing increases in data volume and complexity
Develop, test, document and support scalable data pipelines.
Establish and follow data governance processes and guidelines to ensure data availability, usability, consistency, integrity, and security
Build and implement scalable solutions that align to our data governance standards and architectural road map for data integrations, data storage, reporting, and analytic solutions
Design, implement, and automate deployment of our distributed system for collecting and processing streaming events from multiple sources
Build out new data integrations including APIs to support continuing increases in data volume and complexity.
Establish and follow data governance processes and guidelines to ensure data availability, usability, consistency, integrity, and security.
Build and implement scalable solutions that align to our data governance standards and architectural road maps for data integrations, data storage, reporting, and analytic solutions.
Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making across the organization.
Design and develop data integrations and a data quality framework. Write unit/integration/functional tests and document work.
Design, implement, and automate deployment of our distributed system for collecting and processing streaming events from multiple sources.
Perform data analysis needed to troubleshoot data-related issues and aid in the resolution of data issues.
Guide and mentor junior engineers on coding best practices and optimization.
What you'll need

5+ years of relevant work experience in data engineering, BI or related field
Must have strong experience with Python, PySpark and SWL
Strong experience with Databricks & Snowflake
Familiarity with any cloud platform GCP, AWS, or Azure.
A Bachelor's or more advanced degree is great, but not required
More Information

Our organization follows a hybrid work structure in our service territory (NY & MA and adjacent states) where employees can work remotely or from the office, as needed. Working from the office is encouraged when working on tasks that require a high degree of collaboration. We work with our employees to foster a work schedule that fits your flexible schedule.

At National Grid, we keep the lights on and homes warm. But it’s so much more than that. We keep people connected and society moving. This is no easy feat, and it takes all of us. But National Grid supplies us with the environment to make it happen. As we generate momentum in the energy transition for all, we don’t plan on leaving any of our customers in the dark. So, join us and help bring energy to life.

#LI-Hybrid
#LI-Remote
Salary
$112,000 - $158,000 a year
This position has a career path which provides for advancement opportunities within and across bands as you develop and evolve in the position; gaining experience, expertise and acquiring and applying technical skills. Candidates will be assessed and provided offers against the minimum qualifications of this role and their individual experience.

National Grid is an equal opportunity employer that values a broad diversity of talent, knowledge, experience and expertise. We foster a culture of inclusion that drives employee engagement to deliver superior performance to the communities we serve. National Grid is proud to be an affirmative action employer. We encourage minorities, women, individuals with disabilities and protected veterans to join the National Grid team.


Nearest Major Market: Waltham
Nearest Secondary Market: Boston
Recruitment Advisory",135000,10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1990,$5 to $10 billion (USD),MA,33,data engineer,senior,['python'],"['databricks', 'snowflake', 'aws', 'azure']",[],['snowflake'],[],[],bachelor,
Aspen Insurance Holdings,3.7,"Jersey City, NJ",Data Engineer,"Reference: ASPUS00490
Jersey City, New Jersey
Permanent - Full Time
About us
Since Aspen was founded in 2002, we have become a leading, diversified specialty insurance and reinsurance company. We respond thoughtfully and creatively to find the best outcomes for our clients and business partners through carefully tailored solutions. We believe the way we work is just as important as the work we do, and we are guided by our core values of respect, honesty, trust and professionalism. Aspen is a great place to develop your career offering an exciting and challenging environment where achievement is rewarded.
The role

Aspen’s vision is to be the global reference point for quality in all its markets. To achieve this goal, Aspen has launched our Data and Analytics strategy which introduces a preconnected, 360 view of data to power company wide analytics and embed data focused decision making from every seat. Through the creation of our single version of data truth, we ensure that all data used for decision making is accurate and fit for purpose, building trust in reporting and provide client intimacy.
As a cloud data engineer you will have a passion for sourcing data of all types and building high quality, scalable data pipelines creating out single version of data truth to power analytics across the enterprise. Partnering with our internal leaders, users and clients, you will design and integrate data from customers, external data providers and our internal applications to create a 360-insurance data platform which will be the foundation for our ML/AI services and power the delivery of all insights used for decision making. As a member of our engineering unit, you will support the design and implementation of data integration capabilities making the most use of our Cloud Tech data services. You will have the autonomy to explore and find innovative ways or delivering our data landscape utilizing the tech and data we have access to.
If you have a real drive to make a difference, add value for our clients in an industry that creates resilience and sustainability globally then this is an opportunity that you don’t want to miss out on.

Key accountabilities
Contribute to Aspen's Data & Analytics Strategy to embrace tech and data to support our user’s demand for insights and informed decision making
Contributes to functional strategy and prioritizes deliverables to support delivery of business targets
Manages tactical plan/support to others to achieve positive results for business in line with strategy.
Ensure data pipelines are scalable, repeatable, and secure across enterprise
Explains technical considerations at meetings, including those with internal clients and less experienced team members.
Tests code thoroughly for accuracy of intended purpose.
Reviews end-product with client to ensure adequate understanding of data assets you and other data engineers are delivering.
Co-Create coding and delivery standards embedding a focus across the engineering unit on consistent, high quality data pipelines and access
Experience with integrating large scale data from a variety of sources for business partners to generate insight/decisions.
Translates business specifications into design specifications and code.
Ensures all code is well structured, includes sufficient documentation, and is easy to maintain and reuse.
Gains expertise in tools, technologies, and applications/databases in specific business areas and company-wide systems.
Create awareness across the business of data made available in Single Version of Truth
Works with key stakeholders/business managers to encourage adoption of single version of data truth.
Demonstrate extensive experience in building data pipelines in both Data Warehouse and Data Lake environments
Apply experience in working within cloud-based data infrastructure most notably Azure including tools such as Data Factory, Data Bricks
Apply experience coding in both python and SQL in spark-based environments
Support the design of our single version of data truth data model
Working with the Data Leadership Team, ensure Data Integration Framework for our single version of data truth is designed, supported, and managed in accordance with business needs
Skills & experience
Good knowledge of Data Governance and Data Architecture processes and standards
Knowledge and awareness of technology services which could the data and analytics space
Deep knowledge of data modelling for analytics – i.e. Data Warehousing, Data Lakes, Data Mesh architecture
Aware of data science methodologies and frameworks
Excellent understanding of Agile frameworks and processes
Knowledge of insurance industry’s processes
Sound knowledge of problem analysis, structure analysis, and design techniques
Strong understanding of underlying needs of the business and how own role contributes to these.
Strong coding experience using python, SQL
Experienced in building integration frameworks focused on reusability and consistency across data engineering
Experienced in creating engineering standards and monitoring the adoption and benefits across the engineering unit.
People management – ability to engage and lead a team.
Able to execute within agile processes, tracking capacity and deliverables in collaboration with team members
Resource and budget management
Specific professional qualifications at the level of degree or equivalent within topics such as computer or data science , information systems or similar
At least 5 years of experience of data engineering and design experience in Microsoft onprem, Microsoft Azure and its related data and analytical tools (ADF, DataBricks, PowerBI), Azure DevOps.
Experience within Financial Services (especially Insurance)
Evidence of supporting technical and operational strategy.
Working as part of a senior team within a complex organization (preferably within financial services industry)
Hands-on experience with computer networks, network administration and network installation.
Other
We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.
Apply now
Share",109532,1001 to 5000 Employees,Company - Private,Insurance,Insurance Carriers,2002,$1 to $5 billion (USD),NJ,21,data engineer,na,"['sql', 'python']","['databricks', 'azure']",[],[],['spark'],[],,5-10 years
