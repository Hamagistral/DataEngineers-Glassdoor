company,company_rating,location,job_title,job_description,salary_estimate,company_size,company_type,company_sector,company_industry,company_founded,company_revenue,job_state,company_age,job_simp,seniority,job_languages,job_cloud,job_viz,job_databases,job_librairies,job_education,job_experience
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
Apple,4.2,"Cupertino, CA",Data Engineer,"Summary
Posted: Dec 22, 2021
Weekly Hours: 40
Role Number:200327520
As part of our Video Engineering group, you’ll help deliver creative solutions to various problems that could impact the people all over the world. This Data Engineer will work closely with other members of the Video Engineering group to mine data, implement model evaluation pipeline, analyze large scale data, visualize data, and ensure the delivery is of the highest quality. This position will also require strong coding skills, presentation skills, and collaborating with multiple teams (ex: machine learning, cloud infrastructure support).
Key Qualifications
A curious mind
An obsession for quality
Background in Data science, Data mining, Multivariate statistics, Computer vision, Machine learning
Experience working with large scale data sets
Solid programming skills including:
Python
C/C++
Experience with data visualization and presentation, familiar with data analysis tools such as Tableau
Excellent problem solving and communication skills
Description
The responsibilities of this position includes the following for current and future products: - Implement algorithm evaluation methods - Analyze data and build data analysis tools - Deep-dive failure analysis - Discover new perspectives for old data - Produce / Present meaningful data visualization to higher-ups and across various involved teams
Education & Experience
Masters in Computer Science or relevant experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $50.72 and $76.44/hr, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",106385,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1976,$10+ billion (USD),CA,47,data engineer,na,['python'],[],['tableau'],[],[],,
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
Jane Street,4.4,"New York, NY",Data Engineer,"About the Position
We are looking for a Data Engineer who can help us understand, clean, manage, and share the data that guides our trading. At Jane Street, having a thorough and accurate understanding of data is at the core of the work we do.
Using our mix of in-house and open-source software, you will analyze datasets gathered from a variety of sources, checking for anomalies, matching formats and symbologies, automating ETL processes, and generally making it easier for our traders to generate valuable insights.
You should be excited about digging deep into datasets and explaining your findings to different types of colleagues, working collaboratively with traders and software engineers.
While prior experience with financial data would be nice, we don’t expect you to have a finance background. We’re happy to hire talented engineers and teach them what they need to know.
About You
Top-notch programming skills in any language (Python a plus)
Experience with using SQL and relational databases
Experience with generating data visualizations
Knowledge of statistical techniques, including multivariate regression and time series analysis
Clear and concise communication skills; able to efficiently analyze and deconstruct technical problems
Fluency in English required
Base salary is $175,000 - $300,000. Base salary is only one part of Jane Street total compensation, which includes an annual discretionary bonus.
Jane Street is an Equal Opportunity Employer",237500,1001 to 5000 Employees,Company - Private,Management & Consulting,Research & Development,2000,Unknown / Non-Applicable,NY,23,data engineer,na,"['python', 'sql']",[],[],[],[],,
Steward Health Care,2.7,"Westwood, MA",Data Engineer,"Position Purpose:
Reporting to the Manager of the Data Warehouse team, part of the larger Health Informatics group, the data engineer applies their technical expertise to meet the needs of the department and Steward Health Care Network (SHCN).

Key Responsibilities:
ETL/Automation
Design configurable data process flows with full automation
Develop ETL processes for data loading and data extraction
Schedule ETL processes for full process automation
Data Engineering
Responsible for data analysis to support building data processes and reporting
Design useful and accurate data marts that meet requirements
Apply SQL skills when designing and building data marts and data flows
Quality
Establish and utilize QC processes to ensure data integrity
Incorporate standard error logging and alerts to ensure data is loaded as expected
Documentation
Create and maintain clear documentation

Education / Experience / Other Requirements


Education:
Bachelor's degree in Computer Science, Mathematics, Statistics or related experience


Years of Experience:
5+ years of database related work
2+ years of focus on healthcare data

Specialized Knowledge:
Knowledge of healthcare data
Experience using relational databases, SQL Server experience preferred
Experience using ETL tools (SSIS, Informatica, etc.)
Strong SQL programming skills
Experience with scripting languages (PowerShell, R, Python, etc.)
Experience automating data flows
Experience with Health Catalyst tools preferred, but not required
Deep understanding of database structures and data design.
Creative, flexible, and self-motivated with sound judgment
Strong communication skills




Location: Steward Health Care Network · 1301.72330 Steward Health Care Network
Schedule: Full Time, Day Shift, 40 hours",94536,10000+ Employees,Hospital,Healthcare,Health Care Services & Hospitals,1998,Unknown / Non-Applicable,MA,25,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],['sql server'],[],bachelor,+10 years
"Twitch Interactive, Inc.",3.8,"San Francisco, CA",Data Engineer,"3+ years of experience in data engineering, software engineering, or other related roles. 3+ years in relational database concepts with a solid knowledge of star schema, SQL, SQL Tuning, OLAP, Big Data technologies 3+ years of experience in generating and maintaining data pipelines from various data sources, in collaboration with diverse stakeholders. 3+ years of experience working with Amazon Webservices, S3, EMR, Redshift etc. Experience with best practices for development including query optimization, version control, code reviews, and documentation. Experience with coding languages like Python/Java/Scala
About Us: Twitch is the world's biggest live streaming service, with global communities built around gaming, entertainment, music, sports, cooking, and more. It's where millions of people come together to chat, interact, and make their own entertainment. We're about community, inside and out. You'll find coworkers who are eager to team up, collaborate, and smash (or elegantly solve) problems together. We're on a quest to empower live communities, so if this sounds good to you, see what we're up to on LinkedIn and Twitter, get interviewing tips on Instagram, and discover projects we're solving on our Blog. About the Role: Data is central to Twitch's decision-making process, and data engineers operate at the forefront of this by creating authoritative datasets that drives analysis and decision-making across all of Twitch. In this role you will be shaping the way that business performance is measured, defining how we transform our data, and scaling analytics methods and tools to support our growing business, leading the way for high quality, high velocity decisions. For this role, we're looking for an experienced data engineer to join our Content Data Science team, which is focused on empowering staff throughout Twitch to use and trust our business data. Your responsibilities may range from developing and enhancing our data warehouse which act as authoritative sources of truth across the company, driving data quality and trustworthiness across product verticals and business areas, building self-service business intelligence infrastructure for analysts, as well as connecting into data interfaces that enable everyone in Twitch to discover and analyze the data. In the process, you will have the opportunity to interact with technical and non-technical staff members throughout the company, and will report to the Director of Content Data Science. This position can be located in San Francisco, CA; Irvine, CA; Seattle, WA; New York, NY; and Salt Lake City, UT. You Will: Define and own team level data architecture for trusted, governed, dimensionally-modeled repository of data that enables Twitch staff to quickly and reliably answer their business questions. Keep existing data sources fresh against data quality issues, design, develop and maintain data quality assurance framework and continuously improve the processes for developing new ones raising the level of quality expected from our work. Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost. Improve search, discovery and literacy: Create exploration and visualization interfaces in our BI tools and evangelize the adoption of these sources across the company through education and training programs. Improve business and engineering team processes via data architecture, engineering, test, and operational excellence best practices. Make enhancements that improve data processes.
Bonus Points
A passion for data science and interest in growing / learning data science, machine learning at scale.
A passion for games and the gaming industry
Perks
Medical, Dental, Vision & Disability Insurance
401(k)
Maternity & Parental Leave
Flexible PTO
Amazon Employee Discount
Monthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages, etc.)
We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
Pursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $105,700/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. Applicants should apply via our internal or external career site.",105700,10000+ Employees,Company - Public,Information Technology,Internet & Web Services,1994,$10+ billion (USD),CA,29,data engineer,na,"['python', 'scala', 'sql', 'java']",['redshift'],[],[],[],,+10 years
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Nike,4.1,"Boston, MA",Data Engineer,"Become part of the Converse Team

Converse is a place to explore potential, break barriers and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Converse, it’s about each person bringing skills and passion to a challenging and constantly evolving world to make things better as a team.
Converse, Inc. Boston, MA. Work closely with Project Management and Business teams to completely define specifications to ensure the project acceptance. Involved in preparation of functional and technical specifications with different cross teams. Lead team, defining solution options, providing estimates on effort and risk, and evaluating technical feasibility in Agile development process, including Scrum and Kanban. Work on troubleshooting data and analytics issues and perform root cause analysis to proactively resolve issues. Develop data extracts and feeds from the full spectrum of systems in the Converse ecosystem, including transactional ERP systems, POS data, product and merchandising systems. Engineer data products for a variety of Operations analytics use cases, ranging from reporting and data visualization to advanced analytics/machine learning use cases. Support designing technical specifications and data transformation models for junior developers. Ensure development is on track and meets specifications as defined by product management and the business. Responsible for data integrity of current platform and QA of new releases. Support the development and maintenance of backlog items and solution feature. Participate in sprint planning activities from a development perspective. Responsible for designing cloud-based data architecture using AWS stacks. Design and develop Python data science and data engineering libraries dealing with structured and unstructured data. Work with a variety of database types (SQL/NoSQL, columnar, object-oriented) and diverse data formats. Responsible for ETL with Spark and building data pipelines/orchestrations in Airflow and working on ETL tools like Matillion. Responsible for DevOps toolchain and Continuous Development, Continuous Integration and Automated Testing using Jenkins. Ensure and use data engineering for advanced analytics/data science and Software development skills.
Applicant must have a Bachelor’s degree in Computer Science, Information Systems, or Information Technology and 5 years of progressive post-baccalaureate experience in the job offered or a related occupation. Experience must include:
Data warehousing;
ETL or ELT;
Amazon Web Service (AWS) Cloud Services, including AWS S3, AWS Lambda, AWS EC2, AWS EMR or AWS DynamoDB;
Relational Database Management Systems (RDBMS), such as Oracle, Teradata, SQL Server or Snowflake;
Database Development with writing stored procedures, functions, triggers, cursors or SQL queries;
Hadoop, HDFS, Hive or Spark;
Programming languages, including Java or Python;
Business Intelligence Tools, such as Tableau;
Unix Shell scripting; and
Version control systems, such as Git, Bitbucket or Github
#LI-DNP
Converse is more than a company; it’s a worldwide advocate for self-expression. This belief motivates our employees, permeates our working environment and inspires our products. No two of us look or think exactly alike. We are each one-of-a-kind. Individually and as a culture, we have the freedom to create and grow professionally. Generous benefits packages only sweeten the experience. From Boston to Shanghai, from Brand Design to Finance, Converse is a brand that celebrates the unique and creative people of the world. Together, we’re different.",115797,10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1972,$10+ billion (USD),MA,51,data engineer,na,"['sql', 'shell', 'python', 'java', 'nosql']","['snowflake', 'oracle', 'aws']",['tableau'],"['dynamodb', 'hive', 'sql server']","['airflow', 'hadoop', 'spark']",bachelor,
Aretec Inc,1.0,Remote,Junior Data Engineer,"POSITION TITLE: Junior Data Engineer YEARS OF EXPERIENCE: 1-3
LOCATON: 100% remote
*****Please Note: Aretec, Inc. does not offer Corp - 2 - Corp (C2C) employment. *****
Aretec is looking for a Junior Data Engineer. The Junior Data Engineer will be primarily responsible for design, development, support and enhancement of the data pipelines developed in AWS.

RESPONSIBILITIES:
You'll write clean and functional code on the front- and back-end
You'll write reusable and maintainable code
Coordinate with data migration plans
Ability to communication and collaborate with various teams and vendors.
Participates in functional and technical design.
Participation in Agile activities Scrum, Kanban.
Ensure coding, testing, debugging and implementation activities completed as required.
Flexible and adaptable with the ability to align to changing priorities
The developer should have great communication skills and be able to discuss and develop requirements with multiple levels of staff from corporate and field locations
An interest in and ability to understand financial reporting, accounting concepts and related accounting data
Participate in data flow diagramming and/or process modeling (code architecture)
Documents work and steps to completion as required
Follows AWS best practices to integrate with ecosystem and infrastructure
Ability to partner with domain architects to implement the defined solution architecture including application, infrastructure, data, integration, and security domains

REQUIRED SKILLS:
1-3 years of software engineering experience
1+ years of real industry experience
Experience with website development, web services and API development
Hands-on experience performing data engineering and transformation tasks using Python
Experience implementing backend in Python using frameworks such as Django or Flask
Knowledge of web technologies - both back and front-end development including, but not limited to JavaScript, React, CSS, HTML, T-SQL, and Python
Understand log monitoring and analytics
Experience Meeting both technical and consumer needs
Experience testing software to ensure responsiveness and efficiency
A general knowledge of index migrations, debugging and researching concepts are major pluses
Must be aware of CI/CD pipelines and well-versed in using GitLab for creating required pipelines for CI/CD

EDUCATION: Bachelors Degree in Mathematics/Statistics/Technology/Science/Engineering/Applied Mathematics or related field

CERTIFICATIONS: N/A",102500,51 to 200 Employees,Contract,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']",['aws'],[],[],[],,+10 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Adobe,4.4,"New York, NY",Data Engineer,"Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

Job Description
Adobe Customer Solutions is looking for a full time Data Engineer with experience in building data integrations using AWS technology stack as part of the team's Data as a Service portfolio for Adobe’s Digital Experience enterprise customers.
Customer facing Engineers who enjoy tackling complex technical challenges, have a passion for delighting customers and who are self-motivated to push themselves in a team oriented culture will thrive in our environment
What you'll Do
Collaborate with Data architects, Enterprise architects, Solution consultants and Product engineering teams to gather customer data integration requirements, conceptualize solutions & build required technology stack
Collaborate with enterprise customer's engineering team to identify data sources, profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating customer data sources and third party data sources with Adobe solutions
Develop new features and improve existing data integrations with customer data ecosystem
Encourage team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Collaborate with a Project Manager to bill and forecast time for customer solutions
What you need to succeed

Proven experience in building/operating/maintaining fault tolerant and scalable data processing integrations using AWS
Proven track record in Python programming language
Software development experience working with Apache Airflow, Spark, MongoDB, MySQL
Experience using Docker or Kubernetes is a plus
BS/MS degree in Computer Science or equivalent proven experience
Ability to identify and resolve problems associated with production grade large scale data processing workflows
Excellent interpersonal skills
Experience crafting and maintaining unit tests and continuous integration.
Passion for crafting I ntelligent data pipelines that customers love to use
Strong capacity to handle numerous projects are a must
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists. You will also be surrounded by colleagues who are committed to helping each other grow through our outstanding Check-In approach where feedback flows freely.
If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the significant benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age, sexual orientation, gender identity, disability or veteran status.

Our compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $101,500 -- $194,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.",147900,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1982,$5 to $10 billion (USD),NY,41,data engineer,na,['python'],['aws'],[],"['mysql', 'mongodb']","['airflow', 'spark']",,
Glow Networks,3.5,"Dallas, TX",Data Engineer,"Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.",131400,51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD),TX,20,data engineer,na,"['python', 'scala', 'sql', 'java']","['snowflake', 'databricks', 'aws', 'redshift']",[],['hive'],"['kafka', 'airflow', 'hadoop', 'spark']",bachelor,+10 years
Enterprise Knowledge LLC,4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”",89080,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD),VA,10,data engineer,na,"['python', 'r', 'sql']",['aws'],[],[],"['airflow', 'spark']",,2-5 years
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,['sql'],"['oracle', 'azure']","['power bi', 'ssis']",['sql server'],[],bachelor,
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
Small Batch Standard,4.1,Remote,Junior Data Engineer,"We're the premier, remote accounting, tax, and consulting firm built exclusively to serve the craft brewing industry.
Our mission is to help craft breweries grow profits and build deep successful relationships. And our team is filled with expert, autonomous, adaptable, technology-driven high performers.
Are you up for the challenge?
We're looking for a full-time, remote Junior Data Engineer to join our specialized team. The main objective of this role is to design, develop, implement, and improve both internal and external applications to support our brewery clients and team in accordance with the SBS Core Values.
About The Role
This role will report to our Technology/Product Manager and is accountable for fulfilling the following responsibilities:
Building our data pipeline and analysis applications. A key aspect of the consulting service we provide to clients involves the collection, aggregation, analysis, modeling, and usage of financial data and benchmarks. We use this data both internally to develop and inform strategy, as well as externally through our Benchmarks Assessment (https://sbstandard.com/assessment/) and Compass analysis product (https://sbstandard.com/levelup-compass/). You'll be responsible for working with our team to build out our data pipeline for these tools, and progressively increasing our ability to aggregate, analyze, query, and feed back this data into our reporting, analysis, and consulting work. Platforms we're building with include: SQL, Airflow, Excel Visual Basic for Applications (VBA), Google Apps Script, Intuit/QuickBooks Online.
New process and technology R&D. We're always looking for new opportunities to provide both our team and our clients access to additional tools that give them leverage, automate and streamline processes, and overall make work more efficient. Part of your time will be dedicated to researching, testing, and prototyping new tech and application options.
Participate and contribute to the overall success of our team. Each week the team meets to share wins, progress, and knowledge, as well as identify and solve issues at multiple levels (company, team, individual). Your full participation in this process is critical to ensure that we are operating as a cohesive, high-performance unit.
About You
We're looking for an individual who:
Is a problem solver through-and-through. Everywhere you look, you both (a) see problems to solve, and (b) see solutions and new ways of doing things that just haven't been done yet. You know how to think outside of the box, are willing to “go there” with new ideas and solutions that haven't been done before, and have the confidence to start building, testing, iterating, and making sh*t work.
Is a systems thinker. You understand both the big picture and how the functional components fit together, and have the ability to take a specific analysis outcome and generalize it to fit a wide range of scenarios through structure and sound system design.
Can fail fast, iterate, and learn. You're an independent, self-directed, learner who isn't afraid to “move fast and break stuff” knowing that failure is a prerequisite to success, ESPECIALLY in product development. You may not have traditional credentials, but what you do have is the ability to rapidly learn, adopt, test, and understand new languages, platforms, tools, and solutions.
Is a manager of one. Unlike working within a traditional firm, in this role you'll be in the driver's seat, managing your workflow and workload in order to meet the standard set of deliverables required for each client.
About Our Culture
We're fully remote, with team members and clients located all across the U.S. and have developed our own unique culture we call The SBS Way, within which we operate, evaluate performance, and make decisions using our core values as a guide:
Be Antifragile. Everything we do, good or bad, makes us better. And every experience is an opportunity for learning and continuous improvement.
Play The Long Game. We make decisions, to the best of our ability, in the long-term interest of our firm, our team, our clients, and our broader industry and community.
Embrace Technology. We welcome new technologies with open arms, and are always exploring, testing, and implementing them in the interest of enhancing both our internal capabilities and our client's outcomes.
Build and Trust The Process. Each member of the team is committed to building, following, and improving the processes we use to deliver exceptional results for our clients.
Act as A Team of Expert Knowledge Workers. We openly and willingly collaborate, communicate, and provide rapid, direct feedback in the interest of learning, improving and developing ourselves.
Working At SBS
What it's like working at our firm:
High flexibility. We believe in the ability of our team to determine the best way to complete their work. We measure outputs, not inputs. We don't have time sheets. We don't track hours. We don't pay attention to when and where our team works. Your schedule is yours to make.
High accountability. What we care about most is that we deliver on what we promise to our clients. In this respect, we measure and manage to our deliverable performance metrics and ensure each team member takes ownership over their accomplishment with a high level of quality that aligns with our core values
Great pay for great work. We pay based on the characteristics that matter: position (and its market value), level of mastery, and longevity with the firm. All of which aim to ensure each member of the team feels they are compensated well and can focus on great work.
Merit-based career progression. We have clearly established career tracks, performance benchmarks, and mastery levels set for all of our core positions. How quickly you progress is entirely under your control, with a quarterly review and bi-annual promotion consideration cycle in place to evaluate your progress.
Generous benefits. We offer a generous benefits package that includes medical, dental, and vision insurance enrollment; as well as an IRA match, tech stipend, 3 weeks of paid time off, and entry into our profit share bonus program after two years of service.
Personal and and team development. In addition to our overall continuous learning focus, we also provide support for personal development in the form of expense coverage for continuing education (books, courses, training, certifications, etc.) as well as experiential learning (brewery visits, industry events and conferences, etc.). Each year we also meet in person for an all-expenses-paid annual retreat as a team. No work. Lots of fun. Lots of client beer.
Job Requirements
The following basic requirements must be met:
Previous experience in SQL development and database management.
Previous experience building useful applications in scripting languages like VBA, Google Apps Script, Python, PHP, etc.
Can do effective cross-functional work in a remote environment.
Have crystal clear professional written and verbal communication skills.
Have exacting organizational standards and a calm and friendly attitude.
Available and responsive during normal business hours (9am-5pm Eastern Time, Monday-Friday).
Have a strong, consistent internet connection and a work environment conducive to video calls.
Preferred qualifications include:
Direct previous experience building data pipelines.
Direct previous experience building Airflow workflows and applications.
Experience building out and managing API connections.
Experience working with Quickbooks Online or similar accounting or finance platforms.
Experience using Podio or similar remote project management tools (e.g. Trello, Asana, etc.).
Next Steps
If the position, culture, values, and mission at Small Batch Standard sound like they're the right fit for you, please apply here.",64000,1 to 50 Employees,Company - Private,Financial Services,Accounting & Tax,2010,Unknown / Non-Applicable,Remote,13,data engineer,na,"['r', 'python', 'sql', 'go']",[],['excel'],[],['airflow'],,
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['python', 'sql']",[],[],[],['spark'],bachelor,0-2 years
Tripoint Solutions,4.5,Remote,Data Engineer (Remote),"Tripoint Solutions is seeking a Data Engineer to join our team.
The Data Engineer will be part of a team responsible for ensuring the success of a highly visible, results-driven federal client through the development of a cloud-based next generation system.
This position requires the applicant to parse disparate data sources, including structured and unstructured elements, to find the patterns and meaning in large quantities of data. The successful candidate will leverage machine learning as well as best of breed pipeline technology to process and store a variety of data elements.
Location: This position is eligible for fully remote work. Selected candidates living within a 25 miles radius of the NITAAC office in Rockville, MD will be required to come into the office once a week. The selected candidate must be currently located in, or willing to relocate to, a state supported by Tripoint Solutions corporate offices (AL, DC, FL, IL, LA, MD, MI, MN, MS, NJ, NC, PA, TN, TX, or VA).
The successful candidate will be accountable to:
Creating and maintaining optimal data pipeline architecture.
Assembling large, complex data sets that meet functional / non-functional business requirements.
Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Building the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Keeping data separated and secure across national boundaries through multiple data centers and AWS regions.
Strong interest to learn and stay up to date on relevant technologies, trends, industry standards and identify new ones to implement.
What you bring
Experience, Education & Training:
Bachelor's degree in computer science, Math, Analytics, Statistics, Informatics, Information Technology or equivalent quantitative field.
5 years of experience working in a Data Engineer or Data Scientist role.
Experience with cloud data services (AWS preferred).
Experience solutioning and applying Natural Language Processing (NLP) and or Machine Learning (ML) technologies
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience with Microsoft SQL, database development and design.
Experience building processes supporting data integration, transformation, data structures, metadata, dependency and workload management.
Demonstrated success in manipulating, processing and extracting value from large disconnected data sets.
Demonstrated accomplishments in designing, coding, testing and supporting data analytics and reporting solutions in a cloud environment.
Experience with object-oriented/object function scripting languages: Python, Java
Concept experience; information retrieval, search engine, document data extraction
Preferred experience with AWS cloud services: Textract, Comprehend, GlueMaker, Athena, Notebook
Working knowledge of message queueing, stream processing, and highly scalable ‘big data’ data stores.
Clearance Requirements:
Applicants selected may be subject to a government security investigation and must meet eligibility requirements for potential access to classified information. Accordingly, US Citizenship or Green Card is required.
What we offer
About Tripoint Solutions
We are technology innovators, partnered with state-of-the-art providers, such as AWS, ServiceNow, and UiPath, to drive digital transformation in the federal space. TPS teams are bringing automation and data science into areas of the government that are crying out for fresh tech—making positive impacts felt by tens of thousands of users, countless citizens, and all six branches of the military each day. Our Agile teams are responsible for envisioning, launching, and operating the massive data systems and analytics platforms used to manage $14.5B in government procurements and $200B in military real estate assets globally. At TPS, we apply the power of cloud technologies to help the government think smarter and function better—for everyone.
TPS Company Values
We value and respect each employee's dedicated work and unique contributions; as they directly impact who we are and what we do.
Your talent and innovative thinking bring leading-edge solutions to our customers.
Our success is driven by the dedication of our employees.
Employee-generated solutions have sustained our continued success and customer satisfaction
Benefit Offerings
Tripoint Solutions builds flexibility into health benefit plan choices, covers most of the monthly premiums, and helps employees build a career with impact through our generous professional development program.
We offer all full-time employees:
Medical, Dental, Vision benefits with a national provider network (company pays 100% of Vision and Dental premiums)
Flexible Spending and Health Savings Accounts (FSA & HSA)
Company-paid Life and Disability insurance including Short-Term, Long-Term, and Accidental
Paid-time off (PTO), accruing with each year of service, up to 20 days, plus 11 paid holidays
401(k) Retirement Plan - No waiting period to contribute and company makes 3% contribution of eligible pay in addition to annual profit-sharing contribution option
Eligibility to receive impact bonuses each quarter
Referral Program
Professional Development Reimbursement Program to pursue undergraduate, graduate, training, and certifications
Monthly transportation, parking, and cell phone service reimbursement
COVID-19 Related Information
Tripoint Solutions does not have a vaccination mandate applicable to all employees. However, to protect the health and safety of its employees and to comply with customer requirements, Tripoint Solutions may require employees in certain positions to be fully vaccinated against COVID-19. Vaccination requirements will depend on the status of the federal contractor mandate and customer site requirements. Furthermore, remote work arrangements are subject to change based on customer site requirements.
Tripoint Solutions is an Equal Opportunity Employer/Veterans/Disabled
Job Type: Full-time
Pay: $145,000.00 - $155,000.00 per year
Benefits:
401(k)
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Retirement plan
Tuition reimbursement
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
What cloud services have you worked with?
Do you have experience with Machine Learning or NLP?
Does the advertised salary align to your expectations?
US citizenship or green card is required. Do you meet this requirement?
This is a remote position (See description for details and requirements). Where are you located?
Are you willing to undergo a federal background check?
Education:
Bachelor's (Required)
Experience:
data scientist or data engineer role: 5 years (Required)
cloud services: 2 years (Required)
Microsoft SQL (development and design)?: 2 years (Required)
optimizing ‘big data’ pipelines, architectures and data sets: 2 years (Required)
AWS: 1 year (Preferred)
Python: 1 year (Required)
Java: 1 year (Required)
Work Location: Remote",150000,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,Unknown / Non-Applicable,Remote,10,data engineer,na,"['python', 'sql', 'java']",['aws'],[],[],[],bachelor,0-2 years
Angle Health,4.0,Remote,Data Engineer,"Changing Healthcare For Good
At Angle Health, we believe the healthcare system should be accessible, transparent, and easy to navigate. As a digital-first, data-driven health plan, we are replacing legacy systems with modern infrastructure to deliver our members the care they need when they need it. If you want to build the future of healthcare, we'd love for you to join us.

The Role
As a Data Engineer on the Strategy team at Angle Health, you will be part of an elite team of problem solvers tackling some of the hardest operational challenges across the business. You will be working closely with Technical Product Managers and other engineers in small teams embedded within functions across the company—from sales to operations to finance and more.

In partnership with operational stakeholders, your job will be to quickly understand business workflows, and design and implement technical, data-driven solutions to achieve the intended business outcomes. That may include architecting backend micro-services, building and maintaining robust data pipelines, standing up health metrics and automated alerts, and developing scalable technical solutions to support Angle Health's day-to-day operations.

Every day will be a new learning experience in this role and may span discussing architecture with fellow engineers, wrangling large-scale data, coding a custom web app or micro-service, or speaking with customers and executives.

This role is ideal for entrepreneurial engineers, applied data scientists, and creative, intellectually curious thinkers that want to solve high-value problems in healthcare.

This position may be based in San Francisco, New York City, Salt Lake City, or Remote.

We are currently trialing various titles for the same role. Please consider the following posted roles as the same position: Deployed Engineer, Software Engineer, Business Operations, and Data Engineer. Please note these are the same, so if you have already applied for one position, there is no need to reapply for the others.
What We Value
A strong engineering background in computer science, software engineering, data science, mathematics, or similar technical field is required for this role
Proficiency in programming languages (e.g. Python, SQL, Java, TypeScript/JavaScript, or similar) and data engineering frameworks
A highly analytical mindset and an eagerness to build technical solutions to complex business problems
High attention to detail and intellectual curiosity—you're not satisfied with surface-level answers. You want to dive into the data, the ""how,"" and the ""why"" because ""the way it's always been done"" is not always the way it should be done
Low ego—the outcome matters more than who gets the credit
Demonstrated ability to collaborate effectively in teams of technical and non-technical individuals
Highly organized with an ability to multitask, problem solve, and balance competing priorities in a rapidly changing environment
Because We Value You:

Competitive compensation and stock options
100% company paid comprehensive health, vision & dental insurance for you and your dependents
Supplemental Life, AD&D and Short Term Disability coverage options
Discretionary time off
Opportunity for rapid career progression
Relocation assistance (if relocation is required)
3 months of paid parental leave and flexible return to work policy (after 10 months of employment)
Work-from-home stipend for remote employees
Company provided lunch for in-office employees
401(k) account
Other benefits coming soon!

Backed by a team of world class investors, we are a healthcare startup on a mission to make our health system more effective, accessible, and affordable to everyone. From running large hospitals and health plans to serving on federal healthcare advisory boards to solving the world's hardest problems at Palantir, our team has done it all. As part of this core group at Angle Health, you will have the right balance of support and autonomy to grow both personally and professionally and the opportunity to own large parts of the business and scale with the company.

Angle Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. Angle Health is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities.",106385,1 to 50 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2019,Unknown / Non-Applicable,Remote,4,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
LOVEFOODIES INC,4.2,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",54000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['python', 'sql']",[],['tableau'],[],[],,2-5 years
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['python', 'sql']",[],[],[],[],master,
"Double Line, Inc.",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",85882,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['python', 'sql']","['azure', 'aws']","['tableau', 'power bi']",[],[],,
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['python', 'sql', 'java', 'nosql']","['azure', 'google cloud', 'aws']",[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,+10 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
AgileEngine,5.0,Remote,Senior/Lead Data Engineer,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Work collaboratively with other engineers, architects, data scientists, analytics teams, and business product owners in an agile environment
Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools
Design and build robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Must haves
Bachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience
6 years of experience with Python or Java
4+ years of experience in building data lake, cloud data platform leveraging cloud (GCP/AWS) cloud native architecture, ETL/ELT, and data integration
3 years of development experience with cloud services ( AWS,GCP,AZURE) utilizing various support tools (e.g. GCS, Dataproc, Cloud Data flow, Airflow(Composer), Kafka , Cloud Pub/Sub)
Expertise in developing distributed data processing and Streaming frameworks and architectures (Apache Spark, Apache Beam, Apache Flink, )
In-depth knowledge of NoSQL database technologies (e.g. MongoDB, BigTable, DynamoDB)
Expertise in build and deployment tools – (Visual Studio, PyCharm, Git/Bitbucket/Bamboo, Maven, Jenkins, Nexus, )
5 years of experience and expertise in database design techniques and philosophies (e.g. RDBMS, Document, Star Schema, Kimball Model)
5 years of experience with integration and service frameworks (e.g API Gateways, Apache Camel, Swagger API, Zookeeper, Kafka, messaging tools, microservices)
Expertise with containerized Microservices and REST/GraphQL based API development
Experience leveraging continuous integration/development tools (e.g. Jenkins, Docker, Containers, OpenShift, Kubernetes, and container automation) in a Ci/CD pipeline
Advanced understanding of software development and research tools
Attention to detail and results-oriented, with a strong customer focus
Ability to work as part of a team and independently
Problem-solving and technical communication skills
Ability to prioritize workload to meet tight deadlines
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
Our team of data experts and engineers has helped create a supply chain management platform used by Forbes 500 enterprises and featured by Gartner. We’ve accompanied the platform from an early prototype to a full-fledged solution based on big data, AI, and machine learning. Data quality is integral to our current engagement with this project as we’re expanding the platform functionality.
AgileEngine has also been working on data and ML solutions in multiple industries and verticals like telecom, FinTech, energy efficiency, and more. Our work with our clients’ data has contributed to patented software products, Nasdaq-grade fraud detection, and products used by Deloitte and T-Mobile.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",106385,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable,Remote,13,data engineer,senior,"['python', 'java', 'nosql']","['azure', 'aws', 'gcp']",[],"['dynamodb', 'mongodb']","['kafka', 'airflow', 'spark']",master,2-5 years
"Dovenmuehle Mortgage, Inc.",2.6,"San Francisco, CA",Data Engineer,"Data Engineer
DMI Software, the San Francisco branch of Dovenmuehle Mortgage, Inc, the leading sub-servicer of mortgage loans in the United States, is looking for a talented and enthusiastic data engineer. We work exclusively in Software Development. Our growing office offers the feel of a startup with the backing and security of a long-established company. We aspire to create elegantly scalable products while fostering the continued growth of each team member. The ideal candidate will have 5+ years relevant experience, including Hadoop Ecosystem or similar, and with a scripting language.

Here we believe that the best software is created by an eclectic set of voices, and we strive to nurture an environment rich in differing opinion, belief, and background. Only in this way can we develop revolutionary products capable of meeting the varied needs of an increasingly interconnected world.

What You’ll Be Doing:
Design, implement, automate, and maintain large-scale enterprise ETL processes
Evolve data model and schema based on business and engineering needs
Oversee systems tracking data quality and consistency
Collaborate with data analysts to bridge business goals with data delivery

Requirements:
5+ years data engineering experience
Highly experienced using Python, SQL and and Hadoop
Excellent communication, analytical and problem-solving skills
Keen attention to detail while keeping an eye toward the big picture
You are comfortable with the nuts and bolts of systems programming in the Linux environment (shell/bash scripting)
Experience working in an Agile environment
Excellent presentation and communication skills
Experience profiling, debugging, tracing, and or parallelizing/optimizing Python code
Ideal candidate is one who can adapt and adopt to our existing architectures while also making impactful improvements and suggestions.

Job Type: Full-time",135927,1001 to 5000 Employees,Company - Private,Financial Services,Banking & Lending,1844,Unknown / Non-Applicable,CA,179,data engineer,na,"['python', 'shell', 'sql']",[],[],[],['hadoop'],,+10 years
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],[],"['airflow', 'hadoop', 'spark']",,2-5 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
BOTG LLC,4.2,"Chicago, IL",Data Engineer,"We are looking for a Data Engineer in Chicago, IL (Hybrid) for a direct-hire position.
Job Description:
Position: Data Engineer - Centralized Data Science and Analytics (CDSA)
Location: Chicago, IL (Hybrid)
Duration: Direct-hire position
Client: Direct Client
Note: This is a W2 direct-hire role. Looking for candidates who are open to work independently on W2.
Requirements:
· Experience building and optimizing ""big data"" data pipelines, architectures and data sets.
· Working knowledge of message queuing, stream processing and highly scalable ""big data"" data stores.
· Advanced working SQL knowledge and experience working with cloud and relational databases.
· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
· Experience building processes supporting data transformation, data structures, metadata, dependency and workload management.
· A successful history of manipulating, processing and extracting value from large, disconnected datasets.
· Experience using the following software/tools:
· Relational SQL and NoSQL databases, including Postgres.
· Data pipeline and workflow management tools.
· Azure cloud services.
· Object-oriented/object function scripting languages: Python, PySpark Java, C++, R/RStudio/RSpark.
· CI/CD systems.
· Strong understanding across cloud and infrastructure components (server, storage, network, data, and applications) and ability to deliver end to end cloud infrastructure, architectures, and designs.
· Knowledge and implementation of enterprise scale cloud security platforms and tooling.
· Experience with enterprise applications, solutions, and data center infrastructures.
· Bachelor's degree in computer science or similar field; master's degree a plus.
· Exceptional product, project and client management skills.
· Azure, AWS or any other cloud/data engineering certifications are preferred.
Job Type: Full-time
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Big data: 5 years (Required)
Advanced SQL: 5 years (Required)
Cloud: 3 years (Required)
CI/CD: 3 years (Preferred)
NoSQL: 2 years (Required)
Work Location: Hybrid remote in Chicago, IL 60606",85894,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,"['sql', 'r', 'python', 'java', 'nosql']","['azure', 'aws']",[],[],[],bachelor,2-5 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
CapitalTech Solutions,4.5,Remote,Data Engineer,"Job Description:
12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
Complete Description:
Require the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals:
(1) Establish a data governance program,
(2) Perform a comprehensive data gap analysis,
(3) Design a master data architecture,
(4) Create a data warehouse for all data assets,
(5) Develop a front-end for program staff to quickly access workforce information and visualize program status,
(6) Create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities
(7) Foster relations with other agencies and improve inter-agency data integration.
The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff.
Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.
Develop and maintain an understanding of the data landscape including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.
Support the Data Management Project team to develop and maintain data quality controls.
Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.
Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.
Support the data stewards to troubleshoot and resolve data issues.
Support business users to obtain requirements for enhancements and/or new analytic assets.
Assist in the Development of data asset training and documentation.
Participate in the development and implementation of a data standard.
Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.
Job Types: Full-time, Contract
Pay: $66.00 - $74.00 per hour
Schedule:
8 hour shift
Experience:
in SQL, Python, R, JavaScript, JSON: 10 years (Preferred)
Agile Testing, Automation Testing, Black-box Testing: 10 years (Preferred)
Windows and Linux: 10 years (Preferred)
of BI tool architecture, Tableau: 9 years (Required)
Work Location: Remote",126000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1999,$25 to $100 million (USD),Remote,24,data engineer,na,"['python', 'sql', 'r']",[],['tableau'],[],[],master,5-10 years
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['python', 'sql']","['gcp', 'aws', 'redshift']",[],[],['airflow'],,
Invictus Data,4.2,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500,,,,,-1,,CA,-1,data engineer,senior,"['python', 'scala', 'sql']","['azure', 'aws', 'gcp']",[],['hive'],"['kafka', 'hadoop', 'spark']",,2-5 years
ITExpertUS,2.8,"Chicago, IL",Data Engineer,"Client: Agilisium
Role: Data Engineer
Location: Chicago, IL – Hybrid role – Need to be onsite once/twice in a month.
Type: Long term contract
Job Description
Title: Data Engineer
We are looking for a savvy Data Engineer to join our growing team. The hire will be responsible for expanding and optimizing our data and pipeline architecture and data flow and collection. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. In addition, the Data Engineer will collaborate and support software developers, implementation architecture, and data engineers on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. In addition, they must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
Responsibilities:
Because we work on a programmatic leading edge of many technologies, we need someone who is a creative problem solver, resourceful in getting things done, and can shift productively to or from working independently and collaboratively. This person would also take on the following responsibilities:
Process unstructured data into a form suitable for analysis.
Support the business with ad hoc data analysis and build reliable data pipelines.
Implementation of best practices and IT operations in mission-critical tighter SLA data
pipelines using Airflow
Query Engine Migration from Dremio to Redshift.
We leverage: Multiple AWS Data & Analytic Services(e.g.,Glue, Kinesis, S3) , SQL (e.g.,
PostgreSQL, Redshift, Athena); NoSQL (e.g., DocumentDB, MongoDB); Kafka, Docker, Spark(AWS EMR and DataBricks), Airflow, Dremio, Qubole, etc.
We use AWS extensively, so experience with AWS cloud and AWS Data & Analytics certification will help you hit the ground running.
Business Skills for Data Engineers:
Creative Problem-Solving: Approaching data organization challenges with a clear eye on what is important; employing the suitable approach/methods to make the maximum use of time and human resources.
Effective Collaboration: Carefully listening to management, data engineers, and architects to establish their needs.
Intellectual Curiosity: Exploring new territories and finding creative and pragmatic ways to solve data problems.
Industry Knowledge: Understanding and perspective of how industry functions and how data can be collected, analyzed, and utilized for value creation; maintaining flexibility in the face of big data developments.
Skills and Qualifications:
3 to 5 years of real-world Data Engineering experience.
Programming experience, ideally in Python and other data engineering languages like Scala
Programming knowledge to clean structure and semi-structure datasets.
Experience processing large amounts of structured and unstructured data. Streaming data experience is a plus.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Background in Linux
Build the infrastructure required for optimal extraction, transformation, and loading of data
from various data sources using SQL and other cloud ‘big data’ technologies like DataBricks, Snowflake, Dremio, and Qubole.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience creating a platform on which complex data pipelines are built using orchestration tools like Airflow, and Astronomer.
Experience with real-time sync between OLTP and OLAP using AWS technologies like real- time sync between AWS Aurora and AWS Redshift.
Job Type: Full-time
Salary: Up to $68.00 per hour
Schedule:
8 hour shift
Work Location: On the road
Speak with the employer
+91 6305048577",122400,501 to 1000 Employees,Company - Private,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,"['python', 'scala', 'sql', 'nosql']","['snowflake', 'databricks', 'aws', 'redshift']",[],['mongodb'],"['kafka', 'airflow', 'spark']",,5-10 years
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master,
Ascent Solutions,4.2,Remote,Hadoop Big Data Engineer,"Hadoop Big Data Engineer
Remote
Must Haves • Strong experience in Big Data, Cloudera Distribution 7.x, Cloud migration, RDBMS • Strong Experience with Amazon EMR/Databricks/Cloudera CDP • 4-5 experience building data pipelines using Hadoop components Sqoop, Hive, Solr, MR, Impala, Spark, Spark SQL., HBase. • 4-5 years of programming experience in Python, Java and Scala is must. • Strong experience with REST API development using Python frameworks (Django, Flask etc.) • Micro Services/Web service development experience using Spring framework is highly desirable.
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,na,"['python', 'scala', 'sql', 'java']",['databricks'],[],['hive'],"['hadoop', 'spark']",,5-10 years
IntelliBridge LLC,3.9,"McLean, VA",Data Engineer,"Title: Data Engineer
Location: Permanent remote role
Clearance: Not required: Start date not contingent on a having or completion of a clearance, however one could be offered upon starting for future programs
Overview:
IntelliBridge is seeking a Data Engineer to collaborate with technical and non-technical data and development team members to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of analytics that provide help ensure national security. You'll be able to gain experience in designing cloud architectures while providing critical support to the client's mission. You will be responsible for designing and building smart data pipelines that are secure, robust, and alerting. You will also create innovative ways to combine disparate data sources and build integrated datasets for advanced analytics.
As a direct employee of IntelliBridge, you would receive a benefit package that includes health/dental/vision insurance coverage, 401K with company match, PTO & paid holidays, and annual tuition/training assistance. For more information, please visit our website.
Responsibilities/Duties:
Build and maintain the infrastructure to support integration, extraction, transformation, and loading (ETL) of data from a wide variety of data sources, such as relational SQL and NoSQL databases, and other platform APIs
Design data pipelines that are robust and secure including pipeline monitoring and alerting mechanisms
Create innovative ways to orchestrate data ETL processes
Guide and support the implementation of new data engineering solutions to enable adoption and growth
Integrate disparate data sources into powerful datasets for advanced analytics
Recommend tools and capabilities based on understanding the current environment and knowledge of various on-premises, cloud based, and hybrid capabilities/technologies
Monitor existing metrics, analyze data, and lead partnership with other Data and Analytics personnel to identify and implement system and process improvements
Develop processes to convert aggregated data from teams, collection tools, and dashboards
Configure and manage data analytic frameworks and pipelines using databases and tools
Develop Python packages to improve application capabilities
Apply distributed systems concepts and principles such as consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms
Administrate cloud computing and CI/CD pipelines to include Amazon Web Service (AWS)
Investigate legacy code to determine areas of improvement and automation
Required Qualifications:
Excellent verbal and written communications
Bachelor’s Degree in a STEM filed or Master’s Degree in Operations Research, Industrial Engineering, Applied Mathematics, Statistics, Physics, Computer Science, or related fields
5+ years of experience with Python, SQL, Unix(Linux), and handling semi-structured data (JSON)
3+ years of experience with Elasticsearch, Logstash, and Kibana (ELK stack)
3+ years of experience with Amazon Web Services (AWS) or other cloud provider
Proficient in Docker
Proficient in Agile Development
Proficient in Git Operations
Experience understanding requirements, analyzing data, discovering opportunities, addressing gaps and communicating them to multiple individuals and stakeholders
Demonstrated expertise in technical data engineering on integrating complex applications, systems, software, and project activities and integrating them into cloud-based resources
General knowledge in machine learning for building efficient and accurate data pipelines that occur for downstream users, such as for data scientists to create the models and analytics that produce insight
Preferred Qualifications:
Organizational skills and a love of documentation
Experienced in Airflow
Experience with demonstrated strength in data lake/warehouse technical architecture, infrastructure components, and ETL/ELT pipelines
Experience with geo-spatial data
Experience with deployments via Kubernetes
Experience with configuring and aggregating logs for data analysis using Splunk or ELK solutions
Experience with developing and managing machine images or templates to automate cloud deployments
About Us:
IntelliBridge delivers IT strategy, cloud, cybersecurity, application, data and analytics, enterprise IT, intelligence analysis, and mission operation support services to accelerate technical performance and efficiency for Defense, Civilian, and National Security & Federal Law Enforcement clients.",92898,501 to 1000 Employees,Company - Private,Government & Public Administration,National Agencies,-1,Unknown / Non-Applicable,VA,-1,data engineer,na,"['python', 'sql', 'nosql']",['aws'],[],['elasticsearch'],['airflow'],bachelor,+10 years
Oddball,4.6,Remote,Data Engineer,"Oddball believes that the best products are built when teams understand and value the things they are working on. We value learning and growth and the ability to make a big impact at a small company. We believe that we can make big changes happen and improve the daily lives of millions of people by bringing quality software to the federal space.
As a Database Engineer you will be responsible for helping to develop production-quality analytics tools, utilizing existing data sources to empower stakeholders to inform research and development. Act as a liaison between engineers and analysts. Implement and enforce standards and procedures to ensure data is managed consistently and properly integrated.
Experience Requirements
5+ years of proven data and performance engineering experience.
A proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs.
Experience with system analysis, data analysis or programming, using a variety of computer languages and procedures.
Experience working in Agile environments.
Experience in developing, analyzing, and presenting data models.
Knowledge of data management, data standardization, and data governance.
Ability to analyze source data for potential data quality issues.
Expert in SQL and/or SQL based languages.
Create supporting documentation, such as metadata and diagrams of entity relationships, business processes, and process flow.
Create plans, test files, and scripts for data warehouse testing, ranging from unit to integration testing.
This is a Remote position. This is a salaried role. Oddball offers both a tech, and a continuing education stipend.
Must be located within the United States.
Clearances
Ability to obtain low-level federal clearance is required.
Oddball is an Equal Opportunity Employer and does not discriminate against applicants based on race, religion, color, disability, medical condition, legally protected genetic information, national origin, gender, sexual orientation, marital status, gender identity or expression, sex (including pregnancy, childbirth or related medical conditions), age, veteran status or other legally protected characteristics. Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Work Location: Remote",106385,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,['sql'],[],[],[],[],bachelor,5-10 years
MARVEL TECHNOLOGIES INC,3.7,Remote,Data Engineer,"Primary Skills
SCALA, SPARK, SQL, HADOOP, AWS, HIVE, Spark SQL, HIVE QL, CICD (Continuous Integration/Continuous Delivery), VCS (GIT HUB)
Coding in Scala
Designing in of HADOOP ecosystem
Hands-on experience on AWS tools like EMR, EC2
Hands-on experience of SQL in Big Data: SQL, Spark SQL, Hive QL
Proficient in working with large data sets and pipelines
Proficient with workflow scheduling / orchestration tools
Well versed with CICD process and VCS
Databricks is a big PLUS
Job Types: Full-time, Contract
Pay: $50.00 - $58.00 per hour
Schedule:
8 hour shift
Monday to Friday
Experience:
Spark: 4 years (Required)
Scala: 4 years (Required)
Hadoop: 3 years (Required)
Aws: 3 years (Required)
Hive: 3 years (Required)
CI/CD, VCS: 3 years (Required)
Databricks: 1 year (Required)
Work Location: Remote",97200,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,$5 to $25 million (USD),Remote,-1,data engineer,na,"['scala', 'sql']","['databricks', 'aws']",[],['hive'],"['hadoop', 'spark']",,0-2 years
Grid,4.2,"Oakland, CA",Data Engineer,"Today’s financial system is built to favor those with money. Grid’s mission is to level that playing field by building financial products that help users better manage their financial future. The Grid app lets users access cash, build credit, spend money, file their taxes and lots, lots more.

Grid is a fast growing team that’s deeply passionate about making a difference in the lives of millions. We’re solving huge problems and believe that a merit driven culture allows every team member to play a big role. Join our growing team in our Bay Area headquarters or additional offices across the US.

Our Data Engineers are responsible for converting the data surrounding a customer's financial life into tools that make our customers smarter and better equipped financially for the future. We value making intelligent decisions backed by good data and tools. We're looking for people who share our values, particularly if you have experience analyzing, processing, and learning from large data sets.

We’re looking for a seasoned data engineer who can help us lay the foundation of an exceptional data engineering practice. The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions. You will be writing code, setting style guides and collaborating cross-functionally with product, engineering and leadership.
Problems we work on:
Analytics: Collect all the data for a user into tools that help our customers
Machine Learning: Using a variety of techniques to reach better insights
Data Processing: Managing data & statistics using scalable and efficient technologies
Visualization: Envision our data as beautiful graphs and tools that allow customers to explore their data & ask their own questions
Risk: Analyze data for anomalous patterns and build tools that allow us to find bad actors quickly
We Practice:
Open collaboration
Code reviews
Testing
Agile development
We Use:
Go
Python
MySQL
Google Cloud Platform
Kubernetes
Kubeflow
Docker
Google Pubsub
BigQuery
Firebase
We’re looking for Engineers to:
Design and implement platform services, frameworks and ecosystems
Build a scalable, reliable, operable and performant big data workflow platform for data scientists/engineers, AI/ML engineers, and product/operation team members
Drive efficiency and reliability improvements through design and automation: performance, scaling, observability, and monitoring
Requirements:
Strong programming skills with Python
Strong programming skills with a typed programming language, such as Java, Scala, Go, etc.
Disciplined approach to development, testing, and quality assurance
Excellent communication skills, capable of explaining highly technical problems in English
Understand data processing and ETL, hands on experience building pipelines and using frameworks such as Hive hdfs, Presto, Spark, etc.
Really strong candidates may have:
Really strong candidates may have:
Actively contribute to open source software
Worked with a strong, lean-based development environment
Previous work experience in a start-up environment
Ability to recognize the right tool for the right situation/problem.
Will have strong programming skill in Go",112815,,,,,-1,,CA,-1,data engineer,na,"['python', 'scala', 'java', 'go']","['bigquery', 'google cloud']",[],"['mysql', 'hive']",['spark'],,
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
"Second Wave Delivery Systems, LLC",4.2,Remote,Data Engineer,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker",106385,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020,Unknown / Non-Applicable,Remote,3,data engineer,na,"['python', 'scala', 'sql', 'java']","['bigquery', 'google cloud']",[],[],"['airflow', 'spark']",,
FlexIT Inc,4.0,"Beaverton, OR",Data Engineer,"FlexIT client is looking for a Data Engineer 12 months contract in Beaverton, Oregon.
Looking for local candidates to work on site.
Top skills: Python, SQL , AWS, Spark",106334,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'sql']",['aws'],[],[],['spark'],,
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['python', 'sql']","['gcp', 'aws', 'redshift']",[],[],['airflow'],,
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100714,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
Metrohm Spectro,4.2,"Plainsboro, NJ",Data Engineer,"Metrohm Spectro is an advanced mobile spectroscopic instrumentation leader, developing, manufacturing, and servicing state-of-the-art analytical devices, including portable and handheld Raman analyzers.We provide solutions for the pharmaceutical, biomedical, safety and security, chemical, and academic research industries. We are constantly growing with new products and new opportunities, and are always looking for talented, dedicated employees to join us and grow together as a team.
With the fast growth of our business, we have an immediate vacancy for a full-time Data Engineer for our Plainsboro, NJ location.
Job Description
In this role, you will take responsibility for developing and maintaining databases within software products. You will be required to have hands-on problem solving, from the upkeep and generation of database, to data validation as well as the capability of data processing and analysis, and will be able to perform data processing algorithm validation with the knowledge of data science. To excel in this role, you need to be very organized with a fine eye for detail, and openness to learn new skills to meet growing business needs.
Education
· Bachelor’s of Science degree from an accredited university or college in chemistry, physics, mathematics or computer science.
Experience:
· High-level proficiency in Microsoft Excel or other automated data management tool.
· Experienced in database programming and familiar with all popular database types. Good understanding of MySQL is a plus;
· Knowledge in MATLAB, R, Python or SAS tools for data processing and analysis;
· Knowledge in AI/machine learning and data mining basics;
· Knowledge in C/C++ programming for data processing algorithm;
· High-level proficiency in Microsoft Excel or other automated data management tool;
· Knowledge in Network/Cloud infrastructure will be a plus.
ROLE AND JOB RESPONSIBILITIES
· Develop and maintain database for cross-platform software implementation on all BWTEK spectroscopic products.
· Assist in data process and analysis algorithm design and validation.
· Collaboration with entire software team for product enhancement and new product.
Job Type: Full-time
Application Question(s):
What are your salary expectations?
Work Location: Plainsboro, NJ - on site
Can you commute to this location?
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
5x8
8 hour shift
Monday to Friday
Ability to commute/relocate:
Plainsboro, NJ 08536: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Will you need sponsorship to work in US?
Work Location: In person",106385,,,,,-1,,NJ,-1,data engineer,na,"['r', 'python']",[],"['sas', 'excel']",['mysql'],[],bachelor,
Manufacturers Bank,3.2,"Los Angeles, CA",Data Engineer,"GENERAL SUMMARY:
Manufacturers bank is looking for a Data Engineer with strong software engineering backgrounds, a ""platform engineering mindset"" and deep passion for data. This Engineer will help to shape the technical direction of the data domain within the organization. The Engineer will also partner closely with internal business units and external vendors to develop a high performance, reliable and scalable data environment for the bank.

PRINCIPAL DUTIES & RESPONSIBILITIES:
Develop and automate high-performance data processing systems and visualization to ensure reliability and meet critical business requirements.
Design, develop, implement, document and support Data & Analytics systems which adhere to the industry best practices.
Lead data engineering projects, and overall strategy for data governance, security, privacy, quality, and retention.
Triage and resolve issues reported by Risk, Operations, and other business stakeholders.
Mentor junior engineers and continue promoting data engineering and analytics tooling & standards.",115908,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),CA,-1,data engineer,na,[],[],[],[],[],,
Grid,4.2,"Oakland, CA",Data Engineer,"Today’s financial system is built to favor those with money. Grid’s mission is to level that playing field by building financial products that help users better manage their financial future. The Grid app lets users access cash, build credit, spend money, file their taxes and lots, lots more.

Grid is a fast growing team that’s deeply passionate about making a difference in the lives of millions. We’re solving huge problems and believe that a merit driven culture allows every team member to play a big role. Join our growing team in our Bay Area headquarters or additional offices across the US.

Our Data Engineers are responsible for converting the data surrounding a customer's financial life into tools that make our customers smarter and better equipped financially for the future. We value making intelligent decisions backed by good data and tools. We're looking for people who share our values, particularly if you have experience analyzing, processing, and learning from large data sets.

We’re looking for a seasoned data engineer who can help us lay the foundation of an exceptional data engineering practice. The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions. You will be writing code, setting style guides and collaborating cross-functionally with product, engineering and leadership.
Problems we work on:
Analytics: Collect all the data for a user into tools that help our customers
Machine Learning: Using a variety of techniques to reach better insights
Data Processing: Managing data & statistics using scalable and efficient technologies
Visualization: Envision our data as beautiful graphs and tools that allow customers to explore their data & ask their own questions
Risk: Analyze data for anomalous patterns and build tools that allow us to find bad actors quickly
We Practice:
Open collaboration
Code reviews
Testing
Agile development
We Use:
Go
Python
MySQL
Google Cloud Platform
Kubernetes
Kubeflow
Docker
Google Pubsub
BigQuery
Firebase
We’re looking for Engineers to:
Design and implement platform services, frameworks and ecosystems
Build a scalable, reliable, operable and performant big data workflow platform for data scientists/engineers, AI/ML engineers, and product/operation team members
Drive efficiency and reliability improvements through design and automation: performance, scaling, observability, and monitoring
Requirements:
Strong programming skills with Python
Strong programming skills with a typed programming language, such as Java, Scala, Go, etc.
Disciplined approach to development, testing, and quality assurance
Excellent communication skills, capable of explaining highly technical problems in English
Understand data processing and ETL, hands on experience building pipelines and using frameworks such as Hive hdfs, Presto, Spark, etc.
Really strong candidates may have:
Really strong candidates may have:
Actively contribute to open source software
Worked with a strong, lean-based development environment
Previous work experience in a start-up environment
Ability to recognize the right tool for the right situation/problem.
Will have strong programming skill in Go",112815,,,,,-1,,CA,-1,data engineer,na,"['python', 'scala', 'java', 'go']","['bigquery', 'google cloud']",[],"['mysql', 'hive']",['spark'],,
Edrstaffing,4.2,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],,2-5 years
Govini,3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.",88151,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable,PA,12,data engineer,na,['sql'],[],[],[],[],master,5-10 years
Airbus Americas,4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.",92046,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD),GA,53,data engineer,na,"['python', 'r', 'java']",[],[],[],['spark'],bachelor,2-5 years
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['python', 'sql', 'java', 'nosql']","['azure', 'google cloud', 'aws']",[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,+10 years
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],"['azure', 'databricks']",['power bi'],[],[],,5-10 years
Arthur Grand Technologies Inc,4.8,"Atlanta, GA",AWS Data Engineer,"Role: AWS Data Engineer
Location: Atlanta, GA (Meet customer Monthly once post client selection & onboarding)
JD for AWS Data Engineer
Experience with the core AWS services, plus the specifics mentioned in this job description.
Experience with Docker . Having used ECS , SNS/SQS, Athena, Lambda,S3, RDS and layers in the past is a plus.
Proficiency in at least in Python, Java
Strong notions of security best practices (e.g. using IAM Roles, KMS, etc.).
Experience with monitoring solutions such as CloudWatch, Cloud Trail.
Previous exposure to large-scale systems design.
Knowledge of writing infrastructure as code (IaC) using CloudFormation or Terraform.
Experience with building or maintaining cloud-native applications.
Past experience with the serverless approaches using AWS Lambda is a plus. For example, the Serverless Application Model (AWS SAM).
Job Types: Full-time, Contract
Schedule:
10 hour shift
8 hour shift
Work Location: Remote",94994,51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable,GA,11,data engineer,na,"['python', 'java']",['aws'],[],[],[],,
Nursa,4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!",89485,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable,UT,5,data engineer,na,"['python', 'sql']","['aws', 'redshift']",[],[],[],,+10 years
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
"Second Wave Delivery Systems, LLC",4.2,Remote,Data Engineer,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker",106385,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020,Unknown / Non-Applicable,Remote,3,data engineer,na,"['python', 'scala', 'sql', 'java']","['bigquery', 'google cloud']",[],[],"['airflow', 'spark']",,
Oddball,4.6,Remote,Data Engineer,"Oddball believes that the best products are built when teams understand and value the things they are working on. We value learning and growth and the ability to make a big impact at a small company. We believe that we can make big changes happen and improve the daily lives of millions of people by bringing quality software to the federal space.
As a Database Engineer you will be responsible for helping to develop production-quality analytics tools, utilizing existing data sources to empower stakeholders to inform research and development. Act as a liaison between engineers and analysts. Implement and enforce standards and procedures to ensure data is managed consistently and properly integrated.
Experience Requirements
5+ years of proven data and performance engineering experience.
A proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs.
Experience with system analysis, data analysis or programming, using a variety of computer languages and procedures.
Experience working in Agile environments.
Experience in developing, analyzing, and presenting data models.
Knowledge of data management, data standardization, and data governance.
Ability to analyze source data for potential data quality issues.
Expert in SQL and/or SQL based languages.
Create supporting documentation, such as metadata and diagrams of entity relationships, business processes, and process flow.
Create plans, test files, and scripts for data warehouse testing, ranging from unit to integration testing.
This is a Remote position. This is a salaried role. Oddball offers both a tech, and a continuing education stipend.
Must be located within the United States.
Clearances
Ability to obtain low-level federal clearance is required.
Oddball is an Equal Opportunity Employer and does not discriminate against applicants based on race, religion, color, disability, medical condition, legally protected genetic information, national origin, gender, sexual orientation, marital status, gender identity or expression, sex (including pregnancy, childbirth or related medical conditions), age, veteran status or other legally protected characteristics. Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Work Location: Remote",106385,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,['sql'],[],[],[],[],bachelor,5-10 years
IBR (Imagine Believe Realize),4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",106385,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,Remote,16,data engineer,senior,"['python', 'r', 'java']","['azure', 'aws', 'redshift']","['tableau', 'sas']",['redis'],"['hadoop', 'spark']",bachelor,
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,
Globaleur,4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information
Deploy sophisticated analytics programs, machine learning, and statistical methods
Ensure compliance with data governance and security policies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java
Have working experiences in e-commence, travel, marketing domain is a plus
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.",119136,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017,Unknown / Non-Applicable,CA,6,data engineer,senior,"['sql', 'go', 'python', 'java', 'nosql']","['azure', 'aws']",['tableau'],['mongodb'],[],bachelor,
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master,
Lendem Solutions,4.2,"Plano, TX",Data Engineer,"LENDEM Solutions is looking at add a Data Engineer to our business!
CORE COMPETENCIES
Ability to thrive in a dynamic and fast-paced environment, drive change, and collaborate effectively with a variety of individuals and teams
Strong analytical and problem-solving skills
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
PRINCIPAL DUTIES
Shredding and parsing data to extract meaningful information
Preparing historical and live data for data studies to identify trends and patterns
Performing adhoc analysis to answer specific business questions and provide insights
Working with relational databases to model and query complex data relationships
Understanding and working with MySQL data in several different data environments
Mining consumer loan data utilizing tools such as SQL, Python, R, or other comparable data mining tools.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Engineering, or a related field Master’s degree preferred
3+ years of experience as a data engineer or similar role
REQUIRED SKILLS, ABILITIES, SOFT SKILL FACTORS
· Strong experience in ETL processes, data modeling, and data warehousing
Experience working with graph databases, such as Neo4j or Apache Cassandra
Expertise in programming languages such as SQL, Python
Familiarity with big data technologies, such as Hadoop, Spark, or Kafka
Ability to analyze and manipulate large and complex data sets
Strong problem-solving skills and the ability to work independently and as part of a team
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
If you are a data engineer with a passion for problem-solving and a strong background in ETL processes, data modeling, and graph databases, we encourage you to apply for this exciting opportunity. We offer a competitive salary, comprehensive benefits, and a dynamic work environment where you can continue to grow and develop your skills.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Plano, TX 75024: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 5 years (Preferred)
SQL: 5 years (Preferred)
Work Location: Hybrid remote in Plano, TX 75024",106385,,,,,-1,,TX,-1,data engineer,na,"['python', 'sql', 'r']",[],[],"['mysql', 'neo4j']","['kafka', 'hadoop', 'spark']",bachelor,5-10 years
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],[],"['airflow', 'hadoop', 'spark']",,2-5 years
Okaya Corp,4.2,"Mahwah, NJ",Azure Data Engineer,"Job Title: Azure Data Engineer
Location: Mahwah, NJ
Duration: Full Time
Skills Required:
Azure data factory, data bricks, data lake, automation, and performance optimization of ETL
Strong Hands-on experience in ADF, data bricks, data lake, power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end-to-end CI/CD implementation or Devops process in Data & Analytics context.
Design, plan, develop and update technical docs & BI solutions.
Understand the requirements and define the data load strategy for data refresh.
Create, debug, troubleshoot and deploy solutions.
Work on ETL design
Designing and optimization of ETL Process using ADF
Implementing end-to-end automated ETL processes and monitoring of those processes using various options using Azure
Experience in Azure data factory, data bricks, data lake, automation, and performance optimization of ETL
Job Type: Full-time
Work Location: One location",106385,,,,,-1,,NJ,-1,data engineer,na,[],['azure'],['power bi'],[],[],,
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['python', 'sql', 'go']","['snowflake', 'databricks', 'aws']",['ssis'],[],['spark'],,
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100714,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['python', 'sql', 'java', 'nosql']","['azure', 'google cloud', 'aws']",[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,+10 years
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
IBR (Imagine Believe Realize),4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",106385,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,Remote,16,data engineer,senior,"['python', 'r', 'java']","['azure', 'aws', 'redshift']","['tableau', 'sas']",['redis'],"['hadoop', 'spark']",bachelor,
Airbus Americas,4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.",92046,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD),GA,53,data engineer,na,"['python', 'r', 'java']",[],[],[],['spark'],bachelor,2-5 years
PSRTEK,4.6,"Mount Laurel, NJ",Lead Data Engineer,"AR# 226054
Role: Lead Data Engineer /Databricks (On-site)
Location: Mt. Laurel, NJ
Full-time
Visa status: GC/USC
Must have skills:
Databricks, Python, RDBMS, PowerShell scripting, data warehouse
Detailed JD:
Experience in ETL/Pipeline Development using tools such as Azure Databricks/Apache Spark and Azure
Data Factory with development expertise on batch and real-time data integration
Experience in programming using Python
RDBMS knowledge and experience in writing the Store Procedures
Experience in writing bash and Power shell scripting.
Experience in data ingestion, preparation, integration, and operationalization techniques in optimally addressing the data requirements
Experience in Cloud data warehouse like Azure Synapse, Snowflake analytical warehouse
Experience with Orchestration tools, Azure DevOps, and GitHub
Experience in building end to end architecture for Data Lakes, Data Warehouses and Data Marts
Experience in relational data processing technology like MS SQL, Delta Lake, Spark SQL, SQL Server
Experience to own end-to-end development, including coding, testing, debugging and deployment
Extensive knowledge of ETL and Data Warehousing concepts, strategies, methodologies
Experience working with structured and unstructured data
Familiarity with Azure services like Azure functions, Azure Data Lake Store, Azure Cosmos
Ability to provide solutions that are forward-thinking in data and analytics
Job Type: Full-time
Salary: $120.00 - $130.00 per year
Schedule:
8 hour shift
Experience:
Data Warehouse: 10 years (Required)
Python: 10 years (Required)
PowerShell: 10 years (Required)
Data Bricks: 10 years (Required)
RDBMS: 10 years (Required)
Work Location: On the road
Speak with the employer
+91 609-917-9952",121211,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,NJ,-1,data engineer,senior,"['python', 'shell', 'sql']","['snowflake', 'azure', 'databricks']",[],['sql server'],['spark'],,0-2 years
"Pomeroy Technologies, LLC.",3.0,"Cleveland, OH",Data Engineer,"The Data Engineer is responsible for the development, integration, and implementation of ETL Feeds in support of various modern architecture projects. This role requires hands-on knowledge of modern Integrations Platform solutions, file, and database ETL, and XML transformation. The incumbent will act as a technical resource, participating in the successful design, development, unit testing, implementation, and support of new and existing ETL integrations.
Essential Functions
Must be able to acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design.
Working with manager, gathering business requirements for the development of new applications.
Perform programming assignments based on established standards, methods, and best practices.
Develop ETL solutions as assigned in supporting new or updated business solutions and requirements.
Help business users develop functional requirements for integration.
Provide support during user acceptance testing.
Prepare documentation for code changes.
Code ETL feeds, and conduct tests to verify correct program functionality, reliability, and data integrity.
Maintain and modify ETL feeds; make approved changes by amending application documentation, developing detailed programming logic, and coding changes.
Provide support and resolution for application problems and issues.
Provide user and system documentation for operational and technical support.
Assist in the discovery and investigation of production problems as required.
Support ongoing daily use of ETL environments and Integrations platforms.
Provide on-call support for critical application problems and issues.
Analyze program performance and take action to correct deficiencies based on consultation with clients and approval of supervisor.
required.Work outside the standard office 7.5 hour workday may be
Experience Requirements
2+ years of IT experience.
1+ years of experience building ETL feeds using any ETL tool (Talend, Informatica, etc.)
Knowledge of the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment, and post-production support.
Understanding of XML, JSON, XSD, WSDL, REST, and SOAP APIs.
Experience in developing SQL programs.
Job Type: Full-time
Pay: From $80,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Work Location: Hybrid remote in Cleveland, OH 44115",80000,1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable,OH,41,data engineer,na,['sql'],[],[],[],[],,+10 years
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['python', 'sql']","['gcp', 'aws', 'redshift']",[],[],['airflow'],,
ITExpertUS,2.8,"Chicago, IL",Data Engineer,"Client: Agilisium
Role: Data Engineer
Location: Chicago, IL – Hybrid role – Need to be onsite once/twice in a month.
Type: Long term contract
Job Description
Title: Data Engineer
We are looking for a savvy Data Engineer to join our growing team. The hire will be responsible for expanding and optimizing our data and pipeline architecture and data flow and collection. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. In addition, the Data Engineer will collaborate and support software developers, implementation architecture, and data engineers on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. In addition, they must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
Responsibilities:
Because we work on a programmatic leading edge of many technologies, we need someone who is a creative problem solver, resourceful in getting things done, and can shift productively to or from working independently and collaboratively. This person would also take on the following responsibilities:
Process unstructured data into a form suitable for analysis.
Support the business with ad hoc data analysis and build reliable data pipelines.
Implementation of best practices and IT operations in mission-critical tighter SLA data
pipelines using Airflow
Query Engine Migration from Dremio to Redshift.
We leverage: Multiple AWS Data & Analytic Services(e.g.,Glue, Kinesis, S3) , SQL (e.g.,
PostgreSQL, Redshift, Athena); NoSQL (e.g., DocumentDB, MongoDB); Kafka, Docker, Spark(AWS EMR and DataBricks), Airflow, Dremio, Qubole, etc.
We use AWS extensively, so experience with AWS cloud and AWS Data & Analytics certification will help you hit the ground running.
Business Skills for Data Engineers:
Creative Problem-Solving: Approaching data organization challenges with a clear eye on what is important; employing the suitable approach/methods to make the maximum use of time and human resources.
Effective Collaboration: Carefully listening to management, data engineers, and architects to establish their needs.
Intellectual Curiosity: Exploring new territories and finding creative and pragmatic ways to solve data problems.
Industry Knowledge: Understanding and perspective of how industry functions and how data can be collected, analyzed, and utilized for value creation; maintaining flexibility in the face of big data developments.
Skills and Qualifications:
3 to 5 years of real-world Data Engineering experience.
Programming experience, ideally in Python and other data engineering languages like Scala
Programming knowledge to clean structure and semi-structure datasets.
Experience processing large amounts of structured and unstructured data. Streaming data experience is a plus.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Background in Linux
Build the infrastructure required for optimal extraction, transformation, and loading of data
from various data sources using SQL and other cloud ‘big data’ technologies like DataBricks, Snowflake, Dremio, and Qubole.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience creating a platform on which complex data pipelines are built using orchestration tools like Airflow, and Astronomer.
Experience with real-time sync between OLTP and OLAP using AWS technologies like real- time sync between AWS Aurora and AWS Redshift.
Job Type: Full-time
Salary: Up to $68.00 per hour
Schedule:
8 hour shift
Work Location: On the road
Speak with the employer
+91 6305048577",122400,501 to 1000 Employees,Company - Private,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,"['python', 'scala', 'sql', 'nosql']","['snowflake', 'databricks', 'aws', 'redshift']",[],['mongodb'],"['kafka', 'airflow', 'spark']",,5-10 years
TY Software,4.2,"Dallas, TX",Data Engineer,"Job Title Data Engineer,
Location: Dallas, TX
Type of work- Onsite , C2C
Job Description
Experience 3-5 years
At least 3+ years of enterprise experience in working with data bricks and highly proficient in SQL, Spark, Scala/Python.
Skilled in Big Data Technologies like Spark, Spark SQL, PySpark
Experience with one or more of the major cloud platforms & cloud services such as - Azure/AWS/GCP, Databricks
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Strong analytic skills related to working with unstructured datasets
Working knowledge of highly scalable ‘big data’ data stores
A successful history of manipulating, processing and extracting value from large, disconnected datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Experience developing enterprise software products
Experience with at least one of these object-oriented/object function scripting languages: PySpark/Python, Scala, Java
Build monitoring and automated testing to ensure data consistency and availability
Experience supporting and working with cross-functional teams in a dynamic environment
Experience working in an AGILE environment
Job Types: Full-time, Contract
Salary: $42.15 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75201: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",91944,,,,,-1,,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks', 'aws', 'gcp']",[],[],['spark'],,+10 years
Nursa,4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!",89485,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable,UT,5,data engineer,na,"['python', 'sql']","['aws', 'redshift']",[],[],[],,+10 years
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],"['azure', 'databricks']",['power bi'],[],[],,5-10 years
AgileEngine,5.0,Remote,Senior/Lead Data Engineer,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Work collaboratively with other engineers, architects, data scientists, analytics teams, and business product owners in an agile environment
Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools
Design and build robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Must haves
Bachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience
6 years of experience with Python or Java
4+ years of experience in building data lake, cloud data platform leveraging cloud (GCP/AWS) cloud native architecture, ETL/ELT, and data integration
3 years of development experience with cloud services ( AWS,GCP,AZURE) utilizing various support tools (e.g. GCS, Dataproc, Cloud Data flow, Airflow(Composer), Kafka , Cloud Pub/Sub)
Expertise in developing distributed data processing and Streaming frameworks and architectures (Apache Spark, Apache Beam, Apache Flink, )
In-depth knowledge of NoSQL database technologies (e.g. MongoDB, BigTable, DynamoDB)
Expertise in build and deployment tools – (Visual Studio, PyCharm, Git/Bitbucket/Bamboo, Maven, Jenkins, Nexus, )
5 years of experience and expertise in database design techniques and philosophies (e.g. RDBMS, Document, Star Schema, Kimball Model)
5 years of experience with integration and service frameworks (e.g API Gateways, Apache Camel, Swagger API, Zookeeper, Kafka, messaging tools, microservices)
Expertise with containerized Microservices and REST/GraphQL based API development
Experience leveraging continuous integration/development tools (e.g. Jenkins, Docker, Containers, OpenShift, Kubernetes, and container automation) in a Ci/CD pipeline
Advanced understanding of software development and research tools
Attention to detail and results-oriented, with a strong customer focus
Ability to work as part of a team and independently
Problem-solving and technical communication skills
Ability to prioritize workload to meet tight deadlines
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
Our team of data experts and engineers has helped create a supply chain management platform used by Forbes 500 enterprises and featured by Gartner. We’ve accompanied the platform from an early prototype to a full-fledged solution based on big data, AI, and machine learning. Data quality is integral to our current engagement with this project as we’re expanding the platform functionality.
AgileEngine has also been working on data and ML solutions in multiple industries and verticals like telecom, FinTech, energy efficiency, and more. Our work with our clients’ data has contributed to patented software products, Nasdaq-grade fraud detection, and products used by Deloitte and T-Mobile.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",106385,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable,Remote,13,data engineer,senior,"['python', 'java', 'nosql']","['azure', 'aws', 'gcp']",[],"['dynamodb', 'mongodb']","['kafka', 'airflow', 'spark']",master,2-5 years
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']",[],[],[],[],,
Globaleur,4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information
Deploy sophisticated analytics programs, machine learning, and statistical methods
Ensure compliance with data governance and security policies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java
Have working experiences in e-commence, travel, marketing domain is a plus
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.",119136,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017,Unknown / Non-Applicable,CA,6,data engineer,senior,"['sql', 'go', 'python', 'java', 'nosql']","['azure', 'aws']",['tableau'],['mongodb'],[],bachelor,
Sky Consulting Inc,4.2,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.",103312,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,['python'],['gcp'],[],['hive'],"['kafka', 'hadoop', 'spark']",,
BCVS Group INC,5.0,"Oakland, CA",Data engineer lead,"Role: Data Engineer Lead
Location: Oakland, CA
Description
Qualitest is looking for a Data Architect to join our team.
Must Have-
Seasoned data professional with experience implementing solutions across Data Quality, Data Assurance,
Data Governance and Test Data Management
Ability to assess current state, recommend solutions and build a rollout strategy
Analyze existing data landscape, business processes and create knowledge articles (architectures, data flows, tools)Design, Build and Deliver quality gates across data lifecycle for automated and continuous data quality assurance.
Integrate data testing solutions with Automation, CI/CD Pipelines for Continuous Testing
Working experience on various database technologies such as Oracle, SQL Server, MongoDB, Snowflake, Teradata
Good understanding about QA Test Management, Defect Management and Testing methodologies. Evaluate tools, build proof of concepts, and implement NextGen solutions
Work with customer executives and cross functional teams for seamless project implementation and delivery
Mentor onshore/offshore teams to deliver projects in time and assist in case of delays/clarifications
Experience working on data quality validation tools such as RightData, Icedq, Informatica, Tosca DI, Datagaps
Experience working on TDM tools such as Informatica TDM , IBM Optim, Delphix, CA TDM/Broadcom TDM
Proficient in AWS Cloud Services such as Glue, Lambda, RedShift, Dynamo DB and Databricks
Experience working on Data Warehouse, Data Lake, Big Data platforms
Requirements
Good to Have-
Knowledge about Enterprise Applications such as Salesforce, SAP, Veeva CRM
AI/ML knowledge on building Intelligent data validations and eliminate data anomalies
Job Types: Full-time, Contract
Salary: From $41.80 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Oakland, CA 94619: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 +12019071788",75240,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,['sql'],"['snowflake', 'oracle', 'redshift', 'databricks', 'aws']",['sap'],"['sql server', 'mongodb']",[],,0-2 years
ASCENDING,4.2,"Rockville, MD",Data Engineer,"Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.
This role is only available for W2 or individual contracts. Please no C2C.
100% Remote Work.

Responsibilities:
Analyze system requirements and design responsive algorithms and solutions.
Use big data and cloud technologies to produce production quality code.
Engage in performance tuning and scalability engineering.
Work with team, peers and management to identify objectives and set priorities.
Perform related SDLC engineering activities like sprint planning and estimation.
Work effectively in small agile teams.
Provide creative solutions to problems.
Identify opportunities for improvement and execute.

Requirements:
Minimum 5 years of proven professional experience working in the IT industry.
Degree in Computer Science or related domains.
Experience with cloud based Big Data technologies.
Experience with big data technologies like Hadoop, Spark and Hive.
AWS experience is a big plus.
Proficiency in Hive / Spark SQL / SQL. Experience with Spark.
Experience with one or more programming languages like Scala & Python & Java.
Ability to push the frontier of technology and independently pursue better alternatives.
Kubernetes or AWS EKS experience will be a plus.

Thanks for applying!
U3GJMKlbkr",96611,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,"['python', 'scala', 'sql', 'java']",['aws'],[],['hive'],"['hadoop', 'spark']",,
FalconSmartIT,4.5,"Dover, DE",Big Data Engineer,"Job Title: Big Data Engineer
Location: Toronto, Ontario, Canada
Job Type: Full time


Job Description:


Qualifications :
8+ years of software development experience in Big Data technologies (Spark/Hive/Hadoop)
Experience in working on Hadoop Distribution, good understanding of core concepts and best practices
Good experience in building/tuning Spark pipelines in Scala/Python
Good experience in writing complex Hive queries to derive business critical insights
Good Programming experience with Java/Python/Scala
Experience with AWS Cloud, exposure to Lambda/EMR/Kinesis will be good to have
Experience in NoSQL Technologies - MongoDB, Dynamo DB
Roles and Responsibilities :
Design and implement solutions for problems arising out of large-scale data processing
Attend/drive various architectural, design and status calls with multiple stakeholders
Ensure end-to-end ownership of all tasks being aligned
Design, build & maintain efficient, reusable & reliable code
Test implementation, troubleshoot & correct problems
Capable of working as an individual contributor and within team too
Ensure high quality software development with complete documentation and traceability
Fulfil organizational responsibilities (sharing knowledge & experience with other teams/ groups)
Conduct technical training(s)/session(s), write whitepapers/case studies/blogs etc.",106385,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,DE,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",['aws'],[],"['hive', 'mongodb']","['hadoop', 'spark']",,
"Second Wave Delivery Systems, LLC",4.2,Remote,Data Engineer,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker",106385,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020,Unknown / Non-Applicable,Remote,3,data engineer,na,"['python', 'scala', 'sql', 'java']","['bigquery', 'google cloud']",[],[],"['airflow', 'spark']",,
Edrstaffing,4.2,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],,2-5 years
Monogram Health Renal Services,4.2,"Brentwood, TN",Azure Data Engineer,"Position: Azure Data Engineer

The Azure Data Engineer will work with our business operations and technology teams to build out an enterprise-wide Data Warehouse and related systems in the cloud. Will collaborate with our internal teams and 3rd party vendor in building our future state, business process, system interfaces, governance and technology platforms needed to enable a single source of truth for product and customer data, drive a more sophisticated data environment and be positioned to scale for future business growth.

Roles and Responsibilities
In collaboration with our 3rd party vendor, implement strategic roadmap and operational framework for Enterprise Data Management, in partnership with key stakeholders
Collaborate on the strategy on new cloud data stores and migration of existing data in Azure
Work collaboratively with teams to design, document, and ensure continuous improvement of SDLC best practices for data in the cloud
Be an advocate for quality and security through data quality and accuracy validation and auditing of security best practices
Focus on understanding business problems and determine what aspects require optimization; articulate those aspects in a clear and concise manner
Work collaboratively with cross-functional teams to support Data Models in alignment with business needs
Partner with IT, on design and oversight of APIs and other data sharing capabilities in collaboration with short / long term business needs
Partner with IT to integrate new data management technologies and software engineering tools (automation projects, process improvements, etc.) into existing systems to enable single source of truth for product and customer data
Adhere to identified key performance indicators for quality and compliance metrics to ensure data related policies and standards are followed
Acts as a SME to explain all facets of EDM data exchange and processing, business user involvement, and overall architecture
Support Monogram as necessary with any tasks required to deliver excellent personalized kidney care and perform all other duties as assigned
Uphold the mission and values of Monogram Health in all aspects of your role and activities
Position Requirements
Nashville, TN
BS or equivalent experience in crucial duties and responsibilities
3-5 years’ experience in cloud computing
Solid understanding of enterprise data management in Healthcare Data
Solid understanding of data, databases, data tools (e.g. SQL) and data processing experience
Solid understanding of standard ETL tools and techniques, sourcing, maintaining, and updating data, data warehousing, data cleansing, and other analytical techniques required for data usage.
Experience with database systems, Azure cloud storage, and significant exposure to or experience with modern data and BI platforms (such as Databricks, MSSQL Data Warehouse (Synapse), Snowflake, and Tableau).
Previous experience with data migration to cloud-based environment
Interest and passion in designing and implementing data management, querying, and storage, with a particular focus on purpose-built data stores.
Functional knowledge of data visualization tools such as PowerBI and Tableau.
Strong intuition for business and analytical skills
Team player with excellent communication skills

Benefits

Opportunity to work in a dynamic, fast-paced and innovative value-based provider organization that is transforming the delivery of kidney care
Competitive salary and opportunity to participate in the company’s bonus program
Comprehensive medical, dental, vision and life insurance
Flexible paid leave and vacation policy
401(k) plan with matching contributions

About Monogram Health
Monogram Health is a next-generation, value-based chronic condition risk provider serving patients living with chronic kidney and end-stage renal disease and their related metabolic disorders. Monogram seeks to fill systemic gaps and transform the way nephrology, primary care and chronic condition treatment are delivered. Monogram’s innovative, in-home approach utilizes a national nephrology practice powered by a suite of technology-enabled clinical services, including case and disease management, utilization management and review, and medication therapy management services that improve health outcomes while lowering medical costs across the healthcare continuum. By focusing on increasing access to evidence-based care pathways and addressing social determinants of health, Monogram has emerged as an industry leader in championing greater health equity and improving health outcomes for individuals with chronic kidney and end-stage renal disease.

At Monogram Health we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.
Experience
Required
SQL
healthcare data
cloud computing
Azure
Preferred
Tableau
Snowflake
Data Warehouse
Databricks
Power BI",106385,,,,,-1,,TN,-1,data engineer,na,['sql'],"['snowflake', 'azure', 'databricks']","['tableau', 'power bi']",[],[],,5-10 years
Savvy Technology Solutions,4.2,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location",107339,,,,,-1,,DC,-1,data engineer,senior,"['sql', 'nosql']","['aws', 'redshift']","['tableau', 'ssis']",['mysql'],[],master,2-5 years
Govini,3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.",88151,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable,PA,12,data engineer,na,['sql'],[],[],[],[],master,5-10 years
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100714,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
Invictus Data,4.2,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500,,,,,-1,,CA,-1,data engineer,senior,"['python', 'scala', 'sql']","['azure', 'aws', 'gcp']",[],['hive'],"['kafka', 'hadoop', 'spark']",,2-5 years
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['python', 'sql', 'java', 'nosql']","['azure', 'google cloud', 'aws']",[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,+10 years
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],"['azure', 'databricks']",['power bi'],[],[],,5-10 years
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
IBR (Imagine Believe Realize),4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",106385,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,Remote,16,data engineer,senior,"['python', 'r', 'java']","['azure', 'aws', 'redshift']","['tableau', 'sas']",['redis'],"['hadoop', 'spark']",bachelor,
AgileEngine,5.0,Remote,Senior/Lead Data Engineer,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Work collaboratively with other engineers, architects, data scientists, analytics teams, and business product owners in an agile environment
Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools
Design and build robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Must haves
Bachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience
6 years of experience with Python or Java
4+ years of experience in building data lake, cloud data platform leveraging cloud (GCP/AWS) cloud native architecture, ETL/ELT, and data integration
3 years of development experience with cloud services ( AWS,GCP,AZURE) utilizing various support tools (e.g. GCS, Dataproc, Cloud Data flow, Airflow(Composer), Kafka , Cloud Pub/Sub)
Expertise in developing distributed data processing and Streaming frameworks and architectures (Apache Spark, Apache Beam, Apache Flink, )
In-depth knowledge of NoSQL database technologies (e.g. MongoDB, BigTable, DynamoDB)
Expertise in build and deployment tools – (Visual Studio, PyCharm, Git/Bitbucket/Bamboo, Maven, Jenkins, Nexus, )
5 years of experience and expertise in database design techniques and philosophies (e.g. RDBMS, Document, Star Schema, Kimball Model)
5 years of experience with integration and service frameworks (e.g API Gateways, Apache Camel, Swagger API, Zookeeper, Kafka, messaging tools, microservices)
Expertise with containerized Microservices and REST/GraphQL based API development
Experience leveraging continuous integration/development tools (e.g. Jenkins, Docker, Containers, OpenShift, Kubernetes, and container automation) in a Ci/CD pipeline
Advanced understanding of software development and research tools
Attention to detail and results-oriented, with a strong customer focus
Ability to work as part of a team and independently
Problem-solving and technical communication skills
Ability to prioritize workload to meet tight deadlines
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
Our team of data experts and engineers has helped create a supply chain management platform used by Forbes 500 enterprises and featured by Gartner. We’ve accompanied the platform from an early prototype to a full-fledged solution based on big data, AI, and machine learning. Data quality is integral to our current engagement with this project as we’re expanding the platform functionality.
AgileEngine has also been working on data and ML solutions in multiple industries and verticals like telecom, FinTech, energy efficiency, and more. Our work with our clients’ data has contributed to patented software products, Nasdaq-grade fraud detection, and products used by Deloitte and T-Mobile.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",106385,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable,Remote,13,data engineer,senior,"['python', 'java', 'nosql']","['azure', 'aws', 'gcp']",[],"['dynamodb', 'mongodb']","['kafka', 'airflow', 'spark']",master,2-5 years
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],[],"['airflow', 'hadoop', 'spark']",,2-5 years
Manufacturers Bank,3.2,"Los Angeles, CA",Data Engineer,"GENERAL SUMMARY:
Manufacturers bank is looking for a Data Engineer with strong software engineering backgrounds, a ""platform engineering mindset"" and deep passion for data. This Engineer will help to shape the technical direction of the data domain within the organization. The Engineer will also partner closely with internal business units and external vendors to develop a high performance, reliable and scalable data environment for the bank.

PRINCIPAL DUTIES & RESPONSIBILITIES:
Develop and automate high-performance data processing systems and visualization to ensure reliability and meet critical business requirements.
Design, develop, implement, document and support Data & Analytics systems which adhere to the industry best practices.
Lead data engineering projects, and overall strategy for data governance, security, privacy, quality, and retention.
Triage and resolve issues reported by Risk, Operations, and other business stakeholders.
Mentor junior engineers and continue promoting data engineering and analytics tooling & standards.",115908,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),CA,-1,data engineer,na,[],[],[],[],[],,
Lendem Solutions,4.2,"Plano, TX",Data Engineer,"LENDEM Solutions is looking at add a Data Engineer to our business!
CORE COMPETENCIES
Ability to thrive in a dynamic and fast-paced environment, drive change, and collaborate effectively with a variety of individuals and teams
Strong analytical and problem-solving skills
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
PRINCIPAL DUTIES
Shredding and parsing data to extract meaningful information
Preparing historical and live data for data studies to identify trends and patterns
Performing adhoc analysis to answer specific business questions and provide insights
Working with relational databases to model and query complex data relationships
Understanding and working with MySQL data in several different data environments
Mining consumer loan data utilizing tools such as SQL, Python, R, or other comparable data mining tools.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Engineering, or a related field Master’s degree preferred
3+ years of experience as a data engineer or similar role
REQUIRED SKILLS, ABILITIES, SOFT SKILL FACTORS
· Strong experience in ETL processes, data modeling, and data warehousing
Experience working with graph databases, such as Neo4j or Apache Cassandra
Expertise in programming languages such as SQL, Python
Familiarity with big data technologies, such as Hadoop, Spark, or Kafka
Ability to analyze and manipulate large and complex data sets
Strong problem-solving skills and the ability to work independently and as part of a team
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
If you are a data engineer with a passion for problem-solving and a strong background in ETL processes, data modeling, and graph databases, we encourage you to apply for this exciting opportunity. We offer a competitive salary, comprehensive benefits, and a dynamic work environment where you can continue to grow and develop your skills.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Plano, TX 75024: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 5 years (Preferred)
SQL: 5 years (Preferred)
Work Location: Hybrid remote in Plano, TX 75024",106385,,,,,-1,,TX,-1,data engineer,na,"['python', 'sql', 'r']",[],[],"['mysql', 'neo4j']","['kafka', 'hadoop', 'spark']",bachelor,5-10 years
Oddball,4.6,Remote,Data Engineer,"Oddball believes that the best products are built when teams understand and value the things they are working on. We value learning and growth and the ability to make a big impact at a small company. We believe that we can make big changes happen and improve the daily lives of millions of people by bringing quality software to the federal space.
As a Database Engineer you will be responsible for helping to develop production-quality analytics tools, utilizing existing data sources to empower stakeholders to inform research and development. Act as a liaison between engineers and analysts. Implement and enforce standards and procedures to ensure data is managed consistently and properly integrated.
Experience Requirements
5+ years of proven data and performance engineering experience.
A proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs.
Experience with system analysis, data analysis or programming, using a variety of computer languages and procedures.
Experience working in Agile environments.
Experience in developing, analyzing, and presenting data models.
Knowledge of data management, data standardization, and data governance.
Ability to analyze source data for potential data quality issues.
Expert in SQL and/or SQL based languages.
Create supporting documentation, such as metadata and diagrams of entity relationships, business processes, and process flow.
Create plans, test files, and scripts for data warehouse testing, ranging from unit to integration testing.
This is a Remote position. This is a salaried role. Oddball offers both a tech, and a continuing education stipend.
Must be located within the United States.
Clearances
Ability to obtain low-level federal clearance is required.
Oddball is an Equal Opportunity Employer and does not discriminate against applicants based on race, religion, color, disability, medical condition, legally protected genetic information, national origin, gender, sexual orientation, marital status, gender identity or expression, sex (including pregnancy, childbirth or related medical conditions), age, veteran status or other legally protected characteristics. Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Work Location: Remote",106385,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,['sql'],[],[],[],[],bachelor,5-10 years
Nursa,4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!",89485,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable,UT,5,data engineer,na,"['python', 'sql']","['aws', 'redshift']",[],[],[],,+10 years
"Pomeroy Technologies, LLC.",3.0,"Cleveland, OH",Data Engineer,"The Data Engineer is responsible for the development, integration, and implementation of ETL Feeds in support of various modern architecture projects. This role requires hands-on knowledge of modern Integrations Platform solutions, file, and database ETL, and XML transformation. The incumbent will act as a technical resource, participating in the successful design, development, unit testing, implementation, and support of new and existing ETL integrations.
Essential Functions
Must be able to acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design.
Working with manager, gathering business requirements for the development of new applications.
Perform programming assignments based on established standards, methods, and best practices.
Develop ETL solutions as assigned in supporting new or updated business solutions and requirements.
Help business users develop functional requirements for integration.
Provide support during user acceptance testing.
Prepare documentation for code changes.
Code ETL feeds, and conduct tests to verify correct program functionality, reliability, and data integrity.
Maintain and modify ETL feeds; make approved changes by amending application documentation, developing detailed programming logic, and coding changes.
Provide support and resolution for application problems and issues.
Provide user and system documentation for operational and technical support.
Assist in the discovery and investigation of production problems as required.
Support ongoing daily use of ETL environments and Integrations platforms.
Provide on-call support for critical application problems and issues.
Analyze program performance and take action to correct deficiencies based on consultation with clients and approval of supervisor.
required.Work outside the standard office 7.5 hour workday may be
Experience Requirements
2+ years of IT experience.
1+ years of experience building ETL feeds using any ETL tool (Talend, Informatica, etc.)
Knowledge of the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment, and post-production support.
Understanding of XML, JSON, XSD, WSDL, REST, and SOAP APIs.
Experience in developing SQL programs.
Job Type: Full-time
Pay: From $80,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Work Location: Hybrid remote in Cleveland, OH 44115",80000,1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable,OH,41,data engineer,na,['sql'],[],[],[],[],,+10 years
"Second Wave Delivery Systems, LLC",4.2,Remote,Data Engineer,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker",106385,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020,Unknown / Non-Applicable,Remote,3,data engineer,na,"['python', 'scala', 'sql', 'java']","['bigquery', 'google cloud']",[],[],"['airflow', 'spark']",,
Airbus Americas,4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.",92046,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD),GA,53,data engineer,na,"['python', 'r', 'java']",[],[],[],['spark'],bachelor,2-5 years
Infinity Quest,4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote",105128,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable,NJ,17,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,0-2 years
Business Integra Inc,3.5,"San Francisco, CA",Data Engineer,"Position can be 100% remote but preferred to have candidates who can periodically (2 x month) work at headquarters.
(Data Engineer)
Job Description:
Assigned Personnel to provide data analytic support to the Data Analytics/Data Integration Project for Judicial Branch Statistical Information System (JBSIS) data reporting.
This position will perform high level data engineering and data analytics on a variety of agency data sources, but primarily on the Judicial Branch Statistical Information System (JBSIS).
Partnering with IT staff, this position will reengineer JBSIS to create new technical documentation for JBSIS; create mappings for the Court Statistics Report and other JBSIS products, make policy recommendations, create and/or implement new governance standards, enhance data auditing and data quality controls, and create data visualizations.
These same tasks may be performed with additional agency datasets.
Specific Skills/Qualifications Required
Technical project management and documentation skills.
Ability to analyze issues from system documentation and recommend solutions.
Experience managing technical projects, including conflict resolution, issue escalations, status reporting and resource management.
Experience creating and executing data mappings and scripts to clean, compile and analyze data
Ability to assess and maintain data pipeline, data quality in the database, and address data reporting issues.
Experience developing and implementing testing protocols for data and system quality
Experience in R and Stata.
Experience with data visualization and software such as Tableau and Power BI.
Excellent oral, written, analytical and communication skills with the ability to lead a technical discussion to both technical and non-technical staff.
Excellent analytical, verbal and conflict resolution skills.
Additional Skills/Qualifications Desired:
General:
Understanding of courtroom operations and workflow.
Experience in government (State) setting
Excellent presentation skills for both technical and non-technical audiences, including creating and presenting executive summaries to management and technical committees.
Technical:
Exposure and experience with Cloud computing.
Conceptual understanding of Amazon Web Services, Microsoft Azure, Google Cloud, IBM and Oracle Cloud Platforms.
Prior experience using Snowflake
Experience using Python or other database query languages.
Job Types: Full-time, Contract
Pay: $112,604.02 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Vision insurance
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco, CA 94102: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data science: 9 years (Required)
Work Location: Hybrid remote in San Francisco, CA 94102",131302,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2001,$25 to $100 million (USD),CA,22,data engineer,na,"['r', 'python']","['snowflake', 'oracle', 'azure', 'google cloud']","['tableau', 'power bi']",[],[],,5-10 years
Softcrylic,4.0,Remote,Senior Data Engineer,"Who We Are
For more than 20 years, we have been working with organizations large and small to help solve business challenges through technology. We bring a unique combination of engineering and strategy to Make Data Work for organizations.
Our clients range from the travel and leisure industry to publishing, retail and banking. The common thread between our clients is their commitment to making data work as seen through their investment in those efforts.
In our quest to solve data challenges for our clients, we work with large enterprise, cloud based and marketing technology suites. We have a deep understanding of these solutions so we can help our clients make the most of their investment in an efficient way to have a data driven business.
Why Work at Softcrylic?
Softcrylic provides an engaging, team-focused, and rewarding work environment where people are excited about the work they do and passionate about delivering creative solutions to our clients.
We are looking to add a Senior Data Engineer to our team! This is a 100% Remote role and preference will be given to candidates from Atlanta, NJ or Texas regions.
Job Description:
Softcrylic is looking for a Senior Data Engineerwith strong design, development, and team leadership skills. The person should be working with Clients / Customer and with our internal (onshore and offshore) members to design, develop and rollout data projects. The person to be hands on and have strong leadership/communication and interpersonal skills.
Requirement:
· 5 to 7 years of experience in working as a Data Engineer.
· Strong experience in Python.
· Experience in working on GCP.
· Good experience in Airflow.
· Should have good experience in ETL pipeline design and development.
· Very good experience in SQL
· Experience in working on Redshift.
· Excellent designing and documentation (diagrams) and presentation skills.
· Data Quality Concepts are must have.
· Must know design and development of any of industry leading graph databases.
· Good communication skills.
· Independent thinker, good team player with Data Engineering Design skills.
· Work with minimum guidelines.
Plus:
GCP - Big Query
Agile background
Microsoft Power BI
Graph Database
Job Types: Full-time, Contract
Pay: $130,000.00 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Compensation package:
Performance bonus
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
GCP: 3 years (Preferred)
Python: 4 years (Preferred)
Work Location: Remote
Speak with the employer
+91 609.241.9641",135000,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$5 to $25 million (USD),Remote,23,data engineer,senior,"['python', 'sql']","['redshift', 'gcp']",['power bi'],[],['airflow'],,2-5 years
"MOBE, LLC",3.7,"Minneapolis, MN",Data Engineer (ETL),"MOBE
MOBE guides people to better health and more happiness. Behind our innovative health solutions is uniquely human philosophy. We believe that person-to-person connections and understanding can make a difference in a world where self-care can be complicated, and health care is ever-evolving and complex.
MOBE works with health plans and large employers to identify individuals who are frequent users of health care but aren?t finding resolutions for their underlying health issues. We use a whole-person approach and guidance to impact health outcomes positively.
Supporting people is at the core of our business, employees included. MOBE is a high-growth organization with a culture built on trust and collaboration. Consistent across our teams and offerings is a belief in the power of people doing good together. We genuinely care about people and consider our workforce the most significant asset.
Your Role at MOBE
This is an exciting time at MOBE and we are growing fast. At MOBE, we have a lot of data: eligibility, medical and pharmacy claims, marketing campaign impressions, transcripts from participant interactions, etc.
This position is responsible for providing technical and project expertise to enable MOBE analytics and operations with structured and unstructured data. Responsibilities includes executing and/or leading user story development, data design and architecture, data pipeline development, testing and deployment in the Analytic Data Framework. This role will partner with internal and external business and technology teams to drive project deliverables and ensure high quality delivery of data architecture and integration.
Responsibilities
The Data Engineer ensures the following capabilities and functions:
Translate high level business processes into logical data processing steps
Design data structures and pipelines that are flexible and scalable for MOBE analytics and operational requirements
Support Analytic partners through collaborative and transparent development, information delivery, problem resolution, shared insights, and training
Data processing definition, execution, and documentation, in a time appropriate way, to meet business priorities and requirements
Data quality and maintenance consistent within the Analytic Data Framework
Lead small to moderate sized projects and initiatives, following through on execution of chosen strategies and demonstrating the ability to work through obstacles and changing priorities.
Demonstrate ability and willingness to play multiple roles for different projects (e.g. planning/architecture, project development, hands-on technical resource/support for others, analysis and resolution of data issues)
Identify and constructively communicate the need for improvements or enhancements in MOBE technology assets
All other duties as assigned to help fulfill our Mission and abide by MOBE?s Guiding Principles",97357,51 to 200 Employees,Company - Private,Personal Consumer Services,Beauty & Wellness,2014,Unknown / Non-Applicable,MN,9,data engineer,na,[],[],[],[],[],,
Sky Consulting Inc,4.2,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.",103312,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,['python'],['gcp'],[],['hive'],"['kafka', 'hadoop', 'spark']",,
Savvy Technology Solutions,4.2,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location",107339,,,,,-1,,DC,-1,data engineer,senior,"['sql', 'nosql']","['aws', 'redshift']","['tableau', 'ssis']",['mysql'],[],master,2-5 years
Monogram Health Renal Services,4.2,"Brentwood, TN",Azure Data Engineer,"Position: Azure Data Engineer

The Azure Data Engineer will work with our business operations and technology teams to build out an enterprise-wide Data Warehouse and related systems in the cloud. Will collaborate with our internal teams and 3rd party vendor in building our future state, business process, system interfaces, governance and technology platforms needed to enable a single source of truth for product and customer data, drive a more sophisticated data environment and be positioned to scale for future business growth.

Roles and Responsibilities
In collaboration with our 3rd party vendor, implement strategic roadmap and operational framework for Enterprise Data Management, in partnership with key stakeholders
Collaborate on the strategy on new cloud data stores and migration of existing data in Azure
Work collaboratively with teams to design, document, and ensure continuous improvement of SDLC best practices for data in the cloud
Be an advocate for quality and security through data quality and accuracy validation and auditing of security best practices
Focus on understanding business problems and determine what aspects require optimization; articulate those aspects in a clear and concise manner
Work collaboratively with cross-functional teams to support Data Models in alignment with business needs
Partner with IT, on design and oversight of APIs and other data sharing capabilities in collaboration with short / long term business needs
Partner with IT to integrate new data management technologies and software engineering tools (automation projects, process improvements, etc.) into existing systems to enable single source of truth for product and customer data
Adhere to identified key performance indicators for quality and compliance metrics to ensure data related policies and standards are followed
Acts as a SME to explain all facets of EDM data exchange and processing, business user involvement, and overall architecture
Support Monogram as necessary with any tasks required to deliver excellent personalized kidney care and perform all other duties as assigned
Uphold the mission and values of Monogram Health in all aspects of your role and activities
Position Requirements
Nashville, TN
BS or equivalent experience in crucial duties and responsibilities
3-5 years’ experience in cloud computing
Solid understanding of enterprise data management in Healthcare Data
Solid understanding of data, databases, data tools (e.g. SQL) and data processing experience
Solid understanding of standard ETL tools and techniques, sourcing, maintaining, and updating data, data warehousing, data cleansing, and other analytical techniques required for data usage.
Experience with database systems, Azure cloud storage, and significant exposure to or experience with modern data and BI platforms (such as Databricks, MSSQL Data Warehouse (Synapse), Snowflake, and Tableau).
Previous experience with data migration to cloud-based environment
Interest and passion in designing and implementing data management, querying, and storage, with a particular focus on purpose-built data stores.
Functional knowledge of data visualization tools such as PowerBI and Tableau.
Strong intuition for business and analytical skills
Team player with excellent communication skills

Benefits

Opportunity to work in a dynamic, fast-paced and innovative value-based provider organization that is transforming the delivery of kidney care
Competitive salary and opportunity to participate in the company’s bonus program
Comprehensive medical, dental, vision and life insurance
Flexible paid leave and vacation policy
401(k) plan with matching contributions

About Monogram Health
Monogram Health is a next-generation, value-based chronic condition risk provider serving patients living with chronic kidney and end-stage renal disease and their related metabolic disorders. Monogram seeks to fill systemic gaps and transform the way nephrology, primary care and chronic condition treatment are delivered. Monogram’s innovative, in-home approach utilizes a national nephrology practice powered by a suite of technology-enabled clinical services, including case and disease management, utilization management and review, and medication therapy management services that improve health outcomes while lowering medical costs across the healthcare continuum. By focusing on increasing access to evidence-based care pathways and addressing social determinants of health, Monogram has emerged as an industry leader in championing greater health equity and improving health outcomes for individuals with chronic kidney and end-stage renal disease.

At Monogram Health we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.
Experience
Required
SQL
healthcare data
cloud computing
Azure
Preferred
Tableau
Snowflake
Data Warehouse
Databricks
Power BI",106385,,,,,-1,,TN,-1,data engineer,na,['sql'],"['snowflake', 'azure', 'databricks']","['tableau', 'power bi']",[],[],,5-10 years
Globaleur,4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information
Deploy sophisticated analytics programs, machine learning, and statistical methods
Ensure compliance with data governance and security policies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java
Have working experiences in e-commence, travel, marketing domain is a plus
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.",119136,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017,Unknown / Non-Applicable,CA,6,data engineer,senior,"['sql', 'go', 'python', 'java', 'nosql']","['azure', 'aws']",['tableau'],['mongodb'],[],bachelor,
BCVS Group INC,5.0,"Oakland, CA",Data engineer lead,"Role: Data Engineer Lead
Location: Oakland, CA
Description
Qualitest is looking for a Data Architect to join our team.
Must Have-
Seasoned data professional with experience implementing solutions across Data Quality, Data Assurance,
Data Governance and Test Data Management
Ability to assess current state, recommend solutions and build a rollout strategy
Analyze existing data landscape, business processes and create knowledge articles (architectures, data flows, tools)Design, Build and Deliver quality gates across data lifecycle for automated and continuous data quality assurance.
Integrate data testing solutions with Automation, CI/CD Pipelines for Continuous Testing
Working experience on various database technologies such as Oracle, SQL Server, MongoDB, Snowflake, Teradata
Good understanding about QA Test Management, Defect Management and Testing methodologies. Evaluate tools, build proof of concepts, and implement NextGen solutions
Work with customer executives and cross functional teams for seamless project implementation and delivery
Mentor onshore/offshore teams to deliver projects in time and assist in case of delays/clarifications
Experience working on data quality validation tools such as RightData, Icedq, Informatica, Tosca DI, Datagaps
Experience working on TDM tools such as Informatica TDM , IBM Optim, Delphix, CA TDM/Broadcom TDM
Proficient in AWS Cloud Services such as Glue, Lambda, RedShift, Dynamo DB and Databricks
Experience working on Data Warehouse, Data Lake, Big Data platforms
Requirements
Good to Have-
Knowledge about Enterprise Applications such as Salesforce, SAP, Veeva CRM
AI/ML knowledge on building Intelligent data validations and eliminate data anomalies
Job Types: Full-time, Contract
Salary: From $41.80 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Oakland, CA 94619: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 +12019071788",75240,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,['sql'],"['snowflake', 'oracle', 'redshift', 'databricks', 'aws']",['sap'],"['sql server', 'mongodb']",[],,0-2 years
Boston Globe Media Partners,4.1,"Boston, MA",Data Engineer,"Boston Globe Media is New England's largest newsgathering organization - and much more. We are committed to being an indispensable, trusted, reliable source of round-the-clock information. Through the powerful journalism from our newsroom, engaging content from our content marketing studio, or through targeted advertising solutions, brands and marketers rely on us to reach highly engaged, educated and influential audiences through a variety of media and experiences.
Responsibilities:
Collect, organize, and document often-used data resources (maps, APIs, etc).
Create scripts to scrape data from websites for stakeholders.
With guidance, start creation of a data style guide.
Technology:
Basic knowledge of HTML, CSS, and JavaScript.
Basic familiarity with PHP, Groovy, or another server side scripting language.
Basic familiarity of build tools such as Grunt, Gulp, or Webpack.
Basic familiarity with version control systems such as SVN or Git.
Qualifications:
Understands and follows the team’s agile process.
Adheres to defined coding standards.
Participates in code reviews.
A willingness to adapt and be audience focused, with a curious mindset and a commitment to creating an inclusive work environment
Vaccination Statement:
We require that all BGMP employees (including temporary employees, co-ops, interns, and independent contractors) be vaccinated from COVID-19, unless an exemption from this policy has been granted as an accommodation or otherwise. All BGMP employees, regardless of vaccination status or work location, must provide proof of vaccination status as instructed by the employee's designated Human Resources contact. Employees may request a reasonable accommodation or other exemption from this policy by contacting their designated Human Resources contact. Failure to comply with or enforce any part of this policy, or misrepresentation of compliance with this policy, may result in discipline, up to and including termination of employment, subject to reasonable accommodation and other requirements of applicable federal, state, and local law.
EEO Statement:
Boston Globe Media Partners is an equal employment opportunity employer, and does not discriminate on the basis of race, color, religion, gender, sexual orientation, gender identity or expression, age, disability, national origin, ancestry, genetic information, military or veteran status, pregnancy or pregnancy-related condition or any other protected characteristic. Boston Globe Media Partners is committed to diversity in its most inclusive sense.
wcZyZ7QvrB",110394,1001 to 5000 Employees,Company - Private,Media & Communication,Publishing,-1,$100 to $500 million (USD),MA,-1,data engineer,na,[],[],[],[],[],,
NIVID Technologies,3.7,Remote,Sr. Data Engineer,"We hope this Job meets your skills and expectations. If you are available and interested, please contact me at your earliest convenience. You will be working with a highly skilled team of IT Professionals in a high pace corporate environment. This opportunity will move quickly and candidates will be interviewed in the order they apply. Job Description/ Required Skills: (i) Strong hands-on programming experience in Python (ii) Hands-on experience of API development (from application / software engineering perspective) (iii) AWS Lambda and data streaming ingestion (Kinesis) (iv) AWS tech stack from data engineering stand-point
Job Types: Full-time, Contract, Permanent
Salary: $39.76 - $86.23 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",113400,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$5 to $10 billion (USD),Remote,11,data engineer,senior,"['python', 'sql']",['aws'],[],[],[],,0-2 years
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100714,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
"Fiserv, Inc.",3.2,"Bridgewater, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['python', 'sql', 'java', 'nosql']","['azure', 'google cloud', 'aws']",[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,+10 years
AgileEngine,5.0,Remote,Senior/Lead Data Engineer,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Work collaboratively with other engineers, architects, data scientists, analytics teams, and business product owners in an agile environment
Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools
Design and build robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Must haves
Bachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience
6 years of experience with Python or Java
4+ years of experience in building data lake, cloud data platform leveraging cloud (GCP/AWS) cloud native architecture, ETL/ELT, and data integration
3 years of development experience with cloud services ( AWS,GCP,AZURE) utilizing various support tools (e.g. GCS, Dataproc, Cloud Data flow, Airflow(Composer), Kafka , Cloud Pub/Sub)
Expertise in developing distributed data processing and Streaming frameworks and architectures (Apache Spark, Apache Beam, Apache Flink, )
In-depth knowledge of NoSQL database technologies (e.g. MongoDB, BigTable, DynamoDB)
Expertise in build and deployment tools – (Visual Studio, PyCharm, Git/Bitbucket/Bamboo, Maven, Jenkins, Nexus, )
5 years of experience and expertise in database design techniques and philosophies (e.g. RDBMS, Document, Star Schema, Kimball Model)
5 years of experience with integration and service frameworks (e.g API Gateways, Apache Camel, Swagger API, Zookeeper, Kafka, messaging tools, microservices)
Expertise with containerized Microservices and REST/GraphQL based API development
Experience leveraging continuous integration/development tools (e.g. Jenkins, Docker, Containers, OpenShift, Kubernetes, and container automation) in a Ci/CD pipeline
Advanced understanding of software development and research tools
Attention to detail and results-oriented, with a strong customer focus
Ability to work as part of a team and independently
Problem-solving and technical communication skills
Ability to prioritize workload to meet tight deadlines
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
Our team of data experts and engineers has helped create a supply chain management platform used by Forbes 500 enterprises and featured by Gartner. We’ve accompanied the platform from an early prototype to a full-fledged solution based on big data, AI, and machine learning. Data quality is integral to our current engagement with this project as we’re expanding the platform functionality.
AgileEngine has also been working on data and ML solutions in multiple industries and verticals like telecom, FinTech, energy efficiency, and more. Our work with our clients’ data has contributed to patented software products, Nasdaq-grade fraud detection, and products used by Deloitte and T-Mobile.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",106385,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable,Remote,13,data engineer,senior,"['python', 'java', 'nosql']","['azure', 'aws', 'gcp']",[],"['dynamodb', 'mongodb']","['kafka', 'airflow', 'spark']",master,2-5 years
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
IBR (Imagine Believe Realize),4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",106385,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,Remote,16,data engineer,senior,"['python', 'r', 'java']","['azure', 'aws', 'redshift']","['tableau', 'sas']",['redis'],"['hadoop', 'spark']",bachelor,
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],[],"['airflow', 'hadoop', 'spark']",,2-5 years
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Sky Consulting Inc,4.2,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.",103312,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,['python'],['gcp'],[],['hive'],"['kafka', 'hadoop', 'spark']",,
Plaxonic Technologies,4.6,"New York, NY",GCP Data Engineer,"Bachelor’s Degree in Computer Science or a related discipline
5+ years of applicable engineering experience
Strong proficiency in Python with an emphasis in building data pipelines
Ability to write complex SQL to perform common types of analysis and aggregations
Experience with Apache Airflow or Google Composer
Detail-oriented and document all the work
Ability to work with others from diverse skill-sets and backgrounds
GCP solution architect - certified
Experience in GCP, Big Query
Working experience in Databricks, Spark is expected
Job Types: Full-time, Contract
Benefits:
401(k)
Health insurance
Paid time off
Schedule:
8 hour shift
Work Location: One location
Speak with the employer
+91 (727) 216-7989",117952,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),NY,10,data engineer,na,"['python', 'sql']","['databricks', 'gcp']",[],[],"['airflow', 'spark']",bachelor,
EZOPs Inc,4.2,"New York, NY",Python Data Engineer,"Responsibility:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies.
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems.
Utilize programming languages like Python, ReactJs, JavaScript and Open Source RDBMS and Cloud based data warehousing services such as Snowflake.
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community.
Collaborate with product managers and deliver robust cloud-based solutions.
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply).
At least 1 year of experience in data technologies.
Hands on Experience in application development with Python, Pandas, NumPy, SQL, Docker.
Preferred Qualifications:
2+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud).
1+ year experience working on real-time data and streaming applications like Kafka is big plus.
1+ years of data warehousing experience (Redshift or Snowflake)
1+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
Job Type: Full-time
Benefits:
401(k)
Health insurance
Paid time off
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
New York, NY: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location
Speak with the employer
+91 9599382735",106385,,,,,-1,,NY,-1,data engineer,na,"['scala', 'sql', 'shell', 'python', 'java']","['snowflake', 'google cloud', 'redshift', 'azure', 'aws']",[],[],['kafka'],bachelor,2-5 years
Softcrylic,4.0,Remote,Senior Data Engineer,"Who We Are
For more than 20 years, we have been working with organizations large and small to help solve business challenges through technology. We bring a unique combination of engineering and strategy to Make Data Work for organizations.
Our clients range from the travel and leisure industry to publishing, retail and banking. The common thread between our clients is their commitment to making data work as seen through their investment in those efforts.
In our quest to solve data challenges for our clients, we work with large enterprise, cloud based and marketing technology suites. We have a deep understanding of these solutions so we can help our clients make the most of their investment in an efficient way to have a data driven business.
Why Work at Softcrylic?
Softcrylic provides an engaging, team-focused, and rewarding work environment where people are excited about the work they do and passionate about delivering creative solutions to our clients.
We are looking to add a Senior Data Engineer to our team! This is a 100% Remote role and preference will be given to candidates from Atlanta, NJ or Texas regions.
Job Description:
Softcrylic is looking for a Senior Data Engineerwith strong design, development, and team leadership skills. The person should be working with Clients / Customer and with our internal (onshore and offshore) members to design, develop and rollout data projects. The person to be hands on and have strong leadership/communication and interpersonal skills.
Requirement:
· 5 to 7 years of experience in working as a Data Engineer.
· Strong experience in Python.
· Experience in working on GCP.
· Good experience in Airflow.
· Should have good experience in ETL pipeline design and development.
· Very good experience in SQL
· Experience in working on Redshift.
· Excellent designing and documentation (diagrams) and presentation skills.
· Data Quality Concepts are must have.
· Must know design and development of any of industry leading graph databases.
· Good communication skills.
· Independent thinker, good team player with Data Engineering Design skills.
· Work with minimum guidelines.
Plus:
GCP - Big Query
Agile background
Microsoft Power BI
Graph Database
Job Types: Full-time, Contract
Pay: $130,000.00 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Compensation package:
Performance bonus
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
GCP: 3 years (Preferred)
Python: 4 years (Preferred)
Work Location: Remote
Speak with the employer
+91 609.241.9641",135000,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$5 to $25 million (USD),Remote,23,data engineer,senior,"['python', 'sql']","['redshift', 'gcp']",['power bi'],[],['airflow'],,2-5 years
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
Infinity Quest,4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote",105128,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable,NJ,17,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,0-2 years
NIVID Technologies,3.7,Remote,Sr. Data Engineer,"We hope this Job meets your skills and expectations. If you are available and interested, please contact me at your earliest convenience. You will be working with a highly skilled team of IT Professionals in a high pace corporate environment. This opportunity will move quickly and candidates will be interviewed in the order they apply. Job Description/ Required Skills: (i) Strong hands-on programming experience in Python (ii) Hands-on experience of API development (from application / software engineering perspective) (iii) AWS Lambda and data streaming ingestion (Kinesis) (iv) AWS tech stack from data engineering stand-point
Job Types: Full-time, Contract, Permanent
Salary: $39.76 - $86.23 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",113400,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$5 to $10 billion (USD),Remote,11,data engineer,senior,"['python', 'sql']",['aws'],[],[],[],,0-2 years
"Pomeroy Technologies, LLC.",3.0,"Cleveland, OH",Data Engineer,"The Data Engineer is responsible for the development, integration, and implementation of ETL Feeds in support of various modern architecture projects. This role requires hands-on knowledge of modern Integrations Platform solutions, file, and database ETL, and XML transformation. The incumbent will act as a technical resource, participating in the successful design, development, unit testing, implementation, and support of new and existing ETL integrations.
Essential Functions
Must be able to acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design.
Working with manager, gathering business requirements for the development of new applications.
Perform programming assignments based on established standards, methods, and best practices.
Develop ETL solutions as assigned in supporting new or updated business solutions and requirements.
Help business users develop functional requirements for integration.
Provide support during user acceptance testing.
Prepare documentation for code changes.
Code ETL feeds, and conduct tests to verify correct program functionality, reliability, and data integrity.
Maintain and modify ETL feeds; make approved changes by amending application documentation, developing detailed programming logic, and coding changes.
Provide support and resolution for application problems and issues.
Provide user and system documentation for operational and technical support.
Assist in the discovery and investigation of production problems as required.
Support ongoing daily use of ETL environments and Integrations platforms.
Provide on-call support for critical application problems and issues.
Analyze program performance and take action to correct deficiencies based on consultation with clients and approval of supervisor.
required.Work outside the standard office 7.5 hour workday may be
Experience Requirements
2+ years of IT experience.
1+ years of experience building ETL feeds using any ETL tool (Talend, Informatica, etc.)
Knowledge of the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment, and post-production support.
Understanding of XML, JSON, XSD, WSDL, REST, and SOAP APIs.
Experience in developing SQL programs.
Job Type: Full-time
Pay: From $80,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Work Location: Hybrid remote in Cleveland, OH 44115",80000,1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable,OH,41,data engineer,na,['sql'],[],[],[],[],,+10 years
BCVS Group INC,5.0,"Oakland, CA",Data engineer lead,"Role: Data Engineer Lead
Location: Oakland, CA
Description
Qualitest is looking for a Data Architect to join our team.
Must Have-
Seasoned data professional with experience implementing solutions across Data Quality, Data Assurance,
Data Governance and Test Data Management
Ability to assess current state, recommend solutions and build a rollout strategy
Analyze existing data landscape, business processes and create knowledge articles (architectures, data flows, tools)Design, Build and Deliver quality gates across data lifecycle for automated and continuous data quality assurance.
Integrate data testing solutions with Automation, CI/CD Pipelines for Continuous Testing
Working experience on various database technologies such as Oracle, SQL Server, MongoDB, Snowflake, Teradata
Good understanding about QA Test Management, Defect Management and Testing methodologies. Evaluate tools, build proof of concepts, and implement NextGen solutions
Work with customer executives and cross functional teams for seamless project implementation and delivery
Mentor onshore/offshore teams to deliver projects in time and assist in case of delays/clarifications
Experience working on data quality validation tools such as RightData, Icedq, Informatica, Tosca DI, Datagaps
Experience working on TDM tools such as Informatica TDM , IBM Optim, Delphix, CA TDM/Broadcom TDM
Proficient in AWS Cloud Services such as Glue, Lambda, RedShift, Dynamo DB and Databricks
Experience working on Data Warehouse, Data Lake, Big Data platforms
Requirements
Good to Have-
Knowledge about Enterprise Applications such as Salesforce, SAP, Veeva CRM
AI/ML knowledge on building Intelligent data validations and eliminate data anomalies
Job Types: Full-time, Contract
Salary: From $41.80 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Oakland, CA 94619: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 +12019071788",75240,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,['sql'],"['snowflake', 'oracle', 'redshift', 'databricks', 'aws']",['sap'],"['sql server', 'mongodb']",[],,0-2 years
PepsiCo,4.0,"Plano, TX",Azure Data Engineer,"As a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems.
Responsibilities:
Active contributor to code development in projects and services.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.
Responsible for implementing best practices around systems integration, security, performance and data management.
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.
Develop and optimize procedures to “productionalize” data science models.
Define and manage SLA’s for data products and processes running in production.
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale.
Research in state-of-the-art methodologies.
Create documentation for learnings and knowledge transfer.
Create and audit reusable packages or libraries.
Requirements:
2+ years of overall technology experience that includes at least 2+ years of hands-on software development, data engineering, and systems architecture.
2+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
1+ years in cloud data engineering experience in Azure Certification is a plus.
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.
Experience with version control systems like Github and deployment & CI tools.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus
Understanding of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI)
Covid-19 vaccination may be a condition of employment dependent on role and location. For specific information, please discuss role requirements with the recruiter
Education
BA/BS in Computer Science, Math, Physics, or other technical fields
Skills, Abilities, Knowledge
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.
Proven track record of leading, mentoring data teams.
Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.
Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.
Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.
Foster a team culture of accountability, communication, and self-management.
Proactively drives impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals
Ability to lead others without direct authority in a matrixed environment.
Job Type: Full-time
Pay: $85,000.00 - $90,000.00 per year
Schedule:
Monday to Friday
Work Location: Hybrid remote in Plano, TX 75024",87500,10000+ Employees,Company - Public,Manufacturing,Food & Beverage Manufacturing,1965,$10+ billion (USD),TX,58,data engineer,na,"['python', 'scala', 'sql']",['azure'],[],[],[],,+10 years
Lendem Solutions,4.2,"Plano, TX",Data Engineer,"LENDEM Solutions is looking at add a Data Engineer to our business!
CORE COMPETENCIES
Ability to thrive in a dynamic and fast-paced environment, drive change, and collaborate effectively with a variety of individuals and teams
Strong analytical and problem-solving skills
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
PRINCIPAL DUTIES
Shredding and parsing data to extract meaningful information
Preparing historical and live data for data studies to identify trends and patterns
Performing adhoc analysis to answer specific business questions and provide insights
Working with relational databases to model and query complex data relationships
Understanding and working with MySQL data in several different data environments
Mining consumer loan data utilizing tools such as SQL, Python, R, or other comparable data mining tools.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Engineering, or a related field Master’s degree preferred
3+ years of experience as a data engineer or similar role
REQUIRED SKILLS, ABILITIES, SOFT SKILL FACTORS
· Strong experience in ETL processes, data modeling, and data warehousing
Experience working with graph databases, such as Neo4j or Apache Cassandra
Expertise in programming languages such as SQL, Python
Familiarity with big data technologies, such as Hadoop, Spark, or Kafka
Ability to analyze and manipulate large and complex data sets
Strong problem-solving skills and the ability to work independently and as part of a team
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
If you are a data engineer with a passion for problem-solving and a strong background in ETL processes, data modeling, and graph databases, we encourage you to apply for this exciting opportunity. We offer a competitive salary, comprehensive benefits, and a dynamic work environment where you can continue to grow and develop your skills.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Plano, TX 75024: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 5 years (Preferred)
SQL: 5 years (Preferred)
Work Location: Hybrid remote in Plano, TX 75024",106385,,,,,-1,,TX,-1,data engineer,na,"['python', 'sql', 'r']",[],[],"['mysql', 'neo4j']","['kafka', 'hadoop', 'spark']",bachelor,5-10 years
Ascendion,4.5,Remote,Senior Data Engineer,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote",117000,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable,Remote,1,data engineer,senior,"['scala', 'sql', 'python', 'java', 'nosql']","['azure', 'databricks']",['ssis'],['mongodb'],[],,5-10 years
Arthur Grand Technologies Inc,4.8,"Jersey City, NJ",Azure Tech Lead/ Sr Data Engineer,"Role: Azure Tech Lead/ Sr Data Engineer – Onsite role – Preferred locals
Location: Jersey City, New Jersey / Fort Mill, South Carolina.
Full-time
Mandatory Skills: MS Azure using Azure Data Factory, MS Synapse, Scala, Spark, Data Warehousing
Skills:
Over all 12 to 15 years of experience with Data Management, Data Warehousing and Analytics.
At least 4 to 5 years of experience in Architecting and Implementing Data Solutions.
At least 3 years of experience in implementing the data solutions on MS Azure using Azure Data Factory, MS Synapse.
One to two years of experience in Azure Synapse Analytics is plus.
Installing and configuring ADF integration runtimes and linked services.
At least one hands on experience with Big data platform tool selection POC.
Two years of experience in data migrations to Azure by using data box or Data migration Services.
Apache Spark experience using Scala or PySpark or pre-packaged tools like Databrick is must.
Extensive hands-on experience in data warehousing design, tuning and ETL/ELT process development by using cloud native technologies.
At least one year experience with unified data governance solution using MS Purview.
Developing the CICD pipeline for Azure Infrastructure, version control strategy and Integrate source control ( Azure repos)
In-depth understanding of various storage services offered by Azure.
Experience with implementation of data security, encryption, PII/PSI legislation, identity and access management across sources and environments.
Experience with data process Orchestration, end-to-end design and build process of Near-Real Time and Batch Data Pipelines.
Certification in Azure data engineering and solution architecture Azure is must.
Strong client-facing communication and facilitation skills.
Job Type: Full-time
Salary: $81,075.29 - $186,473.81 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Experience:
Azure: 4 years (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road",133775,51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable,NJ,11,data engineer,senior,"['scala', 'sql']",['azure'],[],[],['spark'],,0-2 years
ConnectiveRx,3.0,"Hanover, NJ",Sr. Data Engineer,"ConnectiveRx is a leading, technology-enabled healthcare services company. We work strategically with hundreds of biopharmaceutical manufacturers to help commercialize and maximize the benefits of specialty and branded medications. Our mission is to simplify how patients get on and stay on therapy. We fulfill our mission by providing our customers with innovative services such as patient and provider messaging, the design and operation of copay, vouchers and patient affordability programs, and hub services, all of which accelerate speed-to-therapy and help improve outcomes for manufacturers, healthcare providers and patients.

ConnectiveRx was formed in 2015 by bringing together the industry-leading business of PSKW, PDR/LDM, Careform (2017) and The Macaluso Group (2018) to advance our technology-driven expertise in providing state-of-the-art commercialization solutions. To learn more about our company, visit ConnectiveRx.com

Job Description

What you will do:
Looking for a seasoned Senior Data Engineer to help us continue to build out our new Enterprise Data Platform. This person must have a strong understanding and demonstrated experience with data streaming architectures that leverage microservice & message-oriented integration patterns and practices within AWS cloud native technologies. This person will help to scale our data ingestion pipelines which are at the core of our Enterprise Data Platform which supports our client reporting as well as our internal analytics & operational teams.

The successful candidate will:
Work with senior leadership, architects, engineers, data analysts, product managers and cloud infrastructure teams to deliver a new features and capabilities.
Write clean, robust, and well-thought-out code with an emphasis on quality, performance, scalability, and maintainability.
Demonstrate strong end to end ownership & craftsmanship - analysis, design, code, test, debug, and deploy
Your ability to traverse the full stack within AWS server-less technologies will be an asset to us as we evaluate the tradeoffs inherent in software engineering. You have the product driven development mindset and can work closely with BA’s and Product teams to breakdown requirements and translate business workflows into scalable technical solutions.

What we’d like from you:
Strong Python & strong SQL
Extensive relational DB experience (Redshift, SQL Server, PostgresSQL) with exposure to document DBs such as DynamoDB. ElasticSearch.
Experience with designing solutions that run in AWS cloud technologies (Lambda, ECS, DynamoDB etc), docker containers
Message oriented architectures, patterns and tools, CQRS, event streaming, Kafka, SQS
Change data capture concepts, Database Triggers, AWS DMS
Data lake concepts, data catalogs, meta data etc
CICD Pipelines
Event store processing, data validation, operational logging via AWS Cloud Watch
Why work with us?
Excellent company culture, fun events, and volunteer opportunities
Competitive benefits (medical, dental, vision & more)
401k package with dollar-for-dollar match-up
Generous PTO and paid holidays days offered
Opportunities to grow professionally and personally
Team-oriented atmosphere
#LI-BJ1

Equal Opportunity Employer: This employer (hereafter the Company) is an equal opportunity employer and does not discriminate in recruitment, hiring, training, promotion, or other employment policies on the basis of age, race, sex, color, religion, national origin, disability, veteran status, genetic information, or any other basis that is prohibited by federal, state, or local law. No question in this application is intended to secure information to be used for such discrimination. In addition, the Company makes reasonable accommodation to the needs of disabled applicants and employees, so long as this does not create an undue hardship on the Company or threaten the health or safety of others at work. This application will be given every consideration, but its receipt does not imply that the applicant will be employed.",115021,1001 to 5000 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2015,Unknown / Non-Applicable,NJ,8,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],"['dynamodb', 'sql server', 'elasticsearch']",['kafka'],,
Airbus Americas,4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.",92046,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD),GA,53,data engineer,na,"['python', 'r', 'java']",[],[],[],['spark'],bachelor,2-5 years
"Second Wave Delivery Systems, LLC",4.2,Remote,Data Engineer,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker",106385,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020,Unknown / Non-Applicable,Remote,3,data engineer,na,"['python', 'scala', 'sql', 'java']","['bigquery', 'google cloud']",[],[],"['airflow', 'spark']",,
Monogram Health Renal Services,4.2,"Brentwood, TN",Azure Data Engineer,"Position: Azure Data Engineer

The Azure Data Engineer will work with our business operations and technology teams to build out an enterprise-wide Data Warehouse and related systems in the cloud. Will collaborate with our internal teams and 3rd party vendor in building our future state, business process, system interfaces, governance and technology platforms needed to enable a single source of truth for product and customer data, drive a more sophisticated data environment and be positioned to scale for future business growth.

Roles and Responsibilities
In collaboration with our 3rd party vendor, implement strategic roadmap and operational framework for Enterprise Data Management, in partnership with key stakeholders
Collaborate on the strategy on new cloud data stores and migration of existing data in Azure
Work collaboratively with teams to design, document, and ensure continuous improvement of SDLC best practices for data in the cloud
Be an advocate for quality and security through data quality and accuracy validation and auditing of security best practices
Focus on understanding business problems and determine what aspects require optimization; articulate those aspects in a clear and concise manner
Work collaboratively with cross-functional teams to support Data Models in alignment with business needs
Partner with IT, on design and oversight of APIs and other data sharing capabilities in collaboration with short / long term business needs
Partner with IT to integrate new data management technologies and software engineering tools (automation projects, process improvements, etc.) into existing systems to enable single source of truth for product and customer data
Adhere to identified key performance indicators for quality and compliance metrics to ensure data related policies and standards are followed
Acts as a SME to explain all facets of EDM data exchange and processing, business user involvement, and overall architecture
Support Monogram as necessary with any tasks required to deliver excellent personalized kidney care and perform all other duties as assigned
Uphold the mission and values of Monogram Health in all aspects of your role and activities
Position Requirements
Nashville, TN
BS or equivalent experience in crucial duties and responsibilities
3-5 years’ experience in cloud computing
Solid understanding of enterprise data management in Healthcare Data
Solid understanding of data, databases, data tools (e.g. SQL) and data processing experience
Solid understanding of standard ETL tools and techniques, sourcing, maintaining, and updating data, data warehousing, data cleansing, and other analytical techniques required for data usage.
Experience with database systems, Azure cloud storage, and significant exposure to or experience with modern data and BI platforms (such as Databricks, MSSQL Data Warehouse (Synapse), Snowflake, and Tableau).
Previous experience with data migration to cloud-based environment
Interest and passion in designing and implementing data management, querying, and storage, with a particular focus on purpose-built data stores.
Functional knowledge of data visualization tools such as PowerBI and Tableau.
Strong intuition for business and analytical skills
Team player with excellent communication skills

Benefits

Opportunity to work in a dynamic, fast-paced and innovative value-based provider organization that is transforming the delivery of kidney care
Competitive salary and opportunity to participate in the company’s bonus program
Comprehensive medical, dental, vision and life insurance
Flexible paid leave and vacation policy
401(k) plan with matching contributions

About Monogram Health
Monogram Health is a next-generation, value-based chronic condition risk provider serving patients living with chronic kidney and end-stage renal disease and their related metabolic disorders. Monogram seeks to fill systemic gaps and transform the way nephrology, primary care and chronic condition treatment are delivered. Monogram’s innovative, in-home approach utilizes a national nephrology practice powered by a suite of technology-enabled clinical services, including case and disease management, utilization management and review, and medication therapy management services that improve health outcomes while lowering medical costs across the healthcare continuum. By focusing on increasing access to evidence-based care pathways and addressing social determinants of health, Monogram has emerged as an industry leader in championing greater health equity and improving health outcomes for individuals with chronic kidney and end-stage renal disease.

At Monogram Health we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.
Experience
Required
SQL
healthcare data
cloud computing
Azure
Preferred
Tableau
Snowflake
Data Warehouse
Databricks
Power BI",106385,,,,,-1,,TN,-1,data engineer,na,['sql'],"['snowflake', 'azure', 'databricks']","['tableau', 'power bi']",[],[],,5-10 years
Talent-One,4.2,"Imperial, CA",Data Engineer,"Responsibilities:
Develop predictive models
Develop optimization models
Develop re-activation and retention models
Advanced analytics to drive incremental revenue
Identify performance metrics definition, algorithm development and automation
Reporting and visualization
Complex data analysis tasks
Data anomaly detection and correction modeling
Conversion of data into stories for internal and external consumption
Cross-team support for CRM and Database Marketing Teams.
Qualifications:
Bachelor’s degree in an Analytical field (Business, Marketing) required.
At least three (3) years casino database experience required, or the equivalent combination of education and experience in data analysis.
SAS programming level 1 or higher certification required.
Ability to use data to solve complex business problems.
Advanced SQL skills
Strong industry experience of Microsoft Office Suite, including Excel, Word, Access, required
Job Type: Full-time
Salary: $1.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",106385,,,,,-1,,CA,-1,data engineer,na,['sql'],[],"['sas', 'excel']",[],[],bachelor,0-2 years
Savvy Technology Solutions,4.2,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location",107339,,,,,-1,,DC,-1,data engineer,senior,"['sql', 'nosql']","['aws', 'redshift']","['tableau', 'ssis']",['mysql'],[],master,2-5 years
Manufacturers Bank,3.2,"Los Angeles, CA",Data Engineer,"GENERAL SUMMARY:
Manufacturers bank is looking for a Data Engineer with strong software engineering backgrounds, a ""platform engineering mindset"" and deep passion for data. This Engineer will help to shape the technical direction of the data domain within the organization. The Engineer will also partner closely with internal business units and external vendors to develop a high performance, reliable and scalable data environment for the bank.

PRINCIPAL DUTIES & RESPONSIBILITIES:
Develop and automate high-performance data processing systems and visualization to ensure reliability and meet critical business requirements.
Design, develop, implement, document and support Data & Analytics systems which adhere to the industry best practices.
Lead data engineering projects, and overall strategy for data governance, security, privacy, quality, and retention.
Triage and resolve issues reported by Risk, Operations, and other business stakeholders.
Mentor junior engineers and continue promoting data engineering and analytics tooling & standards.",115908,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),CA,-1,data engineer,na,[],[],[],[],[],,
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['python', 'sql', 'java', 'nosql']","['azure', 'google cloud', 'aws']",[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,+10 years
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['python', 'sql']",[],[],[],[],master,
LOVEFOODIES INC,4.2,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",54000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['python', 'sql']",[],['tableau'],[],[],,2-5 years
Invictus Data,4.2,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500,,,,,-1,,CA,-1,data engineer,senior,"['python', 'scala', 'sql']","['azure', 'aws', 'gcp']",[],['hive'],"['kafka', 'hadoop', 'spark']",,2-5 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['python', 'sql', 'go']","['snowflake', 'databricks', 'aws']",['ssis'],[],['spark'],,
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],[],"['airflow', 'hadoop', 'spark']",,2-5 years
YT Global Network,5.0,Remote,Data Engineer- Remote,"Data Engineer- Remote
Role: Data and Analytics is an evolving space which includes more software engineering, distributed systems, and cloud skills.
WIll develop, maintain, and enhance the data platform capabilities in an open and collaborative environment to build the central platform.
Will collaborate with internal data customers across IT and the Business to minimize the time from idea inception to analytical insight.
Job responsibilities will include: contributing to data infrastructure design efforts and collaborating with other platforms to integrate infrastructure into the client's systems and testing the feasibility and effectiveness of various technology options; supporting complex tools and solutions to manage orchestration, data pipelines, and infrastructure as code solutions the Data Engineering team builds.
Required skills:
Proven experience in designing, building, and supporting complex data pipelines using a variety of traditional and non-traditional data sources.
Version Control and associated best practices
Advanced programming experience in programming languages used in analytics and data science (e.g. Python, Java, Scala). Comfortable with Linux environments and shell scripting.
Experience with Cloud-based infrastructures (AWS)
Experience working with SQL/NoSQL
Experience utilizing data pipeline orchestration frameworks.
Verbal Communication
Preferred skills and experiences:
Analysis
API Development
CI/CD
Creating Real Time or Streaming Systems
Data Governance
Data Lineage
Data Metadata
Data Testing
Distributed Databases
Domain Knowledge
Schema
Snowflake
Visual Communication
EDUCATION AND/OR EXPERIENCE REQUIRED:
Education and/or experiences listed below are the minimum requirements for job entry.
Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of two years of work experience designing, programming, and supporting software programs or applications.
In lieu of degree, minimum of four years related work experience designing, programming, and supporting software programs or applications may be accepted.
Job Types: Full-time, Contract
Pay: $90.00 - $120.00 per hour
Benefits:
Health insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote",189000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['scala', 'sql', 'shell', 'python', 'java', 'nosql']","['snowflake', 'aws']",[],[],[],bachelor,5-10 years
Edrstaffing,4.2,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],,2-5 years
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],"['azure', 'databricks']",['power bi'],[],[],,5-10 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
Invictus Data,4.2,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500,,,,,-1,,CA,-1,data engineer,senior,"['python', 'scala', 'sql']","['azure', 'aws', 'gcp']",[],['hive'],"['kafka', 'hadoop', 'spark']",,2-5 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
LOVEFOODIES INC,4.2,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",54000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['python', 'sql']",[],['tableau'],[],[],,2-5 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,['sql'],"['oracle', 'azure']","['power bi', 'ssis']",['sql server'],[],bachelor,
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
"Double Line, Inc.",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",85882,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['python', 'sql']","['azure', 'aws']","['tableau', 'power bi']",[],[],,
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['python', 'sql', 'go']","['snowflake', 'databricks', 'aws']",['ssis'],[],['spark'],,
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['python', 'sql']",[],[],[],[],master,
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
Sky Consulting Inc,4.2,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.",103312,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,['python'],['gcp'],[],['hive'],"['kafka', 'hadoop', 'spark']",,
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
Monogram Health Renal Services,4.2,"Brentwood, TN",Azure Data Engineer,"Position: Azure Data Engineer

The Azure Data Engineer will work with our business operations and technology teams to build out an enterprise-wide Data Warehouse and related systems in the cloud. Will collaborate with our internal teams and 3rd party vendor in building our future state, business process, system interfaces, governance and technology platforms needed to enable a single source of truth for product and customer data, drive a more sophisticated data environment and be positioned to scale for future business growth.

Roles and Responsibilities
In collaboration with our 3rd party vendor, implement strategic roadmap and operational framework for Enterprise Data Management, in partnership with key stakeholders
Collaborate on the strategy on new cloud data stores and migration of existing data in Azure
Work collaboratively with teams to design, document, and ensure continuous improvement of SDLC best practices for data in the cloud
Be an advocate for quality and security through data quality and accuracy validation and auditing of security best practices
Focus on understanding business problems and determine what aspects require optimization; articulate those aspects in a clear and concise manner
Work collaboratively with cross-functional teams to support Data Models in alignment with business needs
Partner with IT, on design and oversight of APIs and other data sharing capabilities in collaboration with short / long term business needs
Partner with IT to integrate new data management technologies and software engineering tools (automation projects, process improvements, etc.) into existing systems to enable single source of truth for product and customer data
Adhere to identified key performance indicators for quality and compliance metrics to ensure data related policies and standards are followed
Acts as a SME to explain all facets of EDM data exchange and processing, business user involvement, and overall architecture
Support Monogram as necessary with any tasks required to deliver excellent personalized kidney care and perform all other duties as assigned
Uphold the mission and values of Monogram Health in all aspects of your role and activities
Position Requirements
Nashville, TN
BS or equivalent experience in crucial duties and responsibilities
3-5 years’ experience in cloud computing
Solid understanding of enterprise data management in Healthcare Data
Solid understanding of data, databases, data tools (e.g. SQL) and data processing experience
Solid understanding of standard ETL tools and techniques, sourcing, maintaining, and updating data, data warehousing, data cleansing, and other analytical techniques required for data usage.
Experience with database systems, Azure cloud storage, and significant exposure to or experience with modern data and BI platforms (such as Databricks, MSSQL Data Warehouse (Synapse), Snowflake, and Tableau).
Previous experience with data migration to cloud-based environment
Interest and passion in designing and implementing data management, querying, and storage, with a particular focus on purpose-built data stores.
Functional knowledge of data visualization tools such as PowerBI and Tableau.
Strong intuition for business and analytical skills
Team player with excellent communication skills

Benefits

Opportunity to work in a dynamic, fast-paced and innovative value-based provider organization that is transforming the delivery of kidney care
Competitive salary and opportunity to participate in the company’s bonus program
Comprehensive medical, dental, vision and life insurance
Flexible paid leave and vacation policy
401(k) plan with matching contributions

About Monogram Health
Monogram Health is a next-generation, value-based chronic condition risk provider serving patients living with chronic kidney and end-stage renal disease and their related metabolic disorders. Monogram seeks to fill systemic gaps and transform the way nephrology, primary care and chronic condition treatment are delivered. Monogram’s innovative, in-home approach utilizes a national nephrology practice powered by a suite of technology-enabled clinical services, including case and disease management, utilization management and review, and medication therapy management services that improve health outcomes while lowering medical costs across the healthcare continuum. By focusing on increasing access to evidence-based care pathways and addressing social determinants of health, Monogram has emerged as an industry leader in championing greater health equity and improving health outcomes for individuals with chronic kidney and end-stage renal disease.

At Monogram Health we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.
Experience
Required
SQL
healthcare data
cloud computing
Azure
Preferred
Tableau
Snowflake
Data Warehouse
Databricks
Power BI",106385,,,,,-1,,TN,-1,data engineer,na,['sql'],"['snowflake', 'azure', 'databricks']","['tableau', 'power bi']",[],[],,5-10 years
Infinity Quest,4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote",105128,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable,NJ,17,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,0-2 years
Teamware Solutions (quantum leap consulting).,4.6,"South San Francisco, CA",Data Engineer - Onsite,"Hi,
Data Engineer
Bay Area, CA – Onsite(Hybrid)
Client: Decision Minds/PANW
Duration: Contract
Exp Level: 10+ Years
Must have skill: Google cloud exp
Job Responsibilities:
Expert in data engineering and GCP data technologies.
Work with client teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
Work with Agile and DevOps techniques and implementation approaches in the delivery
Key responsibilities: Architecture, Design and Development
Required Skills:
10+ Year experience in BI and Analytics
Hands on and deep experience ( at least 2 years) working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, ]etc.).
Experience in Spark (Scala/Python/Java) and Kafka, Airflow
Data Engineering and Lifecycle (including non-functional requirements and operations) management.
E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.
Experience with SQL and NoSQL modern data stores.
Thanks & Regards
Jagadeesh
Teamware Solutions Inc |2838 E. Long Lake Road,Suite# 210, TROY, MI 48085
Job Type: Full-time
Salary: $60.00 - $65.00 per hour
Experience level:
10 years
11+ years
9 years
Schedule:
On call
Ability to commute/relocate:
South San Francisco, CA 94080: Reliably commute or planning to relocate before starting work (Required)
Experience:
Google Cloud Platform: 4 years (Preferred)
Data Engineer: 9 years (Preferred)
Spark: 4 years (Preferred)
Work Location: One location",112500,1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,Unknown / Non-Applicable,CA,20,data engineer,na,"['scala', 'sql', 'python', 'java', 'nosql']","['bigquery', 'google cloud', 'gcp']",[],[],"['kafka', 'airflow', 'spark']",,2-5 years
"MOBE, LLC",3.7,"Minneapolis, MN",Data Engineer (ETL),"MOBE
MOBE guides people to better health and more happiness. Behind our innovative health solutions is uniquely human philosophy. We believe that person-to-person connections and understanding can make a difference in a world where self-care can be complicated, and health care is ever-evolving and complex.
MOBE works with health plans and large employers to identify individuals who are frequent users of health care but aren?t finding resolutions for their underlying health issues. We use a whole-person approach and guidance to impact health outcomes positively.
Supporting people is at the core of our business, employees included. MOBE is a high-growth organization with a culture built on trust and collaboration. Consistent across our teams and offerings is a belief in the power of people doing good together. We genuinely care about people and consider our workforce the most significant asset.
Your Role at MOBE
This is an exciting time at MOBE and we are growing fast. At MOBE, we have a lot of data: eligibility, medical and pharmacy claims, marketing campaign impressions, transcripts from participant interactions, etc.
This position is responsible for providing technical and project expertise to enable MOBE analytics and operations with structured and unstructured data. Responsibilities includes executing and/or leading user story development, data design and architecture, data pipeline development, testing and deployment in the Analytic Data Framework. This role will partner with internal and external business and technology teams to drive project deliverables and ensure high quality delivery of data architecture and integration.
Responsibilities
The Data Engineer ensures the following capabilities and functions:
Translate high level business processes into logical data processing steps
Design data structures and pipelines that are flexible and scalable for MOBE analytics and operational requirements
Support Analytic partners through collaborative and transparent development, information delivery, problem resolution, shared insights, and training
Data processing definition, execution, and documentation, in a time appropriate way, to meet business priorities and requirements
Data quality and maintenance consistent within the Analytic Data Framework
Lead small to moderate sized projects and initiatives, following through on execution of chosen strategies and demonstrating the ability to work through obstacles and changing priorities.
Demonstrate ability and willingness to play multiple roles for different projects (e.g. planning/architecture, project development, hands-on technical resource/support for others, analysis and resolution of data issues)
Identify and constructively communicate the need for improvements or enhancements in MOBE technology assets
All other duties as assigned to help fulfill our Mission and abide by MOBE?s Guiding Principles",97357,51 to 200 Employees,Company - Private,Personal Consumer Services,Beauty & Wellness,2014,Unknown / Non-Applicable,MN,9,data engineer,na,[],[],[],[],[],,
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
LOVEFOODIES INC,4.2,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",54000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['python', 'sql']",[],['tableau'],[],[],,2-5 years
Arthur Grand Technologies Inc,4.8,"Atlanta, GA",AWS Data Engineer,"Role: AWS Data Engineer
Location: Atlanta, GA (Meet customer Monthly once post client selection & onboarding)
JD for AWS Data Engineer
Experience with the core AWS services, plus the specifics mentioned in this job description.
Experience with Docker . Having used ECS , SNS/SQS, Athena, Lambda,S3, RDS and layers in the past is a plus.
Proficiency in at least in Python, Java
Strong notions of security best practices (e.g. using IAM Roles, KMS, etc.).
Experience with monitoring solutions such as CloudWatch, Cloud Trail.
Previous exposure to large-scale systems design.
Knowledge of writing infrastructure as code (IaC) using CloudFormation or Terraform.
Experience with building or maintaining cloud-native applications.
Past experience with the serverless approaches using AWS Lambda is a plus. For example, the Serverless Application Model (AWS SAM).
Job Types: Full-time, Contract
Schedule:
10 hour shift
8 hour shift
Work Location: Remote",94994,51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable,GA,11,data engineer,na,"['python', 'java']",['aws'],[],[],[],,
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,['sql'],"['oracle', 'azure']","['power bi', 'ssis']",['sql server'],[],bachelor,
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['python', 'sql']",[],[],[],[],master,
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['python', 'sql', 'java', 'nosql']","['azure', 'google cloud', 'aws']",[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,+10 years
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['python', 'sql', 'go']","['snowflake', 'databricks', 'aws']",['ssis'],[],['spark'],,
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],"['azure', 'databricks']",['power bi'],[],[],,5-10 years
Edrstaffing,4.2,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],,2-5 years
"Double Line, Inc.",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",85882,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['python', 'sql']","['azure', 'aws']","['tableau', 'power bi']",[],[],,
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
Invictus Data,4.2,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500,,,,,-1,,CA,-1,data engineer,senior,"['python', 'scala', 'sql']","['azure', 'aws', 'gcp']",[],['hive'],"['kafka', 'hadoop', 'spark']",,2-5 years
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,['sql'],"['oracle', 'azure']","['power bi', 'ssis']",['sql server'],[],bachelor,
"Double Line, Inc.",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",85882,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['python', 'sql']","['azure', 'aws']","['tableau', 'power bi']",[],[],,
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['python', 'sql', 'go']","['snowflake', 'databricks', 'aws']",['ssis'],[],['spark'],,
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['python', 'sql']",[],[],[],[],master,
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['python', 'sql']",[],[],[],['spark'],bachelor,0-2 years
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],"['azure', 'databricks']",['power bi'],[],[],,5-10 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],[],"['airflow', 'hadoop', 'spark']",,2-5 years
Arthur Grand Technologies Inc,4.8,"Mount Laurel, NJ",Lead Informatica / Data Engineer,"Role: Lead Informatica / Data Engineer – On Prem –ETL (Onsite role) / Senior Informatica / Mid-Level Informatica
Location: Mount Laurel, NJ / Charlotte, NC
Duration : FTE
Client :: Hexaware / TD Bank
Key Skills: Informatica Power Centre, Autosys, Unix
Must Have
More than 12+ years of IT experience in Datawarehouse and ETL
Hands-on Experience on ETL Informatica Power Centre
Experience on Autosys, Unix and scripting knowledge on Python, Shell Scripts
Experience on Oracle Database
Ability to understand ETL Design, Source to target mapping (STTM) and create ETL specifications documents
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have
Any cloud experience on Azure or AWS or Informatica cloud connector
Any relevant certifications
Job Type: Full-time
Salary: $69,919.38 - $166,922.18 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 3 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road",118421,51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable,NJ,11,data engineer,senior,"['python', 'shell', 'sql']","['oracle', 'azure', 'aws']",[],[],[],,0-2 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
Invictus Data,4.2,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500,,,,,-1,,CA,-1,data engineer,senior,"['python', 'scala', 'sql']","['azure', 'aws', 'gcp']",[],['hive'],"['kafka', 'hadoop', 'spark']",,2-5 years
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['python', 'sql', 'go']","['snowflake', 'databricks', 'aws']",['ssis'],[],['spark'],,
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],"['azure', 'databricks']",['power bi'],[],[],,5-10 years
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108451,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['python', 'sql', 'java', 'nosql']","['azure', 'google cloud', 'aws']",[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,+10 years
Edrstaffing,4.2,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],,2-5 years
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['python', 'sql']",[],[],[],[],master,
AgileEngine,5.0,Remote,Senior/Lead Data Engineer,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Work collaboratively with other engineers, architects, data scientists, analytics teams, and business product owners in an agile environment
Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools
Design and build robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Must haves
Bachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience
6 years of experience with Python or Java
4+ years of experience in building data lake, cloud data platform leveraging cloud (GCP/AWS) cloud native architecture, ETL/ELT, and data integration
3 years of development experience with cloud services ( AWS,GCP,AZURE) utilizing various support tools (e.g. GCS, Dataproc, Cloud Data flow, Airflow(Composer), Kafka , Cloud Pub/Sub)
Expertise in developing distributed data processing and Streaming frameworks and architectures (Apache Spark, Apache Beam, Apache Flink, )
In-depth knowledge of NoSQL database technologies (e.g. MongoDB, BigTable, DynamoDB)
Expertise in build and deployment tools – (Visual Studio, PyCharm, Git/Bitbucket/Bamboo, Maven, Jenkins, Nexus, )
5 years of experience and expertise in database design techniques and philosophies (e.g. RDBMS, Document, Star Schema, Kimball Model)
5 years of experience with integration and service frameworks (e.g API Gateways, Apache Camel, Swagger API, Zookeeper, Kafka, messaging tools, microservices)
Expertise with containerized Microservices and REST/GraphQL based API development
Experience leveraging continuous integration/development tools (e.g. Jenkins, Docker, Containers, OpenShift, Kubernetes, and container automation) in a Ci/CD pipeline
Advanced understanding of software development and research tools
Attention to detail and results-oriented, with a strong customer focus
Ability to work as part of a team and independently
Problem-solving and technical communication skills
Ability to prioritize workload to meet tight deadlines
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
Our team of data experts and engineers has helped create a supply chain management platform used by Forbes 500 enterprises and featured by Gartner. We’ve accompanied the platform from an early prototype to a full-fledged solution based on big data, AI, and machine learning. Data quality is integral to our current engagement with this project as we’re expanding the platform functionality.
AgileEngine has also been working on data and ML solutions in multiple industries and verticals like telecom, FinTech, energy efficiency, and more. Our work with our clients’ data has contributed to patented software products, Nasdaq-grade fraud detection, and products used by Deloitte and T-Mobile.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",106385,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable,Remote,13,data engineer,senior,"['python', 'java', 'nosql']","['azure', 'aws', 'gcp']",[],"['dynamodb', 'mongodb']","['kafka', 'airflow', 'spark']",master,2-5 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,['sql'],"['oracle', 'azure']","['power bi', 'ssis']",['sql server'],[],bachelor,
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['python', 'sql', 'go']","['snowflake', 'databricks', 'aws']",['ssis'],[],['spark'],,
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['python', 'sql']",[],[],[],['spark'],bachelor,0-2 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],[],"['airflow', 'hadoop', 'spark']",,2-5 years
Savvy Technology Solutions,4.2,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location",107339,,,,,-1,,DC,-1,data engineer,senior,"['sql', 'nosql']","['aws', 'redshift']","['tableau', 'ssis']",['mysql'],[],master,2-5 years
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
Predict Health,4.2,"Arlington, VA",Data Engineer,"Overview of the Role
We are looking for an exceptional Data Engineer who is interested in joining a small and rapidly growing Product and Services team at our company. We view our data-driven products and services as strategic assets, and you will be a key part of our team and company. The successful candidate will have 5+ years of experience designing and implementing data solutions and services. This role will report to our VP, Product Engineering and will be an instrumental member of our product development team.
The Data Engineer will participate in the design of new information intensive product and service offerings. They will be responsible for the development and implementation of the data pipelines and services underlying these new products. They will deepen their understanding of the political, policy, financial and business aspects of the health plan industry so as to be able to provide insight into the best manner to collect and manage information related to these businesses.
The Data Engineer will work closely with the Product Management function to understand customer needs and help identify capabilities for new offerings. They will also partner with the Data Science team to understand the analytical needs of the business and assist in the design and implementation of data solutions to support these efforts.
About Predict Health, Inc.
Predict Health is transforming how health plans, providers and their partners increase retention, quality, and lifetime value of their customer or patient relationships. The core competency of the organization is the curation, analysis, and presentation of information to support targeted operational improvements in healthcare enterprises. The company is an early-stage enterprise based in Washington, D.C. which focuses on three solution categories: Member Acquisition, Quality Management, and Retention Management.
Our company, Predict Health was founded in 2019 and is led by an experienced team of healthcare experts, data scientists and healthcare entrepreneurs. We are focused on building services and solutions designed to improve the healthcare experience of our nation’s seniors. We founded our company after seeing first-hand how challenging it is for the seniors in our lives - including our parents, grandparents and friends - to deal with the complexities of Medicare with little or no real help.
Detailed Job Description
The Data Engineer will initially support the deployment of an Azure solution to help collect, aggregate, transform, and distribute information related to Predict Health’s current service offerings. The Data Engineer is responsible for designing the solution, building the data integration and associated pipelines, design and deploying data structures required to support current products and analytical efforts, and automating the operational aspects of the data platform. The Data Engineer responsibilities will include:
· Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as, but not limited to, Azure Synapse, Azure Data Factory and Azure pipelines.
· Translate business requirements from product owners and analysts into technical code.
· Identify, fix and document bugs, bottlenecks in workflows and pipelines.
· Enforce security compliance, fine tune performance and promote code quality standards.
· Identify and curate new data sources required to support business requirements
· Explore, learn the latest Azure data management technologies to add to current competencies.
· Support development of data intensive solutions in a fast-paced, dynamic environment
· Support Business Intelligence Applications (e.g. Power BI) as required.
Qualifications
Requires minimum of five years relevant experience in design, development, implementation, and testing of data services
· Experience in data structures, data modeling, data warehousing and data pipeline development on Azure
· 3+ years of hands-on experience in the design, implementation and performance tuning of Azure Data Factory, Synapse, or other Azure data services
· Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure
· Experience developing software code in one or more programming languages (Java, Python, etc.)
· Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform
· Experience with data migration
· Experience defining system architectures and exploring technical feasibility trade-offs.
· Ability to prototype and evaluate applications and interaction methodologies.
Other Knowledge, Skills, and Abilities Required
· Demonstrated capability for problem solving, decision making, sound judgement and assertiveness.
· Ability to change direction to meet demand in a high-paced work environment
· Ability to focus on details and technicalities while simultaneously retaining a comprehensive vision of organizational goals
· Excellent collaborator and able to influence and direct work contributions from others to meet critical program deliverables and timelines
· Advanced interpersonal, written, and oral communication skills.
· Proficiency in Microsoft Office including Excel and PowerPoint
What we Offer
This is a full-time position and compensation, and benefits will be highly competitive.
Predict Health is committed to bringing together people from different backgrounds and perspectives to deliver real results for our clients and end-users. We recruit, employ, compensate, and promote regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.
This position will be based in our Arlington, VA office. We offer a flexible work schedule with a minimum of three days in the office each week.
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Employee discount
Flexible schedule
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Arlington, VA: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",115000,,,,,-1,,VA,-1,data engineer,na,"['python', 'java']",['azure'],"['power bi', 'excel']",[],[],,+10 years
"Double Line, Inc.",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",85882,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['python', 'sql']","['azure', 'aws']","['tableau', 'power bi']",[],[],,
IBR (Imagine Believe Realize),4.5,"Melbourne, FL",Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",118866,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,FL,16,data engineer,senior,"['python', 'r', 'java']","['azure', 'aws', 'redshift']","['tableau', 'sas']",['redis'],"['hadoop', 'spark']",bachelor,
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,['sql'],"['oracle', 'azure']","['power bi', 'ssis']",['sql server'],[],bachelor,
Predict Health,4.2,"Arlington, VA",Data Engineer,"Overview of the Role
We are looking for an exceptional Data Engineer who is interested in joining a small and rapidly growing Product and Services team at our company. We view our data-driven products and services as strategic assets, and you will be a key part of our team and company. The successful candidate will have 5+ years of experience designing and implementing data solutions and services. This role will report to our VP, Product Engineering and will be an instrumental member of our product development team.
The Data Engineer will participate in the design of new information intensive product and service offerings. They will be responsible for the development and implementation of the data pipelines and services underlying these new products. They will deepen their understanding of the political, policy, financial and business aspects of the health plan industry so as to be able to provide insight into the best manner to collect and manage information related to these businesses.
The Data Engineer will work closely with the Product Management function to understand customer needs and help identify capabilities for new offerings. They will also partner with the Data Science team to understand the analytical needs of the business and assist in the design and implementation of data solutions to support these efforts.
About Predict Health, Inc.
Predict Health is transforming how health plans, providers and their partners increase retention, quality, and lifetime value of their customer or patient relationships. The core competency of the organization is the curation, analysis, and presentation of information to support targeted operational improvements in healthcare enterprises. The company is an early-stage enterprise based in Washington, D.C. which focuses on three solution categories: Member Acquisition, Quality Management, and Retention Management.
Our company, Predict Health was founded in 2019 and is led by an experienced team of healthcare experts, data scientists and healthcare entrepreneurs. We are focused on building services and solutions designed to improve the healthcare experience of our nation’s seniors. We founded our company after seeing first-hand how challenging it is for the seniors in our lives - including our parents, grandparents and friends - to deal with the complexities of Medicare with little or no real help.
Detailed Job Description
The Data Engineer will initially support the deployment of an Azure solution to help collect, aggregate, transform, and distribute information related to Predict Health’s current service offerings. The Data Engineer is responsible for designing the solution, building the data integration and associated pipelines, design and deploying data structures required to support current products and analytical efforts, and automating the operational aspects of the data platform. The Data Engineer responsibilities will include:
· Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as, but not limited to, Azure Synapse, Azure Data Factory and Azure pipelines.
· Translate business requirements from product owners and analysts into technical code.
· Identify, fix and document bugs, bottlenecks in workflows and pipelines.
· Enforce security compliance, fine tune performance and promote code quality standards.
· Identify and curate new data sources required to support business requirements
· Explore, learn the latest Azure data management technologies to add to current competencies.
· Support development of data intensive solutions in a fast-paced, dynamic environment
· Support Business Intelligence Applications (e.g. Power BI) as required.
Qualifications
Requires minimum of five years relevant experience in design, development, implementation, and testing of data services
· Experience in data structures, data modeling, data warehousing and data pipeline development on Azure
· 3+ years of hands-on experience in the design, implementation and performance tuning of Azure Data Factory, Synapse, or other Azure data services
· Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure
· Experience developing software code in one or more programming languages (Java, Python, etc.)
· Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform
· Experience with data migration
· Experience defining system architectures and exploring technical feasibility trade-offs.
· Ability to prototype and evaluate applications and interaction methodologies.
Other Knowledge, Skills, and Abilities Required
· Demonstrated capability for problem solving, decision making, sound judgement and assertiveness.
· Ability to change direction to meet demand in a high-paced work environment
· Ability to focus on details and technicalities while simultaneously retaining a comprehensive vision of organizational goals
· Excellent collaborator and able to influence and direct work contributions from others to meet critical program deliverables and timelines
· Advanced interpersonal, written, and oral communication skills.
· Proficiency in Microsoft Office including Excel and PowerPoint
What we Offer
This is a full-time position and compensation, and benefits will be highly competitive.
Predict Health is committed to bringing together people from different backgrounds and perspectives to deliver real results for our clients and end-users. We recruit, employ, compensate, and promote regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.
This position will be based in our Arlington, VA office. We offer a flexible work schedule with a minimum of three days in the office each week.
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Employee discount
Flexible schedule
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Arlington, VA: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",115000,,,,,-1,,VA,-1,data engineer,na,"['python', 'java']",['azure'],"['power bi', 'excel']",[],[],,+10 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],[],"['airflow', 'hadoop', 'spark']",,2-5 years
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['python', 'sql']",[],[],[],['spark'],bachelor,0-2 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['python', 'sql', 'go']","['snowflake', 'databricks', 'aws']",['ssis'],[],['spark'],,
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['python', 'sql']","['gcp', 'aws', 'redshift']",[],[],['airflow'],,
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
MARVEL TECHNOLOGIES INC,3.7,Remote,Data Engineer,"Primary Skills
SCALA, SPARK, SQL, HADOOP, AWS, HIVE, Spark SQL, HIVE QL, CICD (Continuous Integration/Continuous Delivery), VCS (GIT HUB)
Coding in Scala
Designing in of HADOOP ecosystem
Hands-on experience on AWS tools like EMR, EC2
Hands-on experience of SQL in Big Data: SQL, Spark SQL, Hive QL
Proficient in working with large data sets and pipelines
Proficient with workflow scheduling / orchestration tools
Well versed with CICD process and VCS
Databricks is a big PLUS
Job Types: Full-time, Contract
Pay: $50.00 - $58.00 per hour
Schedule:
8 hour shift
Monday to Friday
Experience:
Spark: 4 years (Required)
Scala: 4 years (Required)
Hadoop: 3 years (Required)
Aws: 3 years (Required)
Hive: 3 years (Required)
CI/CD, VCS: 3 years (Required)
Databricks: 1 year (Required)
Work Location: Remote",97200,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,$5 to $25 million (USD),Remote,-1,data engineer,na,"['scala', 'sql']","['databricks', 'aws']",[],['hive'],"['hadoop', 'spark']",,0-2 years
"Double Line, Inc.",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w",85882,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD),TX,14,data engineer,na,"['python', 'sql']","['azure', 'aws']","['tableau', 'power bi']",[],[],,
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
Nursa,4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!",89485,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable,UT,5,data engineer,na,"['python', 'sql']","['aws', 'redshift']",[],[],[],,+10 years
The Sunwater Institute,4.2,"Rockville, MD",Data Engineer,"The Sunwater Institute is a nonprofit, nonpartisan public policy organization focused on improving the foundations of our social systems. The Sunwater Institute’s current programs focus on improving congressional performance, preserving and advancing constitutional rights and liberties, strengthening and enlarging property rights, and exploring the foundations of group decision-making.
The Sunwater Institute’s interdisciplinary, long-term orientation and focus on the building blocks of social systems sets it apart as a haven for bold, fresh ideas and free, constructive dialogue. The Sunwater Institute is located in North Bethesda and maintains a presence on Capitol Hill.
POSITION
The Sunwater Institute is seeking a Data Engineer who has a background in engineering or computer science and is highly skilled in SQL and database services technologies. The ideal candidate will possess strong problem-solving skills, excellent communication, and a great technical aptitude. The individual in this position will provide project support and help drive team initiatives. The candidate will be creative, and comfortable working in small teams and coordinating with US and non-US-based developers.
Responsibilities
Develop data models to store and manage large amounts of structured and unstructured data
Design efficient ETL processes to move and integrate data from different sources into the enterprise warehouse
Write complex queries using SQL Server etc. to analyze large datasets
Identify trends, patterns, and correlations within datasets by performing exploratory analysis
Ensure high quality of data by implementing best practices on storage, maintenance, indexing techniques, etc
Coordinate project status updates between teams and communicate deliverables and timelines
REQUIREMENTS
Bachelor’s degree in Computer Science or Engineering related field
4+ years of relevant experience in a similar role
Strong knowledge of relational databases including SQL Server
Proficient skills in writing complex queries using SQL language; developing stored procs, views, etc.
Experience with end-to-end ETL (Extract Transform Load) Processes
Familiarity with Big Data platforms such as AWS etc.
Ability to develop effective reports that help drive business decisions
Must be able to work effectively with teams but be able to work independently on tasks to meet tight deadlines
Exceptional communication skills verbal, written, and active listening
Extreme attention to detail and accuracy
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Vision insurance
Compensation package:
Yearly bonus
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Rockville, MD 20852: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",106385,,,,,-1,,MD,-1,data engineer,na,['sql'],['aws'],[],['sql server'],[],bachelor,2-5 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,['sql'],"['oracle', 'azure']","['power bi', 'ssis']",['sql server'],[],bachelor,
Plaxonic Technologies,4.6,"Dallas, TX",Data Engineer,"Job Description – Sr. Data Engineer:
Minimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark
Ability to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark
Hands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies
Strong skill in understanding database architecture, data models and writing complex SQL queries/code
Hands on experience with integration of different data sources (Files, DBs, APIs)
Ability to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation
Knowledge of various ETL techniques
Experience with data pipeline and workflow management tools [Azure DevOps]
Strong analytic skill to work with unstructured data
Experience working with Agile teams scrums.
Strong customer handling skills and communication skills
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75287: Reliably commute or planning to relocate before starting work (Required)
Experience:
Microsoft Excel: 1 year (Preferred)
Microsoft Office: 1 year (Preferred)
Compliance management: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: One location
Speak with the employer
+91 7272167989",98641,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),TX,10,data engineer,na,['sql'],"['azure', 'databricks']",['excel'],[],[],,0-2 years
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
Oddball,4.6,Remote,Data Engineer,"Oddball believes that the best products are built when teams understand and value the things they are working on. We value learning and growth and the ability to make a big impact at a small company. We believe that we can make big changes happen and improve the daily lives of millions of people by bringing quality software to the federal space.
As a Database Engineer you will be responsible for helping to develop production-quality analytics tools, utilizing existing data sources to empower stakeholders to inform research and development. Act as a liaison between engineers and analysts. Implement and enforce standards and procedures to ensure data is managed consistently and properly integrated.
Experience Requirements
5+ years of proven data and performance engineering experience.
A proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs.
Experience with system analysis, data analysis or programming, using a variety of computer languages and procedures.
Experience working in Agile environments.
Experience in developing, analyzing, and presenting data models.
Knowledge of data management, data standardization, and data governance.
Ability to analyze source data for potential data quality issues.
Expert in SQL and/or SQL based languages.
Create supporting documentation, such as metadata and diagrams of entity relationships, business processes, and process flow.
Create plans, test files, and scripts for data warehouse testing, ranging from unit to integration testing.
This is a Remote position. This is a salaried role. Oddball offers both a tech, and a continuing education stipend.
Must be located within the United States.
Clearances
Ability to obtain low-level federal clearance is required.
Oddball is an Equal Opportunity Employer and does not discriminate against applicants based on race, religion, color, disability, medical condition, legally protected genetic information, national origin, gender, sexual orientation, marital status, gender identity or expression, sex (including pregnancy, childbirth or related medical conditions), age, veteran status or other legally protected characteristics. Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Work Location: Remote",106385,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,['sql'],[],[],[],[],bachelor,5-10 years
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
Edrstaffing,4.2,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],,2-5 years
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['python', 'sql']","['gcp', 'aws', 'redshift']",[],[],['airflow'],,
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['python', 'sql', 'go']","['snowflake', 'databricks', 'aws']",['ssis'],[],['spark'],,
Predict Health,4.2,"Arlington, VA",Data Engineer,"Overview of the Role
We are looking for an exceptional Data Engineer who is interested in joining a small and rapidly growing Product and Services team at our company. We view our data-driven products and services as strategic assets, and you will be a key part of our team and company. The successful candidate will have 5+ years of experience designing and implementing data solutions and services. This role will report to our VP, Product Engineering and will be an instrumental member of our product development team.
The Data Engineer will participate in the design of new information intensive product and service offerings. They will be responsible for the development and implementation of the data pipelines and services underlying these new products. They will deepen their understanding of the political, policy, financial and business aspects of the health plan industry so as to be able to provide insight into the best manner to collect and manage information related to these businesses.
The Data Engineer will work closely with the Product Management function to understand customer needs and help identify capabilities for new offerings. They will also partner with the Data Science team to understand the analytical needs of the business and assist in the design and implementation of data solutions to support these efforts.
About Predict Health, Inc.
Predict Health is transforming how health plans, providers and their partners increase retention, quality, and lifetime value of their customer or patient relationships. The core competency of the organization is the curation, analysis, and presentation of information to support targeted operational improvements in healthcare enterprises. The company is an early-stage enterprise based in Washington, D.C. which focuses on three solution categories: Member Acquisition, Quality Management, and Retention Management.
Our company, Predict Health was founded in 2019 and is led by an experienced team of healthcare experts, data scientists and healthcare entrepreneurs. We are focused on building services and solutions designed to improve the healthcare experience of our nation’s seniors. We founded our company after seeing first-hand how challenging it is for the seniors in our lives - including our parents, grandparents and friends - to deal with the complexities of Medicare with little or no real help.
Detailed Job Description
The Data Engineer will initially support the deployment of an Azure solution to help collect, aggregate, transform, and distribute information related to Predict Health’s current service offerings. The Data Engineer is responsible for designing the solution, building the data integration and associated pipelines, design and deploying data structures required to support current products and analytical efforts, and automating the operational aspects of the data platform. The Data Engineer responsibilities will include:
· Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as, but not limited to, Azure Synapse, Azure Data Factory and Azure pipelines.
· Translate business requirements from product owners and analysts into technical code.
· Identify, fix and document bugs, bottlenecks in workflows and pipelines.
· Enforce security compliance, fine tune performance and promote code quality standards.
· Identify and curate new data sources required to support business requirements
· Explore, learn the latest Azure data management technologies to add to current competencies.
· Support development of data intensive solutions in a fast-paced, dynamic environment
· Support Business Intelligence Applications (e.g. Power BI) as required.
Qualifications
Requires minimum of five years relevant experience in design, development, implementation, and testing of data services
· Experience in data structures, data modeling, data warehousing and data pipeline development on Azure
· 3+ years of hands-on experience in the design, implementation and performance tuning of Azure Data Factory, Synapse, or other Azure data services
· Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure
· Experience developing software code in one or more programming languages (Java, Python, etc.)
· Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform
· Experience with data migration
· Experience defining system architectures and exploring technical feasibility trade-offs.
· Ability to prototype and evaluate applications and interaction methodologies.
Other Knowledge, Skills, and Abilities Required
· Demonstrated capability for problem solving, decision making, sound judgement and assertiveness.
· Ability to change direction to meet demand in a high-paced work environment
· Ability to focus on details and technicalities while simultaneously retaining a comprehensive vision of organizational goals
· Excellent collaborator and able to influence and direct work contributions from others to meet critical program deliverables and timelines
· Advanced interpersonal, written, and oral communication skills.
· Proficiency in Microsoft Office including Excel and PowerPoint
What we Offer
This is a full-time position and compensation, and benefits will be highly competitive.
Predict Health is committed to bringing together people from different backgrounds and perspectives to deliver real results for our clients and end-users. We recruit, employ, compensate, and promote regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.
This position will be based in our Arlington, VA office. We offer a flexible work schedule with a minimum of three days in the office each week.
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Employee discount
Flexible schedule
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Arlington, VA: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",115000,,,,,-1,,VA,-1,data engineer,na,"['python', 'java']",['azure'],"['power bi', 'excel']",[],[],,+10 years
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
Ascent Solutions,4.2,Remote,Hadoop Big Data Engineer,"Hadoop Big Data Engineer
Remote
Must Haves • Strong experience in Big Data, Cloudera Distribution 7.x, Cloud migration, RDBMS • Strong Experience with Amazon EMR/Databricks/Cloudera CDP • 4-5 experience building data pipelines using Hadoop components Sqoop, Hive, Solr, MR, Impala, Spark, Spark SQL., HBase. • 4-5 years of programming experience in Python, Java and Scala is must. • Strong experience with REST API development using Python frameworks (Django, Flask etc.) • Micro Services/Web service development experience using Spring framework is highly desirable.
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,na,"['python', 'scala', 'sql', 'java']",['databricks'],[],['hive'],"['hadoop', 'spark']",,5-10 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,['sql'],"['oracle', 'azure']","['power bi', 'ssis']",['sql server'],[],bachelor,
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
IBR (Imagine Believe Realize),4.5,"Melbourne, FL",Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",118866,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,FL,16,data engineer,senior,"['python', 'r', 'java']","['azure', 'aws', 'redshift']","['tableau', 'sas']",['redis'],"['hadoop', 'spark']",bachelor,
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85831,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
Plaxonic Technologies,4.6,"Dallas, TX",Data Engineer,"Job Description – Sr. Data Engineer:
Minimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark
Ability to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark
Hands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies
Strong skill in understanding database architecture, data models and writing complex SQL queries/code
Hands on experience with integration of different data sources (Files, DBs, APIs)
Ability to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation
Knowledge of various ETL techniques
Experience with data pipeline and workflow management tools [Azure DevOps]
Strong analytic skill to work with unstructured data
Experience working with Agile teams scrums.
Strong customer handling skills and communication skills
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75287: Reliably commute or planning to relocate before starting work (Required)
Experience:
Microsoft Excel: 1 year (Preferred)
Microsoft Office: 1 year (Preferred)
Compliance management: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: One location
Speak with the employer
+91 7272167989",98641,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),TX,10,data engineer,na,['sql'],"['azure', 'databricks']",['excel'],[],[],,0-2 years
Invictus Data,4.2,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500,,,,,-1,,CA,-1,data engineer,senior,"['python', 'scala', 'sql']","['azure', 'aws', 'gcp']",[],['hive'],"['kafka', 'hadoop', 'spark']",,2-5 years
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
Ascent Technologies,4.2,Remote,Data Engineer,"Job Description: Involved in resource / volume ramp-up plan/ backfilling resources planned for rotation and resignation cases Tracking Performance of individual Projects within the delivery/SIPs. Participate / presenting SD performance during client visit Publish Daily, Weekly and Monthly reports on team productivity Reviewing the weekly / monthly dashboards Publishing dashboards to customer People Management Drive the team to reach the defined targets Ensure training and development of the team members Mentor team members on technical and communication aspects. onsite anni data enginneer-pheonix,az python,charlottee,nc ba-houston tx
Job Types: Full-time, Contract, Permanent
Salary: $39.87 - $86.24 per hour
Benefits:
Dental insurance
Compensation package:
1099 contract
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91 7842516676",113490,,,,,-1,,Remote,-1,data engineer,na,"['python', 'sql']",[],[],[],[],,0-2 years
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['python', 'sql']",[],[],[],['spark'],bachelor,0-2 years
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],[],"['airflow', 'hadoop', 'spark']",,2-5 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master,
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
Invictus Data,4.2,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500,,,,,-1,,CA,-1,data engineer,senior,"['python', 'scala', 'sql']","['azure', 'aws', 'gcp']",[],['hive'],"['kafka', 'hadoop', 'spark']",,2-5 years
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
Empower Federal Credit Union,4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000",99876,201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,-1,$25 to $100 million (USD),NY,-1,data engineer,na,['sql'],"['oracle', 'azure']","['power bi', 'ssis']",['sql server'],[],bachelor,
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['python', 'sql']",[],[],[],['spark'],bachelor,0-2 years
Enterprise Knowledge LLC,4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”",89080,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD),VA,10,data engineer,na,"['python', 'r', 'sql']",['aws'],[],[],"['airflow', 'spark']",,2-5 years
LOVEFOODIES INC,4.2,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",54000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['python', 'sql']",[],['tableau'],[],[],,2-5 years
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['python', 'sql', 'go']","['snowflake', 'databricks', 'aws']",['ssis'],[],['spark'],,
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master,
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],[],"['airflow', 'hadoop', 'spark']",,2-5 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['python', 'sql', 'go']","['snowflake', 'databricks', 'aws']",['ssis'],[],['spark'],,
Enterprise Knowledge LLC,4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”",89080,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD),VA,10,data engineer,na,"['python', 'r', 'sql']",['aws'],[],[],"['airflow', 'spark']",,2-5 years
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['python', 'sql']",[],[],[],['spark'],bachelor,0-2 years
IBR (Imagine Believe Realize),4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",106385,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,Remote,16,data engineer,senior,"['python', 'r', 'java']","['azure', 'aws', 'redshift']","['tableau', 'sas']",['redis'],"['hadoop', 'spark']",bachelor,
Invictus Data,4.2,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500,,,,,-1,,CA,-1,data engineer,senior,"['python', 'scala', 'sql']","['azure', 'aws', 'gcp']",[],['hive'],"['kafka', 'hadoop', 'spark']",,2-5 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
IBR (Imagine Believe Realize),4.5,"Melbourne, FL",Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",118866,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,FL,16,data engineer,senior,"['python', 'r', 'java']","['azure', 'aws', 'redshift']","['tableau', 'sas']",['redis'],"['hadoop', 'spark']",bachelor,
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master,
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],[],"['airflow', 'hadoop', 'spark']",,2-5 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['python', 'sql', 'go']","['snowflake', 'databricks', 'aws']",['ssis'],[],['spark'],,
Enterprise Knowledge LLC,4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”",89080,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD),VA,10,data engineer,na,"['python', 'r', 'sql']",['aws'],[],[],"['airflow', 'spark']",,2-5 years
Invictus Data,4.2,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500,,,,,-1,,CA,-1,data engineer,senior,"['python', 'scala', 'sql']","['azure', 'aws', 'gcp']",[],['hive'],"['kafka', 'hadoop', 'spark']",,2-5 years
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['python', 'sql']",[],[],[],['spark'],bachelor,0-2 years
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],[],"['airflow', 'hadoop', 'spark']",,2-5 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master,
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
Enterprise Knowledge LLC,4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”",89080,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD),VA,10,data engineer,na,"['python', 'r', 'sql']",['aws'],[],[],"['airflow', 'spark']",,2-5 years
Certec Consulting,5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)",92845,Unknown,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,WI,-1,data engineer,na,"['python', 'sql', 'go']","['snowflake', 'databricks', 'aws']",['ssis'],[],['spark'],,
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['python', 'sql']",[],[],[],['spark'],bachelor,0-2 years
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
IBR (Imagine Believe Realize),4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",106385,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,Remote,16,data engineer,senior,"['python', 'r', 'java']","['azure', 'aws', 'redshift']","['tableau', 'sas']",['redis'],"['hadoop', 'spark']",bachelor,
Ascent Technologies,4.2,Remote,Data Engineer,"Job Description: Involved in resource / volume ramp-up plan/ backfilling resources planned for rotation and resignation cases Tracking Performance of individual Projects within the delivery/SIPs. Participate / presenting SD performance during client visit Publish Daily, Weekly and Monthly reports on team productivity Reviewing the weekly / monthly dashboards Publishing dashboards to customer People Management Drive the team to reach the defined targets Ensure training and development of the team members Mentor team members on technical and communication aspects. onsite anni data enginneer-pheonix,az python,charlottee,nc ba-houston tx
Job Types: Full-time, Contract, Permanent
Salary: $39.87 - $86.24 per hour
Benefits:
Dental insurance
Compensation package:
1099 contract
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91 7842516676",113490,,,,,-1,,Remote,-1,data engineer,na,"['python', 'sql']",[],[],[],[],,0-2 years
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['python', 'sql']",[],[],[],[],master,
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
"Rocket Travel, Inc.",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.",147500,51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable,IL,11,data engineer,senior,"['python', 'sql']","['aws', 'redshift']",[],[],"['airflow', 'hadoop', 'spark']",,2-5 years
E-Logic INC,4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.",84277,1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD),DC,16,data engineer,na,[],[],[],[],[],master,
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
EMONICS LLC,3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location",150000,1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD),NJ,7,data engineer,na,[],"['azure', 'databricks']",['power bi'],[],[],,5-10 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
IBR (Imagine Believe Realize),4.5,"Melbourne, FL",Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",118866,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable,FL,16,data engineer,senior,"['python', 'r', 'java']","['azure', 'aws', 'redshift']","['tableau', 'sas']",['redis'],"['hadoop', 'spark']",bachelor,
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
Angle Health,4.0,Remote,Data Engineer,"Changing Healthcare For Good
At Angle Health, we believe the healthcare system should be accessible, transparent, and easy to navigate. As a digital-first, data-driven health plan, we are replacing legacy systems with modern infrastructure to deliver our members the care they need when they need it. If you want to build the future of healthcare, we'd love for you to join us.

The Role
As a Data Engineer on the Strategy team at Angle Health, you will be part of an elite team of problem solvers tackling some of the hardest operational challenges across the business. You will be working closely with Technical Product Managers and other engineers in small teams embedded within functions across the company—from sales to operations to finance and more.

In partnership with operational stakeholders, your job will be to quickly understand business workflows, and design and implement technical, data-driven solutions to achieve the intended business outcomes. That may include architecting backend micro-services, building and maintaining robust data pipelines, standing up health metrics and automated alerts, and developing scalable technical solutions to support Angle Health's day-to-day operations.

Every day will be a new learning experience in this role and may span discussing architecture with fellow engineers, wrangling large-scale data, coding a custom web app or micro-service, or speaking with customers and executives.

This role is ideal for entrepreneurial engineers, applied data scientists, and creative, intellectually curious thinkers that want to solve high-value problems in healthcare.

This position may be based in San Francisco, New York City, Salt Lake City, or Remote.

We are currently trialing various titles for the same role. Please consider the following posted roles as the same position: Deployed Engineer, Software Engineer, Business Operations, and Data Engineer. Please note these are the same, so if you have already applied for one position, there is no need to reapply for the others.
What We Value
A strong engineering background in computer science, software engineering, data science, mathematics, or similar technical field is required for this role
Proficiency in programming languages (e.g. Python, SQL, Java, TypeScript/JavaScript, or similar) and data engineering frameworks
A highly analytical mindset and an eagerness to build technical solutions to complex business problems
High attention to detail and intellectual curiosity—you're not satisfied with surface-level answers. You want to dive into the data, the ""how,"" and the ""why"" because ""the way it's always been done"" is not always the way it should be done
Low ego—the outcome matters more than who gets the credit
Demonstrated ability to collaborate effectively in teams of technical and non-technical individuals
Highly organized with an ability to multitask, problem solve, and balance competing priorities in a rapidly changing environment
Because We Value You:

Competitive compensation and stock options
100% company paid comprehensive health, vision & dental insurance for you and your dependents
Supplemental Life, AD&D and Short Term Disability coverage options
Discretionary time off
Opportunity for rapid career progression
Relocation assistance (if relocation is required)
3 months of paid parental leave and flexible return to work policy (after 10 months of employment)
Work-from-home stipend for remote employees
Company provided lunch for in-office employees
401(k) account
Other benefits coming soon!

Backed by a team of world class investors, we are a healthcare startup on a mission to make our health system more effective, accessible, and affordable to everyone. From running large hospitals and health plans to serving on federal healthcare advisory boards to solving the world's hardest problems at Palantir, our team has done it all. As part of this core group at Angle Health, you will have the right balance of support and autonomy to grow both personally and professionally and the opportunity to own large parts of the business and scale with the company.

Angle Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. Angle Health is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities.",106385,1 to 50 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2019,Unknown / Non-Applicable,Remote,4,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,
Enterprise Knowledge LLC,4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”",89080,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD),VA,10,data engineer,na,"['python', 'r', 'sql']",['aws'],[],[],"['airflow', 'spark']",,2-5 years
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,
Buchanan & Edwards,3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location",138500,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD),DC,25,data engineer,na,"['python', 'sql']",[],[],[],['spark'],bachelor,0-2 years
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74927,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Jane Street,4.4,"New York, NY",Data Engineer,"About the Position
We are looking for a Data Engineer who can help us understand, clean, manage, and share the data that guides our trading. At Jane Street, having a thorough and accurate understanding of data is at the core of the work we do.
Using our mix of in-house and open-source software, you will analyze datasets gathered from a variety of sources, checking for anomalies, matching formats and symbologies, automating ETL processes, and generally making it easier for our traders to generate valuable insights.
You should be excited about digging deep into datasets and explaining your findings to different types of colleagues, working collaboratively with traders and software engineers.
While prior experience with financial data would be nice, we don’t expect you to have a finance background. We’re happy to hire talented engineers and teach them what they need to know.
About You
Top-notch programming skills in any language (Python a plus)
Experience with using SQL and relational databases
Experience with generating data visualizations
Knowledge of statistical techniques, including multivariate regression and time series analysis
Clear and concise communication skills; able to efficiently analyze and deconstruct technical problems
Fluency in English required
Base salary is $175,000 - $300,000. Base salary is only one part of Jane Street total compensation, which includes an annual discretionary bonus.
Jane Street is an Equal Opportunity Employer",237500,1001 to 5000 Employees,Company - Private,Management & Consulting,Research & Development,2000,Unknown / Non-Applicable,NY,23,data engineer,na,"['python', 'sql']",[],[],[],[],,
"Twitch Interactive, Inc.",3.8,"San Francisco, CA",Data Engineer,"3+ years of experience in data engineering, software engineering, or other related roles. 3+ years in relational database concepts with a solid knowledge of star schema, SQL, SQL Tuning, OLAP, Big Data technologies 3+ years of experience in generating and maintaining data pipelines from various data sources, in collaboration with diverse stakeholders. 3+ years of experience working with Amazon Webservices, S3, EMR, Redshift etc. Experience with best practices for development including query optimization, version control, code reviews, and documentation. Experience with coding languages like Python/Java/Scala
About Us: Twitch is the world's biggest live streaming service, with global communities built around gaming, entertainment, music, sports, cooking, and more. It's where millions of people come together to chat, interact, and make their own entertainment. We're about community, inside and out. You'll find coworkers who are eager to team up, collaborate, and smash (or elegantly solve) problems together. We're on a quest to empower live communities, so if this sounds good to you, see what we're up to on LinkedIn and Twitter, get interviewing tips on Instagram, and discover projects we're solving on our Blog. About the Role: Data is central to Twitch's decision-making process, and data engineers operate at the forefront of this by creating authoritative datasets that drives analysis and decision-making across all of Twitch. In this role you will be shaping the way that business performance is measured, defining how we transform our data, and scaling analytics methods and tools to support our growing business, leading the way for high quality, high velocity decisions. For this role, we're looking for an experienced data engineer to join our Content Data Science team, which is focused on empowering staff throughout Twitch to use and trust our business data. Your responsibilities may range from developing and enhancing our data warehouse which act as authoritative sources of truth across the company, driving data quality and trustworthiness across product verticals and business areas, building self-service business intelligence infrastructure for analysts, as well as connecting into data interfaces that enable everyone in Twitch to discover and analyze the data. In the process, you will have the opportunity to interact with technical and non-technical staff members throughout the company, and will report to the Director of Content Data Science. This position can be located in San Francisco, CA; Irvine, CA; Seattle, WA; New York, NY; and Salt Lake City, UT. You Will: Define and own team level data architecture for trusted, governed, dimensionally-modeled repository of data that enables Twitch staff to quickly and reliably answer their business questions. Keep existing data sources fresh against data quality issues, design, develop and maintain data quality assurance framework and continuously improve the processes for developing new ones raising the level of quality expected from our work. Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost. Improve search, discovery and literacy: Create exploration and visualization interfaces in our BI tools and evangelize the adoption of these sources across the company through education and training programs. Improve business and engineering team processes via data architecture, engineering, test, and operational excellence best practices. Make enhancements that improve data processes.
Bonus Points
A passion for data science and interest in growing / learning data science, machine learning at scale.
A passion for games and the gaming industry
Perks
Medical, Dental, Vision & Disability Insurance
401(k)
Maternity & Parental Leave
Flexible PTO
Amazon Employee Discount
Monthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages, etc.)
We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
Pursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $105,700/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. Applicants should apply via our internal or external career site.",105700,10000+ Employees,Company - Public,Information Technology,Internet & Web Services,1994,$10+ billion (USD),CA,29,data engineer,na,"['python', 'scala', 'sql', 'java']",['redshift'],[],[],[],,+10 years
Aretec Inc,1.0,Remote,Junior Data Engineer,"POSITION TITLE: Junior Data Engineer YEARS OF EXPERIENCE: 1-3
LOCATON: 100% remote
*****Please Note: Aretec, Inc. does not offer Corp - 2 - Corp (C2C) employment. *****
Aretec is looking for a Junior Data Engineer. The Junior Data Engineer will be primarily responsible for design, development, support and enhancement of the data pipelines developed in AWS.

RESPONSIBILITIES:
You'll write clean and functional code on the front- and back-end
You'll write reusable and maintainable code
Coordinate with data migration plans
Ability to communication and collaborate with various teams and vendors.
Participates in functional and technical design.
Participation in Agile activities Scrum, Kanban.
Ensure coding, testing, debugging and implementation activities completed as required.
Flexible and adaptable with the ability to align to changing priorities
The developer should have great communication skills and be able to discuss and develop requirements with multiple levels of staff from corporate and field locations
An interest in and ability to understand financial reporting, accounting concepts and related accounting data
Participate in data flow diagramming and/or process modeling (code architecture)
Documents work and steps to completion as required
Follows AWS best practices to integrate with ecosystem and infrastructure
Ability to partner with domain architects to implement the defined solution architecture including application, infrastructure, data, integration, and security domains

REQUIRED SKILLS:
1-3 years of software engineering experience
1+ years of real industry experience
Experience with website development, web services and API development
Hands-on experience performing data engineering and transformation tasks using Python
Experience implementing backend in Python using frameworks such as Django or Flask
Knowledge of web technologies - both back and front-end development including, but not limited to JavaScript, React, CSS, HTML, T-SQL, and Python
Understand log monitoring and analytics
Experience Meeting both technical and consumer needs
Experience testing software to ensure responsiveness and efficiency
A general knowledge of index migrations, debugging and researching concepts are major pluses
Must be aware of CI/CD pipelines and well-versed in using GitLab for creating required pipelines for CI/CD

EDUCATION: Bachelors Degree in Mathematics/Statistics/Technology/Science/Engineering/Applied Mathematics or related field

CERTIFICATIONS: N/A",102500,51 to 200 Employees,Contract,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']",['aws'],[],[],[],,+10 years
Small Batch Standard,4.1,Remote,Junior Data Engineer,"We're the premier, remote accounting, tax, and consulting firm built exclusively to serve the craft brewing industry.
Our mission is to help craft breweries grow profits and build deep successful relationships. And our team is filled with expert, autonomous, adaptable, technology-driven high performers.
Are you up for the challenge?
We're looking for a full-time, remote Junior Data Engineer to join our specialized team. The main objective of this role is to design, develop, implement, and improve both internal and external applications to support our brewery clients and team in accordance with the SBS Core Values.
About The Role
This role will report to our Technology/Product Manager and is accountable for fulfilling the following responsibilities:
Building our data pipeline and analysis applications. A key aspect of the consulting service we provide to clients involves the collection, aggregation, analysis, modeling, and usage of financial data and benchmarks. We use this data both internally to develop and inform strategy, as well as externally through our Benchmarks Assessment (https://sbstandard.com/assessment/) and Compass analysis product (https://sbstandard.com/levelup-compass/). You'll be responsible for working with our team to build out our data pipeline for these tools, and progressively increasing our ability to aggregate, analyze, query, and feed back this data into our reporting, analysis, and consulting work. Platforms we're building with include: SQL, Airflow, Excel Visual Basic for Applications (VBA), Google Apps Script, Intuit/QuickBooks Online.
New process and technology R&D. We're always looking for new opportunities to provide both our team and our clients access to additional tools that give them leverage, automate and streamline processes, and overall make work more efficient. Part of your time will be dedicated to researching, testing, and prototyping new tech and application options.
Participate and contribute to the overall success of our team. Each week the team meets to share wins, progress, and knowledge, as well as identify and solve issues at multiple levels (company, team, individual). Your full participation in this process is critical to ensure that we are operating as a cohesive, high-performance unit.
About You
We're looking for an individual who:
Is a problem solver through-and-through. Everywhere you look, you both (a) see problems to solve, and (b) see solutions and new ways of doing things that just haven't been done yet. You know how to think outside of the box, are willing to “go there” with new ideas and solutions that haven't been done before, and have the confidence to start building, testing, iterating, and making sh*t work.
Is a systems thinker. You understand both the big picture and how the functional components fit together, and have the ability to take a specific analysis outcome and generalize it to fit a wide range of scenarios through structure and sound system design.
Can fail fast, iterate, and learn. You're an independent, self-directed, learner who isn't afraid to “move fast and break stuff” knowing that failure is a prerequisite to success, ESPECIALLY in product development. You may not have traditional credentials, but what you do have is the ability to rapidly learn, adopt, test, and understand new languages, platforms, tools, and solutions.
Is a manager of one. Unlike working within a traditional firm, in this role you'll be in the driver's seat, managing your workflow and workload in order to meet the standard set of deliverables required for each client.
About Our Culture
We're fully remote, with team members and clients located all across the U.S. and have developed our own unique culture we call The SBS Way, within which we operate, evaluate performance, and make decisions using our core values as a guide:
Be Antifragile. Everything we do, good or bad, makes us better. And every experience is an opportunity for learning and continuous improvement.
Play The Long Game. We make decisions, to the best of our ability, in the long-term interest of our firm, our team, our clients, and our broader industry and community.
Embrace Technology. We welcome new technologies with open arms, and are always exploring, testing, and implementing them in the interest of enhancing both our internal capabilities and our client's outcomes.
Build and Trust The Process. Each member of the team is committed to building, following, and improving the processes we use to deliver exceptional results for our clients.
Act as A Team of Expert Knowledge Workers. We openly and willingly collaborate, communicate, and provide rapid, direct feedback in the interest of learning, improving and developing ourselves.
Working At SBS
What it's like working at our firm:
High flexibility. We believe in the ability of our team to determine the best way to complete their work. We measure outputs, not inputs. We don't have time sheets. We don't track hours. We don't pay attention to when and where our team works. Your schedule is yours to make.
High accountability. What we care about most is that we deliver on what we promise to our clients. In this respect, we measure and manage to our deliverable performance metrics and ensure each team member takes ownership over their accomplishment with a high level of quality that aligns with our core values
Great pay for great work. We pay based on the characteristics that matter: position (and its market value), level of mastery, and longevity with the firm. All of which aim to ensure each member of the team feels they are compensated well and can focus on great work.
Merit-based career progression. We have clearly established career tracks, performance benchmarks, and mastery levels set for all of our core positions. How quickly you progress is entirely under your control, with a quarterly review and bi-annual promotion consideration cycle in place to evaluate your progress.
Generous benefits. We offer a generous benefits package that includes medical, dental, and vision insurance enrollment; as well as an IRA match, tech stipend, 3 weeks of paid time off, and entry into our profit share bonus program after two years of service.
Personal and and team development. In addition to our overall continuous learning focus, we also provide support for personal development in the form of expense coverage for continuing education (books, courses, training, certifications, etc.) as well as experiential learning (brewery visits, industry events and conferences, etc.). Each year we also meet in person for an all-expenses-paid annual retreat as a team. No work. Lots of fun. Lots of client beer.
Job Requirements
The following basic requirements must be met:
Previous experience in SQL development and database management.
Previous experience building useful applications in scripting languages like VBA, Google Apps Script, Python, PHP, etc.
Can do effective cross-functional work in a remote environment.
Have crystal clear professional written and verbal communication skills.
Have exacting organizational standards and a calm and friendly attitude.
Available and responsive during normal business hours (9am-5pm Eastern Time, Monday-Friday).
Have a strong, consistent internet connection and a work environment conducive to video calls.
Preferred qualifications include:
Direct previous experience building data pipelines.
Direct previous experience building Airflow workflows and applications.
Experience building out and managing API connections.
Experience working with Quickbooks Online or similar accounting or finance platforms.
Experience using Podio or similar remote project management tools (e.g. Trello, Asana, etc.).
Next Steps
If the position, culture, values, and mission at Small Batch Standard sound like they're the right fit for you, please apply here.",64000,1 to 50 Employees,Company - Private,Financial Services,Accounting & Tax,2010,Unknown / Non-Applicable,Remote,13,data engineer,na,"['r', 'python', 'sql', 'go']",[],['excel'],[],['airflow'],,
Cloudbc Labs,3.9,Remote,Anaplan Data Engineer,"Job Title :: Anaplan Data Engineer
Location :: Remote
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Remote",117000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,Unknown / Non-Applicable,Remote,8,data engineer,na,[],[],[],[],[],,
LOVEFOODIES INC,4.2,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",54000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['python', 'sql']",[],['tableau'],[],[],,2-5 years
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
The Sunwater Institute,4.2,"Rockville, MD",Data Engineer,"The Sunwater Institute is a nonprofit, nonpartisan public policy organization focused on improving the foundations of our social systems. The Sunwater Institute’s current programs focus on improving congressional performance, preserving and advancing constitutional rights and liberties, strengthening and enlarging property rights, and exploring the foundations of group decision-making.
The Sunwater Institute’s interdisciplinary, long-term orientation and focus on the building blocks of social systems sets it apart as a haven for bold, fresh ideas and free, constructive dialogue. The Sunwater Institute is located in North Bethesda and maintains a presence on Capitol Hill.
POSITION
The Sunwater Institute is seeking a Data Engineer who has a background in engineering or computer science and is highly skilled in SQL and database services technologies. The ideal candidate will possess strong problem-solving skills, excellent communication, and a great technical aptitude. The individual in this position will provide project support and help drive team initiatives. The candidate will be creative, and comfortable working in small teams and coordinating with US and non-US-based developers.
Responsibilities
Develop data models to store and manage large amounts of structured and unstructured data
Design efficient ETL processes to move and integrate data from different sources into the enterprise warehouse
Write complex queries using SQL Server etc. to analyze large datasets
Identify trends, patterns, and correlations within datasets by performing exploratory analysis
Ensure high quality of data by implementing best practices on storage, maintenance, indexing techniques, etc
Coordinate project status updates between teams and communicate deliverables and timelines
REQUIREMENTS
Bachelor’s degree in Computer Science or Engineering related field
4+ years of relevant experience in a similar role
Strong knowledge of relational databases including SQL Server
Proficient skills in writing complex queries using SQL language; developing stored procs, views, etc.
Experience with end-to-end ETL (Extract Transform Load) Processes
Familiarity with Big Data platforms such as AWS etc.
Ability to develop effective reports that help drive business decisions
Must be able to work effectively with teams but be able to work independently on tasks to meet tight deadlines
Exceptional communication skills verbal, written, and active listening
Extreme attention to detail and accuracy
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Vision insurance
Compensation package:
Yearly bonus
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Rockville, MD 20852: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",106385,,,,,-1,,MD,-1,data engineer,na,['sql'],['aws'],[],['sql server'],[],bachelor,2-5 years
Nike,4.1,"Boston, MA",Data Engineer,"Become part of the Converse Team

Converse is a place to explore potential, break barriers and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Converse, it’s about each person bringing skills and passion to a challenging and constantly evolving world to make things better as a team.
Converse, Inc. Boston, MA. Work closely with Project Management and Business teams to completely define specifications to ensure the project acceptance. Involved in preparation of functional and technical specifications with different cross teams. Lead team, defining solution options, providing estimates on effort and risk, and evaluating technical feasibility in Agile development process, including Scrum and Kanban. Work on troubleshooting data and analytics issues and perform root cause analysis to proactively resolve issues. Develop data extracts and feeds from the full spectrum of systems in the Converse ecosystem, including transactional ERP systems, POS data, product and merchandising systems. Engineer data products for a variety of Operations analytics use cases, ranging from reporting and data visualization to advanced analytics/machine learning use cases. Support designing technical specifications and data transformation models for junior developers. Ensure development is on track and meets specifications as defined by product management and the business. Responsible for data integrity of current platform and QA of new releases. Support the development and maintenance of backlog items and solution feature. Participate in sprint planning activities from a development perspective. Responsible for designing cloud-based data architecture using AWS stacks. Design and develop Python data science and data engineering libraries dealing with structured and unstructured data. Work with a variety of database types (SQL/NoSQL, columnar, object-oriented) and diverse data formats. Responsible for ETL with Spark and building data pipelines/orchestrations in Airflow and working on ETL tools like Matillion. Responsible for DevOps toolchain and Continuous Development, Continuous Integration and Automated Testing using Jenkins. Ensure and use data engineering for advanced analytics/data science and Software development skills.
Applicant must have a Bachelor’s degree in Computer Science, Information Systems, or Information Technology and 5 years of progressive post-baccalaureate experience in the job offered or a related occupation. Experience must include:
Data warehousing;
ETL or ELT;
Amazon Web Service (AWS) Cloud Services, including AWS S3, AWS Lambda, AWS EC2, AWS EMR or AWS DynamoDB;
Relational Database Management Systems (RDBMS), such as Oracle, Teradata, SQL Server or Snowflake;
Database Development with writing stored procedures, functions, triggers, cursors or SQL queries;
Hadoop, HDFS, Hive or Spark;
Programming languages, including Java or Python;
Business Intelligence Tools, such as Tableau;
Unix Shell scripting; and
Version control systems, such as Git, Bitbucket or Github
#LI-DNP
Converse is more than a company; it’s a worldwide advocate for self-expression. This belief motivates our employees, permeates our working environment and inspires our products. No two of us look or think exactly alike. We are each one-of-a-kind. Individually and as a culture, we have the freedom to create and grow professionally. Generous benefits packages only sweeten the experience. From Boston to Shanghai, from Brand Design to Finance, Converse is a brand that celebrates the unique and creative people of the world. Together, we’re different.",115797,10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1972,$10+ billion (USD),MA,51,data engineer,na,"['sql', 'shell', 'python', 'java', 'nosql']","['snowflake', 'oracle', 'aws']",['tableau'],"['dynamodb', 'hive', 'sql server']","['airflow', 'hadoop', 'spark']",bachelor,
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85907,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
Decision Point Healthcare,5.0,"Boston, MA",Junior Data Engineer,"Boston, MA
Full-Time

We need your help.
We’re looking for a self-motivated and detail-oriented data engineer to join our DataOps team. This opportunity will enable you contribute to the operation, support, and enhancement of our mission critical data operations platform and the development of data pipelines. Our data operations platform supports high volume, high velocity data ingestion and curation to support our existing, and rapidly expanding, health plan client base.
What you’ll do:
As a data engineer, you will be responsible for the execution and management of inbound client and internal service-based data pipelines. This encompasses the development, management, and operation of our client data hubs, including data intake, data quality assessment/evaluation, data curation and enrichment processes. Our client data hubs consist of various health related data sources supporting Decision Point services including our AI/ML platform, analytics platform, and OPUS application suite. If this interests you, read on.
The position:
Develop and apply scalable data integration (ETL/ELT) processes (including ingestion, cleansing, curation, unification, etc.)
Support various components of the data pipelines, including ingestion, validation, cleansing and curation
Manage and ensure the success of ongoing data pipeline routines
Collaborate with our implementation team to assist with the identification and reconciliation of data anomalies
Create and maintain documentation on data pipelines
Engage with our software engineering team to ensure precise data points per application specifications
Provide periodic support to our customer success team
Skills & experience:
BS / MS in Computer Science, Engineering, or applicable experience
Expertise with SQL, database design and data manipulation methodologies
Expertise with ETL/ELT and the development of automated validation and data pipelines
Strong data profiling and analytic skills; Ability to discover and highlight unique patterns/trends within data to identify and solve complex problems
Keen understanding of EDW, master data management and other database design principles
Comfortable working with high volume data in a variety of formats
Experience with Pandas/Numpy in a production environment
Experience with CI/CD and version control tools: Git preferred
Experience working within hybrid cloud environment; AWS experience is a plus
Excellent verbal and written communication
Excellent listener and collaborator with senior leaders, peers, and staff
Familiarity with data engineering and workflow management frameworks such as Airflow and dbt
Familiarity with healthcare data is a plus
A little bit about Decision Point:
We are a rapidly growing healthcare technology company changing the fundamentals of patient and provider engagement. For years, health plans have relied on descriptive data and reactive engagement. We empower our clients to understand and predict the whole member journey, enabling sustained improvements in member health outcomes and plan performance. We combine the latest, most practical technologies and a deep understanding of healthcare, bringing innovative, pragmatic solutions to an industry that touches us all.",92443,1 to 50 Employees,Company - Public,,,2013,Less than $1 million (USD),MA,10,data engineer,na,['sql'],['aws'],[],[],['airflow'],master,
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
Schneider Electric,4.2,"Columbia, SC",Manufacturing Data Engineer,"Job Description:
Schneider Electric has an opportunity for a Manufacturing Data Engineer in our Columbia, South Carolina location. The Manufacturing Data Engineer is a key contributor to the data strategies for the Columbia plant, specifically, in the areas of system design and performance.

What will you do?
Specializes in several areas of knowledge regarding production / manufacturing processes: process design, ergonomics, capacity, simulation tools, investment, and cost analysis
Supports, maintains, upgrades and troubleshoots applicable system infrastructure
Develops a variety of applications and reports, leveraging large data sets to facilitate business operations and boost organizational efficiency.
Analyzes business processes and develops reports, tools, scripts and procedures to bridge the gap between legacy systems
Provides end-user support, including researching user complaints/issues, answering technical questions, and/or assisting with application revisions.
Manages Enterprise Resource Planning (ERP) by updating material, Bill of Materials (BOMs), and routings when modifications are required by Engineering
What qualifications will make you successful?
Bachelor’s degree in a field of Engineering ME or EE
Good communication skills
Relevant experience preferred 3 plus years
Experience in Excel, Macros, SQL, and Tableau

Qualifications:
What's in it for me?
Schneider Electric offers a robust benefits package to support our employees such as flexible work arrangements, paid family leave, 401(k)+ match, and more.
Who will you report to?
Manufacturing Engineering Manager

Let us learn about you! Apply today.

About Our Company:
Why us?
Schneider Electric is leading the digital transformation of energy management and automation. Our technologies enable the world to use energy in a safe, efficient and sustainable manner. We strive to promote a global economy that is both ecologically viable and highly productive.

€25.7bn global revenue
137 000+ employees in 100+ countries
45% of revenue from IoT
5% of revenue devoted for R&D

You must submit an online application to be considered for any position with us. This position will be posted until filled

It is the policy of Schneider Electric to provide equal employment and advancement opportunities in the areas of recruiting, hiring, training, transferring, and promoting all qualified individuals regardless of race, religion, color, gender, disability, national origin, ancestry, age, military status, sexual orientation, marital status, or any other legally protected characteristic or conduct. Concerning agencies: Schneider Electric does not accept unsolicited resumes and will not be responsible for fees related to such.",90065,10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1836,$10+ billion (USD),SC,187,data engineer,na,"['r', 'sql']",[],"['tableau', 'excel']",[],[],bachelor,
Archon Resources,3.4,"Tulsa, OK",Data Engineer,"EDI/Data Engineer:
6-month contract to hire. Must be in Tulsa/OKC and able to be on site weekly.
Needed: Azure Data Factory; Microsoft SQL; Oracle; Experience with big data
Nice to Haves: Experience monitoring data in and out; data warehouse provisioning to feed analytical requirements
________________________________________________________________
JOB SUMMARY:
The Data Engineer will be responsible for expanding, optimizing and monitoring our data and data pipeline architecture, as well as optimizing data flow and collection across organizational teams. The Data Engineer will support our software engineers, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
KEY RESPONSIBILITIES:
Create and maintain optimal data pipeline architecture to support our next generation of products and data initiatives.
Assemble large, complex data sets that meet functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.
Experience in the development of SSIS, ETL and other standardized data management tools.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Performs other duties as required.
QUALIFICATIONS:
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong project management and organizational skills.
Ability to work independently, handle multiple tasks and projects simultaneously.
EDUCATION/EXPERIENCE:
Bachelors degree or equivalent experience required.
Project management skills preferred.
Willingness to work in a high-tech, continually evolving, innovative environment.
Job Types: Full-time, Contract
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible schedule
Health insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Tulsa, OK 74105: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Tulsa, OK 74105",108000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,OK,-1,data engineer,na,['sql'],"['oracle', 'azure']",['ssis'],[],[],,0-2 years
Chevron,4.1,"Denver, CO",Wells Data Engineer,"Chevron is accepting online applications for the position Wells Data Engineer through 03/08/2023 at 11:59 p.m. (CST).
Chevron’s strategy is straight-forward: be a leader in efficient and lower carbon production of traditional energy, in high demand today and for decades to come, while growing lower carbon businesses that will be a bigger part of the future. To achieve these goals, we’ll build on the assets, experience, capabilities, and relationships we’ve developed over 140 years to incubate and grow new business.

Technology will play a crucial role in unlocking ever cleaner and more affordable sources of energy. Chevron is seeking innovative, technology professionals with a desire to thrive in the global digital environment and help us lead the global energy transition. An IT career at Chevron offers you the opportunity to work in a technical environment with a global reach. You’ll find that we make a business of investing in our people and encouraging your professional development through a learning culture and challenging on-the-job opportunities. We differentiate ourselves through the application of cutting-edge technology, and by taking a collaborative approach that includes in-house expertise, proprietary solutions, and strategic partnerships. We also offer flexible work schedules and very competitive benefits.

Join Chevron IT. Lend us your skills and enjoy a great career with Chevron.
Wells Data Engineer responsible for multiple aspects of RBU Engineering Team data management and quality, automation, system connectivity, visualizations. This role is expanded through supporting performance related processes, benchmarking, and cost/metrics tracking in our Competitive Performance framework. The Data Engineer is also responsible for providing technical and analytical support in the area of data entry and standard templates for drilling, workover, intervention, and abandonments project management and execution.
The role

Wells Data Engineer responsible for multiple aspects of RBU Engineering Team data management and quality, automation, system connectivity, visualizations. This role is expanded through supporting performance related processes, benchmarking, and cost/metrics tracking in our Competitive Performance framework. The Data Engineer is also responsible for providing technical and analytical support in the area of data entry and standard templates for drilling, workover, intervention, and abandonments project management and execution.
Responsibilities for this position may include but are not limited to:

Understanding data systems architecture and management
Data sources, data quality, and QA/QC mechanisms needed for systems of record
Administrator level ownership of multiple data systems such as WellView, SharePoint, WellSafe systems, reporting systems, and data systems
Working with multiple data systems to provide clear and impactful data visualizations and tracking for the Wells leadership team
Working with the Wells Performance Engineer to support cost, metrics, and performance tracking processes
Support WellView well/job creation, data quality review and support, field training, and regulatory data
Utilizing PowerBI, Spotfire, or Excel to collate cost tracking information in Wellview and Siteview
Required to have strong written communication & excellent organizational skills.
Required Qualifications:

Data science or engineering related training and related systems
Advanced skills in Excel and other Microsoft Office Suite applications
Good communicator and able to collaborate effectively with multiple stakeholders (i.e. engineers, regulatory, operations, etc…)
Preferred Qualifications:

Experience working with WellView, PowerBI and ArcGIS applications
Experience in supporting oil and gas well drilling, completion, workover, and asset retirement operations.
Flexible Working
Chevron offers a complete package and provides career development opportunities to all employees. We do this through on-boarding, training and development, mentoring, volunteering opportunities and employee networking groups. We advocate work-life balance and offer employees access to various health and wellness programs.
What type of flex work does the position offer?
We offer alternative work schedules including 9/80 (work 9-hour days, with every other Friday off)
We offer a hybrid work model - work remotely from home 2-3 days a week

Relocation & International Considerations
Relocation [ may / will not be] considered.
Expatriate assignments [ may / will not be ] considered.

Chevron regrets that it is unable to sponsor employment Visas or consider individuals on time-limited Visa status for this position.
Working with us
Chevron is one of the world’s leading integrated energy companies. We believe affordable, reliable and ever-cleaner energy is essential to achieving a more prosperous and sustainable world. Chevron produces crude oil and natural gas; manufactures transportation fuels, lubricants, petrochemicals and additives; and develops technologies that enhance our business and the industry. We are focused on lowering the carbon intensity in our operations and seeking to grow lower carbon businesses along with our traditional business lines. More information about Chevron is available at
www.chevron.com
.
Benefits
Chevron offers competitive compensation and benefits programs which includes, but is not limited to, variable pay, healthcare coverage, retirement plan, insurance, time off programs, training and development opportunities and a range of allowances connected to specific work situations. Details of such benefits and allowances are available at
https://hr2.chevron.com/
.

The compensation and reference to benefits for this role is listed on this posting in compliance with Colorado law. The selected candidate’s salary will be determined based on his or her skills, experience, and qualifications.

Regulatory Disclosure for US Positions

The compensation and reference to benefits for this role is listed on this posting in compliance with applicable law. Please note that the compensation and benefits listed below are only applicable for U.S. payroll offers.
The anticipated salary range for this position is $ 112,200 – $ 221,900 The selected candidate’s compensation will be determined based on their skills, experience, and qualifications.
Chevron offers competitive compensation and benefits programs which includes, but is not limited to, variable pay, health care coverage, retirement plan, protection coverage, time off and leave programs, training and development opportunities and a range of allowances connected to specific work situations. Details are available at
http://hr2.chevron.com/
.
Regulatory Disclosure for US Positions:
Chevron is an Equal Opportunity / Affirmative Action employer. Qualified applicants will receive consideration for employment without regard to race, color, religious creed, sex (including pregnancy, childbirth, breast-feeding and related medical conditions), sexual orientation, gender identity, gender expression, national origin or ancestry, age, mental or physical disability (including medical condition), military or veteran status, political preference, marital status, citizenship, genetic information or other status protected by law or regulation.
We are committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or an accommodation, please email us at
emplymnt@chevron.com
.
Chevron participates in E-Verify in certain locations as required by law.",167050,10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1879,$10+ billion (USD),CO,144,data engineer,na,[],[],['excel'],[],[],,0-2 years
Steward Health Care,2.7,"Westwood, MA",Data Engineer,"Position Purpose:
Reporting to the Manager of the Data Warehouse team, part of the larger Health Informatics group, the data engineer applies their technical expertise to meet the needs of the department and Steward Health Care Network (SHCN).

Key Responsibilities:
ETL/Automation
Design configurable data process flows with full automation
Develop ETL processes for data loading and data extraction
Schedule ETL processes for full process automation
Data Engineering
Responsible for data analysis to support building data processes and reporting
Design useful and accurate data marts that meet requirements
Apply SQL skills when designing and building data marts and data flows
Quality
Establish and utilize QC processes to ensure data integrity
Incorporate standard error logging and alerts to ensure data is loaded as expected
Documentation
Create and maintain clear documentation

Education / Experience / Other Requirements


Education:
Bachelor's degree in Computer Science, Mathematics, Statistics or related experience


Years of Experience:
5+ years of database related work
2+ years of focus on healthcare data

Specialized Knowledge:
Knowledge of healthcare data
Experience using relational databases, SQL Server experience preferred
Experience using ETL tools (SSIS, Informatica, etc.)
Strong SQL programming skills
Experience with scripting languages (PowerShell, R, Python, etc.)
Experience automating data flows
Experience with Health Catalyst tools preferred, but not required
Deep understanding of database structures and data design.
Creative, flexible, and self-motivated with sound judgment
Strong communication skills




Location: Steward Health Care Network · 1301.72330 Steward Health Care Network
Schedule: Full Time, Day Shift, 40 hours",94536,10000+ Employees,Hospital,Healthcare,Health Care Services & Hospitals,1998,Unknown / Non-Applicable,MA,25,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],['sql server'],[],bachelor,+10 years
Crisp Inc,3.6,Remote,Data Engineer,"Here at Crisp, we value the strength in teamwork, and strongly believe that it’s the key to Crisp’s success. By bringing together bright, motivated creators wherever they live and work, we leverage diverse experiences and backgrounds to understand the challenges facing our food system and solve them together. Come join us, and help build the type of business you’d like to be a part of.
Crisp is a socially conscious, distributed team. We give you the opportunity to solve challenges in the global food industry while living where you’re most comfortable and working in areas where you can help foster and grow the community that you are a part of. We believe in transparency, diversity, and merit, and foster a culture of empowerment, personal impact and career growth.
As a Data Engineer at Crisp, you will help unlock the potential of our customers’ data by highlighting and elevating the semantic context. Your responsibilities will include data cleansing, semantic labeling, normalization, and using modern BI Technology to efficiently convey insights to our customers. Being part of the engineering team, you will not only help clients leverage our data platform, you will also help evolve the platform itself by being a subject matter expert involved in product development.
This is an evolving role with ample opportunity for growth. Whether you are coming from a startup or corporate background, you appreciate the significant impact to be had in smaller organizations and you relish the ability to shape your own role and the future of the company.
Signs of a great candidate for Crisp:
Collaborative: You know that your colleagues’ perspectives will make our customers successful. Similarly, you use your strengths to help us grow together. You propose ways for us to be more valuable to our customers.
Customer focused: Our customers are at the forefront of your day. You prioritize our customers’ voices to ensure their needs are met.
Ambitious, curious, and resourceful: You are innately curious, and you aren’t afraid to work hard. You are self-driven and able to find creative results on your own, but you also take direction well. You are driven to succeed because your hard work and results make you proud.
Disciplined and reliable: You enjoy the benefits of working on a distributed team while consistently delivering what you have committed to. When you hit a snag, you communicate and reset expectations early.
Appreciative of honest feedback: You know that the best way to learn and grow is through constructive feedback delivered kindly. You view feedback given to you as an opportunity to get better and strive to do the same for others.
Work smarter and harder: You often identify a problem, create a solution, and bring it to a state of completion - with others, or even on your own. You find ways of eliminating or automating stuff that is uninteresting or wasteful.
Signs of a great candidate for a Data Engineer:
Data oriented: You think about data in a rigorously structured manner. You live by the mantra “garbage in, garbage out” and are deeply experienced in the art of data cleansing.
Focus on the business problem: You are passionate about using visualizations to tell stories and glean actionable insights. In order to tell the story the right way, you need to understand how the business works and how to communicate with different stakeholders.
Brings business context to data engineering: You are the bridge between the business problem and the data pipeline. You are experienced in codifying business context via a semantic definition layer. You enable automation of labor intensive workflows.
Strong sense of aesthetics and user experience: You feel strongly about not just making Business Intelligence visually appealing, but also ensuring that it’s easy to learn and a pleasure to use.
Deep tooling expertise: Many BI tools have a point and click design layer, but you have a deep understanding of the modeling layer and how it interacts with the underlying data store. You are familiar with one or more tools that facilitate data exploration for the purposes of data cleansing or normalization. You hold strong opinions, born from experience, on the features that make a great semantic definition metadata capture tool. You are adept at transforming data in analytical databases using SQL.
We are building a team of people with a breadth of combined experiences so that we can collaboratively enable our customers to be successful. There are no hard requirements on specific background, experience or geographical location. Instead we’re looking for individuals that are capable, reliable, and hoping to grow along with us. Do you have strengths you can share? If so, we’d love to hear from you!",106385,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2018,Unknown / Non-Applicable,Remote,5,data engineer,na,['sql'],[],[],[],[],,
Horizon Health Alliance,4.2,"Getzville, NY",Data Engineer,"Are you seeking a rewarding and fulfilling career in the Mental Health and Addictions field?
Apply to be a Data Engineer today!
What will your day look like?
At Horizon, you will enjoy a supportive, team-based work environment. Have a question? There is always someone there to help! We offer a seamless onboarding experience that will ensure your success in your new role.
As a Data Engineer at Horizon you will...
Acquire and assemble datasets that align with business needs
Identifying, designing, and implementing internal process improvements including optimizing data delivery, and automating manual processes
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency
Working with stakeholders including executive teams and assisting them with data-related technical issues
Working with stakeholders to support their data infrastructure needs and understand company objectives
Build, test, and maintain database pipeline architectures
Create new data validation methods and data analysis tools
Assist and support development of data governance policies
Create and distribute corporate reports using industry standard reporting tools and software such as SQL, Power BI, and Python.
Develop and maintain the workflows and security controls related to data extract, transform, and load (ETL) of corporate data
Providing technical expertise in data storage structures, data mining, and data cleansing.
Develop and maintain databases, data marts, models, data sets, and other key technical solutions
Why choose Horizon to build your career?
Besides the fact that we’ve been named a Best Place to Work for 14 (yes, 14!) years in a row? At Horizon, you can be assured that you will make difference in the lives of others. Even better, your teammates will be just as motivated to make a difference!
What we offer that you’ll love…
Company Culture: At Horizon, we pride ourselves on cultivating an atmosphere of teamwork where all employees feel heard and valued.
Diversity & Inclusion: We are committed to equity, racial justice, and equal opportunity for all, and strive toward this goal through the work of our Diversity, Equity, Inclusion and Belonging Council, frequent trainings, ongoing conversations, affinity groups, and more.
Trainings, Trainings, and More Trainings: We have an entire team dedicated to your personal development and professional growth.
Team Building, Connection, and Relationships: At Horizon, we’re more than co-workers, we’re a community. We support each other, celebrate our achievements and milestones together, and have fun together!
Retirement: We know you want to retire comfortably and we’re here to help! Horizon offers 401(k) AND profit-sharing programs to make sure you’re set for the future.
Student Loan Assistance: We help pay off our team members' student loans every month. One year after joining, you’ll have been able to pay off an extra $600!
PTO & Holidays: We believe self-care is essential. Combined with holidays, you’ll earn up to 22.5 paid days in your first year. By your 3rd anniversary, that's almost doubled with up to 37.5 paid days off!
What makes you a great candidate?
We can’t wait to learn more about you! Here are a few specifics of what you’ll need for the job:
Bachelor's degree in Computer Science, Information Systems, Engineering or equivalent
1+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL and reporting/analytic tools
Strong experience in coding languages like SQL and Python
Fluent in relational based systems and writing complex SQL
Strong analytical and problem-solving skills
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Thorough knowledge of Microsoft Office
Ability to work with multidisciplinary teams
Ability to multitask and manage competing deadlines
Ability to communicate appropriately with all levels of management
Excellent understanding of Microsoft Office suite
Ability to build and optimize data sets
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytic skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Location:
55 Dodge Road, Getzville, NY 14068
Hours:
Full-time position, Monday - Friday; 8:00am-5:00pm
Disclaimer:
Horizon endorses public health measures including vaccinations. We encourage all applicants to be mindful of the fact that Horizon is a healthcare agency proving in person services throughout our community.
This information is intended to provide a general overview of the position; it is not a full job description.

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)",84681,1 to 50 Employees,Nonprofit Organization,,,-1,Unknown / Non-Applicable,NY,-1,data engineer,na,"['python', 'sql']",[],['power bi'],[],[],bachelor,+10 years
Epsilon,3.9,"Irving, TX",AWS Data Engineer,"Job Description

As a data engineer, you will design and maintain data platform road maps and data structures that support business and technology objectives. Naturally inquisitive and open to the deep exploration of underlying data, finding actionable insights, and working with functional competencies to drive identified actions. You also enjoy working both freely and as part of a team and have the confidence to influence and communicate with stakeholders at all levels, and to work in a fast-paced complex environment with conflicting priorities.
About the role:
Reporting into the delivery leader, you will deliver consumable, contemporary, and immediate data content to support and drive business decisions. The key focus of the role is to deliver a custom solution to support various business critical requirements. You will be involved in all aspects of data engineering from delivery planning, estimating and analysis, all the way through to data architecture and pipeline design, delivery, and production implementation. From the beginning you will be involved in the design and implementation of complex data solutions ranging from batch to streaming and event-driven architectures, across cloud, on-premise, and hybrid client technology landscapes.
Brief Description of Role:
We are looking for 3+ years of experience in data engineering in a customer or business facing capacity and experience in the following:
Ability to understand and articulate requirements to technical and non-technical audiences
Stakeholder management and communication skills, including prioritizing, problem solving and interpersonal relationship building
Strong experience in SDLC delivery, including waterfall, hybrid and Agile methodologies
Experience of implementing and delivering data solutions and pipelines on AWS Cloud Platform.
A strong understanding of data modelling, data structures, databases, and ETL processes
An in-depth understanding of large-scale data sets, including both structured and unstructured data
Knowledge and experience of delivering CI/CD and DevOps capabilities in a data environment
Develop new inbound data source ingestions required within the multi-tiered data platform to support analytics and marketing automation solutions
Supports data pipelines – Builds the required dimensions, rules, segments, and aggregates
Support all database operations: performance monitoring, pipeline ingestion, maintenance, etc.
Monitor platform health - data loads, extracts, failures, performance tuning
Create/modify data structures/pipelines
Leveraging capabilities of Databricks Lakehouse functionality as needed to build Common/Conformed layers within the data lake
Develop, document, and test software and environment setup to ensure that the outcome meets the needs of end-users and achieves business goals
Qualifications
The following skills are required:
Tech Stack: AWS pipeline, Glue, Databricks, Python, SQL, Spark, etc.
Building the Data Lake using AWS technologies like S3, EKS, ECS, AWS Glue, AWS KMS, EMR
Extensive experience in ETL and audience segmentation
Developing sustainable, scalable, and adaptable data pipelines
Attention to detail in design, documentation, and test coverage of delivered tasks
Strong written and verbal communication skills, team player
In addition, the candidate should have strong business acumen, interpersonal skills, and communication skills, yet also be able to work independently.
At least 3 years of experience with designing and developing Data Pipelines for Data Ingestion or Transformation using AWS technologies
At least 2 years of experience in the following Big Data frameworks: File Format (Parquet, etc.), Resource Management, Distributed Processing
At least 3 years of experience developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 1-2 years of experience with Spark programming (PySpark)
At least 2 years of experience with Databricks implementations
Familiarity with the concepts of “delta lake” and “lakehouse” technologies
The following skills are nice to have, and expertise is not required:
Adobe (Campaign, Audience Manager, Analytics)
MLFlow
Microsoft Power BI
SAP Business Objects

Additional Information

When you’re one of us, you get to run with the best. For decades, we’ve been helping marketers from the world’s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon’s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC:
Culture: https://www.epsilon.com/us/about-us/our-culture-epsilon
DE&I: https://www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR: https://www.epsilon.com/us/about-us/corporate-social-responsibility
Life at Epsilon: https://www.epsilon.com/us/about-us/epic-blog
Great People Deserve Great Benefits
We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.
#LI-SJ1
REF186919L",88775,5001 to 10000 Employees,Subsidiary or Business Segment,Media & Communication,Advertising & Public Relations,1969,$1 to $5 billion (USD),TX,54,data engineer,na,"['python', 'sql']","['databricks', 'aws']","['power bi', 'sap']",[],['spark'],,2-5 years
CINQCARE,4.0,"Washington, DC",Data Engineer,"Overview
The Data Engineer is a critical member of our growing data science team. In this role, you will have the chance to define and develop a core data asset which provides a representative and culturally aware view of the individuals and the communities that CINQCARE serves. You will work to evolve this asset over time using a product roadmap that includes identification and closure of gaps in existing data, introduction of new data sources, generation of proprietary data while quantifying and eliminating areas of structural biases. CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations and to so, we must also seek to fix gaps in data that ignore and marginalizes the Black and Brown communities that we serve.

An idea candidate for this role will embody CINQCARE’s core values, including, Trusted, Empathetic, Committed, Humble, Creative and Community-Minded. At CINQCARE, we don’t have patients or customers – we have Family Members.
Job Responsibilities
The Data Engineer will have the following responsibilities:
The Data Engineer will be responsible for the design, development and delivery of data pipelines and value-added data assets, leveraging a variety of data warehousing methodologies and disciplines to ingest the data from heterogeneous sources into cloud-based Data-Lake Environment in AWS.
Manage initiatives & projects of significant complexity and risk. Excellent business and communication skills to be able to work with business owners providing input to prioritized roadmaps, develop work estimates, and ensure successful delivery to support strategic planning and initiatives, improve organizational performance, and advance progress towards CINQCARE’s goals.
Assist in the overall architecture of the ETL Design and proactively provide inputs in designing, implementing, and automating the data pipelines.
Investigate and mine data to identify potential issues within the data pipelines, notify end-users and propose adequate solutions.
Ensure data quality and integrity within the data lake in the AWS environment with a focus on compliance to HIPAA and state level compliance requirements.
Oversee user permissions and configurations for adherence to documented access management standards and policies.
Independently (with minimal oversight) develop and maintain trusted advisor relationships with business, clinical, and operations leaders at the senior leadership level and with external partners, that include guidance for optimizing use of analytic capabilities and deliverables, and prioritization based on strategic vision.
Use coding/scripting pipelines and APIs to uncover and turn data into assets that are analysis-friendly using AWS services like Athena Data Catalogue, Quick Sight or any other big data tool on AWS.
Create a high-quality catalog of all pertinent data with the primary goal of establishing a single source of truth and significantly increasing productivity by reducing the time required for data search and discovery.
Crossing team boundaries, educate/advise on data projects, on how to combine and aggregate client data across platforms or technologies, and how to make the greatest use of data.
Lead consistent adherence to the Software Development Life Cycle framework and governance processes including, but not limited to leading planning sessions, collecting, and documenting requirements, identifying design patterns, create and define custom transformations, aggregations, and other data manipulations, developing data pipelines, creating documentation, developing test plans, performing unit testing, conducting peer review sessions.
Use knowledge of healthcare industry, market environment, and clinical and business workflows and activities, to inform solution design and development to execute high-quality or differentiated solutions in an established problem space.
Perform other job-related duties as assigned.

General Duties
The Data Engineer should have the following duties:
Leadership: The Data Engineer will lead the continued build out of the data asset to create business value, including collaborating with their team to design, develop, and execute those strategies and solutions to deliver desired outcomes.
Strategy: The Data Engineer will contribute to the business strategy and roadmap: (1) improve outcomes for CINQCARE Family Members; (2) enhance the efficacy of other CINQCARE. business divisions; and (3) develop and deliver external market opportunities for CINQCARE products and services. In establishing the business strategy, the Data Engineer will define and innovate sustainable revenue models to drive profitability of the Company.
Collaboration: The Data Engineer will ensure that AI capabilities form a cohesive offering, including by working closely with other business divisions to learn their needs, internalize their knowledge, and define solutions to achieve the business objectives of CINQCARE.
Knowledge: The Data Engineer will provide subject matter expertise in the AI solutions, including determining and recommended approaches for designing and building elegant data structures in support of existing reporting tools and custom visualization platforms.
Culture: The Data Engineer is accountable for creating a productive, collaborative, safe and inclusive work environment for their team and as part of the larger Company.
Qualifications
The Data Engineer should have the following qualifications:
Education: Bachelor’s degree in Computer Science, Engineering, Software Engineering, or related field; Master’s degree preferred.
Experience: The ideal candidate should have at least 3+ years of experience in healthcare data engineering. Experience with a variety of data projects and environments, whether on-prem or in-cloud (5+ years in SQL Server, ETL Tools, Business Intelligence & Analysis, Architecture). Familiarity with the Microsoft Stack; experience with other platforms is a plus. Mastery of Python and SQL. Strong foundational knowledge of data lakes and AWS products such as AWS Glue. Experience with healthcare eligibility and claims, implementing APIs, HL7/FHIR standards, ETL scheduling solutions, SQL, and healthcare data security.
Entrepreneurial: CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations. This position is accountable for ensuring CINQCARE is positioned to innovatively deliver on its promise.
Communication: Strong analytical and collaboration skills is required. Excellent verbal, written communication and presentation skills; ability to clearly articulate and present concepts and models in an accessible manner to CINQCARE’s team, investors, partners, and other stakeholders.
Relationships: Ability to build and effectively manage relationships with business leaders and external constituents.
Culture. Good judgement, impeccable ethics, and a strong team player; desire to succeed and grow in a fast-paced, demanding, and entrepreneurial Company.

Location: New York, NY
Compensation: $100,000-$120,000
My3ehHtP4z",110000,201 to 500 Employees,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql']",['aws'],[],['sql server'],[],bachelor,+10 years
MARVEL TECHNOLOGIES INC,3.7,Remote,Data Engineer,"Primary Skills
SCALA, SPARK, SQL, HADOOP, AWS, HIVE, Spark SQL, HIVE QL, CICD (Continuous Integration/Continuous Delivery), VCS (GIT HUB)
Coding in Scala
Designing in of HADOOP ecosystem
Hands-on experience on AWS tools like EMR, EC2
Hands-on experience of SQL in Big Data: SQL, Spark SQL, Hive QL
Proficient in working with large data sets and pipelines
Proficient with workflow scheduling / orchestration tools
Well versed with CICD process and VCS
Databricks is a big PLUS
Job Types: Full-time, Contract
Pay: $50.00 - $58.00 per hour
Schedule:
8 hour shift
Monday to Friday
Experience:
Spark: 4 years (Required)
Scala: 4 years (Required)
Hadoop: 3 years (Required)
Aws: 3 years (Required)
Hive: 3 years (Required)
CI/CD, VCS: 3 years (Required)
Databricks: 1 year (Required)
Work Location: Remote",97200,51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,$5 to $25 million (USD),Remote,-1,data engineer,na,"['scala', 'sql']","['databricks', 'aws']",[],['hive'],"['hadoop', 'spark']",,0-2 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
Liberty Source,3.0,"Hampton, VA",Data Engineer,"Company Background:

Liberty Source PBC combines state-of-the-art technology with a human overlay, enabling our clients to realize greater returns from their investments in artificial intelligence, machine learning, business intelligence and deep analytics platforms. We work with our clients’ Data Science teams, Data Operations staff, Data Quality functions, and other key stakeholders to refine and enhance the data that is vital to the success of their advanced technology initiatives. We are the Data Fitness experts.

Our specialized recruiting mission focuses on the talents of veterans and families of active-duty military personnel to fulfill our brand promise of 100% U.S.-based operations and staff.

Founded in 2014, Liberty Source PBC is based in Hampton, Virginia and is a Certified B Corporation.

Position Summary:

Are you ready to put your data wrangling skills to the test and take your career to the next level? Liberty Source is seeking a Data Engineer to join our rapidly growing team. The right individual will expand and optimize our data and data pipeline architecture and build the systems that collect, manage, and convert raw – and often unstructured - data into usable information for our clients.

The ideal candidate can demonstrate experience in optimizing data flow, has collected and distributed data across teams and across companies, has built data systems (both large and small) from the ground up, and isn’t afraid to wade into the lake for some data cleansing and transformation. We’re looking for an individual who is self-directed and comfortable supporting the data needs of multiple teams, systems, and products, and excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. Beyond technical prowess, our new Data Engineer will have the soft skills for clearly communicating highly complex data ideas and issues to the leadership team.

Position Responsibilities:

Create and maintain optimal data pipeline architecture
Build the infrastructure required for efficient ETL (extraction, transformation, and loading) of data from a wide variety of data sources using SQL, Hadoop and AWS ‘big data’ technologies.
Build analytic views that utilize the data pipeline to provide actionable insights into operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Leverage data to solve business problems, building and maintaining the infrastructure to answer questions and improve processes
Help streamline our data science customer workflows, adding value to our product offerings and building out the customer lifecycle and retention models
Work closely with customer data science and business intelligence teams to develop data models and pipelines for research, reporting, and machine learning
Be an advocate for best practices and continued learning

Requirements:

Bachelor’s degree in computer science, information technology, engineering, or related discipline.
Five or more years of experience in a Data Engineer role utilizing Python and data visualization/exploration tools.
Advanced SQL capabilities and experience working with relational databases, query authoring (SQL) as well as hands-on experience with a variety of data constructs.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Demonstrated ability to work with unstructured data.
Proven ability to build processes supporting data transformation, data structures, metadata, dependency.
Professional certifications such as Cloudera Certified Professional (CCP), IBM Certified Data Engineer or Google’s Certified Professional is a plus.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong project management and organizational skills.
Experience supporting and working with cross-functional, innovation-oriented teams in a dynamic environment.
Great communication skills, especially for explaining technical concepts to nontechnical business leaders.

Benefits:

Paid Time Off (PTO) and 10 paid holidays
Medical, dental, vision, life insurance, and other ancillary benefits
401k Plan

A pre-employment background check is required.

To learn more about our business, please visit our website at https://liberty-source.com/",91094,51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2014,$5 to $25 million (USD),VA,9,data engineer,na,"['python', 'sql']",['aws'],[],[],['hadoop'],bachelor,
Lenovo,4.0,"Morrisville, NC",Data Engineer,"General Information
Req #
WD00037891
Career area:
Hardware Engineering
Country/Region:
United States of America
State:
North Carolina
City:
Morrisville
Date:
Wednesday, January 18, 2023
Working time:
Full-time
Additional Locations:
Morrisville - North Carolina - United States of America
Why Work at Lenovo
Here at Lenovo, we believe in smarter technology that builds a brighter, more sustainable and inclusive future for our customers, colleagues, communities, and the planet.

And we go big. No, not big—huge.

We’re not just a US$70 billion revenue Fortune Global 500 company, we’re one of Fortune’s Most Admired. We’re transforming the world through intelligent transformation, offering the world’s most complete portfolio of smart devices, infrastructure, and solutions. With more than 71,500 employees doing business in 180 markets, we help millions—not just the select few—experience our version of a smarter future.

The one thing that’s missing? Well… you...
Description and Requirements
Lenovo’s Infrastructure Solutions Group (ISG) is seeking a qualified candidate to join our global tools, data, and automation development team. This team designs, develops, deploys, and maintains software applications for productivity and automation solutions.
What You'll Do
The position will develop automation to drive efficiency and effectiveness, automate manual tasks, and extend existing automation platforms for others to build on. Additionally, this role will provide Technical Leadership to a worldwide team and utilize problem solving skills to provide solutions based on data for worldwide product assurance issues. Responsibilities Include:
Work with team to develop data strategies: data model, tools, storage, parsing, etc.
Define the physical components in data.
Define the configuration of a collection of components over time in data.
Define characteristics of a specific configuration over time in data.
Integrate with existing business data
Coordinate with functional teams within Lenovo to develop and automate data pipelines to continuously supply our analysts with clean and reliable data.
Work with to existing team to integrate, adapt, or identify new tools to efficiently collect, clean, prepare, and store data for analysis


Basic Qualifications:
Bachelor’s degree in Computer Science, Mathematics, Engineering, or in a related field
4+ years’ experience with object-oriented/object function scripting languages: Python, Scala, etc
4+ years’ experience building processes supporting data transformation, data structures, metadata, dependency and workload management
4+ years’ experience in data schema, data pipeline design and database management
4+ years’ experience in optimizing data pipelines, architectures and data sets Fluency in structured and unstructured data and management through modern data transformation methodologies


Preferred Qualifications:
4+ years’ experience with designing and managing data in modern ETL architect like Spark, Kafka, Hadoop, Snowflakes
Experience with cloud service environment
Working experience with NoSQL
Experience with Power BI ETL pipeline
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Strong analytical, problem solving, verbal and written communication skills



This position must sit in Morrisville, NC.
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any federal, state, or local protected class.
Lenovo adopted a COVID-19 Vaccination Policy for US-based employees. As a condition of employment, employees must adhere to Lenovo’s US Vaccination Policy and be fully vaccinated against COVID-19, subject to any applicable accommodations. To be fully vaccinated means individuals must receive the full series of a vaccine either approved by the FDA or WHO and listed by the CDC (e.g. two dose of the Moderna, AstraZeneca or Pfizer-BioNTech vaccines; or one dose of the Johnson & Johnson vaccine). This applies to all US-based employees, contractors and interns, regardless of work location. As a condition of employment, you must provide proof that you are fully vaccinated or follow Lenovo’s accommodation process.
TO BE DELETED - Multiple Cities (OLD)
Morrisville - North Carolina - United States of America
Multiple Countries (Posting Locations)
United States of America
Multiple States (Posting Locations)
North Carolina
Multiple Cities (Posting Locations)
Morrisville - North Carolina - United States of America",102019,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1984,$10+ billion (USD),NC,39,data engineer,na,"['python', 'scala', 'nosql', 'go']",[],['power bi'],[],"['kafka', 'hadoop', 'spark']",bachelor,+10 years
"i6 Group, Ltd.",4.2,Remote,Junior Data Engineer,"Junior Data Engineer
Job Location: Fully Remote – UK/EU
Job Type: Full-time, Permanent
Relocation: No
We’re pioneering the use of data to improve efficiency, transparency, and sustainability. Today, our aviation fuel management technology connects business functions, drives operational efficiencies, and enables environmental accountability across the globe. Our ambition is big, we’re transforming the aviation industry, and we need great people to make that happen.
Bring Your Skills and Passion to Life at i6:
As our company grows and matures from a start-up into a scale-up with an increase in customers and multi-tenancy applications, the opportunity for a curious and inspired Junior Data Engineer has opened up in our Data Team. This is your chance to join a team of innovative and like-minded engineers, analysts, and innovators pushing the boundaries of their products and themselves, working with our high-profile global customers to craft truly outstanding SaaS solutions. You will be responsible for creating, owning and driving the data solutions, including transforming data into critical assets for the business while ensuring security, reliability, privacy and performance by design.

Your Career at i6:
In your first month you’ll…
Ask lots of questions and begin to explore and really get to know our products, our clients and gain deeper insight into the Aviation industry
Start to build an idea of the challenges and requirements of our clients, think about what it really means to connect our biosphere to highly varied client architectures managing multiple interactions a second and dealing in near real time
Begin to translate business issues and requirements into data solutions
Maintain monitoring and alerting across the data processing systems
Start to learn and develop your skills in implementing data processes, ensuring consistency & re-usability
Start to work closely with data analysts ensuring data and its required structure & availability is accessible for reporting

Within 6 mths you’ll…
Start to develop the knowledge, expertise and confidence to identify pain points , challenge the status quo, then pioneer new ideas, develop designs and implement solutions helping us scale faster and drive the business forward
Participate fully in, and contribute to, sprint planning, retrospectives, standups and other ceremonies in a constructive and can-do manner
Begin to gather data requirements, working closely with business stakeholders and analysts and design & build the required integrations into the data pipeline following industry best practices on ETL, ELT processes & data warehousing to support project delivery
Working closely with the infrastructure team ensuring business data applications are well served and prioritise when required
Working closely with Solutions Architects, Developers, Product Owners & Managers to ensure adherence to data models when developing new products or applications
Start implementing data analysts reporting requirements into the data warehouse or pipeline

Within 1 year, you’ll…
Be able to proactively identify gaps & solutions in the data functionality requirements of the business
Effectively manage challenges associated with handling large volumes of data while working towards tight deadlines
Be proposing, researching & supporting the implementation of technical solutions that help the business achieve its commercial objectives in a cost-efficient and scalable manner
Feeling proud of what you have achieved, the influence you have had and how you have made positive changes for yourself, your team, i6, your clients and the planet
What you’ll bring to the role:
Bright, ambitious, humble and most of all inquisitive, you love telling stories with data, building reports and propositions that give real insight and impact, ensuring our products remain the best in the market. You love data and are a true evangelist, able to use your outstanding analytical and architecture skills to power internal insight and supercharge our data focused products. Data is the secret sauce to your superpower, and you are ready to share your expertise to help launch us to the next level.
What you will be working on: (You don't need all of this, but outstanding SQL is an essential)
Enterprise Systems
Near Real-Time Applications
Good understanding and experience using the following technologies: SQL, Python, Airflow, DBT, BigQuery, GitHub, GitHub Actions
Nice to have understanding of the following technologies: BI Tools (e.g. Tableau), Docker, Kubernetes, Google Cloud Platform, AWS, Azure Cloud
Bonus knowledge: Node.js, Typescript, MongoDB
SaaS
A dedicated, creative, and highly collaborative team of developers, engineers and analysts
A massive variety of systems and products for our major clients across the globe

Our Interview Process:

Stage 1: Initial screening interview with Talent Manager
Stage 2: In depth technical interview our Senior Data Engineer
Stage 3: Cultural interview with our CTO
Stage 4: Offer
Stage 5: Hand in notice and join our awesome team at i6

A Career With i6 Group

Our Values, The Six I’s That Make i6:
Improve - we’re open to new ideas and deliver an amazing customer experience.
Influence - we build strong relationships and make great decisions.
Impress - we work hard and smart to deliver great work on time.
Innovation - we share ideas, experiment and find new ways to solve challenges.
Intelligence - we continue to develop our skills, knowledge and behaviours.
Integrity - we respect and embrace differences in people.

Our Benefits

Subsidised subscriptions to Headspace, gym membership and health insurance (UK Only)
Various discounts and perks at high street shops, supermarkets and online vendors
Fully remote – work from anywhere within the UK/ Europe (with occasional travel to Farnborough Airport)
Clear goals, targets, and progression plan so you can maximise your career
Cross training opportunities
Sustainable workplace - Carbon footprint offset

Diversity & Inclusion: Equal opportunities for everyone.
We’re embracing diversity in all its forms and fostering an inclusive environment for all people to do their best work. This is integral to our mission of driving operational efficiency and environmental accountability across the world.

QTPX6EGOKQ",106385,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,Unknown / Non-Applicable,Remote,10,data engineer,na,"['python', 'sql']","['bigquery', 'azure', 'google cloud', 'aws']",['tableau'],['mongodb'],['airflow'],,
Kaizen Dynamics,4.2,Remote,Data Engineer,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and system administrator for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Support the System Administration of associated tools and software of data and analytics landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, and analytic products. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
ETL processes development: 10 years (Preferred)
Work Location: Remote",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,[],[],[],[],[],master,0-2 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['python', 'sql']",[],[],[],[],master,
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
CINQCARE,4.0,"Washington, DC",Data Engineer,"Overview
The Data Engineer is a critical member of our growing data science team. In this role, you will have the chance to define and develop a core data asset which provides a representative and culturally aware view of the individuals and the communities that CINQCARE serves. You will work to evolve this asset over time using a product roadmap that includes identification and closure of gaps in existing data, introduction of new data sources, generation of proprietary data while quantifying and eliminating areas of structural biases. CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations and to so, we must also seek to fix gaps in data that ignore and marginalizes the Black and Brown communities that we serve.

An idea candidate for this role will embody CINQCARE’s core values, including, Trusted, Empathetic, Committed, Humble, Creative and Community-Minded. At CINQCARE, we don’t have patients or customers – we have Family Members.
Job Responsibilities
The Data Engineer will have the following responsibilities:
The Data Engineer will be responsible for the design, development and delivery of data pipelines and value-added data assets, leveraging a variety of data warehousing methodologies and disciplines to ingest the data from heterogeneous sources into cloud-based Data-Lake Environment in AWS.
Manage initiatives & projects of significant complexity and risk. Excellent business and communication skills to be able to work with business owners providing input to prioritized roadmaps, develop work estimates, and ensure successful delivery to support strategic planning and initiatives, improve organizational performance, and advance progress towards CINQCARE’s goals.
Assist in the overall architecture of the ETL Design and proactively provide inputs in designing, implementing, and automating the data pipelines.
Investigate and mine data to identify potential issues within the data pipelines, notify end-users and propose adequate solutions.
Ensure data quality and integrity within the data lake in the AWS environment with a focus on compliance to HIPAA and state level compliance requirements.
Oversee user permissions and configurations for adherence to documented access management standards and policies.
Independently (with minimal oversight) develop and maintain trusted advisor relationships with business, clinical, and operations leaders at the senior leadership level and with external partners, that include guidance for optimizing use of analytic capabilities and deliverables, and prioritization based on strategic vision.
Use coding/scripting pipelines and APIs to uncover and turn data into assets that are analysis-friendly using AWS services like Athena Data Catalogue, Quick Sight or any other big data tool on AWS.
Create a high-quality catalog of all pertinent data with the primary goal of establishing a single source of truth and significantly increasing productivity by reducing the time required for data search and discovery.
Crossing team boundaries, educate/advise on data projects, on how to combine and aggregate client data across platforms or technologies, and how to make the greatest use of data.
Lead consistent adherence to the Software Development Life Cycle framework and governance processes including, but not limited to leading planning sessions, collecting, and documenting requirements, identifying design patterns, create and define custom transformations, aggregations, and other data manipulations, developing data pipelines, creating documentation, developing test plans, performing unit testing, conducting peer review sessions.
Use knowledge of healthcare industry, market environment, and clinical and business workflows and activities, to inform solution design and development to execute high-quality or differentiated solutions in an established problem space.
Perform other job-related duties as assigned.

General Duties
The Data Engineer should have the following duties:
Leadership: The Data Engineer will lead the continued build out of the data asset to create business value, including collaborating with their team to design, develop, and execute those strategies and solutions to deliver desired outcomes.
Strategy: The Data Engineer will contribute to the business strategy and roadmap: (1) improve outcomes for CINQCARE Family Members; (2) enhance the efficacy of other CINQCARE. business divisions; and (3) develop and deliver external market opportunities for CINQCARE products and services. In establishing the business strategy, the Data Engineer will define and innovate sustainable revenue models to drive profitability of the Company.
Collaboration: The Data Engineer will ensure that AI capabilities form a cohesive offering, including by working closely with other business divisions to learn their needs, internalize their knowledge, and define solutions to achieve the business objectives of CINQCARE.
Knowledge: The Data Engineer will provide subject matter expertise in the AI solutions, including determining and recommended approaches for designing and building elegant data structures in support of existing reporting tools and custom visualization platforms.
Culture: The Data Engineer is accountable for creating a productive, collaborative, safe and inclusive work environment for their team and as part of the larger Company.
Qualifications
The Data Engineer should have the following qualifications:
Education: Bachelor’s degree in Computer Science, Engineering, Software Engineering, or related field; Master’s degree preferred.
Experience: The ideal candidate should have at least 3+ years of experience in healthcare data engineering. Experience with a variety of data projects and environments, whether on-prem or in-cloud (5+ years in SQL Server, ETL Tools, Business Intelligence & Analysis, Architecture). Familiarity with the Microsoft Stack; experience with other platforms is a plus. Mastery of Python and SQL. Strong foundational knowledge of data lakes and AWS products such as AWS Glue. Experience with healthcare eligibility and claims, implementing APIs, HL7/FHIR standards, ETL scheduling solutions, SQL, and healthcare data security.
Entrepreneurial: CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations. This position is accountable for ensuring CINQCARE is positioned to innovatively deliver on its promise.
Communication: Strong analytical and collaboration skills is required. Excellent verbal, written communication and presentation skills; ability to clearly articulate and present concepts and models in an accessible manner to CINQCARE’s team, investors, partners, and other stakeholders.
Relationships: Ability to build and effectively manage relationships with business leaders and external constituents.
Culture. Good judgement, impeccable ethics, and a strong team player; desire to succeed and grow in a fast-paced, demanding, and entrepreneurial Company.

Location: New York, NY
Compensation: $100,000-$120,000
My3ehHtP4z",110000,201 to 500 Employees,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql']",['aws'],[],['sql server'],[],bachelor,+10 years
LOVEFOODIES INC,4.2,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",54000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['python', 'sql']",[],['tableau'],[],[],,2-5 years
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108511,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['python', 'sql', 'java', 'nosql']","['azure', 'google cloud', 'aws']",[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,+10 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
Angle Health,4.0,Remote,Data Engineer,"Changing Healthcare For Good
At Angle Health, we believe the healthcare system should be accessible, transparent, and easy to navigate. As a digital-first, data-driven health plan, we are replacing legacy systems with modern infrastructure to deliver our members the care they need when they need it. If you want to build the future of healthcare, we'd love for you to join us.

The Role
As a Data Engineer on the Strategy team at Angle Health, you will be part of an elite team of problem solvers tackling some of the hardest operational challenges across the business. You will be working closely with Technical Product Managers and other engineers in small teams embedded within functions across the company—from sales to operations to finance and more.

In partnership with operational stakeholders, your job will be to quickly understand business workflows, and design and implement technical, data-driven solutions to achieve the intended business outcomes. That may include architecting backend micro-services, building and maintaining robust data pipelines, standing up health metrics and automated alerts, and developing scalable technical solutions to support Angle Health's day-to-day operations.

Every day will be a new learning experience in this role and may span discussing architecture with fellow engineers, wrangling large-scale data, coding a custom web app or micro-service, or speaking with customers and executives.

This role is ideal for entrepreneurial engineers, applied data scientists, and creative, intellectually curious thinkers that want to solve high-value problems in healthcare.

This position may be based in San Francisco, New York City, Salt Lake City, or Remote.

We are currently trialing various titles for the same role. Please consider the following posted roles as the same position: Deployed Engineer, Software Engineer, Business Operations, and Data Engineer. Please note these are the same, so if you have already applied for one position, there is no need to reapply for the others.
What We Value
A strong engineering background in computer science, software engineering, data science, mathematics, or similar technical field is required for this role
Proficiency in programming languages (e.g. Python, SQL, Java, TypeScript/JavaScript, or similar) and data engineering frameworks
A highly analytical mindset and an eagerness to build technical solutions to complex business problems
High attention to detail and intellectual curiosity—you're not satisfied with surface-level answers. You want to dive into the data, the ""how,"" and the ""why"" because ""the way it's always been done"" is not always the way it should be done
Low ego—the outcome matters more than who gets the credit
Demonstrated ability to collaborate effectively in teams of technical and non-technical individuals
Highly organized with an ability to multitask, problem solve, and balance competing priorities in a rapidly changing environment
Because We Value You:

Competitive compensation and stock options
100% company paid comprehensive health, vision & dental insurance for you and your dependents
Supplemental Life, AD&D and Short Term Disability coverage options
Discretionary time off
Opportunity for rapid career progression
Relocation assistance (if relocation is required)
3 months of paid parental leave and flexible return to work policy (after 10 months of employment)
Work-from-home stipend for remote employees
Company provided lunch for in-office employees
401(k) account
Other benefits coming soon!

Backed by a team of world class investors, we are a healthcare startup on a mission to make our health system more effective, accessible, and affordable to everyone. From running large hospitals and health plans to serving on federal healthcare advisory boards to solving the world's hardest problems at Palantir, our team has done it all. As part of this core group at Angle Health, you will have the right balance of support and autonomy to grow both personally and professionally and the opportunity to own large parts of the business and scale with the company.

Angle Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. Angle Health is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities.",106385,1 to 50 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2019,Unknown / Non-Applicable,Remote,4,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74801,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85907,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
The Sunwater Institute,4.2,"Rockville, MD",Data Engineer,"The Sunwater Institute is a nonprofit, nonpartisan public policy organization focused on improving the foundations of our social systems. The Sunwater Institute’s current programs focus on improving congressional performance, preserving and advancing constitutional rights and liberties, strengthening and enlarging property rights, and exploring the foundations of group decision-making.
The Sunwater Institute’s interdisciplinary, long-term orientation and focus on the building blocks of social systems sets it apart as a haven for bold, fresh ideas and free, constructive dialogue. The Sunwater Institute is located in North Bethesda and maintains a presence on Capitol Hill.
POSITION
The Sunwater Institute is seeking a Data Engineer who has a background in engineering or computer science and is highly skilled in SQL and database services technologies. The ideal candidate will possess strong problem-solving skills, excellent communication, and a great technical aptitude. The individual in this position will provide project support and help drive team initiatives. The candidate will be creative, and comfortable working in small teams and coordinating with US and non-US-based developers.
Responsibilities
Develop data models to store and manage large amounts of structured and unstructured data
Design efficient ETL processes to move and integrate data from different sources into the enterprise warehouse
Write complex queries using SQL Server etc. to analyze large datasets
Identify trends, patterns, and correlations within datasets by performing exploratory analysis
Ensure high quality of data by implementing best practices on storage, maintenance, indexing techniques, etc
Coordinate project status updates between teams and communicate deliverables and timelines
REQUIREMENTS
Bachelor’s degree in Computer Science or Engineering related field
4+ years of relevant experience in a similar role
Strong knowledge of relational databases including SQL Server
Proficient skills in writing complex queries using SQL language; developing stored procs, views, etc.
Experience with end-to-end ETL (Extract Transform Load) Processes
Familiarity with Big Data platforms such as AWS etc.
Ability to develop effective reports that help drive business decisions
Must be able to work effectively with teams but be able to work independently on tasks to meet tight deadlines
Exceptional communication skills verbal, written, and active listening
Extreme attention to detail and accuracy
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Vision insurance
Compensation package:
Yearly bonus
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Rockville, MD 20852: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",106385,,,,,-1,,MD,-1,data engineer,na,['sql'],['aws'],[],['sql server'],[],bachelor,2-5 years
Adobe,4.4,"New York, NY",Data Engineer,"Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

Job Description
Adobe Customer Solutions is looking for a full time Data Engineer with experience in building data integrations using AWS technology stack as part of the team's Data as a Service portfolio for Adobe’s Digital Experience enterprise customers.
Customer facing Engineers who enjoy tackling complex technical challenges, have a passion for delighting customers and who are self-motivated to push themselves in a team oriented culture will thrive in our environment
What you'll Do
Collaborate with Data architects, Enterprise architects, Solution consultants and Product engineering teams to gather customer data integration requirements, conceptualize solutions & build required technology stack
Collaborate with enterprise customer's engineering team to identify data sources, profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating customer data sources and third party data sources with Adobe solutions
Develop new features and improve existing data integrations with customer data ecosystem
Encourage team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Collaborate with a Project Manager to bill and forecast time for customer solutions
What you need to succeed

Proven experience in building/operating/maintaining fault tolerant and scalable data processing integrations using AWS
Proven track record in Python programming language
Software development experience working with Apache Airflow, Spark, MongoDB, MySQL
Experience using Docker or Kubernetes is a plus
BS/MS degree in Computer Science or equivalent proven experience
Ability to identify and resolve problems associated with production grade large scale data processing workflows
Excellent interpersonal skills
Experience crafting and maintaining unit tests and continuous integration.
Passion for crafting I ntelligent data pipelines that customers love to use
Strong capacity to handle numerous projects are a must
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists. You will also be surrounded by colleagues who are committed to helping each other grow through our outstanding Check-In approach where feedback flows freely.
If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the significant benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age, sexual orientation, gender identity, disability or veteran status.

Our compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $101,500 -- $194,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.",147900,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1982,$5 to $10 billion (USD),NY,41,data engineer,na,['python'],['aws'],[],"['mysql', 'mongodb']","['airflow', 'spark']",,
Airbus Americas,4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.",92046,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD),GA,53,data engineer,na,"['python', 'r', 'java']",[],[],[],['spark'],bachelor,2-5 years
ITExpertUS,2.8,"Chicago, IL",Data Engineer,"Client: Agilisium
Role: Data Engineer
Location: Chicago, IL – Hybrid role – Need to be onsite once/twice in a month.
Type: Long term contract
Job Description
Title: Data Engineer
We are looking for a savvy Data Engineer to join our growing team. The hire will be responsible for expanding and optimizing our data and pipeline architecture and data flow and collection. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. In addition, the Data Engineer will collaborate and support software developers, implementation architecture, and data engineers on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. In addition, they must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
Responsibilities:
Because we work on a programmatic leading edge of many technologies, we need someone who is a creative problem solver, resourceful in getting things done, and can shift productively to or from working independently and collaboratively. This person would also take on the following responsibilities:
Process unstructured data into a form suitable for analysis.
Support the business with ad hoc data analysis and build reliable data pipelines.
Implementation of best practices and IT operations in mission-critical tighter SLA data
pipelines using Airflow
Query Engine Migration from Dremio to Redshift.
We leverage: Multiple AWS Data & Analytic Services(e.g.,Glue, Kinesis, S3) , SQL (e.g.,
PostgreSQL, Redshift, Athena); NoSQL (e.g., DocumentDB, MongoDB); Kafka, Docker, Spark(AWS EMR and DataBricks), Airflow, Dremio, Qubole, etc.
We use AWS extensively, so experience with AWS cloud and AWS Data & Analytics certification will help you hit the ground running.
Business Skills for Data Engineers:
Creative Problem-Solving: Approaching data organization challenges with a clear eye on what is important; employing the suitable approach/methods to make the maximum use of time and human resources.
Effective Collaboration: Carefully listening to management, data engineers, and architects to establish their needs.
Intellectual Curiosity: Exploring new territories and finding creative and pragmatic ways to solve data problems.
Industry Knowledge: Understanding and perspective of how industry functions and how data can be collected, analyzed, and utilized for value creation; maintaining flexibility in the face of big data developments.
Skills and Qualifications:
3 to 5 years of real-world Data Engineering experience.
Programming experience, ideally in Python and other data engineering languages like Scala
Programming knowledge to clean structure and semi-structure datasets.
Experience processing large amounts of structured and unstructured data. Streaming data experience is a plus.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Background in Linux
Build the infrastructure required for optimal extraction, transformation, and loading of data
from various data sources using SQL and other cloud ‘big data’ technologies like DataBricks, Snowflake, Dremio, and Qubole.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience creating a platform on which complex data pipelines are built using orchestration tools like Airflow, and Astronomer.
Experience with real-time sync between OLTP and OLAP using AWS technologies like real- time sync between AWS Aurora and AWS Redshift.
Job Type: Full-time
Salary: Up to $68.00 per hour
Schedule:
8 hour shift
Work Location: On the road
Speak with the employer
+91 6305048577",122400,501 to 1000 Employees,Company - Private,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,"['python', 'scala', 'sql', 'nosql']","['snowflake', 'databricks', 'aws', 'redshift']",[],['mongodb'],"['kafka', 'airflow', 'spark']",,5-10 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74801,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
Sanametrix,3.8,Remote,AWS Data Engineer,"Candidates Must be able to obtain/maintain a Public Trust (US Citizenship is required). This position is full-time w/benefits and 100% remote!
Sanametrix, Inc. is a fast-growing small business headquartered in Arlington, VA. We are dedicated to providing federal agencies with legendary customer service and focused solutions for their business and technology needs. This role is responsible for building data pipelines for transferring data from source systems (virtual machines, Microsoft SQL Server) into AWS Cloud using AWS Native Tools. This resource has strong data modeling and scripting experience and has a strong knowledge of AWS Data Services.
Responsibilities:
Perform data processing, algorithm / structures, pipeline orchestration, data quality, governance, discovery.
Work with structured and unstructured data, blob data
Develop and work with APIs
Collect and organize data using data warehousing technique and file storage technologies
Perform ELT and ETL processes
Gather data requirements
Develop and maintain scalable data pipelines and build out new API integrations to support continuing increases in data volume and complexity.
Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the organization.
Implement processes and systems to monitor data quality, to ensure production data accuracy, and ensure key stakeholder and business process access.
Write unit/integration tests, contribute to engineering wiki, and documents.
Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Work closely with a team of front-end and back-end engineers, product managers, and analysts.
Design data integrations and dataquality framework based on established requirements.
Must be able to obtain a Public Trust (US Citizenship required to obtain Public Trust)
Qualifications & Skills:
Scripting
SQL & Scripting
Python
Spark
Linux / shell scripting
Services / Tools (six or more)
S3 Lambda
Redshift
Lake Formation
Glue ETL
Kinesis
DMS
Glue catalog/Crawlers
Git
Jira
Airflow /Orchestration
Education, Experience, and Licensing Requirements:
BS or MS degree in Computer Science or a related technical field
4+ years of Python or Java development experience
4+ years of SQL or NoSQL experience
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining data processing systems
AWS Certified is preferred
Job Type: Full-time
Pay: $110,000.00 - $120,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Work Location: Remote",115000,Unknown,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'shell', 'python', 'java', 'nosql']","['aws', 'redshift']",[],['sql server'],"['airflow', 'spark']",,5-10 years
Arthur Grand Technologies Inc,4.8,"Atlanta, GA",AWS Data Engineer,"Role: AWS Data Engineer
Location: Atlanta, GA (Meet customer Monthly once post client selection & onboarding)
JD for AWS Data Engineer
Experience with the core AWS services, plus the specifics mentioned in this job description.
Experience with Docker . Having used ECS , SNS/SQS, Athena, Lambda,S3, RDS and layers in the past is a plus.
Proficiency in at least in Python, Java
Strong notions of security best practices (e.g. using IAM Roles, KMS, etc.).
Experience with monitoring solutions such as CloudWatch, Cloud Trail.
Previous exposure to large-scale systems design.
Knowledge of writing infrastructure as code (IaC) using CloudFormation or Terraform.
Experience with building or maintaining cloud-native applications.
Past experience with the serverless approaches using AWS Lambda is a plus. For example, the Serverless Application Model (AWS SAM).
Job Types: Full-time, Contract
Schedule:
10 hour shift
8 hour shift
Work Location: Remote",94994,51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable,GA,11,data engineer,na,"['python', 'java']",['aws'],[],[],[],,
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
Epsilon,3.9,"Irving, TX",AWS Data Engineer,"Job Description

As a data engineer, you will design and maintain data platform road maps and data structures that support business and technology objectives. Naturally inquisitive and open to the deep exploration of underlying data, finding actionable insights, and working with functional competencies to drive identified actions. You also enjoy working both freely and as part of a team and have the confidence to influence and communicate with stakeholders at all levels, and to work in a fast-paced complex environment with conflicting priorities.
About the role:
Reporting into the delivery leader, you will deliver consumable, contemporary, and immediate data content to support and drive business decisions. The key focus of the role is to deliver a custom solution to support various business critical requirements. You will be involved in all aspects of data engineering from delivery planning, estimating and analysis, all the way through to data architecture and pipeline design, delivery, and production implementation. From the beginning you will be involved in the design and implementation of complex data solutions ranging from batch to streaming and event-driven architectures, across cloud, on-premise, and hybrid client technology landscapes.
Brief Description of Role:
We are looking for 3+ years of experience in data engineering in a customer or business facing capacity and experience in the following:
Ability to understand and articulate requirements to technical and non-technical audiences
Stakeholder management and communication skills, including prioritizing, problem solving and interpersonal relationship building
Strong experience in SDLC delivery, including waterfall, hybrid and Agile methodologies
Experience of implementing and delivering data solutions and pipelines on AWS Cloud Platform.
A strong understanding of data modelling, data structures, databases, and ETL processes
An in-depth understanding of large-scale data sets, including both structured and unstructured data
Knowledge and experience of delivering CI/CD and DevOps capabilities in a data environment
Develop new inbound data source ingestions required within the multi-tiered data platform to support analytics and marketing automation solutions
Supports data pipelines – Builds the required dimensions, rules, segments, and aggregates
Support all database operations: performance monitoring, pipeline ingestion, maintenance, etc.
Monitor platform health - data loads, extracts, failures, performance tuning
Create/modify data structures/pipelines
Leveraging capabilities of Databricks Lakehouse functionality as needed to build Common/Conformed layers within the data lake
Develop, document, and test software and environment setup to ensure that the outcome meets the needs of end-users and achieves business goals
Qualifications
The following skills are required:
Tech Stack: AWS pipeline, Glue, Databricks, Python, SQL, Spark, etc.
Building the Data Lake using AWS technologies like S3, EKS, ECS, AWS Glue, AWS KMS, EMR
Extensive experience in ETL and audience segmentation
Developing sustainable, scalable, and adaptable data pipelines
Attention to detail in design, documentation, and test coverage of delivered tasks
Strong written and verbal communication skills, team player
In addition, the candidate should have strong business acumen, interpersonal skills, and communication skills, yet also be able to work independently.
At least 3 years of experience with designing and developing Data Pipelines for Data Ingestion or Transformation using AWS technologies
At least 2 years of experience in the following Big Data frameworks: File Format (Parquet, etc.), Resource Management, Distributed Processing
At least 3 years of experience developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 1-2 years of experience with Spark programming (PySpark)
At least 2 years of experience with Databricks implementations
Familiarity with the concepts of “delta lake” and “lakehouse” technologies
The following skills are nice to have, and expertise is not required:
Adobe (Campaign, Audience Manager, Analytics)
MLFlow
Microsoft Power BI
SAP Business Objects

Additional Information

When you’re one of us, you get to run with the best. For decades, we’ve been helping marketers from the world’s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon’s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC:
Culture: https://www.epsilon.com/us/about-us/our-culture-epsilon
DE&I: https://www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR: https://www.epsilon.com/us/about-us/corporate-social-responsibility
Life at Epsilon: https://www.epsilon.com/us/about-us/epic-blog
Great People Deserve Great Benefits
We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.
#LI-SJ1
REF186919L",88775,5001 to 10000 Employees,Subsidiary or Business Segment,Media & Communication,Advertising & Public Relations,1969,$1 to $5 billion (USD),TX,54,data engineer,na,"['python', 'sql']","['databricks', 'aws']","['power bi', 'sap']",[],['spark'],,2-5 years
Govini,3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.",88194,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable,PA,12,data engineer,na,['sql'],[],[],[],[],master,5-10 years
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['python', 'sql']",[],[],[],[],master,
CINQCARE,4.0,"Washington, DC",Data Engineer,"Overview
The Data Engineer is a critical member of our growing data science team. In this role, you will have the chance to define and develop a core data asset which provides a representative and culturally aware view of the individuals and the communities that CINQCARE serves. You will work to evolve this asset over time using a product roadmap that includes identification and closure of gaps in existing data, introduction of new data sources, generation of proprietary data while quantifying and eliminating areas of structural biases. CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations and to so, we must also seek to fix gaps in data that ignore and marginalizes the Black and Brown communities that we serve.

An idea candidate for this role will embody CINQCARE’s core values, including, Trusted, Empathetic, Committed, Humble, Creative and Community-Minded. At CINQCARE, we don’t have patients or customers – we have Family Members.
Job Responsibilities
The Data Engineer will have the following responsibilities:
The Data Engineer will be responsible for the design, development and delivery of data pipelines and value-added data assets, leveraging a variety of data warehousing methodologies and disciplines to ingest the data from heterogeneous sources into cloud-based Data-Lake Environment in AWS.
Manage initiatives & projects of significant complexity and risk. Excellent business and communication skills to be able to work with business owners providing input to prioritized roadmaps, develop work estimates, and ensure successful delivery to support strategic planning and initiatives, improve organizational performance, and advance progress towards CINQCARE’s goals.
Assist in the overall architecture of the ETL Design and proactively provide inputs in designing, implementing, and automating the data pipelines.
Investigate and mine data to identify potential issues within the data pipelines, notify end-users and propose adequate solutions.
Ensure data quality and integrity within the data lake in the AWS environment with a focus on compliance to HIPAA and state level compliance requirements.
Oversee user permissions and configurations for adherence to documented access management standards and policies.
Independently (with minimal oversight) develop and maintain trusted advisor relationships with business, clinical, and operations leaders at the senior leadership level and with external partners, that include guidance for optimizing use of analytic capabilities and deliverables, and prioritization based on strategic vision.
Use coding/scripting pipelines and APIs to uncover and turn data into assets that are analysis-friendly using AWS services like Athena Data Catalogue, Quick Sight or any other big data tool on AWS.
Create a high-quality catalog of all pertinent data with the primary goal of establishing a single source of truth and significantly increasing productivity by reducing the time required for data search and discovery.
Crossing team boundaries, educate/advise on data projects, on how to combine and aggregate client data across platforms or technologies, and how to make the greatest use of data.
Lead consistent adherence to the Software Development Life Cycle framework and governance processes including, but not limited to leading planning sessions, collecting, and documenting requirements, identifying design patterns, create and define custom transformations, aggregations, and other data manipulations, developing data pipelines, creating documentation, developing test plans, performing unit testing, conducting peer review sessions.
Use knowledge of healthcare industry, market environment, and clinical and business workflows and activities, to inform solution design and development to execute high-quality or differentiated solutions in an established problem space.
Perform other job-related duties as assigned.

General Duties
The Data Engineer should have the following duties:
Leadership: The Data Engineer will lead the continued build out of the data asset to create business value, including collaborating with their team to design, develop, and execute those strategies and solutions to deliver desired outcomes.
Strategy: The Data Engineer will contribute to the business strategy and roadmap: (1) improve outcomes for CINQCARE Family Members; (2) enhance the efficacy of other CINQCARE. business divisions; and (3) develop and deliver external market opportunities for CINQCARE products and services. In establishing the business strategy, the Data Engineer will define and innovate sustainable revenue models to drive profitability of the Company.
Collaboration: The Data Engineer will ensure that AI capabilities form a cohesive offering, including by working closely with other business divisions to learn their needs, internalize their knowledge, and define solutions to achieve the business objectives of CINQCARE.
Knowledge: The Data Engineer will provide subject matter expertise in the AI solutions, including determining and recommended approaches for designing and building elegant data structures in support of existing reporting tools and custom visualization platforms.
Culture: The Data Engineer is accountable for creating a productive, collaborative, safe and inclusive work environment for their team and as part of the larger Company.
Qualifications
The Data Engineer should have the following qualifications:
Education: Bachelor’s degree in Computer Science, Engineering, Software Engineering, or related field; Master’s degree preferred.
Experience: The ideal candidate should have at least 3+ years of experience in healthcare data engineering. Experience with a variety of data projects and environments, whether on-prem or in-cloud (5+ years in SQL Server, ETL Tools, Business Intelligence & Analysis, Architecture). Familiarity with the Microsoft Stack; experience with other platforms is a plus. Mastery of Python and SQL. Strong foundational knowledge of data lakes and AWS products such as AWS Glue. Experience with healthcare eligibility and claims, implementing APIs, HL7/FHIR standards, ETL scheduling solutions, SQL, and healthcare data security.
Entrepreneurial: CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations. This position is accountable for ensuring CINQCARE is positioned to innovatively deliver on its promise.
Communication: Strong analytical and collaboration skills is required. Excellent verbal, written communication and presentation skills; ability to clearly articulate and present concepts and models in an accessible manner to CINQCARE’s team, investors, partners, and other stakeholders.
Relationships: Ability to build and effectively manage relationships with business leaders and external constituents.
Culture. Good judgement, impeccable ethics, and a strong team player; desire to succeed and grow in a fast-paced, demanding, and entrepreneurial Company.

Location: New York, NY
Compensation: $100,000-$120,000
My3ehHtP4z",110000,201 to 500 Employees,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql']",['aws'],[],['sql server'],[],bachelor,+10 years
Epsilon,3.9,"Irving, TX",AWS Data Engineer,"Job Description

As a data engineer, you will design and maintain data platform road maps and data structures that support business and technology objectives. Naturally inquisitive and open to the deep exploration of underlying data, finding actionable insights, and working with functional competencies to drive identified actions. You also enjoy working both freely and as part of a team and have the confidence to influence and communicate with stakeholders at all levels, and to work in a fast-paced complex environment with conflicting priorities.

About the role:

Reporting into the delivery leader, you will deliver consumable, contemporary, and immediate data content to support and drive business decisions. The key focus of the role is to deliver a custom solution to support various business critical requirements. You will be involved in all aspects of data engineering from delivery planning, estimating and analysis, all the way through to data architecture and pipeline design, delivery, and production implementation. From the beginning you will be involved in the design and implementation of complex data solutions ranging from batch to streaming and event-driven architectures, across cloud, on-premise, and hybrid client technology landscapes.

Brief Description of Role:

We are looking for 3+ years of experience in data engineering in a customer or business facing capacity and experience in the following:
Ability to understand and articulate requirements to technical and non-technical audiences
Stakeholder management and communication skills, including prioritizing, problem solving and interpersonal relationship building
Strong experience in SDLC delivery, including waterfall, hybrid and Agile methodologies
Experience of implementing and delivering data solutions and pipelines on AWS Cloud Platform.
A strong understanding of data modelling, data structures, databases, and ETL processes
An in-depth understanding of large-scale data sets, including both structured and unstructured data
Knowledge and experience of delivering CI/CD and DevOps capabilities in a data environment
Develop new inbound data source ingestions required within the multi-tiered data platform to support analytics and marketing automation solutions
Supports data pipelines - Builds the required dimensions, rules, segments, and aggregates
Support all database operations: performance monitoring, pipeline ingestion, maintenance, etc.
Monitor platform health - data loads, extracts, failures, performance tuning
Create/modify data structures/pipelines
Leveraging capabilities of Databricks Lakehouse functionality as needed to build Common/Conformed layers within the data lake
Develop, document, and test software and environment setup to ensure that the outcome meets the needs of end-users and achieves business goals

Qualifications

The following skills are required:
Tech Stack: AWS pipeline, Glue, Databricks, Python, SQL, Spark, etc.
Building the Data Lake using AWS technologies like S3, EKS, ECS, AWS Glue, AWS KMS, EMR
Extensive experience in ETL and audience segmentation
Developing sustainable, scalable, and adaptable data pipelines
Attention to detail in design, documentation, and test coverage of delivered tasks
Strong written and verbal communication skills, team player
In addition, the candidate should have strong business acumen, interpersonal skills, and communication skills, yet also be able to work independently.
At least 3 years of experience with designing and developing Data Pipelines for Data Ingestion or Transformation using AWS technologies
At least 2 years of experience in the following Big Data frameworks: File Format (Parquet, etc.), Resource Management, Distributed Processing
At least 3 years of experience developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps
At least 1-2 years of experience with Spark programming (PySpark)
At least 2 years of experience with Databricks implementations
Familiarity with the concepts of ""delta lake"" and ""lakehouse"" technologies

The following skills are nice to have, and expertise is not required:
Adobe (Campaign, Audience Manager, Analytics)
MLFlow
Microsoft Power BI
SAP Business Objects

Additional Information

When you're one of us, you get to run with the best. For decades, we've been helping marketers from the world's top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon's best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC:
Culture: https://www.epsilon.com/us/about-us/our-culture-epsilon
DE&I: https://www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR: https://www.epsilon.com/us/about-us/corporate-social-responsibility
Life at Epsilon: https://www.epsilon.com/us/about-us/epic-blog

Great People Deserve Great Benefits

We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.

#LI-SJ1

REF186919L",90830,10000+ Employees,Company - Public,Media & Communication,Advertising & Public Relations,1926,$5 to $10 billion (USD),TX,97,data engineer,na,"['python', 'sql']","['databricks', 'aws']","['power bi', 'sap']",[],['spark'],,2-5 years
Angle Health,4.0,Remote,Data Engineer,"Changing Healthcare For Good
At Angle Health, we believe the healthcare system should be accessible, transparent, and easy to navigate. As a digital-first, data-driven health plan, we are replacing legacy systems with modern infrastructure to deliver our members the care they need when they need it. If you want to build the future of healthcare, we'd love for you to join us.

The Role
As a Data Engineer on the Strategy team at Angle Health, you will be part of an elite team of problem solvers tackling some of the hardest operational challenges across the business. You will be working closely with Technical Product Managers and other engineers in small teams embedded within functions across the company—from sales to operations to finance and more.

In partnership with operational stakeholders, your job will be to quickly understand business workflows, and design and implement technical, data-driven solutions to achieve the intended business outcomes. That may include architecting backend micro-services, building and maintaining robust data pipelines, standing up health metrics and automated alerts, and developing scalable technical solutions to support Angle Health's day-to-day operations.

Every day will be a new learning experience in this role and may span discussing architecture with fellow engineers, wrangling large-scale data, coding a custom web app or micro-service, or speaking with customers and executives.

This role is ideal for entrepreneurial engineers, applied data scientists, and creative, intellectually curious thinkers that want to solve high-value problems in healthcare.

This position may be based in San Francisco, New York City, Salt Lake City, or Remote.

We are currently trialing various titles for the same role. Please consider the following posted roles as the same position: Deployed Engineer, Software Engineer, Business Operations, and Data Engineer. Please note these are the same, so if you have already applied for one position, there is no need to reapply for the others.
What We Value
A strong engineering background in computer science, software engineering, data science, mathematics, or similar technical field is required for this role
Proficiency in programming languages (e.g. Python, SQL, Java, TypeScript/JavaScript, or similar) and data engineering frameworks
A highly analytical mindset and an eagerness to build technical solutions to complex business problems
High attention to detail and intellectual curiosity—you're not satisfied with surface-level answers. You want to dive into the data, the ""how,"" and the ""why"" because ""the way it's always been done"" is not always the way it should be done
Low ego—the outcome matters more than who gets the credit
Demonstrated ability to collaborate effectively in teams of technical and non-technical individuals
Highly organized with an ability to multitask, problem solve, and balance competing priorities in a rapidly changing environment
Because We Value You:

Competitive compensation and stock options
100% company paid comprehensive health, vision & dental insurance for you and your dependents
Supplemental Life, AD&D and Short Term Disability coverage options
Discretionary time off
Opportunity for rapid career progression
Relocation assistance (if relocation is required)
3 months of paid parental leave and flexible return to work policy (after 10 months of employment)
Work-from-home stipend for remote employees
Company provided lunch for in-office employees
401(k) account
Other benefits coming soon!

Backed by a team of world class investors, we are a healthcare startup on a mission to make our health system more effective, accessible, and affordable to everyone. From running large hospitals and health plans to serving on federal healthcare advisory boards to solving the world's hardest problems at Palantir, our team has done it all. As part of this core group at Angle Health, you will have the right balance of support and autonomy to grow both personally and professionally and the opportunity to own large parts of the business and scale with the company.

Angle Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. Angle Health is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities.",106385,1 to 50 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2019,Unknown / Non-Applicable,Remote,4,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,
"AGM Tech Solutions, LLC",4.8,"Alpharetta, GA","Senior Data Engineer (Must be local to Atlanta, GA)","Sr. Data Engineer
Location: Must be local to Atlanta Metro area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $75.00 - $85.00 per hour
Experience level:
7 years
Schedule:
Monday to Friday
Application Question(s):
How many years of experience do you have as a Data Engineer?
How many years of experience do you have building data pipelines with Spark or Databricks?
Can you work on W2 without sponsorship? No C2C engagements for this position.
Are you currently located in Atlanta Metropolitan area?
Experience:
Python: 4 years (Preferred)
Spark: 3 years (Preferred)
AWS: 3 years (Preferred)
Work Location: One location",144000,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,2-5 years
Cloudbc Labs,3.9,Remote,Anaplan Data Engineer,"Job Title :: Anaplan Data Engineer
Location :: Remote
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Remote",117000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,Unknown / Non-Applicable,Remote,8,data engineer,na,[],[],[],[],[],,
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
Govini,3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.",88194,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable,PA,12,data engineer,na,['sql'],[],[],[],[],master,5-10 years
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
Grid,4.2,"Oakland, CA",Data Engineer,"Today’s financial system is built to favor those with money. Grid’s mission is to level that playing field by building financial products that help users better manage their financial future. The Grid app lets users access cash, build credit, spend money, file their taxes and lots, lots more.

Grid is a fast growing team that’s deeply passionate about making a difference in the lives of millions. We’re solving huge problems and believe that a merit driven culture allows every team member to play a big role. Join our growing team in our Bay Area headquarters or additional offices across the US.

Our Data Engineers are responsible for converting the data surrounding a customer's financial life into tools that make our customers smarter and better equipped financially for the future. We value making intelligent decisions backed by good data and tools. We're looking for people who share our values, particularly if you have experience analyzing, processing, and learning from large data sets.

We’re looking for a seasoned data engineer who can help us lay the foundation of an exceptional data engineering practice. The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions. You will be writing code, setting style guides and collaborating cross-functionally with product, engineering and leadership.
Problems we work on:
Analytics: Collect all the data for a user into tools that help our customers
Machine Learning: Using a variety of techniques to reach better insights
Data Processing: Managing data & statistics using scalable and efficient technologies
Visualization: Envision our data as beautiful graphs and tools that allow customers to explore their data & ask their own questions
Risk: Analyze data for anomalous patterns and build tools that allow us to find bad actors quickly
We Practice:
Open collaboration
Code reviews
Testing
Agile development
We Use:
Go
Python
MySQL
Google Cloud Platform
Kubernetes
Kubeflow
Docker
Google Pubsub
BigQuery
Firebase
We’re looking for Engineers to:
Design and implement platform services, frameworks and ecosystems
Build a scalable, reliable, operable and performant big data workflow platform for data scientists/engineers, AI/ML engineers, and product/operation team members
Drive efficiency and reliability improvements through design and automation: performance, scaling, observability, and monitoring
Requirements:
Strong programming skills with Python
Strong programming skills with a typed programming language, such as Java, Scala, Go, etc.
Disciplined approach to development, testing, and quality assurance
Excellent communication skills, capable of explaining highly technical problems in English
Understand data processing and ETL, hands on experience building pipelines and using frameworks such as Hive hdfs, Presto, Spark, etc.
Really strong candidates may have:
Really strong candidates may have:
Actively contribute to open source software
Worked with a strong, lean-based development environment
Previous work experience in a start-up environment
Ability to recognize the right tool for the right situation/problem.
Will have strong programming skill in Go",112869,,,,,-1,,CA,-1,data engineer,na,"['python', 'scala', 'java', 'go']","['bigquery', 'google cloud']",[],"['mysql', 'hive']",['spark'],,
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74801,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
The Sunwater Institute,4.2,"Rockville, MD",Data Engineer,"The Sunwater Institute is a nonprofit, nonpartisan public policy organization focused on improving the foundations of our social systems. The Sunwater Institute’s current programs focus on improving congressional performance, preserving and advancing constitutional rights and liberties, strengthening and enlarging property rights, and exploring the foundations of group decision-making.
The Sunwater Institute’s interdisciplinary, long-term orientation and focus on the building blocks of social systems sets it apart as a haven for bold, fresh ideas and free, constructive dialogue. The Sunwater Institute is located in North Bethesda and maintains a presence on Capitol Hill.
POSITION
The Sunwater Institute is seeking a Data Engineer who has a background in engineering or computer science and is highly skilled in SQL and database services technologies. The ideal candidate will possess strong problem-solving skills, excellent communication, and a great technical aptitude. The individual in this position will provide project support and help drive team initiatives. The candidate will be creative, and comfortable working in small teams and coordinating with US and non-US-based developers.
Responsibilities
Develop data models to store and manage large amounts of structured and unstructured data
Design efficient ETL processes to move and integrate data from different sources into the enterprise warehouse
Write complex queries using SQL Server etc. to analyze large datasets
Identify trends, patterns, and correlations within datasets by performing exploratory analysis
Ensure high quality of data by implementing best practices on storage, maintenance, indexing techniques, etc
Coordinate project status updates between teams and communicate deliverables and timelines
REQUIREMENTS
Bachelor’s degree in Computer Science or Engineering related field
4+ years of relevant experience in a similar role
Strong knowledge of relational databases including SQL Server
Proficient skills in writing complex queries using SQL language; developing stored procs, views, etc.
Experience with end-to-end ETL (Extract Transform Load) Processes
Familiarity with Big Data platforms such as AWS etc.
Ability to develop effective reports that help drive business decisions
Must be able to work effectively with teams but be able to work independently on tasks to meet tight deadlines
Exceptional communication skills verbal, written, and active listening
Extreme attention to detail and accuracy
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Vision insurance
Compensation package:
Yearly bonus
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Rockville, MD 20852: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",106385,,,,,-1,,MD,-1,data engineer,na,['sql'],['aws'],[],['sql server'],[],bachelor,2-5 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Plaxonic Technologies,4.6,"Dallas, TX",Data Engineer,"Job Description – Sr. Data Engineer:
Minimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark
Ability to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark
Hands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies
Strong skill in understanding database architecture, data models and writing complex SQL queries/code
Hands on experience with integration of different data sources (Files, DBs, APIs)
Ability to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation
Knowledge of various ETL techniques
Experience with data pipeline and workflow management tools [Azure DevOps]
Strong analytic skill to work with unstructured data
Experience working with Agile teams scrums.
Strong customer handling skills and communication skills
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75287: Reliably commute or planning to relocate before starting work (Required)
Experience:
Microsoft Excel: 1 year (Preferred)
Microsoft Office: 1 year (Preferred)
Compliance management: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: One location
Speak with the employer
+91 7272167989",98641,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),TX,10,data engineer,na,['sql'],"['azure', 'databricks']",['excel'],[],[],,0-2 years
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85907,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
Angle Health,4.0,Remote,Data Engineer,"Changing Healthcare For Good
At Angle Health, we believe the healthcare system should be accessible, transparent, and easy to navigate. As a digital-first, data-driven health plan, we are replacing legacy systems with modern infrastructure to deliver our members the care they need when they need it. If you want to build the future of healthcare, we'd love for you to join us.

The Role
As a Data Engineer on the Strategy team at Angle Health, you will be part of an elite team of problem solvers tackling some of the hardest operational challenges across the business. You will be working closely with Technical Product Managers and other engineers in small teams embedded within functions across the company—from sales to operations to finance and more.

In partnership with operational stakeholders, your job will be to quickly understand business workflows, and design and implement technical, data-driven solutions to achieve the intended business outcomes. That may include architecting backend micro-services, building and maintaining robust data pipelines, standing up health metrics and automated alerts, and developing scalable technical solutions to support Angle Health's day-to-day operations.

Every day will be a new learning experience in this role and may span discussing architecture with fellow engineers, wrangling large-scale data, coding a custom web app or micro-service, or speaking with customers and executives.

This role is ideal for entrepreneurial engineers, applied data scientists, and creative, intellectually curious thinkers that want to solve high-value problems in healthcare.

This position may be based in San Francisco, New York City, Salt Lake City, or Remote.

We are currently trialing various titles for the same role. Please consider the following posted roles as the same position: Deployed Engineer, Software Engineer, Business Operations, and Data Engineer. Please note these are the same, so if you have already applied for one position, there is no need to reapply for the others.
What We Value
A strong engineering background in computer science, software engineering, data science, mathematics, or similar technical field is required for this role
Proficiency in programming languages (e.g. Python, SQL, Java, TypeScript/JavaScript, or similar) and data engineering frameworks
A highly analytical mindset and an eagerness to build technical solutions to complex business problems
High attention to detail and intellectual curiosity—you're not satisfied with surface-level answers. You want to dive into the data, the ""how,"" and the ""why"" because ""the way it's always been done"" is not always the way it should be done
Low ego—the outcome matters more than who gets the credit
Demonstrated ability to collaborate effectively in teams of technical and non-technical individuals
Highly organized with an ability to multitask, problem solve, and balance competing priorities in a rapidly changing environment
Because We Value You:

Competitive compensation and stock options
100% company paid comprehensive health, vision & dental insurance for you and your dependents
Supplemental Life, AD&D and Short Term Disability coverage options
Discretionary time off
Opportunity for rapid career progression
Relocation assistance (if relocation is required)
3 months of paid parental leave and flexible return to work policy (after 10 months of employment)
Work-from-home stipend for remote employees
Company provided lunch for in-office employees
401(k) account
Other benefits coming soon!

Backed by a team of world class investors, we are a healthcare startup on a mission to make our health system more effective, accessible, and affordable to everyone. From running large hospitals and health plans to serving on federal healthcare advisory boards to solving the world's hardest problems at Palantir, our team has done it all. As part of this core group at Angle Health, you will have the right balance of support and autonomy to grow both personally and professionally and the opportunity to own large parts of the business and scale with the company.

Angle Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. Angle Health is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities.",106385,1 to 50 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2019,Unknown / Non-Applicable,Remote,4,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
CINQCARE,4.0,"Washington, DC",Data Engineer,"Overview
The Data Engineer is a critical member of our growing data science team. In this role, you will have the chance to define and develop a core data asset which provides a representative and culturally aware view of the individuals and the communities that CINQCARE serves. You will work to evolve this asset over time using a product roadmap that includes identification and closure of gaps in existing data, introduction of new data sources, generation of proprietary data while quantifying and eliminating areas of structural biases. CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations and to so, we must also seek to fix gaps in data that ignore and marginalizes the Black and Brown communities that we serve.

An idea candidate for this role will embody CINQCARE’s core values, including, Trusted, Empathetic, Committed, Humble, Creative and Community-Minded. At CINQCARE, we don’t have patients or customers – we have Family Members.
Job Responsibilities
The Data Engineer will have the following responsibilities:
The Data Engineer will be responsible for the design, development and delivery of data pipelines and value-added data assets, leveraging a variety of data warehousing methodologies and disciplines to ingest the data from heterogeneous sources into cloud-based Data-Lake Environment in AWS.
Manage initiatives & projects of significant complexity and risk. Excellent business and communication skills to be able to work with business owners providing input to prioritized roadmaps, develop work estimates, and ensure successful delivery to support strategic planning and initiatives, improve organizational performance, and advance progress towards CINQCARE’s goals.
Assist in the overall architecture of the ETL Design and proactively provide inputs in designing, implementing, and automating the data pipelines.
Investigate and mine data to identify potential issues within the data pipelines, notify end-users and propose adequate solutions.
Ensure data quality and integrity within the data lake in the AWS environment with a focus on compliance to HIPAA and state level compliance requirements.
Oversee user permissions and configurations for adherence to documented access management standards and policies.
Independently (with minimal oversight) develop and maintain trusted advisor relationships with business, clinical, and operations leaders at the senior leadership level and with external partners, that include guidance for optimizing use of analytic capabilities and deliverables, and prioritization based on strategic vision.
Use coding/scripting pipelines and APIs to uncover and turn data into assets that are analysis-friendly using AWS services like Athena Data Catalogue, Quick Sight or any other big data tool on AWS.
Create a high-quality catalog of all pertinent data with the primary goal of establishing a single source of truth and significantly increasing productivity by reducing the time required for data search and discovery.
Crossing team boundaries, educate/advise on data projects, on how to combine and aggregate client data across platforms or technologies, and how to make the greatest use of data.
Lead consistent adherence to the Software Development Life Cycle framework and governance processes including, but not limited to leading planning sessions, collecting, and documenting requirements, identifying design patterns, create and define custom transformations, aggregations, and other data manipulations, developing data pipelines, creating documentation, developing test plans, performing unit testing, conducting peer review sessions.
Use knowledge of healthcare industry, market environment, and clinical and business workflows and activities, to inform solution design and development to execute high-quality or differentiated solutions in an established problem space.
Perform other job-related duties as assigned.

General Duties
The Data Engineer should have the following duties:
Leadership: The Data Engineer will lead the continued build out of the data asset to create business value, including collaborating with their team to design, develop, and execute those strategies and solutions to deliver desired outcomes.
Strategy: The Data Engineer will contribute to the business strategy and roadmap: (1) improve outcomes for CINQCARE Family Members; (2) enhance the efficacy of other CINQCARE. business divisions; and (3) develop and deliver external market opportunities for CINQCARE products and services. In establishing the business strategy, the Data Engineer will define and innovate sustainable revenue models to drive profitability of the Company.
Collaboration: The Data Engineer will ensure that AI capabilities form a cohesive offering, including by working closely with other business divisions to learn their needs, internalize their knowledge, and define solutions to achieve the business objectives of CINQCARE.
Knowledge: The Data Engineer will provide subject matter expertise in the AI solutions, including determining and recommended approaches for designing and building elegant data structures in support of existing reporting tools and custom visualization platforms.
Culture: The Data Engineer is accountable for creating a productive, collaborative, safe and inclusive work environment for their team and as part of the larger Company.
Qualifications
The Data Engineer should have the following qualifications:
Education: Bachelor’s degree in Computer Science, Engineering, Software Engineering, or related field; Master’s degree preferred.
Experience: The ideal candidate should have at least 3+ years of experience in healthcare data engineering. Experience with a variety of data projects and environments, whether on-prem or in-cloud (5+ years in SQL Server, ETL Tools, Business Intelligence & Analysis, Architecture). Familiarity with the Microsoft Stack; experience with other platforms is a plus. Mastery of Python and SQL. Strong foundational knowledge of data lakes and AWS products such as AWS Glue. Experience with healthcare eligibility and claims, implementing APIs, HL7/FHIR standards, ETL scheduling solutions, SQL, and healthcare data security.
Entrepreneurial: CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations. This position is accountable for ensuring CINQCARE is positioned to innovatively deliver on its promise.
Communication: Strong analytical and collaboration skills is required. Excellent verbal, written communication and presentation skills; ability to clearly articulate and present concepts and models in an accessible manner to CINQCARE’s team, investors, partners, and other stakeholders.
Relationships: Ability to build and effectively manage relationships with business leaders and external constituents.
Culture. Good judgement, impeccable ethics, and a strong team player; desire to succeed and grow in a fast-paced, demanding, and entrepreneurial Company.

Location: New York, NY
Compensation: $100,000-$120,000
My3ehHtP4z",110000,201 to 500 Employees,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql']",['aws'],[],['sql server'],[],bachelor,+10 years
Horizon Health Alliance,4.2,"Getzville, NY",Data Engineer,"Are you seeking a rewarding and fulfilling career in the Mental Health and Addictions field?
Apply to be a Data Engineer today!
What will your day look like?
At Horizon, you will enjoy a supportive, team-based work environment. Have a question? There is always someone there to help! We offer a seamless onboarding experience that will ensure your success in your new role.
As a Data Engineer at Horizon you will...
Acquire and assemble datasets that align with business needs
Identifying, designing, and implementing internal process improvements including optimizing data delivery, and automating manual processes
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency
Working with stakeholders including executive teams and assisting them with data-related technical issues
Working with stakeholders to support their data infrastructure needs and understand company objectives
Build, test, and maintain database pipeline architectures
Create new data validation methods and data analysis tools
Assist and support development of data governance policies
Create and distribute corporate reports using industry standard reporting tools and software such as SQL, Power BI, and Python.
Develop and maintain the workflows and security controls related to data extract, transform, and load (ETL) of corporate data
Providing technical expertise in data storage structures, data mining, and data cleansing.
Develop and maintain databases, data marts, models, data sets, and other key technical solutions
Why choose Horizon to build your career?
Besides the fact that we’ve been named a Best Place to Work for 14 (yes, 14!) years in a row? At Horizon, you can be assured that you will make difference in the lives of others. Even better, your teammates will be just as motivated to make a difference!
What we offer that you’ll love…
Company Culture: At Horizon, we pride ourselves on cultivating an atmosphere of teamwork where all employees feel heard and valued.
Diversity & Inclusion: We are committed to equity, racial justice, and equal opportunity for all, and strive toward this goal through the work of our Diversity, Equity, Inclusion and Belonging Council, frequent trainings, ongoing conversations, affinity groups, and more.
Trainings, Trainings, and More Trainings: We have an entire team dedicated to your personal development and professional growth.
Team Building, Connection, and Relationships: At Horizon, we’re more than co-workers, we’re a community. We support each other, celebrate our achievements and milestones together, and have fun together!
Retirement: We know you want to retire comfortably and we’re here to help! Horizon offers 401(k) AND profit-sharing programs to make sure you’re set for the future.
Student Loan Assistance: We help pay off our team members' student loans every month. One year after joining, you’ll have been able to pay off an extra $600!
PTO & Holidays: We believe self-care is essential. Combined with holidays, you’ll earn up to 22.5 paid days in your first year. By your 3rd anniversary, that's almost doubled with up to 37.5 paid days off!
What makes you a great candidate?
We can’t wait to learn more about you! Here are a few specifics of what you’ll need for the job:
Bachelor's degree in Computer Science, Information Systems, Engineering or equivalent
1+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL and reporting/analytic tools
Strong experience in coding languages like SQL and Python
Fluent in relational based systems and writing complex SQL
Strong analytical and problem-solving skills
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Thorough knowledge of Microsoft Office
Ability to work with multidisciplinary teams
Ability to multitask and manage competing deadlines
Ability to communicate appropriately with all levels of management
Excellent understanding of Microsoft Office suite
Ability to build and optimize data sets
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytic skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Location:
55 Dodge Road, Getzville, NY 14068
Hours:
Full-time position, Monday - Friday; 8:00am-5:00pm
Disclaimer:
Horizon endorses public health measures including vaccinations. We encourage all applicants to be mindful of the fact that Horizon is a healthcare agency proving in person services throughout our community.
This information is intended to provide a general overview of the position; it is not a full job description.

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)",84681,1 to 50 Employees,Nonprofit Organization,,,-1,Unknown / Non-Applicable,NY,-1,data engineer,na,"['python', 'sql']",[],['power bi'],[],[],bachelor,+10 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
OPENMIND TECHNOLOGIES INC,4.2,"King of Prussia, PA","Data Engineer - Hybrid Onsite - King of Prussia, PA","Details:
JOB DESCRIPTION:
We are working with a client in need of a talented Data Engineer to support our client located in King of Prussia, PA. This contractor will be working on a high-level corporate initiative to build an enhanced customer service model for a leading utilities organization. This contractor will be working with our client on Data Engineering topics, including creating relevant data models, developing powerful data pipelines, exposing them through various mechanisms including APIs, and using data visualization tools to efficiently present data.
Responsibilities:
· Partner with our client’s leadership teams, engineers, program managers and data analysts to understand data needs.
· Design, build and launch efficient and reliable data pipelines transforming data into useful report ready datasets.
· Use your data and analytics experience to ‘see what’s missing,’ identifying and addressing data gaps, build monitors to detect data quality issues and partner to establish a self-serve environment.
· Broad range of partners equates to a broad range of projects and deliverables, including ML Models, datasets, measurements, services, tools and process.
· Leverage data and business principles to automate data flow, detect business exceptions, build diagnostic capabilities, and improve both business and data knowledge base.
· Build data expertise and own data quality for your areas.
QUALIFICATIONS:
· At least 4+ years' of advanced SQL experience (including at least one SQL DBMS and one no SQL).
· 4+ years' of Python development experience.
· 3+ years' experience with Data Modeling.
· Experience analyzing data to discover opportunities and address gaps.
· Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. SnowFlake, Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
· BSc/BA in Data Science, Computer Science, Engineering.
· Familiarity with SAP order generation and invoicing modules
Job Types: Full-time, Contract
Salary: $75.00 - $85.00 per hour
Experience level:
10 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
advanced SQL (including at least one SQL DBMS, one no SQL: 4 years (Preferred)
Python development: 4 years (Preferred)
Data Modeling: 3 years (Preferred)
analyzing data to discover opportunities and address gaps: 1 year (Preferred)
cloud or on-prem Big Data/MPP analytics platform: 1 year (Preferred)
SnowFlake, Netezza, Teradata, AWS Redshift, Google BigQuery,: 1 year (Preferred)
Azure Data Warehouse: 1 year (Preferred)
SAP order generation and invoicing modules: 1 year (Preferred)
Work Location: One location",144000,Unknown,Private Practice / Firm,Information Technology,Enterprise Software & Network Solutions,-1,Unknown / Non-Applicable,PA,-1,data engineer,na,"['python', 'sql']","['bigquery', 'snowflake', 'redshift', 'azure', 'aws']",['sap'],[],[],,0-2 years
PLAXONIC,4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642",117500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),GA,10,data engineer,na,['sql'],"['bigquery', 'snowflake', 'oracle', 'redshift', 'azure', 'aws']",['sas'],[],[],,5-10 years
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['python', 'sql']","['gcp', 'aws', 'redshift']",[],[],['airflow'],,
FlexIT Inc,4.0,"Beaverton, OR",Full Stack Data Engineer,"APLA is building capabilities around the company's data foundation to build data sources that are needed for reporting and analytics
The type of engineer were looking for is a Full Stack Data Engineer
Knowledge of data visualization engineering as well as consumption and view build engineering",106106,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,[],[],[],[],[],,
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,[],[],[],[],[],bachelor,
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
Govini,3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.",88194,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable,PA,12,data engineer,na,['sql'],[],[],[],[],master,5-10 years
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
The Sunwater Institute,4.2,"Rockville, MD",Data Engineer,"The Sunwater Institute is a nonprofit, nonpartisan public policy organization focused on improving the foundations of our social systems. The Sunwater Institute’s current programs focus on improving congressional performance, preserving and advancing constitutional rights and liberties, strengthening and enlarging property rights, and exploring the foundations of group decision-making.
The Sunwater Institute’s interdisciplinary, long-term orientation and focus on the building blocks of social systems sets it apart as a haven for bold, fresh ideas and free, constructive dialogue. The Sunwater Institute is located in North Bethesda and maintains a presence on Capitol Hill.
POSITION
The Sunwater Institute is seeking a Data Engineer who has a background in engineering or computer science and is highly skilled in SQL and database services technologies. The ideal candidate will possess strong problem-solving skills, excellent communication, and a great technical aptitude. The individual in this position will provide project support and help drive team initiatives. The candidate will be creative, and comfortable working in small teams and coordinating with US and non-US-based developers.
Responsibilities
Develop data models to store and manage large amounts of structured and unstructured data
Design efficient ETL processes to move and integrate data from different sources into the enterprise warehouse
Write complex queries using SQL Server etc. to analyze large datasets
Identify trends, patterns, and correlations within datasets by performing exploratory analysis
Ensure high quality of data by implementing best practices on storage, maintenance, indexing techniques, etc
Coordinate project status updates between teams and communicate deliverables and timelines
REQUIREMENTS
Bachelor’s degree in Computer Science or Engineering related field
4+ years of relevant experience in a similar role
Strong knowledge of relational databases including SQL Server
Proficient skills in writing complex queries using SQL language; developing stored procs, views, etc.
Experience with end-to-end ETL (Extract Transform Load) Processes
Familiarity with Big Data platforms such as AWS etc.
Ability to develop effective reports that help drive business decisions
Must be able to work effectively with teams but be able to work independently on tasks to meet tight deadlines
Exceptional communication skills verbal, written, and active listening
Extreme attention to detail and accuracy
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Vision insurance
Compensation package:
Yearly bonus
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Rockville, MD 20852: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",106385,,,,,-1,,MD,-1,data engineer,na,['sql'],['aws'],[],['sql server'],[],bachelor,2-5 years
CINQCARE,4.0,"Washington, DC",Data Engineer,"Overview
The Data Engineer is a critical member of our growing data science team. In this role, you will have the chance to define and develop a core data asset which provides a representative and culturally aware view of the individuals and the communities that CINQCARE serves. You will work to evolve this asset over time using a product roadmap that includes identification and closure of gaps in existing data, introduction of new data sources, generation of proprietary data while quantifying and eliminating areas of structural biases. CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations and to so, we must also seek to fix gaps in data that ignore and marginalizes the Black and Brown communities that we serve.

An idea candidate for this role will embody CINQCARE’s core values, including, Trusted, Empathetic, Committed, Humble, Creative and Community-Minded. At CINQCARE, we don’t have patients or customers – we have Family Members.
Job Responsibilities
The Data Engineer will have the following responsibilities:
The Data Engineer will be responsible for the design, development and delivery of data pipelines and value-added data assets, leveraging a variety of data warehousing methodologies and disciplines to ingest the data from heterogeneous sources into cloud-based Data-Lake Environment in AWS.
Manage initiatives & projects of significant complexity and risk. Excellent business and communication skills to be able to work with business owners providing input to prioritized roadmaps, develop work estimates, and ensure successful delivery to support strategic planning and initiatives, improve organizational performance, and advance progress towards CINQCARE’s goals.
Assist in the overall architecture of the ETL Design and proactively provide inputs in designing, implementing, and automating the data pipelines.
Investigate and mine data to identify potential issues within the data pipelines, notify end-users and propose adequate solutions.
Ensure data quality and integrity within the data lake in the AWS environment with a focus on compliance to HIPAA and state level compliance requirements.
Oversee user permissions and configurations for adherence to documented access management standards and policies.
Independently (with minimal oversight) develop and maintain trusted advisor relationships with business, clinical, and operations leaders at the senior leadership level and with external partners, that include guidance for optimizing use of analytic capabilities and deliverables, and prioritization based on strategic vision.
Use coding/scripting pipelines and APIs to uncover and turn data into assets that are analysis-friendly using AWS services like Athena Data Catalogue, Quick Sight or any other big data tool on AWS.
Create a high-quality catalog of all pertinent data with the primary goal of establishing a single source of truth and significantly increasing productivity by reducing the time required for data search and discovery.
Crossing team boundaries, educate/advise on data projects, on how to combine and aggregate client data across platforms or technologies, and how to make the greatest use of data.
Lead consistent adherence to the Software Development Life Cycle framework and governance processes including, but not limited to leading planning sessions, collecting, and documenting requirements, identifying design patterns, create and define custom transformations, aggregations, and other data manipulations, developing data pipelines, creating documentation, developing test plans, performing unit testing, conducting peer review sessions.
Use knowledge of healthcare industry, market environment, and clinical and business workflows and activities, to inform solution design and development to execute high-quality or differentiated solutions in an established problem space.
Perform other job-related duties as assigned.

General Duties
The Data Engineer should have the following duties:
Leadership: The Data Engineer will lead the continued build out of the data asset to create business value, including collaborating with their team to design, develop, and execute those strategies and solutions to deliver desired outcomes.
Strategy: The Data Engineer will contribute to the business strategy and roadmap: (1) improve outcomes for CINQCARE Family Members; (2) enhance the efficacy of other CINQCARE. business divisions; and (3) develop and deliver external market opportunities for CINQCARE products and services. In establishing the business strategy, the Data Engineer will define and innovate sustainable revenue models to drive profitability of the Company.
Collaboration: The Data Engineer will ensure that AI capabilities form a cohesive offering, including by working closely with other business divisions to learn their needs, internalize their knowledge, and define solutions to achieve the business objectives of CINQCARE.
Knowledge: The Data Engineer will provide subject matter expertise in the AI solutions, including determining and recommended approaches for designing and building elegant data structures in support of existing reporting tools and custom visualization platforms.
Culture: The Data Engineer is accountable for creating a productive, collaborative, safe and inclusive work environment for their team and as part of the larger Company.
Qualifications
The Data Engineer should have the following qualifications:
Education: Bachelor’s degree in Computer Science, Engineering, Software Engineering, or related field; Master’s degree preferred.
Experience: The ideal candidate should have at least 3+ years of experience in healthcare data engineering. Experience with a variety of data projects and environments, whether on-prem or in-cloud (5+ years in SQL Server, ETL Tools, Business Intelligence & Analysis, Architecture). Familiarity with the Microsoft Stack; experience with other platforms is a plus. Mastery of Python and SQL. Strong foundational knowledge of data lakes and AWS products such as AWS Glue. Experience with healthcare eligibility and claims, implementing APIs, HL7/FHIR standards, ETL scheduling solutions, SQL, and healthcare data security.
Entrepreneurial: CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations. This position is accountable for ensuring CINQCARE is positioned to innovatively deliver on its promise.
Communication: Strong analytical and collaboration skills is required. Excellent verbal, written communication and presentation skills; ability to clearly articulate and present concepts and models in an accessible manner to CINQCARE’s team, investors, partners, and other stakeholders.
Relationships: Ability to build and effectively manage relationships with business leaders and external constituents.
Culture. Good judgement, impeccable ethics, and a strong team player; desire to succeed and grow in a fast-paced, demanding, and entrepreneurial Company.

Location: New York, NY
Compensation: $100,000-$120,000
My3ehHtP4z",110000,201 to 500 Employees,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql']",['aws'],[],['sql server'],[],bachelor,+10 years
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85907,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
Grid,4.2,"Oakland, CA",Data Engineer,"Today’s financial system is built to favor those with money. Grid’s mission is to level that playing field by building financial products that help users better manage their financial future. The Grid app lets users access cash, build credit, spend money, file their taxes and lots, lots more.

Grid is a fast growing team that’s deeply passionate about making a difference in the lives of millions. We’re solving huge problems and believe that a merit driven culture allows every team member to play a big role. Join our growing team in our Bay Area headquarters or additional offices across the US.

Our Data Engineers are responsible for converting the data surrounding a customer's financial life into tools that make our customers smarter and better equipped financially for the future. We value making intelligent decisions backed by good data and tools. We're looking for people who share our values, particularly if you have experience analyzing, processing, and learning from large data sets.

We’re looking for a seasoned data engineer who can help us lay the foundation of an exceptional data engineering practice. The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions. You will be writing code, setting style guides and collaborating cross-functionally with product, engineering and leadership.
Problems we work on:
Analytics: Collect all the data for a user into tools that help our customers
Machine Learning: Using a variety of techniques to reach better insights
Data Processing: Managing data & statistics using scalable and efficient technologies
Visualization: Envision our data as beautiful graphs and tools that allow customers to explore their data & ask their own questions
Risk: Analyze data for anomalous patterns and build tools that allow us to find bad actors quickly
We Practice:
Open collaboration
Code reviews
Testing
Agile development
We Use:
Go
Python
MySQL
Google Cloud Platform
Kubernetes
Kubeflow
Docker
Google Pubsub
BigQuery
Firebase
We’re looking for Engineers to:
Design and implement platform services, frameworks and ecosystems
Build a scalable, reliable, operable and performant big data workflow platform for data scientists/engineers, AI/ML engineers, and product/operation team members
Drive efficiency and reliability improvements through design and automation: performance, scaling, observability, and monitoring
Requirements:
Strong programming skills with Python
Strong programming skills with a typed programming language, such as Java, Scala, Go, etc.
Disciplined approach to development, testing, and quality assurance
Excellent communication skills, capable of explaining highly technical problems in English
Understand data processing and ETL, hands on experience building pipelines and using frameworks such as Hive hdfs, Presto, Spark, etc.
Really strong candidates may have:
Really strong candidates may have:
Actively contribute to open source software
Worked with a strong, lean-based development environment
Previous work experience in a start-up environment
Ability to recognize the right tool for the right situation/problem.
Will have strong programming skill in Go",112869,,,,,-1,,CA,-1,data engineer,na,"['python', 'scala', 'java', 'go']","['bigquery', 'google cloud']",[],"['mysql', 'hive']",['spark'],,
Horizon Health Alliance,4.2,"Getzville, NY",Data Engineer,"Are you seeking a rewarding and fulfilling career in the Mental Health and Addictions field?
Apply to be a Data Engineer today!
What will your day look like?
At Horizon, you will enjoy a supportive, team-based work environment. Have a question? There is always someone there to help! We offer a seamless onboarding experience that will ensure your success in your new role.
As a Data Engineer at Horizon you will...
Acquire and assemble datasets that align with business needs
Identifying, designing, and implementing internal process improvements including optimizing data delivery, and automating manual processes
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency
Working with stakeholders including executive teams and assisting them with data-related technical issues
Working with stakeholders to support their data infrastructure needs and understand company objectives
Build, test, and maintain database pipeline architectures
Create new data validation methods and data analysis tools
Assist and support development of data governance policies
Create and distribute corporate reports using industry standard reporting tools and software such as SQL, Power BI, and Python.
Develop and maintain the workflows and security controls related to data extract, transform, and load (ETL) of corporate data
Providing technical expertise in data storage structures, data mining, and data cleansing.
Develop and maintain databases, data marts, models, data sets, and other key technical solutions
Why choose Horizon to build your career?
Besides the fact that we’ve been named a Best Place to Work for 14 (yes, 14!) years in a row? At Horizon, you can be assured that you will make difference in the lives of others. Even better, your teammates will be just as motivated to make a difference!
What we offer that you’ll love…
Company Culture: At Horizon, we pride ourselves on cultivating an atmosphere of teamwork where all employees feel heard and valued.
Diversity & Inclusion: We are committed to equity, racial justice, and equal opportunity for all, and strive toward this goal through the work of our Diversity, Equity, Inclusion and Belonging Council, frequent trainings, ongoing conversations, affinity groups, and more.
Trainings, Trainings, and More Trainings: We have an entire team dedicated to your personal development and professional growth.
Team Building, Connection, and Relationships: At Horizon, we’re more than co-workers, we’re a community. We support each other, celebrate our achievements and milestones together, and have fun together!
Retirement: We know you want to retire comfortably and we’re here to help! Horizon offers 401(k) AND profit-sharing programs to make sure you’re set for the future.
Student Loan Assistance: We help pay off our team members' student loans every month. One year after joining, you’ll have been able to pay off an extra $600!
PTO & Holidays: We believe self-care is essential. Combined with holidays, you’ll earn up to 22.5 paid days in your first year. By your 3rd anniversary, that's almost doubled with up to 37.5 paid days off!
What makes you a great candidate?
We can’t wait to learn more about you! Here are a few specifics of what you’ll need for the job:
Bachelor's degree in Computer Science, Information Systems, Engineering or equivalent
1+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL and reporting/analytic tools
Strong experience in coding languages like SQL and Python
Fluent in relational based systems and writing complex SQL
Strong analytical and problem-solving skills
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Thorough knowledge of Microsoft Office
Ability to work with multidisciplinary teams
Ability to multitask and manage competing deadlines
Ability to communicate appropriately with all levels of management
Excellent understanding of Microsoft Office suite
Ability to build and optimize data sets
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytic skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Location:
55 Dodge Road, Getzville, NY 14068
Hours:
Full-time position, Monday - Friday; 8:00am-5:00pm
Disclaimer:
Horizon endorses public health measures including vaccinations. We encourage all applicants to be mindful of the fact that Horizon is a healthcare agency proving in person services throughout our community.
This information is intended to provide a general overview of the position; it is not a full job description.

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)",84681,1 to 50 Employees,Nonprofit Organization,,,-1,Unknown / Non-Applicable,NY,-1,data engineer,na,"['python', 'sql']",[],['power bi'],[],[],bachelor,+10 years
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['python', 'sql']","['gcp', 'aws', 'redshift']",[],[],['airflow'],,
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74801,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Plaxonic Technologies,4.6,"Dallas, TX",Data Engineer,"Job Description – Sr. Data Engineer:
Minimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark
Ability to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark
Hands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies
Strong skill in understanding database architecture, data models and writing complex SQL queries/code
Hands on experience with integration of different data sources (Files, DBs, APIs)
Ability to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation
Knowledge of various ETL techniques
Experience with data pipeline and workflow management tools [Azure DevOps]
Strong analytic skill to work with unstructured data
Experience working with Agile teams scrums.
Strong customer handling skills and communication skills
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75287: Reliably commute or planning to relocate before starting work (Required)
Experience:
Microsoft Excel: 1 year (Preferred)
Microsoft Office: 1 year (Preferred)
Compliance management: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: One location
Speak with the employer
+91 7272167989",98641,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),TX,10,data engineer,na,['sql'],"['azure', 'databricks']",['excel'],[],[],,0-2 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
Angle Health,4.0,Remote,Data Engineer,"Changing Healthcare For Good
At Angle Health, we believe the healthcare system should be accessible, transparent, and easy to navigate. As a digital-first, data-driven health plan, we are replacing legacy systems with modern infrastructure to deliver our members the care they need when they need it. If you want to build the future of healthcare, we'd love for you to join us.

The Role
As a Data Engineer on the Strategy team at Angle Health, you will be part of an elite team of problem solvers tackling some of the hardest operational challenges across the business. You will be working closely with Technical Product Managers and other engineers in small teams embedded within functions across the company—from sales to operations to finance and more.

In partnership with operational stakeholders, your job will be to quickly understand business workflows, and design and implement technical, data-driven solutions to achieve the intended business outcomes. That may include architecting backend micro-services, building and maintaining robust data pipelines, standing up health metrics and automated alerts, and developing scalable technical solutions to support Angle Health's day-to-day operations.

Every day will be a new learning experience in this role and may span discussing architecture with fellow engineers, wrangling large-scale data, coding a custom web app or micro-service, or speaking with customers and executives.

This role is ideal for entrepreneurial engineers, applied data scientists, and creative, intellectually curious thinkers that want to solve high-value problems in healthcare.

This position may be based in San Francisco, New York City, Salt Lake City, or Remote.

We are currently trialing various titles for the same role. Please consider the following posted roles as the same position: Deployed Engineer, Software Engineer, Business Operations, and Data Engineer. Please note these are the same, so if you have already applied for one position, there is no need to reapply for the others.
What We Value
A strong engineering background in computer science, software engineering, data science, mathematics, or similar technical field is required for this role
Proficiency in programming languages (e.g. Python, SQL, Java, TypeScript/JavaScript, or similar) and data engineering frameworks
A highly analytical mindset and an eagerness to build technical solutions to complex business problems
High attention to detail and intellectual curiosity—you're not satisfied with surface-level answers. You want to dive into the data, the ""how,"" and the ""why"" because ""the way it's always been done"" is not always the way it should be done
Low ego—the outcome matters more than who gets the credit
Demonstrated ability to collaborate effectively in teams of technical and non-technical individuals
Highly organized with an ability to multitask, problem solve, and balance competing priorities in a rapidly changing environment
Because We Value You:

Competitive compensation and stock options
100% company paid comprehensive health, vision & dental insurance for you and your dependents
Supplemental Life, AD&D and Short Term Disability coverage options
Discretionary time off
Opportunity for rapid career progression
Relocation assistance (if relocation is required)
3 months of paid parental leave and flexible return to work policy (after 10 months of employment)
Work-from-home stipend for remote employees
Company provided lunch for in-office employees
401(k) account
Other benefits coming soon!

Backed by a team of world class investors, we are a healthcare startup on a mission to make our health system more effective, accessible, and affordable to everyone. From running large hospitals and health plans to serving on federal healthcare advisory boards to solving the world's hardest problems at Palantir, our team has done it all. As part of this core group at Angle Health, you will have the right balance of support and autonomy to grow both personally and professionally and the opportunity to own large parts of the business and scale with the company.

Angle Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. Angle Health is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities.",106385,1 to 50 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2019,Unknown / Non-Applicable,Remote,4,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Edrstaffing,4.2,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],,2-5 years
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
Agiles Enterprise,3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road",136537,1 to 50 Employees,Company - Private,,,-1,Less than $1 million (USD),VA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,
Nursa,4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!",89545,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable,UT,5,data engineer,na,"['python', 'sql']","['aws', 'redshift']",[],[],[],,+10 years
Airbus Americas,4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.",92046,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD),GA,53,data engineer,na,"['python', 'r', 'java']",[],[],[],['spark'],bachelor,2-5 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
ITEOM,4.2,Remote,Data Engineer - Remote,"Our client is seeking a Data Engineer who would be responsible for design of data warehouse schemas as well as end-to-end design and implementation of fault tolerant and scalable data processing pipelines using a variety of technologies for orchestrating data movement, primarily Snowflake , DBT and Airflow . You'll work with a team of data engineers to design and implement solutions that provide business-critical insights.
Salary : Mid-Level - 130K to160K+ bonuses and equity options
Location: REMOTE WORK 100% is an option: (locations limited to CO, AZ, TX, VA, CA, GA, HI, MN, MI, NJ, NY, NC, OR, PA, WA)
How You’ll Do It
Working collaboratively with a team of other data engineers and developers of varying experience levels and areas of expertise, you'll be responsible for:
Responsibilities
Design and implement data warehouse schemas to store large data sets that can be easily and efficiently queried and joined together for reporting and analytical purposes.
Design and develop ETL/ELT pipelines to efficiently move and aggregate data, while optimizing for maximum code and data reuse.
Collaborate with other team members on improvements to existing systems
Investigate data anomalies and provide quick resolutions.
Provide technical support to business users and analysts.
What We’re Looking For
TOP SKILLS: ETL, Data Warehousing, SQL, Jinga Scripting, Python and Snowflake.
3+ years of working experience with large scale data warehouse systems.
Very strong knowledge of SQL and data manipulation best practices
Experience in building efficient and fault tolerant ELT/ETL data pipelines
Very strong knowledge of working with large scale datasets and data modeling and data warehouse design
Experience with Snowflake is preferable
Knowledge of DBT, Jinja scripting and Airflow is a big plus
Nice to Have
Experience working cross functionally with product and engineering teams
Desire to wear many hats and work your tail off for a bit - great earning potential and equity available.
Benefits: A diverse benefit package, sign on bonus, options, etc. is offered through our client.
About ITEOM: ITEOM Digital Technology Talent Partners collaborates with top technologists and top employers to align purpose and passion and re-incorporate belonging to make the job search and work experience more human. We are an equal opportunity employer; however, this role is not eligible for sponsorship at this time. Employees eligible for full time W2 employment are encouraged to apply.",145000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']",['snowflake'],[],[],['airflow'],,+10 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
Edrstaffing,4.2,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],,2-5 years
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74801,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Sanametrix,3.8,Remote,AWS Data Engineer,"Candidates Must be able to obtain/maintain a Public Trust (US Citizenship is required). This position is full-time w/benefits and 100% remote!
Sanametrix, Inc. is a fast-growing small business headquartered in Arlington, VA. We are dedicated to providing federal agencies with legendary customer service and focused solutions for their business and technology needs. This role is responsible for building data pipelines for transferring data from source systems (virtual machines, Microsoft SQL Server) into AWS Cloud using AWS Native Tools. This resource has strong data modeling and scripting experience and has a strong knowledge of AWS Data Services.
Responsibilities:
Perform data processing, algorithm / structures, pipeline orchestration, data quality, governance, discovery.
Work with structured and unstructured data, blob data
Develop and work with APIs
Collect and organize data using data warehousing technique and file storage technologies
Perform ELT and ETL processes
Gather data requirements
Develop and maintain scalable data pipelines and build out new API integrations to support continuing increases in data volume and complexity.
Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the organization.
Implement processes and systems to monitor data quality, to ensure production data accuracy, and ensure key stakeholder and business process access.
Write unit/integration tests, contribute to engineering wiki, and documents.
Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Work closely with a team of front-end and back-end engineers, product managers, and analysts.
Design data integrations and dataquality framework based on established requirements.
Must be able to obtain a Public Trust (US Citizenship required to obtain Public Trust)
Qualifications & Skills:
Scripting
SQL & Scripting
Python
Spark
Linux / shell scripting
Services / Tools (six or more)
S3 Lambda
Redshift
Lake Formation
Glue ETL
Kinesis
DMS
Glue catalog/Crawlers
Git
Jira
Airflow /Orchestration
Education, Experience, and Licensing Requirements:
BS or MS degree in Computer Science or a related technical field
4+ years of Python or Java development experience
4+ years of SQL or NoSQL experience
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining data processing systems
AWS Certified is preferred
Job Type: Full-time
Pay: $110,000.00 - $120,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Monday to Friday
Work Location: Remote",115000,Unknown,Company - Private,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['sql', 'shell', 'python', 'java', 'nosql']","['aws', 'redshift']",[],['sql server'],"['airflow', 'spark']",,5-10 years
Cloudbc Labs,3.9,Remote,Anaplan Data Engineer,"Job Title :: Anaplan Data Engineer
Location :: Remote
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Remote",117000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,Unknown / Non-Applicable,Remote,8,data engineer,na,[],[],[],[],[],,
"Fiserv, Inc.",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.",108511,10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD),NJ,39,data engineer,na,"['python', 'sql', 'java', 'nosql']","['azure', 'google cloud', 'aws']",[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,+10 years
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85907,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
Ascendion,4.5,Remote,Senior Data Engineer,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote",117000,1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable,Remote,1,data engineer,senior,"['scala', 'sql', 'python', 'java', 'nosql']","['azure', 'databricks']",['ssis'],['mongodb'],[],,5-10 years
Infinity Quest,4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote",105190,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable,NJ,17,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,0-2 years
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['python', 'sql']",[],[],[],[],master,
Ascent Solutions,4.2,Remote,Hadoop Big Data Engineer,"Hadoop Big Data Engineer
Remote
Must Haves • Strong experience in Big Data, Cloudera Distribution 7.x, Cloud migration, RDBMS • Strong Experience with Amazon EMR/Databricks/Cloudera CDP • 4-5 experience building data pipelines using Hadoop components Sqoop, Hive, Solr, MR, Impala, Spark, Spark SQL., HBase. • 4-5 years of programming experience in Python, Java and Scala is must. • Strong experience with REST API development using Python frameworks (Django, Flask etc.) • Micro Services/Web service development experience using Spring framework is highly desirable.
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,na,"['python', 'scala', 'sql', 'java']",['databricks'],[],['hive'],"['hadoop', 'spark']",,5-10 years
Globaleur,4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes.
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders.
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information.
Deploy sophisticated analytics programs, machine learning, and statistical methods.
Ensure compliance with data governance and security policies.
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition.
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences.
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields.
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing.
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java.
Have working experiences in e-commence, travel, marketing domain is a plus.
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies.
Excellent problem solving and troubleshooting skills.
Process oriented with great documentation skills.
Excellent oral and written communication skills with a keen sense of customer service.


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.",119183,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017,Unknown / Non-Applicable,CA,6,data engineer,senior,"['sql', 'go', 'python', 'java', 'nosql']","['azure', 'aws']",['tableau'],['mongodb'],[],bachelor,
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
Edrstaffing,4.2,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],,2-5 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
Ascent Solutions,4.2,Remote,Hadoop Big Data Engineer,"Hadoop Big Data Engineer
Remote
Must Haves • Strong experience in Big Data, Cloudera Distribution 7.x, Cloud migration, RDBMS • Strong Experience with Amazon EMR/Databricks/Cloudera CDP • 4-5 experience building data pipelines using Hadoop components Sqoop, Hive, Solr, MR, Impala, Spark, Spark SQL., HBase. • 4-5 years of programming experience in Python, Java and Scala is must. • Strong experience with REST API development using Python frameworks (Django, Flask etc.) • Micro Services/Web service development experience using Spring framework is highly desirable.
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,na,"['python', 'scala', 'sql', 'java']",['databricks'],[],['hive'],"['hadoop', 'spark']",,5-10 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
Infinity Quest,4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote",105190,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable,NJ,17,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,0-2 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74801,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
ITEOM,4.2,Remote,Data Engineer - Remote,"Our client is seeking a Data Engineer who would be responsible for design of data warehouse schemas as well as end-to-end design and implementation of fault tolerant and scalable data processing pipelines using a variety of technologies for orchestrating data movement, primarily Snowflake , DBT and Airflow . You'll work with a team of data engineers to design and implement solutions that provide business-critical insights.
Salary : Mid-Level - 130K to160K+ bonuses and equity options
Location: REMOTE WORK 100% is an option: (locations limited to CO, AZ, TX, VA, CA, GA, HI, MN, MI, NJ, NY, NC, OR, PA, WA)
How You’ll Do It
Working collaboratively with a team of other data engineers and developers of varying experience levels and areas of expertise, you'll be responsible for:
Responsibilities
Design and implement data warehouse schemas to store large data sets that can be easily and efficiently queried and joined together for reporting and analytical purposes.
Design and develop ETL/ELT pipelines to efficiently move and aggregate data, while optimizing for maximum code and data reuse.
Collaborate with other team members on improvements to existing systems
Investigate data anomalies and provide quick resolutions.
Provide technical support to business users and analysts.
What We’re Looking For
TOP SKILLS: ETL, Data Warehousing, SQL, Jinga Scripting, Python and Snowflake.
3+ years of working experience with large scale data warehouse systems.
Very strong knowledge of SQL and data manipulation best practices
Experience in building efficient and fault tolerant ELT/ETL data pipelines
Very strong knowledge of working with large scale datasets and data modeling and data warehouse design
Experience with Snowflake is preferable
Knowledge of DBT, Jinja scripting and Airflow is a big plus
Nice to Have
Experience working cross functionally with product and engineering teams
Desire to wear many hats and work your tail off for a bit - great earning potential and equity available.
Benefits: A diverse benefit package, sign on bonus, options, etc. is offered through our client.
About ITEOM: ITEOM Digital Technology Talent Partners collaborates with top technologists and top employers to align purpose and passion and re-incorporate belonging to make the job search and work experience more human. We are an equal opportunity employer; however, this role is not eligible for sponsorship at this time. Employees eligible for full time W2 employment are encouraged to apply.",145000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']",['snowflake'],[],[],['airflow'],,+10 years
"ESRI, Inc.",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1",98800,1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD),CA,54,data engineer,na,"['python', 'sql']",[],[],[],[],master,
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
Cloudbc Labs,3.9,Remote,Anaplan Data Engineer,"Job Title :: Anaplan Data Engineer
Location :: Remote
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Remote",117000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,Unknown / Non-Applicable,Remote,8,data engineer,na,[],[],[],[],[],,
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
Invictus Data,4.2,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location",125500,,,,,-1,,CA,-1,data engineer,senior,"['python', 'scala', 'sql']","['azure', 'aws', 'gcp']",[],['hive'],"['kafka', 'hadoop', 'spark']",,2-5 years
The Sunwater Institute,4.2,"Rockville, MD",Data Engineer,"The Sunwater Institute is a nonprofit, nonpartisan public policy organization focused on improving the foundations of our social systems. The Sunwater Institute’s current programs focus on improving congressional performance, preserving and advancing constitutional rights and liberties, strengthening and enlarging property rights, and exploring the foundations of group decision-making.
The Sunwater Institute’s interdisciplinary, long-term orientation and focus on the building blocks of social systems sets it apart as a haven for bold, fresh ideas and free, constructive dialogue. The Sunwater Institute is located in North Bethesda and maintains a presence on Capitol Hill.
POSITION
The Sunwater Institute is seeking a Data Engineer who has a background in engineering or computer science and is highly skilled in SQL and database services technologies. The ideal candidate will possess strong problem-solving skills, excellent communication, and a great technical aptitude. The individual in this position will provide project support and help drive team initiatives. The candidate will be creative, and comfortable working in small teams and coordinating with US and non-US-based developers.
Responsibilities
Develop data models to store and manage large amounts of structured and unstructured data
Design efficient ETL processes to move and integrate data from different sources into the enterprise warehouse
Write complex queries using SQL Server etc. to analyze large datasets
Identify trends, patterns, and correlations within datasets by performing exploratory analysis
Ensure high quality of data by implementing best practices on storage, maintenance, indexing techniques, etc
Coordinate project status updates between teams and communicate deliverables and timelines
REQUIREMENTS
Bachelor’s degree in Computer Science or Engineering related field
4+ years of relevant experience in a similar role
Strong knowledge of relational databases including SQL Server
Proficient skills in writing complex queries using SQL language; developing stored procs, views, etc.
Experience with end-to-end ETL (Extract Transform Load) Processes
Familiarity with Big Data platforms such as AWS etc.
Ability to develop effective reports that help drive business decisions
Must be able to work effectively with teams but be able to work independently on tasks to meet tight deadlines
Exceptional communication skills verbal, written, and active listening
Extreme attention to detail and accuracy
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Vision insurance
Compensation package:
Yearly bonus
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Rockville, MD 20852: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",106385,,,,,-1,,MD,-1,data engineer,na,['sql'],['aws'],[],['sql server'],[],bachelor,2-5 years
PCS Global Tech,4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road",70000,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,Unknown / Non-Applicable,CA,-1,data engineer,na,"['python', 'sql', 'java']",[],[],[],[],,0-2 years
LOVEFOODIES INC,4.2,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",54000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['python', 'sql']",[],['tableau'],[],[],,2-5 years
Horizon Health Alliance,4.2,"Getzville, NY",Data Engineer,"Are you seeking a rewarding and fulfilling career in the Mental Health and Addictions field?
Apply to be a Data Engineer today!
What will your day look like?
At Horizon, you will enjoy a supportive, team-based work environment. Have a question? There is always someone there to help! We offer a seamless onboarding experience that will ensure your success in your new role.
As a Data Engineer at Horizon you will...
Acquire and assemble datasets that align with business needs
Identifying, designing, and implementing internal process improvements including optimizing data delivery, and automating manual processes
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency
Working with stakeholders including executive teams and assisting them with data-related technical issues
Working with stakeholders to support their data infrastructure needs and understand company objectives
Build, test, and maintain database pipeline architectures
Create new data validation methods and data analysis tools
Assist and support development of data governance policies
Create and distribute corporate reports using industry standard reporting tools and software such as SQL, Power BI, and Python.
Develop and maintain the workflows and security controls related to data extract, transform, and load (ETL) of corporate data
Providing technical expertise in data storage structures, data mining, and data cleansing.
Develop and maintain databases, data marts, models, data sets, and other key technical solutions
Why choose Horizon to build your career?
Besides the fact that we’ve been named a Best Place to Work for 14 (yes, 14!) years in a row? At Horizon, you can be assured that you will make difference in the lives of others. Even better, your teammates will be just as motivated to make a difference!
What we offer that you’ll love…
Company Culture: At Horizon, we pride ourselves on cultivating an atmosphere of teamwork where all employees feel heard and valued.
Diversity & Inclusion: We are committed to equity, racial justice, and equal opportunity for all, and strive toward this goal through the work of our Diversity, Equity, Inclusion and Belonging Council, frequent trainings, ongoing conversations, affinity groups, and more.
Trainings, Trainings, and More Trainings: We have an entire team dedicated to your personal development and professional growth.
Team Building, Connection, and Relationships: At Horizon, we’re more than co-workers, we’re a community. We support each other, celebrate our achievements and milestones together, and have fun together!
Retirement: We know you want to retire comfortably and we’re here to help! Horizon offers 401(k) AND profit-sharing programs to make sure you’re set for the future.
Student Loan Assistance: We help pay off our team members' student loans every month. One year after joining, you’ll have been able to pay off an extra $600!
PTO & Holidays: We believe self-care is essential. Combined with holidays, you’ll earn up to 22.5 paid days in your first year. By your 3rd anniversary, that's almost doubled with up to 37.5 paid days off!
What makes you a great candidate?
We can’t wait to learn more about you! Here are a few specifics of what you’ll need for the job:
Bachelor's degree in Computer Science, Information Systems, Engineering or equivalent
1+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL and reporting/analytic tools
Strong experience in coding languages like SQL and Python
Fluent in relational based systems and writing complex SQL
Strong analytical and problem-solving skills
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Thorough knowledge of Microsoft Office
Ability to work with multidisciplinary teams
Ability to multitask and manage competing deadlines
Ability to communicate appropriately with all levels of management
Excellent understanding of Microsoft Office suite
Ability to build and optimize data sets
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytic skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Location:
55 Dodge Road, Getzville, NY 14068
Hours:
Full-time position, Monday - Friday; 8:00am-5:00pm
Disclaimer:
Horizon endorses public health measures including vaccinations. We encourage all applicants to be mindful of the fact that Horizon is a healthcare agency proving in person services throughout our community.
This information is intended to provide a general overview of the position; it is not a full job description.

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)",84681,1 to 50 Employees,Nonprofit Organization,,,-1,Unknown / Non-Applicable,NY,-1,data engineer,na,"['python', 'sql']",[],['power bi'],[],[],bachelor,+10 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74801,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
The Sunwater Institute,4.2,"Rockville, MD",Data Engineer,"The Sunwater Institute is a nonprofit, nonpartisan public policy organization focused on improving the foundations of our social systems. The Sunwater Institute’s current programs focus on improving congressional performance, preserving and advancing constitutional rights and liberties, strengthening and enlarging property rights, and exploring the foundations of group decision-making.
The Sunwater Institute’s interdisciplinary, long-term orientation and focus on the building blocks of social systems sets it apart as a haven for bold, fresh ideas and free, constructive dialogue. The Sunwater Institute is located in North Bethesda and maintains a presence on Capitol Hill.
POSITION
The Sunwater Institute is seeking a Data Engineer who has a background in engineering or computer science and is highly skilled in SQL and database services technologies. The ideal candidate will possess strong problem-solving skills, excellent communication, and a great technical aptitude. The individual in this position will provide project support and help drive team initiatives. The candidate will be creative, and comfortable working in small teams and coordinating with US and non-US-based developers.
Responsibilities
Develop data models to store and manage large amounts of structured and unstructured data
Design efficient ETL processes to move and integrate data from different sources into the enterprise warehouse
Write complex queries using SQL Server etc. to analyze large datasets
Identify trends, patterns, and correlations within datasets by performing exploratory analysis
Ensure high quality of data by implementing best practices on storage, maintenance, indexing techniques, etc
Coordinate project status updates between teams and communicate deliverables and timelines
REQUIREMENTS
Bachelor’s degree in Computer Science or Engineering related field
4+ years of relevant experience in a similar role
Strong knowledge of relational databases including SQL Server
Proficient skills in writing complex queries using SQL language; developing stored procs, views, etc.
Experience with end-to-end ETL (Extract Transform Load) Processes
Familiarity with Big Data platforms such as AWS etc.
Ability to develop effective reports that help drive business decisions
Must be able to work effectively with teams but be able to work independently on tasks to meet tight deadlines
Exceptional communication skills verbal, written, and active listening
Extreme attention to detail and accuracy
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Vision insurance
Compensation package:
Yearly bonus
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Rockville, MD 20852: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",106385,,,,,-1,,MD,-1,data engineer,na,['sql'],['aws'],[],['sql server'],[],bachelor,2-5 years
Data Crunch Corp,4.2,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",108000,,,,,-1,,Remote,-1,data engineer,senior,"['python', 'sql']",['aws'],['tableau'],[],[],,
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['python', 'sql']","['gcp', 'aws', 'redshift']",[],[],['airflow'],,
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85907,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
Govini,3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.",88194,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable,PA,12,data engineer,na,['sql'],[],[],[],[],master,5-10 years
Grid,4.2,"Oakland, CA",Data Engineer,"Today’s financial system is built to favor those with money. Grid’s mission is to level that playing field by building financial products that help users better manage their financial future. The Grid app lets users access cash, build credit, spend money, file their taxes and lots, lots more.

Grid is a fast growing team that’s deeply passionate about making a difference in the lives of millions. We’re solving huge problems and believe that a merit driven culture allows every team member to play a big role. Join our growing team in our Bay Area headquarters or additional offices across the US.

Our Data Engineers are responsible for converting the data surrounding a customer's financial life into tools that make our customers smarter and better equipped financially for the future. We value making intelligent decisions backed by good data and tools. We're looking for people who share our values, particularly if you have experience analyzing, processing, and learning from large data sets.

We’re looking for a seasoned data engineer who can help us lay the foundation of an exceptional data engineering practice. The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions. You will be writing code, setting style guides and collaborating cross-functionally with product, engineering and leadership.
Problems we work on:
Analytics: Collect all the data for a user into tools that help our customers
Machine Learning: Using a variety of techniques to reach better insights
Data Processing: Managing data & statistics using scalable and efficient technologies
Visualization: Envision our data as beautiful graphs and tools that allow customers to explore their data & ask their own questions
Risk: Analyze data for anomalous patterns and build tools that allow us to find bad actors quickly
We Practice:
Open collaboration
Code reviews
Testing
Agile development
We Use:
Go
Python
MySQL
Google Cloud Platform
Kubernetes
Kubeflow
Docker
Google Pubsub
BigQuery
Firebase
We’re looking for Engineers to:
Design and implement platform services, frameworks and ecosystems
Build a scalable, reliable, operable and performant big data workflow platform for data scientists/engineers, AI/ML engineers, and product/operation team members
Drive efficiency and reliability improvements through design and automation: performance, scaling, observability, and monitoring
Requirements:
Strong programming skills with Python
Strong programming skills with a typed programming language, such as Java, Scala, Go, etc.
Disciplined approach to development, testing, and quality assurance
Excellent communication skills, capable of explaining highly technical problems in English
Understand data processing and ETL, hands on experience building pipelines and using frameworks such as Hive hdfs, Presto, Spark, etc.
Really strong candidates may have:
Really strong candidates may have:
Actively contribute to open source software
Worked with a strong, lean-based development environment
Previous work experience in a start-up environment
Ability to recognize the right tool for the right situation/problem.
Will have strong programming skill in Go",112869,,,,,-1,,CA,-1,data engineer,na,"['python', 'scala', 'java', 'go']","['bigquery', 'google cloud']",[],"['mysql', 'hive']",['spark'],,
Ascent Technologies,4.2,Remote,Data Engineer,"Job Description: Involved in resource / volume ramp-up plan/ backfilling resources planned for rotation and resignation cases Tracking Performance of individual Projects within the delivery/SIPs. Participate / presenting SD performance during client visit Publish Daily, Weekly and Monthly reports on team productivity Reviewing the weekly / monthly dashboards Publishing dashboards to customer People Management Drive the team to reach the defined targets Ensure training and development of the team members Mentor team members on technical and communication aspects. onsite anni data enginneer-pheonix,az python,charlottee,nc ba-houston tx
Job Types: Full-time, Contract, Permanent
Salary: $39.87 - $86.24 per hour
Benefits:
Dental insurance
Compensation package:
1099 contract
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91 7842516676",113490,,,,,-1,,Remote,-1,data engineer,na,"['python', 'sql']",[],[],[],[],,0-2 years
Savvy Technology Solutions,4.2,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location",107339,,,,,-1,,DC,-1,data engineer,senior,"['sql', 'nosql']","['aws', 'redshift']","['tableau', 'ssis']",['mysql'],[],master,2-5 years
plaxonic,4.6,"Hunt Valley, MD",AWS python data engineer,"AWS Python Data Engineer
Must Have: AWS Databricks Python, Spark, PySpark
Location Hartford, CT or St. Paul, MN or Hunt Valley, MD
Roles & responsibilities:
Acts as a single point of contact for data migration to AWS projects for customer
Provides innovative and cost-effective solution using AWS, Spark, python & customer suggested toolset
Optimizes the use of all available resources
Develops solutions to meet business needs that reflect a clear understanding of the objectives, practices and procedures of the corporation, department and business unit
As a leader in the Cloud Engineering you will be responsible for the overseeing development
Learn/adapt quickly to new Technologies as per the business need
Develop a team of Operations Excellence, building tools and capabilities that the Development teams leverage to maintain high levels of performance, scalability, security and availability
Skills:
The Candidate must have 3-5 yrs of experience in PySpark & Python
Hands on experience on AWS Cloud platform especially S3, lamda, EC2, EMR
Experience on spark scripting
Has working knowledge on migrating relational and dimensional databases on AWS Cloud platform
Relevant experience with ETL methods and with retrieving data from dimensional data models and data warehouses.
Strong experience with relational databases and data access methods, especially SQL.
Knowledge of Amazon AWS architecture and design
Job Type: Full-time
Schedule:
8 hour shift
Experience:
Python: 3 years (Preferred)
Work Location: On the road",81908,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD),MD,10,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],,2-5 years
ITExpertUS,2.8,"Chicago, IL",Data Engineer,"Client: Agilisium
Role: Data Engineer
Location: Chicago, IL – Hybrid role – Need to be onsite once/twice in a month.
Type: Long term contract
Job Description
Title: Data Engineer
We are looking for a savvy Data Engineer to join our growing team. The hire will be responsible for expanding and optimizing our data and pipeline architecture and data flow and collection. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. In addition, the Data Engineer will collaborate and support software developers, implementation architecture, and data engineers on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. In addition, they must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
Responsibilities:
Because we work on a programmatic leading edge of many technologies, we need someone who is a creative problem solver, resourceful in getting things done, and can shift productively to or from working independently and collaboratively. This person would also take on the following responsibilities:
Process unstructured data into a form suitable for analysis.
Support the business with ad hoc data analysis and build reliable data pipelines.
Implementation of best practices and IT operations in mission-critical tighter SLA data
pipelines using Airflow
Query Engine Migration from Dremio to Redshift.
We leverage: Multiple AWS Data & Analytic Services(e.g.,Glue, Kinesis, S3) , SQL (e.g.,
PostgreSQL, Redshift, Athena); NoSQL (e.g., DocumentDB, MongoDB); Kafka, Docker, Spark(AWS EMR and DataBricks), Airflow, Dremio, Qubole, etc.
We use AWS extensively, so experience with AWS cloud and AWS Data & Analytics certification will help you hit the ground running.
Business Skills for Data Engineers:
Creative Problem-Solving: Approaching data organization challenges with a clear eye on what is important; employing the suitable approach/methods to make the maximum use of time and human resources.
Effective Collaboration: Carefully listening to management, data engineers, and architects to establish their needs.
Intellectual Curiosity: Exploring new territories and finding creative and pragmatic ways to solve data problems.
Industry Knowledge: Understanding and perspective of how industry functions and how data can be collected, analyzed, and utilized for value creation; maintaining flexibility in the face of big data developments.
Skills and Qualifications:
3 to 5 years of real-world Data Engineering experience.
Programming experience, ideally in Python and other data engineering languages like Scala
Programming knowledge to clean structure and semi-structure datasets.
Experience processing large amounts of structured and unstructured data. Streaming data experience is a plus.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Background in Linux
Build the infrastructure required for optimal extraction, transformation, and loading of data
from various data sources using SQL and other cloud ‘big data’ technologies like DataBricks, Snowflake, Dremio, and Qubole.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience creating a platform on which complex data pipelines are built using orchestration tools like Airflow, and Astronomer.
Experience with real-time sync between OLTP and OLAP using AWS technologies like real- time sync between AWS Aurora and AWS Redshift.
Job Type: Full-time
Salary: Up to $68.00 per hour
Schedule:
8 hour shift
Work Location: On the road
Speak with the employer
+91 6305048577",122400,501 to 1000 Employees,Company - Private,,,-1,Unknown / Non-Applicable,IL,-1,data engineer,na,"['python', 'scala', 'sql', 'nosql']","['snowflake', 'databricks', 'aws', 'redshift']",[],['mongodb'],"['kafka', 'airflow', 'spark']",,5-10 years
CINQCARE,4.0,"Washington, DC",Data Engineer,"Overview
The Data Engineer is a critical member of our growing data science team. In this role, you will have the chance to define and develop a core data asset which provides a representative and culturally aware view of the individuals and the communities that CINQCARE serves. You will work to evolve this asset over time using a product roadmap that includes identification and closure of gaps in existing data, introduction of new data sources, generation of proprietary data while quantifying and eliminating areas of structural biases. CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations and to so, we must also seek to fix gaps in data that ignore and marginalizes the Black and Brown communities that we serve.

An idea candidate for this role will embody CINQCARE’s core values, including, Trusted, Empathetic, Committed, Humble, Creative and Community-Minded. At CINQCARE, we don’t have patients or customers – we have Family Members.
Job Responsibilities
The Data Engineer will have the following responsibilities:
The Data Engineer will be responsible for the design, development and delivery of data pipelines and value-added data assets, leveraging a variety of data warehousing methodologies and disciplines to ingest the data from heterogeneous sources into cloud-based Data-Lake Environment in AWS.
Manage initiatives & projects of significant complexity and risk. Excellent business and communication skills to be able to work with business owners providing input to prioritized roadmaps, develop work estimates, and ensure successful delivery to support strategic planning and initiatives, improve organizational performance, and advance progress towards CINQCARE’s goals.
Assist in the overall architecture of the ETL Design and proactively provide inputs in designing, implementing, and automating the data pipelines.
Investigate and mine data to identify potential issues within the data pipelines, notify end-users and propose adequate solutions.
Ensure data quality and integrity within the data lake in the AWS environment with a focus on compliance to HIPAA and state level compliance requirements.
Oversee user permissions and configurations for adherence to documented access management standards and policies.
Independently (with minimal oversight) develop and maintain trusted advisor relationships with business, clinical, and operations leaders at the senior leadership level and with external partners, that include guidance for optimizing use of analytic capabilities and deliverables, and prioritization based on strategic vision.
Use coding/scripting pipelines and APIs to uncover and turn data into assets that are analysis-friendly using AWS services like Athena Data Catalogue, Quick Sight or any other big data tool on AWS.
Create a high-quality catalog of all pertinent data with the primary goal of establishing a single source of truth and significantly increasing productivity by reducing the time required for data search and discovery.
Crossing team boundaries, educate/advise on data projects, on how to combine and aggregate client data across platforms or technologies, and how to make the greatest use of data.
Lead consistent adherence to the Software Development Life Cycle framework and governance processes including, but not limited to leading planning sessions, collecting, and documenting requirements, identifying design patterns, create and define custom transformations, aggregations, and other data manipulations, developing data pipelines, creating documentation, developing test plans, performing unit testing, conducting peer review sessions.
Use knowledge of healthcare industry, market environment, and clinical and business workflows and activities, to inform solution design and development to execute high-quality or differentiated solutions in an established problem space.
Perform other job-related duties as assigned.

General Duties
The Data Engineer should have the following duties:
Leadership: The Data Engineer will lead the continued build out of the data asset to create business value, including collaborating with their team to design, develop, and execute those strategies and solutions to deliver desired outcomes.
Strategy: The Data Engineer will contribute to the business strategy and roadmap: (1) improve outcomes for CINQCARE Family Members; (2) enhance the efficacy of other CINQCARE. business divisions; and (3) develop and deliver external market opportunities for CINQCARE products and services. In establishing the business strategy, the Data Engineer will define and innovate sustainable revenue models to drive profitability of the Company.
Collaboration: The Data Engineer will ensure that AI capabilities form a cohesive offering, including by working closely with other business divisions to learn their needs, internalize their knowledge, and define solutions to achieve the business objectives of CINQCARE.
Knowledge: The Data Engineer will provide subject matter expertise in the AI solutions, including determining and recommended approaches for designing and building elegant data structures in support of existing reporting tools and custom visualization platforms.
Culture: The Data Engineer is accountable for creating a productive, collaborative, safe and inclusive work environment for their team and as part of the larger Company.
Qualifications
The Data Engineer should have the following qualifications:
Education: Bachelor’s degree in Computer Science, Engineering, Software Engineering, or related field; Master’s degree preferred.
Experience: The ideal candidate should have at least 3+ years of experience in healthcare data engineering. Experience with a variety of data projects and environments, whether on-prem or in-cloud (5+ years in SQL Server, ETL Tools, Business Intelligence & Analysis, Architecture). Familiarity with the Microsoft Stack; experience with other platforms is a plus. Mastery of Python and SQL. Strong foundational knowledge of data lakes and AWS products such as AWS Glue. Experience with healthcare eligibility and claims, implementing APIs, HL7/FHIR standards, ETL scheduling solutions, SQL, and healthcare data security.
Entrepreneurial: CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations. This position is accountable for ensuring CINQCARE is positioned to innovatively deliver on its promise.
Communication: Strong analytical and collaboration skills is required. Excellent verbal, written communication and presentation skills; ability to clearly articulate and present concepts and models in an accessible manner to CINQCARE’s team, investors, partners, and other stakeholders.
Relationships: Ability to build and effectively manage relationships with business leaders and external constituents.
Culture. Good judgement, impeccable ethics, and a strong team player; desire to succeed and grow in a fast-paced, demanding, and entrepreneurial Company.

Location: New York, NY
Compensation: $100,000-$120,000
My3ehHtP4z",110000,201 to 500 Employees,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql']",['aws'],[],['sql server'],[],bachelor,+10 years
Horizon Health Alliance,4.2,"Getzville, NY",Data Engineer,"Are you seeking a rewarding and fulfilling career in the Mental Health and Addictions field?
Apply to be a Data Engineer today!
What will your day look like?
At Horizon, you will enjoy a supportive, team-based work environment. Have a question? There is always someone there to help! We offer a seamless onboarding experience that will ensure your success in your new role.
As a Data Engineer at Horizon you will...
Acquire and assemble datasets that align with business needs
Identifying, designing, and implementing internal process improvements including optimizing data delivery, and automating manual processes
Building required infrastructure for optimal extraction, transformation and loading of data from various data sources
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency
Working with stakeholders including executive teams and assisting them with data-related technical issues
Working with stakeholders to support their data infrastructure needs and understand company objectives
Build, test, and maintain database pipeline architectures
Create new data validation methods and data analysis tools
Assist and support development of data governance policies
Create and distribute corporate reports using industry standard reporting tools and software such as SQL, Power BI, and Python.
Develop and maintain the workflows and security controls related to data extract, transform, and load (ETL) of corporate data
Providing technical expertise in data storage structures, data mining, and data cleansing.
Develop and maintain databases, data marts, models, data sets, and other key technical solutions
Why choose Horizon to build your career?
Besides the fact that we’ve been named a Best Place to Work for 14 (yes, 14!) years in a row? At Horizon, you can be assured that you will make difference in the lives of others. Even better, your teammates will be just as motivated to make a difference!
What we offer that you’ll love…
Company Culture: At Horizon, we pride ourselves on cultivating an atmosphere of teamwork where all employees feel heard and valued.
Diversity & Inclusion: We are committed to equity, racial justice, and equal opportunity for all, and strive toward this goal through the work of our Diversity, Equity, Inclusion and Belonging Council, frequent trainings, ongoing conversations, affinity groups, and more.
Trainings, Trainings, and More Trainings: We have an entire team dedicated to your personal development and professional growth.
Team Building, Connection, and Relationships: At Horizon, we’re more than co-workers, we’re a community. We support each other, celebrate our achievements and milestones together, and have fun together!
Retirement: We know you want to retire comfortably and we’re here to help! Horizon offers 401(k) AND profit-sharing programs to make sure you’re set for the future.
Student Loan Assistance: We help pay off our team members' student loans every month. One year after joining, you’ll have been able to pay off an extra $600!
PTO & Holidays: We believe self-care is essential. Combined with holidays, you’ll earn up to 22.5 paid days in your first year. By your 3rd anniversary, that's almost doubled with up to 37.5 paid days off!
What makes you a great candidate?
We can’t wait to learn more about you! Here are a few specifics of what you’ll need for the job:
Bachelor's degree in Computer Science, Information Systems, Engineering or equivalent
1+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL and reporting/analytic tools
Strong experience in coding languages like SQL and Python
Fluent in relational based systems and writing complex SQL
Strong analytical and problem-solving skills
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Thorough knowledge of Microsoft Office
Ability to work with multidisciplinary teams
Ability to multitask and manage competing deadlines
Ability to communicate appropriately with all levels of management
Excellent understanding of Microsoft Office suite
Ability to build and optimize data sets
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytic skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Location:
55 Dodge Road, Getzville, NY 14068
Hours:
Full-time position, Monday - Friday; 8:00am-5:00pm
Disclaimer:
Horizon endorses public health measures including vaccinations. We encourage all applicants to be mindful of the fact that Horizon is a healthcare agency proving in person services throughout our community.
This information is intended to provide a general overview of the position; it is not a full job description.

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)",84681,1 to 50 Employees,Nonprofit Organization,,,-1,Unknown / Non-Applicable,NY,-1,data engineer,na,"['python', 'sql']",[],['power bi'],[],[],bachelor,+10 years
"WeVideo, Inc.",4.4,"Mountain View, CA","Senior Data Engineer, Analytics","Job type: Full-time position. Location: Mountain View, CA

What inspires you? This is the question that drives most career decisions.

Is it working with a fantastic team that is dedicated to a common goal? Is it the ability to make a significant impact on the success of a product and company? Perhaps it is to contribute to a product that ignites the creativity of content creators, small businesses, educators, and schoolchildren worldwide?

We have an immediate opening for a Senior Data Engineer. You will be working within the Analytics team at WeVideo to help build out and grow our data infrastructure. The team plays a central role in shaping the data ecosystem in our company, and enables business performance by providing users across the company with the insights, tools, infrastructure, and consulting to make data-driven decisions. Our org is hungry for data and needs you to help us get to the next level.

What you will do in this role:
Be the lead engineer that builds and automates data pipelines for our Product data, as well as enable Data Integration projects.
Develop and improve the technical architecture of current data warehouse (BigQuery)
Design data models optimized for aggregation, visualization and advanced analytics (machine learning)
Design, implement and maintain data pipelines from beginning to end. Our Product data pipeline is high velocity and we need to re-architect it using Apache Kafka. Our Business data pipes are standard fare, and we use ‘modern data stack’ tooling (ELT, reverse ETL, DBT, self-serve BI)
Facilitate data integrations and write optimized data transformations in SQL/Python
Implement automated QA and data quality checking systems for pipelines and warehouse
Leverage automation using Apache Airflow where possible to help the team scale
Skills and knowledge you possess:
2+ years in data engineering with a Bachelor's degree in CS, Data Science, or similar technical field, or equivalent professional experience
Demonstrated success building and automating batch and streaming data pipelines, as well as end-to-end Monitoring and Alerting solutions
High fluency with advanced SQL in BigQuery (or Redshift) environment
Proficient in creating high quality, fast services and projects in Python
Experience building data pipes using Apache Kafka or Pub/Sub, Apache Airflow, GCP/AWS
Strong communication skills and interest in working with Marketing & Sales teams
Benefits & perks :
Potential remote opportunity
Medical, dental, vision and 401(k)
Generous PTO policy
Free lunch and snacks
Enough free caffeine to keep you up for 2 weeks straight
Employee development resources
Why you might like working here:
We’re a small, close-knit team that enjoys working and learning from each other.
People stick around. Some of your future colleagues have been for over 8+ years.
Our users love us; just take a look at the tweets shared by teachers.
About WeVideo:
WeVideo is a powerful, easy to use, cloud-based video creation platform that is the digital editing and storytelling choice of more than 22 million consumers, students, businesses, and third-party media solutions. WeVideo is available from virtually any computer or device at home, school, work, or on-the-go to capture, edit, view, and share videos. Built for the future in HTML5, WeVideo brings maximum speed, responsiveness, security, and expandability to browser-based video editing. WeVideo is a Google Play Editors' Choice selection with more than 9 million downloads to date. WeVideo is also the exclusive digital storytelling solution of Google’s Education Creative Bundle for Chromebooks and a Microsoft Education Partner. More than 6,500 schools use WeVideo to enhance classroom learning.",150960,Unknown,Company - Private,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['python', 'sql', 'go']","['bigquery', 'gcp', 'aws', 'redshift']",[],[],"['kafka', 'airflow']",bachelor,+10 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
FlexIT Inc,4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.",100840,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,OR,-1,data engineer,na,"['python', 'scala', 'java', 'nosql']",[],[],"['dynamodb', 'hive']","['kafka', 'hadoop', 'spark']",,
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74801,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
ITEOM,4.2,Remote,Data Engineer - Remote,"Our client is seeking a Data Engineer who would be responsible for design of data warehouse schemas as well as end-to-end design and implementation of fault tolerant and scalable data processing pipelines using a variety of technologies for orchestrating data movement, primarily Snowflake , DBT and Airflow . You'll work with a team of data engineers to design and implement solutions that provide business-critical insights.
Salary : Mid-Level - 130K to160K+ bonuses and equity options
Location: REMOTE WORK 100% is an option: (locations limited to CO, AZ, TX, VA, CA, GA, HI, MN, MI, NJ, NY, NC, OR, PA, WA)
How You’ll Do It
Working collaboratively with a team of other data engineers and developers of varying experience levels and areas of expertise, you'll be responsible for:
Responsibilities
Design and implement data warehouse schemas to store large data sets that can be easily and efficiently queried and joined together for reporting and analytical purposes.
Design and develop ETL/ELT pipelines to efficiently move and aggregate data, while optimizing for maximum code and data reuse.
Collaborate with other team members on improvements to existing systems
Investigate data anomalies and provide quick resolutions.
Provide technical support to business users and analysts.
What We’re Looking For
TOP SKILLS: ETL, Data Warehousing, SQL, Jinga Scripting, Python and Snowflake.
3+ years of working experience with large scale data warehouse systems.
Very strong knowledge of SQL and data manipulation best practices
Experience in building efficient and fault tolerant ELT/ETL data pipelines
Very strong knowledge of working with large scale datasets and data modeling and data warehouse design
Experience with Snowflake is preferable
Knowledge of DBT, Jinja scripting and Airflow is a big plus
Nice to Have
Experience working cross functionally with product and engineering teams
Desire to wear many hats and work your tail off for a bit - great earning potential and equity available.
Benefits: A diverse benefit package, sign on bonus, options, etc. is offered through our client.
About ITEOM: ITEOM Digital Technology Talent Partners collaborates with top technologists and top employers to align purpose and passion and re-incorporate belonging to make the job search and work experience more human. We are an equal opportunity employer; however, this role is not eligible for sponsorship at this time. Employees eligible for full time W2 employment are encouraged to apply.",145000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']",['snowflake'],[],[],['airflow'],,+10 years
The Sunwater Institute,4.2,"Rockville, MD",Data Engineer,"The Sunwater Institute is a nonprofit, nonpartisan public policy organization focused on improving the foundations of our social systems. The Sunwater Institute’s current programs focus on improving congressional performance, preserving and advancing constitutional rights and liberties, strengthening and enlarging property rights, and exploring the foundations of group decision-making.
The Sunwater Institute’s interdisciplinary, long-term orientation and focus on the building blocks of social systems sets it apart as a haven for bold, fresh ideas and free, constructive dialogue. The Sunwater Institute is located in North Bethesda and maintains a presence on Capitol Hill.
POSITION
The Sunwater Institute is seeking a Data Engineer who has a background in engineering or computer science and is highly skilled in SQL and database services technologies. The ideal candidate will possess strong problem-solving skills, excellent communication, and a great technical aptitude. The individual in this position will provide project support and help drive team initiatives. The candidate will be creative, and comfortable working in small teams and coordinating with US and non-US-based developers.
Responsibilities
Develop data models to store and manage large amounts of structured and unstructured data
Design efficient ETL processes to move and integrate data from different sources into the enterprise warehouse
Write complex queries using SQL Server etc. to analyze large datasets
Identify trends, patterns, and correlations within datasets by performing exploratory analysis
Ensure high quality of data by implementing best practices on storage, maintenance, indexing techniques, etc
Coordinate project status updates between teams and communicate deliverables and timelines
REQUIREMENTS
Bachelor’s degree in Computer Science or Engineering related field
4+ years of relevant experience in a similar role
Strong knowledge of relational databases including SQL Server
Proficient skills in writing complex queries using SQL language; developing stored procs, views, etc.
Experience with end-to-end ETL (Extract Transform Load) Processes
Familiarity with Big Data platforms such as AWS etc.
Ability to develop effective reports that help drive business decisions
Must be able to work effectively with teams but be able to work independently on tasks to meet tight deadlines
Exceptional communication skills verbal, written, and active listening
Extreme attention to detail and accuracy
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Vision insurance
Compensation package:
Yearly bonus
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Rockville, MD 20852: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",106385,,,,,-1,,MD,-1,data engineer,na,['sql'],['aws'],[],['sql server'],[],bachelor,2-5 years
Govini,3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.",88194,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable,PA,12,data engineer,na,['sql'],[],[],[],[],master,5-10 years
Edrstaffing,4.2,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],,2-5 years
Phasorsoft LLC,4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person",115786,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,na,"['python', 'scala', 'sql', 'java']","['azure', 'databricks']",[],[],['spark'],bachelor,+10 years
Cloudbc Labs,3.9,Remote,Anaplan Data Engineer,"Job Title :: Anaplan Data Engineer
Location :: Remote
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Remote",117000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,Unknown / Non-Applicable,Remote,8,data engineer,na,[],[],[],[],[],,
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
Xiar tech inc,3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",93420,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,TX,-1,data engineer,senior,"['python', 'sql']",[],[],[],['airflow'],,0-2 years
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85907,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
Monogram Health Renal Services,4.2,"Brentwood, TN",Azure Data Engineer,"Position: Azure Data Engineer

The Azure Data Engineer will work with our business operations and technology teams to build out an enterprise-wide Data Warehouse and related systems in the cloud. Will collaborate with our internal teams and 3rd party vendor in building our future state, business process, system interfaces, governance and technology platforms needed to enable a single source of truth for product and customer data, drive a more sophisticated data environment and be positioned to scale for future business growth.

Roles and Responsibilities
In collaboration with our 3rd party vendor, implement strategic roadmap and operational framework for Enterprise Data Management, in partnership with key stakeholders
Collaborate on the strategy on new cloud data stores and migration of existing data in Azure
Work collaboratively with teams to design, document, and ensure continuous improvement of SDLC best practices for data in the cloud
Be an advocate for quality and security through data quality and accuracy validation and auditing of security best practices
Focus on understanding business problems and determine what aspects require optimization; articulate those aspects in a clear and concise manner
Work collaboratively with cross-functional teams to support Data Models in alignment with business needs
Partner with IT, on design and oversight of APIs and other data sharing capabilities in collaboration with short / long term business needs
Partner with IT to integrate new data management technologies and software engineering tools (automation projects, process improvements, etc.) into existing systems to enable single source of truth for product and customer data
Adhere to identified key performance indicators for quality and compliance metrics to ensure data related policies and standards are followed
Acts as a SME to explain all facets of EDM data exchange and processing, business user involvement, and overall architecture
Support Monogram as necessary with any tasks required to deliver excellent personalized kidney care and perform all other duties as assigned
Uphold the mission and values of Monogram Health in all aspects of your role and activities
Position Requirements
Nashville, TN
BS or equivalent experience in crucial duties and responsibilities
3-5 years’ experience in cloud computing
Solid understanding of enterprise data management in Healthcare Data
Solid understanding of data, databases, data tools (e.g. SQL) and data processing experience
Solid understanding of standard ETL tools and techniques, sourcing, maintaining, and updating data, data warehousing, data cleansing, and other analytical techniques required for data usage.
Experience with database systems, Azure cloud storage, and significant exposure to or experience with modern data and BI platforms (such as Databricks, MSSQL Data Warehouse (Synapse), Snowflake, and Tableau).
Previous experience with data migration to cloud-based environment
Interest and passion in designing and implementing data management, querying, and storage, with a particular focus on purpose-built data stores.
Functional knowledge of data visualization tools such as PowerBI and Tableau.
Strong intuition for business and analytical skills
Team player with excellent communication skills

Benefits

Opportunity to work in a dynamic, fast-paced and innovative value-based provider organization that is transforming the delivery of kidney care
Competitive salary and opportunity to participate in the company’s bonus program
Comprehensive medical, dental, vision and life insurance
Flexible paid leave and vacation policy
401(k) plan with matching contributions

About Monogram Health
Monogram Health is a next-generation, value-based chronic condition risk provider serving patients living with chronic kidney and end-stage renal disease and their related metabolic disorders. Monogram seeks to fill systemic gaps and transform the way nephrology, primary care and chronic condition treatment are delivered. Monogram’s innovative, in-home approach utilizes a national nephrology practice powered by a suite of technology-enabled clinical services, including case and disease management, utilization management and review, and medication therapy management services that improve health outcomes while lowering medical costs across the healthcare continuum. By focusing on increasing access to evidence-based care pathways and addressing social determinants of health, Monogram has emerged as an industry leader in championing greater health equity and improving health outcomes for individuals with chronic kidney and end-stage renal disease.

At Monogram Health we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.
Experience
Required
SQL
healthcare data
cloud computing
Azure
Preferred
Tableau
Snowflake
Data Warehouse
Databricks
Power BI",106385,,,,,-1,,TN,-1,data engineer,na,['sql'],"['snowflake', 'azure', 'databricks']","['tableau', 'power bi']",[],[],,5-10 years
Sky Consulting Inc,4.2,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.",103312,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,['python'],['gcp'],[],['hive'],"['kafka', 'hadoop', 'spark']",,
LOVEFOODIES INC,4.2,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",54000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['python', 'sql']",[],['tableau'],[],[],,2-5 years
NN Tech,5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",117000,1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable,VA,5,data engineer,senior,[],[],[],[],[],,
Clairvoyant,4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",121500,51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']","['databricks', 'aws']",[],[],['spark'],master,0-2 years
Peach IT Professionals,3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",99000,51 to 200 Employees,Company - Private,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,"['r', 'python', 'sql']",[],['ssis'],[],['hadoop'],,
APLOMB Technologies,4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",72500,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,-1,$1 to $5 million (USD),NJ,-1,data engineer,na,['sql'],[],[],[],[],,0-2 years
Cognira,4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.",122989,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable,GA,8,data engineer,senior,"['python', 'scala', 'java', 'nosql']",[],[],[],['spark'],,
AGM Tech Solutions,4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",138600,51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD),GA,5,data engineer,senior,"['python', 'sql', 'java']","['azure', 'databricks', 'aws', 'redshift']",[],['hive'],"['hadoop', 'spark']",,5-10 years
ATCS Inc.,3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR",74801,501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable,GA,37,data engineer,na,"['python', 'scala', 'sql']","['azure', 'databricks']",[],[],['spark'],,
PRO IT,5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",121754,1 to 50 Employees,Company - Private,,,-1,Unknown / Non-Applicable,NJ,-1,data engineer,na,"['python', 'sql']","['oracle', 'aws', 'redshift']",[],['dynamodb'],"['kafka', 'spark']",bachelor,0-2 years
Amazee Global Ventures Inc,5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",112500,1 to 50 Employees,Company - Public,,,2019,Less than $1 million (USD),TX,4,data engineer,na,['nosql'],"['aws', 'gcp']",[],[],"['airflow', 'hadoop']",,0-2 years
Hired by Matrix,4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",103500,51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD),WI,37,data engineer,na,"['python', 'scala', 'sql']","['bigquery', 'google cloud', 'gcp', 'azure', 'databricks', 'aws']","['tableau', 'power bi']",[],"['hadoop', 'spark']",bachelor,5-10 years
TheHive,4.2,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",63000,,,,,-1,,MN,-1,data engineer,na,"['python', 'sql']","['snowflake', 'azure']",[],['sql server'],['kafka'],,
Futuretech Consultants LLC,4.2,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",76500,,,,,-1,,MS,-1,data engineer,na,['sql'],['snowflake'],['ssis'],[],[],bachelor,2-5 years
Skytech Consultancy Services,5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",117000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MD,-1,data engineer,na,['sql'],['oracle'],['tableau'],[],[],bachelor,5-10 years
Edrstaffing,4.2,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker",120000,,,,,-1,,MA,-1,data engineer,na,['python'],[],[],[],[],,2-5 years
The Sunwater Institute,4.2,"Rockville, MD",Data Engineer,"The Sunwater Institute is a nonprofit, nonpartisan public policy organization focused on improving the foundations of our social systems. The Sunwater Institute’s current programs focus on improving congressional performance, preserving and advancing constitutional rights and liberties, strengthening and enlarging property rights, and exploring the foundations of group decision-making.
The Sunwater Institute’s interdisciplinary, long-term orientation and focus on the building blocks of social systems sets it apart as a haven for bold, fresh ideas and free, constructive dialogue. The Sunwater Institute is located in North Bethesda and maintains a presence on Capitol Hill.
POSITION
The Sunwater Institute is seeking a Data Engineer who has a background in engineering or computer science and is highly skilled in SQL and database services technologies. The ideal candidate will possess strong problem-solving skills, excellent communication, and a great technical aptitude. The individual in this position will provide project support and help drive team initiatives. The candidate will be creative, and comfortable working in small teams and coordinating with US and non-US-based developers.
Responsibilities
Develop data models to store and manage large amounts of structured and unstructured data
Design efficient ETL processes to move and integrate data from different sources into the enterprise warehouse
Write complex queries using SQL Server etc. to analyze large datasets
Identify trends, patterns, and correlations within datasets by performing exploratory analysis
Ensure high quality of data by implementing best practices on storage, maintenance, indexing techniques, etc
Coordinate project status updates between teams and communicate deliverables and timelines
REQUIREMENTS
Bachelor’s degree in Computer Science or Engineering related field
4+ years of relevant experience in a similar role
Strong knowledge of relational databases including SQL Server
Proficient skills in writing complex queries using SQL language; developing stored procs, views, etc.
Experience with end-to-end ETL (Extract Transform Load) Processes
Familiarity with Big Data platforms such as AWS etc.
Ability to develop effective reports that help drive business decisions
Must be able to work effectively with teams but be able to work independently on tasks to meet tight deadlines
Exceptional communication skills verbal, written, and active listening
Extreme attention to detail and accuracy
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Vision insurance
Compensation package:
Yearly bonus
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Rockville, MD 20852: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",106385,,,,,-1,,MD,-1,data engineer,na,['sql'],['aws'],[],['sql server'],[],bachelor,2-5 years
ITEOM,4.2,Remote,Data Engineer - Remote,"Our client is seeking a Data Engineer who would be responsible for design of data warehouse schemas as well as end-to-end design and implementation of fault tolerant and scalable data processing pipelines using a variety of technologies for orchestrating data movement, primarily Snowflake , DBT and Airflow . You'll work with a team of data engineers to design and implement solutions that provide business-critical insights.
Salary : Mid-Level - 130K to160K+ bonuses and equity options
Location: REMOTE WORK 100% is an option: (locations limited to CO, AZ, TX, VA, CA, GA, HI, MN, MI, NJ, NY, NC, OR, PA, WA)
How You’ll Do It
Working collaboratively with a team of other data engineers and developers of varying experience levels and areas of expertise, you'll be responsible for:
Responsibilities
Design and implement data warehouse schemas to store large data sets that can be easily and efficiently queried and joined together for reporting and analytical purposes.
Design and develop ETL/ELT pipelines to efficiently move and aggregate data, while optimizing for maximum code and data reuse.
Collaborate with other team members on improvements to existing systems
Investigate data anomalies and provide quick resolutions.
Provide technical support to business users and analysts.
What We’re Looking For
TOP SKILLS: ETL, Data Warehousing, SQL, Jinga Scripting, Python and Snowflake.
3+ years of working experience with large scale data warehouse systems.
Very strong knowledge of SQL and data manipulation best practices
Experience in building efficient and fault tolerant ELT/ETL data pipelines
Very strong knowledge of working with large scale datasets and data modeling and data warehouse design
Experience with Snowflake is preferable
Knowledge of DBT, Jinja scripting and Airflow is a big plus
Nice to Have
Experience working cross functionally with product and engineering teams
Desire to wear many hats and work your tail off for a bit - great earning potential and equity available.
Benefits: A diverse benefit package, sign on bonus, options, etc. is offered through our client.
About ITEOM: ITEOM Digital Technology Talent Partners collaborates with top technologists and top employers to align purpose and passion and re-incorporate belonging to make the job search and work experience more human. We are an equal opportunity employer; however, this role is not eligible for sponsorship at this time. Employees eligible for full time W2 employment are encouraged to apply.",145000,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,na,"['python', 'sql']",['snowflake'],[],[],['airflow'],,+10 years
Govini,3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.",88194,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable,PA,12,data engineer,na,['sql'],[],[],[],[],master,5-10 years
Monogram Health Renal Services,4.2,"Brentwood, TN",Azure Data Engineer,"Position: Azure Data Engineer

The Azure Data Engineer will work with our business operations and technology teams to build out an enterprise-wide Data Warehouse and related systems in the cloud. Will collaborate with our internal teams and 3rd party vendor in building our future state, business process, system interfaces, governance and technology platforms needed to enable a single source of truth for product and customer data, drive a more sophisticated data environment and be positioned to scale for future business growth.

Roles and Responsibilities
In collaboration with our 3rd party vendor, implement strategic roadmap and operational framework for Enterprise Data Management, in partnership with key stakeholders
Collaborate on the strategy on new cloud data stores and migration of existing data in Azure
Work collaboratively with teams to design, document, and ensure continuous improvement of SDLC best practices for data in the cloud
Be an advocate for quality and security through data quality and accuracy validation and auditing of security best practices
Focus on understanding business problems and determine what aspects require optimization; articulate those aspects in a clear and concise manner
Work collaboratively with cross-functional teams to support Data Models in alignment with business needs
Partner with IT, on design and oversight of APIs and other data sharing capabilities in collaboration with short / long term business needs
Partner with IT to integrate new data management technologies and software engineering tools (automation projects, process improvements, etc.) into existing systems to enable single source of truth for product and customer data
Adhere to identified key performance indicators for quality and compliance metrics to ensure data related policies and standards are followed
Acts as a SME to explain all facets of EDM data exchange and processing, business user involvement, and overall architecture
Support Monogram as necessary with any tasks required to deliver excellent personalized kidney care and perform all other duties as assigned
Uphold the mission and values of Monogram Health in all aspects of your role and activities
Position Requirements
Nashville, TN
BS or equivalent experience in crucial duties and responsibilities
3-5 years’ experience in cloud computing
Solid understanding of enterprise data management in Healthcare Data
Solid understanding of data, databases, data tools (e.g. SQL) and data processing experience
Solid understanding of standard ETL tools and techniques, sourcing, maintaining, and updating data, data warehousing, data cleansing, and other analytical techniques required for data usage.
Experience with database systems, Azure cloud storage, and significant exposure to or experience with modern data and BI platforms (such as Databricks, MSSQL Data Warehouse (Synapse), Snowflake, and Tableau).
Previous experience with data migration to cloud-based environment
Interest and passion in designing and implementing data management, querying, and storage, with a particular focus on purpose-built data stores.
Functional knowledge of data visualization tools such as PowerBI and Tableau.
Strong intuition for business and analytical skills
Team player with excellent communication skills

Benefits

Opportunity to work in a dynamic, fast-paced and innovative value-based provider organization that is transforming the delivery of kidney care
Competitive salary and opportunity to participate in the company’s bonus program
Comprehensive medical, dental, vision and life insurance
Flexible paid leave and vacation policy
401(k) plan with matching contributions

About Monogram Health
Monogram Health is a next-generation, value-based chronic condition risk provider serving patients living with chronic kidney and end-stage renal disease and their related metabolic disorders. Monogram seeks to fill systemic gaps and transform the way nephrology, primary care and chronic condition treatment are delivered. Monogram’s innovative, in-home approach utilizes a national nephrology practice powered by a suite of technology-enabled clinical services, including case and disease management, utilization management and review, and medication therapy management services that improve health outcomes while lowering medical costs across the healthcare continuum. By focusing on increasing access to evidence-based care pathways and addressing social determinants of health, Monogram has emerged as an industry leader in championing greater health equity and improving health outcomes for individuals with chronic kidney and end-stage renal disease.

At Monogram Health we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.
Experience
Required
SQL
healthcare data
cloud computing
Azure
Preferred
Tableau
Snowflake
Data Warehouse
Databricks
Power BI",106385,,,,,-1,,TN,-1,data engineer,na,['sql'],"['snowflake', 'azure', 'databricks']","['tableau', 'power bi']",[],[],,5-10 years
"Iyka Enterprises, Inc.",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115",85907,1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,-1,Less than $1 million (USD),IL,-1,data engineer,na,['sql'],['azure'],['tableau'],['sql server'],[],,
Infinity Quest,4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote",105190,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable,NJ,17,data engineer,na,"['python', 'sql']","['azure', 'databricks']",[],[],[],,0-2 years
urpan technology,4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",85500,1001 to 5000 Employees,Company - Public,,,-1,Unknown / Non-Applicable,Remote,-1,data engineer,senior,"['python', 'sql']",[],[],[],[],,0-2 years
Kaizen Dynamics,4.2,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",153000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,DC,-1,data engineer,na,"['python', 'sql', 'r']",[],"['tableau', 'excel']",[],[],master,0-2 years
Sky Consulting Inc,4.2,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.",103312,1 to 50 Employees,Company - Public,,,-1,Unknown / Non-Applicable,MN,-1,data engineer,na,['python'],['gcp'],[],['hive'],"['kafka', 'hadoop', 'spark']",,
PrizePicks,4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1",101755,1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable,GA,8,data engineer,na,"['python', 'sql']","['gcp', 'aws', 'redshift']",[],[],['airflow'],,
Cloudbc Labs,3.9,Remote,Anaplan Data Engineer,"Job Title :: Anaplan Data Engineer
Location :: Remote
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Remote",117000,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2015,Unknown / Non-Applicable,Remote,8,data engineer,na,[],[],[],[],[],,
LOVEFOODIES INC,4.2,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",54000,Unknown,Company - Public,,,-1,Unknown / Non-Applicable,CA,-1,data engineer,senior,"['python', 'sql']",[],['tableau'],[],[],,2-5 years
ERPMark Inc,4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person",130851,1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,-1,Less than $1 million (USD),CA,-1,data engineer,na,"['python', 'sql']","['azure', 'databricks', 'aws']",[],[],['airflow'],,0-2 years
