company,company_rating,location,job_title,job_description,salary_estimate,company_size,company_type,company_sector,company_industry,company_founded,company_revenue
"Quirch Foods
3.0",3.0,"Coral Gables, FL",Data Engineer,"About Quirch Foods
In business for over 50 years, Quirch Foods is a global distributor and exporter of fresh and frozen food products with a unique focus on ethnic cultures. We service our customers from 5 distribution centers strategically located in the United States. We have consistently been ranked among the top 50 exporters in the US and are one of the largest importers of seafood. Our customers include independent grocers, chain supermarkets, foodservice distributors, cruise lines, restaurants and food processors/manufacturers. We have an extensive product list with over 8,000 SKU's and carry niche brands such as Panamei, Mambo and Chiquita along with national brands like Tyson, IBP, Excel, JBS, Iowa Premium, Smithfield, Dietz & Watson, along with many others. We are headquartered in Coral Gables, FL.
Quirch is a portfolio company of Palladium Equity Partners, a leading mid-market private equity fund based in New York City, which partnered with the Executive Team to capitalize on Quirch Foods’ record of excellence and to grow it decisively beyond its current business.
Position Summary
At Quirch Foods we believe in leveraging technology as a source of competitive advantage. This includes customized ERP Solutions, Business Intelligence, EDI, Web and Mobile Solutions. We are currently looking for a Data Engineer to join our development efforts.
Essential Duties and Responsibilities:
Develop, test and maintain custom business solutions by leveraging state of the art software development practices using an agile methodology
Evaluate business requirement and help define problems and develop solutions
Participate in design and architecture discussions with business leaders, end users and IT team members
Document business requirements and solution, document code and provide support for creation of end user documentation
Update business knowledge, technical skills and soft skills, leverage educational opportunities, participate in professional organizations
Experience:
MS SQL Programming
Visual Studio\C#\WPS Experience
Microsoft SSAS, SSIS, SSRS desired but not required
Biztalk experience desired but not required
Skills and Requirements:
Bachelor’s degree in Computer Science, Engineering, Math or equivalent and\or related experience and training preferred
Strong analytical and problem-solving skills
Strong collaborator and team player
Great organizational skills, attention to detail and follow thru
Effective oral and written communication skills
Extensive familiarity with data management principles
Benefits:
Professional growth and developmental opportunities to grow your skills using state of the art technology
Comprehensive benefits package that includes: Medical, Dental, Prescription Drug Plan, Disability Plan, Life insurance Plan
401K savings Plan
Paid Holidays
Personal Time off
Employee Discounts
Quirch Foods is an Equal Opportunity Employer (EOE). Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. All applicants must be eligible to work in the United States.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Coral Gables, FL 33134: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What is your desired compensation?
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Data Engineering: 1 year (Preferred)
Work Location: In person","$89,340 /yr (est.)",201 to 500 Employees,Company - Private,Retail & Wholesale,Wholesale,1967,$500 million to $1 billion (USD)
"ECI - Sacramento
4.4",4.4,Remote,Healthcare Data Quality/Engineer/SAS SME (FT),"Healthcare Data Quality / Data Engineer / SAS SME

Senior Healthcare SAS Data Quality Management SME
Location: remote
24-months.
Must have 10-years working in Information Technology or related field, and those years must include the following experiences:
Minimum of three (3) years of experience interpreting, analyzing and applying HIPAA health care transaction requirements.
Minimum of three (3) years of functional experience within an organization with a large, complex data set.
Minimum of three (3) years of experience gathering, documenting, and designing and applying data quality requirements and data standards.
Minimum of three (3) years of experience with implementing electronic tools/software to support data quality requirements.
Minimum of three (3) years of experience establishing a framework for enterprise data quality.

Others:
Experience working on Medi-Cal, or another Medicaid, as a Data Quality Analyst, Business Analyst, or Business Intelligence Analyst.
Experience with tools focused on business intelligence dashboards and reports, such as Statistical Analysis Software (SAS), Microsoft PowerBI or Tableau.
Experience applying data quality management concepts and the Data Management Body of Knowledge (DMBOK) or similar methodology.
Experience in the creation and implementation of data quality service level agreements.
A bachelor's degree from an accredited college or university.



About ECI - Sacramento:

Estrada Consulting, Inc. (ECI) delivers technology-enabled services and solutions to clients all over the USA and British Columbia. We provide system integration, custom application development, data warehouse and business intelligence, project management, custom reporting solutions and consulting services to mid-size and large enterprises in all major industries. The Company headquarters is in Sacramento, California, and was established in year 2000. Visit http://www.estradaci.com/ to learn about our projects, managed services, awards and certifications delivering value for a range of businesses and government agencies.",$70.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,Unknown / Non-Applicable
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
TEKletics,#N/A,"Scottsdale, AZ",Azure Data Engineer,"Azure Data Engineer
Tekletics is an information technology company focused on providing quality resources to support our customers needs. Successful candidates will work with world class organizations to deliver projects. Each candidate is required to have a strong work ethic and the ability to handle high pressure situations.
A little about this role:
As the Azure Data Engineer, you are primarily responsible for the collection and transformation of data across a multitude of data sources. This individual is also responsible for the optimization of the environment, structure, and processes associated with said data.
A day in the life:
Data Warehouse - As the Azure Data Engineer, you are responsible for the data warehouse design, development, testing, support, and configuration. You will review business requests for data warehouse data and data warehouse usage. You will also research data sources for new and better data feeds ensuring consistency and integration with existing warehouse structure.
Data Collection – You will be responsible for developing automated data pipelines and/or data integrations within the Azure Synapse environment. You will use SQL, Python scripts and Azure Functions to automate data collection from a wide variety of sources.
o API utilization – ability to leverage REST APIs as needed.
Data Transformation – You will create BI (Business Intelligence) and Data Warehousing cube design. You will create and manage ETL/ELT processes to transform and load data into data warehouse for reporting and analytics.
Data Optimization – As the Azure Data Engineer, you will create and maintain standards and policies. You will identify, design, and implement internal process improvements, including automation of manual processes and optimization of data delivery. You will continuously improve data reliability, efficiency, and quality.
Training – Identify and demonstrate techniques to optimize reporting for Data Visualization Analyst, provide and participate in internal and external training sessions, and produce documentation to support understanding and learnings around the Presence data/reporting environment.
This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Projects and responsibilities may change at any time with or without notice due to our business, industry, and/or market changes.
What we are looking for:
Previous experience using SQL, Python scripts and Azure Functions
Experienced in Azure Synapse environment
Experience designing, building, and maintaining data processing system
Dependable, extroverted, diplomatic person, able to problem-solve successfully with a wide variety of people and issues
Attention to detail and strong organizational skills, self-motivated
Ability to work independently while being a strong team player
Ability to mentor junior level developers
Passion for innovation and “can do” attitude to thrive in a fast-paced environment
Proficient in time management and adhering to deadlines
Knowledge and interest of the natural products/brands and retail landscape is a plus
Proficient computer (MS Office applications) and data-mining skills
Flexibility to successfully multi-task in a fast-paced environment with a positive attitude
Regular and predictable attendance is required
Ability to manage time and deadlines
Job Type: Full-time
Pay: $79,947.00 - $142,509.31 per year
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Scottsdale, AZ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 3 years (Required)
SQL: 3 years (Required)
Data warehouse: 3 years (Required)
Work Location: In person","$111,228 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Pfizer
4.1",4.1,"Tampa, FL","Senior Associate, Regulatory Quality Assurance Data Engineer","Pfizer has established a chief digital office which will lead the transformation of Pfizer into a digital powerhouse that will generate patient superior patient experiences that will result in better health outcomes.
As a member of the Digital Health Medicines and AI, Corporate Functions Data Solutions & Engineering team, you will help to realize Pfizer Digital strategy on the cloud by designing and deploying analytic solutions, work with high performing teams, and building lasting relationships with our business colleagues. This digital program will provide you with solid foundational understanding of the data types that drives business insights through data science and enterprise BI reporting solutions.
This role requires you to provide solution development and operations support for our Regulatory Quality Assurance (RQA) Domains (Audit Planning, Audits Made Easy, RQA Metrics, etc.). It will require you to maintain, enhance and provide operations support of our RQA solutions while leveraging our Data Hub, Dataiku, Snowflake, and data visualization product (Spotfire).
As the Senior Associate RQA Data Engineer, you will help in the pursuit of the Pfizer Digital strategy on the cloud by designing data products that drive business insights. You will be hands on and have opportunities to lead a group of vendors resources. You will be expected to partner with Digital Colleagues, (Enterprise Architecture, Client Partners, and Data Scientists, etc.) and Business Unit stakeholders to develop and sustain the RQA analytics strategy and data architecture. Data engineers also partner with solution development teams to ensure use case delivery goals are met while adhering to data architecture principles, guidelines, and standards.
Role Responsibilities
Reporting to the Senior Manager, Finance Sales Procurement Solution Delivery & Engineering, and as the Senior Associate RQA Data Engineer you will be responsible for data modeling, building/enhancing data interfaces, web applications, and visualizations that deliver insights that drive impactful business outcomes.
High level responsibilities may include (but are not limited to):
Leading the gathering, analysis and documentation of business and technical requirements
Create test plans, test scripts, and perform data validation
Enhance the design of the existing RQA Cloud Data Lake, Dataiku workflows, interface APIs, Web Apps, and our Spotfire dashboards.
Design automated solutions for building, testing, monitoring, and deploying ETL tools.
Develop internal APIs and DSS data solutions for thousands of end-users to power Audit applications and promote connectivity.
Opportunities to lead development teams that are driven to build impactful data products and visualizations
Perform root cause analysis and resolve Level 3 production and data issues
Coordinate with backend engineering team to analyze data to improve the efficiency, and speed of the application as well maintain elevated levels of data quality and data consistency.
Tune SQL queries, reports and ETL pipelines
Build and maintain data dictionary and process documentation
Present solutions to leadership, management, architects, and developers.
Work in conjunction with our cloud engineering staff, and partner with project managers, and analysts to deliver insights to the business
Professional Experience and Educational Requirements
Required:
Applicant must have a Bachelor’s degree with three years of relevant experience; OR Master’s degree with one year of relevant experience; OR Associate's degree with six years of relevant experience; OR eight years of relevant experience with a high school diploma or equivalent
At least 2 years of experience in data engineering, and/or reporting & analytics
Fundamental understanding of Data warehousing, data modeling, and data transformation
Exposure to web application development platforms such as Angular/Spring Boot; this is a must have.
Fundamental understanding of Cloud data warehouse solutions (Snowflake, Redshift, Spark, etc).
Working experience with one or more general purpose programming languages, including but not limited to: SQL, Java, Scala, Python, or JavaScript
Nice to Have:
Prior experience with data preparation and ETL: Dataiku, Informatica, Talend, Alteryx, etc is a major plus
Understanding of compliance and audit processes for GxP, SOX etc. is a plus
Cloud computing, machine learning, text analysis, NLP & Web development experience is a plus
Knowledge of Graph Databases (Neo4j, Titan, etc.) and query syntax is a plus
Experience with Semantic technologies and approaches is a plus
Full stack development using infrastructure cloud services (AWS preferred) and cloud-native tools and design patterns (Containers, Serverless, Docker, etc) is a plus and Open-Source technologies is a plus
Experience with sourcing and modeling data from application APIs
Prior experience with AWS Cloud stack (ECC, S3, Redshift) or Google Cloud Platform
Experience designing complex and inter - dependent data models for analytic, Machine learning use cases
Experience with Software engineering best-practices, including but not limited to version control (Git, TFS, Subversion, etc.), CI/CD (Jenkins, Maven, Gradle, etc.), automated unit testing, Dev Ops
Professional and Leadership Characteristics
Creative: Able to bring forth innovative ideas to improve our existing practices and takes calculated risks to innovate new capabilities within Pfizer Digital Business Analytics, with a focus on data products and analytics solutions
Analytical Thinker: Understands how to synthesize facts and information from varied data sources, both new and pre-existing, into discernable insights and perspectives; takes a problem-solving approach by connecting analytical thinking with an understanding of business drivers and how BA can provide value to the organization
Adaptable: Demonstrates flexibility in the face of shifting targets, thrives in new situations
Pioneering: Pushes self and others to think about new innovation and digital frontiers and ways to conquer them
Ambiguity Tolerant: Successfully navigates ambiguity to keep the organization on target and deliver against established timelines
Strong Data and Information Manager: Understands and uses analytical skills/tools to produce data in a clean, organized way to drive objective insights
Exceptional Communicator: Can understand, translate, and distill the complex, technical findings of the team into commentary that facilitates effective decision making by senior leaders; can readily align interpersonal style with the individual needs of customers
Highly Collaborative: Manages projects with and through others; shares responsibility and credit; develops self and others through teamwork; comfortable providing guidance and sharing expertise with others to help them develop their skills and perform at their best; helps others take appropriate risks; communicates frequently with team members earning respect and trust of the team
Proactive Self-Starter: Takes an active role in one’s own professional development; stays abreast of analytical trends, and cutting-edge applications of data

Work Location Assignment: Flexible
The annual base salary for this position ranges from $72,700.00 to $121,200.00. In addition, this position offers an annual bonus with a target of 7.5% of the base salary. Benefits offered include a retirement savings plan, paid vacation, holiday and personal days, paid caregiver/parental and medical leave, and health benefits to include medical, prescription drug, dental and vision coverage in accordance with the terms and conditions of the applicable plans. Salary range does not apply to the Tampa, FL location.
Relocation assistance may be available based on business needs and/or eligibility.
Sunshine Act
Pfizer reports payments and other transfers of value to health care providers as required by federal and state transparency laws and implementing regulations. These laws and regulations require Pfizer to provide government agencies with information such as a health care provider’s name, address and the type of payments or other value received, generally for public disclosure. Subject to further legal review and statutory or regulatory clarification, which Pfizer intends to pursue, reimbursement of recruiting expenses for licensed physicians may constitute a reportable transfer of value under the federal transparency law commonly known as the Sunshine Act. Therefore, if you are a licensed physician who incurs recruiting expenses as a result of interviewing with Pfizer that we pay or reimburse, your name, address and the amount of payments made currently will be reported to the government. If you have questions regarding this matter, please do not hesitate to contact your Talent Acquisition representative.
EEO & Employment Eligibility
Pfizer is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Pfizer also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Pfizer is an E-Verify employer.
Information & Business Tech
#LI-PFE","$96,950 /yr (est.)",10000+ Employees,Company - Public,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,1849,$10+ billion (USD)
"Solect
3.4",3.4,"San Francisco, CA",Senior Data Engineer,"Job Locations US-CA-San Francisco | US-TX-Houston
Posted Date 1 month ago(4/7/2023 5:15 PM)
# of Openings
1
Category
Enterprise Technology
Job ID
2023-2250
Overview
Company Overview
Pattern Energy is a leading renewable energy company that develops, constructs, owns, and operates high-quality wind and solar generation, transmission, and energy storage facilities. Our mission is to transition the world to renewable energy through the sustainable development and responsible operation of facilities with respect for the environment, communities, and cultures where we have a presence.

Our approach begins and ends with establishing trust, accountability, and transparency. Our company values of creative spirit, pride of ownership, follow-through, and a team-first attitude drive us to pursue our mission every day. Our culture supports our values by fostering innovative and critical thinking and a deep belief in living up to our promises.

Headquartered in the United States, Pattern has a global portfolio of more than 35 power facilities and transmission assets, serving various customers that provide low-cost clean energy to millions of consumers.
Responsibilities
Job Purpose
Pattern Energy is embarking on a major business transformation to scale processes and systems to enable the significant growth of our business. One critical aspect of this transformation is building an enterprise data architecture capable of supporting strong enterprise analytical capabilities and interconnected data across functions for improved and real-time decision making. This new role of Senior Data Engineer will be instrumental in building this future.

The Senior Data Engineer will help guide Pattern’s journey by building novel and modern tools, pipelines, and data systems. Through lived experience this role will act in a key strategic resource in our transition from siloed, disparate data assets to a grand, unified, data estate. The senior data engineer will lead development of data systems, namely data lakehouse pipeline, and analytics environments to support data lifecycles across the business. These tools will help drive our efforts toward democratization of data, reducing duplication of effort, and adding value to the data stream.

Key Accountabilities
Work with the Data team to understand and interpret use cases around the business and develop tools/systems to support data lifecycle from the point of production to the point of consumption.
Implement modern technology and concepts to enable Pattern teams’ design of enterprise-grade systems capable of achieving data goals.
Develop pipelines and analytics tools, including automations wherever possible
Templatize these solutions to help our adoption of infrastructure-as-code
Qualifications
Experience/Qualifications/Education Required
Previous experience in building enterprise data lifecycle and cloud solutions.
5-7+ years experience, specific to data management, analytics, or data reporting.
B.S. in a technical field with M.S. preferred.
Significant experience delivering Data solutions using the Azure cloud stack
Experience with database, data lake, lakehouse design concepts and the use of SQL.
Experience with transformations and reporting solutions, namely Power BI
Experience with environment management, release management, code versioning, deployment methodologies, and CI/CD tools
Additional Requirements

Demonstrated excellence and ability to learn new programming skills and languages.
Strong understanding of security and governance principles, including access policies
Proven record of excellent communication and cooperation with varied stakeholders.
Renewable energy industry experience is strongly preferred.
Demonstrated experience building scalable data models & ingestion pipelines from a variety of systems including Enterprise Applications (e.g. ERP, CRM), Operational Assets (e.g. SCADA), Big Data (e.g. Weather or product configuration simulators).
Technical Skills
Experience throughout the Microsoft Integrated Data Platform (IDP) and related stacks including, but not limited to:
Azure, AAD, Synapse, Purview, Data Lake, Delta Lake, and Lakehouse design
Azure Data Factory, Data Explorer, Logic Apps, Power Automate
Databricks; PySpark, and optimized resource consumption, therein
The expected starting pay range for this role is $95,000 - $129,000 USD. This range is an estimate and base pay may be above or below the ranges based on several factors including but not limited to location, work experience, certifications, and education. In addition to base pay, Pattern’s compensation program includes a bonus structure for full-time employees of all levels. We also provide a comprehensive benefits package which includes medical, dental, vision, short and long-term disability, life insurance, voluntary benefits, family care benefits, employee assistance program, paid time off and bonding leave, paid holidays, 401(k)/RRSP retirement savings plan with employer contribution, and employee referral bonuses.

Pattern Energy Group is an Equal Opportunity Employer.","$112,000 /yr (est.)",51 to 200 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,2009,$25 to $100 million (USD)
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Nisum
4.0",4.0,Remote,Data Engineer GB4790,"Location: Remote, USA
Team: Data Science & Analytics
Work Type: Full Time
Nisum is a leading global digital commerce firm headquartered in California, with services spanning digital strategy and transformation, insights and analytics, blockchain, business agility, and custom software development. Founded in 2000 with the customer-centric motto “Building Success Together®,” Nisum has grown to over 1,800 professionals across the United States, Chile,Colombia, India, Pakistan and Canada. A preferred advisor to leading Fortune 500 brands, Nisum enables clients to achieve direct business growth by building the advanced technology they need to reach end customers in today’s world, with immersive and seamless experiences across digital and physical channels.

What You'll Do
Defines, designs, develops, and test software components/applications using Spark, Sql, and PySpark.
Building solutions using a variety of open-source tools Microsoft Azure services and a proven track record in delivering high-quality work to tight deadlines.
Ability to work with the customer as part of the Agile model of delivery.
Design and Build Modern Data Pipelines and maintain data warehouse schematics, layouts, architectures, and relational/non-relational databases for data access and Advanced Analytics
What You Know
Hands-on distributed computing development experience using PySpark, Spark SQL, and Databricks delta tables.
Should have experience in structured streaming, stateless, and state full.
Familiarity with Azure resources (AKV, Managed identity, SPN, ADLS, etc..).
Must have good craftsmanship skills including unit testing, and code quality, and be a creative thinker/problem solver.
Experience in CICD and operation tooling integrations is a plus.
#Li-Remote
Education
Bachelor’s degree in Computer Science, Information Systems, Engineering, Computer Applications, or related field
Benefits
In addition to competitive salaries and benefits packages, Nisum US offers its employees some unique and fun extras:
Professional Development - We offer in-house technical training and professional learning programs aimed at developing skills across a broad spectrum of topics such as technology, leadership, role-based training, and process expertise. We also offer an annual stipend for employees to attend external courses in order to maintain professional certifications
Health & Wellness Benefits - We believe that your health and welfare are important, and we strive to ensure that you have affordable options available to you, including some plans that are subsidized for employees and their families by up to 90%. We also have dental and vision plans in the US where Nisum pays 100% of premiums for employees
Volunteerism Pay - We believe in giving back and in the US, our employees are eligible for up to 40 hours of paid time off each year to volunteer towards the causes that they are most passionate about. This is in addition to personal PTO and paid holidays
Additional Benefits - We offer all the other important benefits to keep employees and their families healthy and financially secure, such as 401(k) retirement savings with a company match, pre-tax parking and transit programs, disability insurance, and Basic Life/AD&D, alongside exclusive employee discounts on a wide variety of products and services
Compensation Band
$120-125k per year
Nisum is an Equal Opportunity Employer and we are proud of our ongoing efforts to foster diversity and inclusion in the workplace.","$122,500 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,Unknown / Non-Applicable
"CEDENT
5.0",5.0,"Phoenix, AZ",Big Data Support Engineer for cloud migration project-Remote,"Title: Big Data Support Engineer for cloud migration project @ Phoenix, AZ or Remote
Terms of Hire: 12 months.
Salary: $ Open K/ YR + Benefits.
Job Summary

Years of Experience: 5 – 7
Bachelor’s Degree: Computer Science or engineering
Start Date: ASAP
Interview Type: Video

Job duties:
Big Data Engineer position for cloud migration project
Writing pyspark & hive code to build use case.
Design solution
Troubleshoot production issue
Discussing business requirement with product team
Write and Execute test case.
Skill Requirements:
5 to 7 years of experience in big data space
Should be good in Hadoop concepts and technology
Hands on working experience on Pig, Hive, Sqoop, Spark, Python, Shell scripting.
Good understanding of Data processing using big data tools.
Experience in optimizing Hadoop jobs and spark jobs
Experience and knowledge of building REST web services.
AWS cloud experience is must
Programming experience in UNIX and Python
Good communication skill
SHOULD HAVE EXPERIENCE WITH OOZIE SCHEDULING
SHOULD BE FAMILIAR WITH AGILE METHODOLOGY
SHOULD HAVE STRONG HOLD ON SQL QUERIES
SHOULD BE AWARE OF USING GIT REPO AN D RESPECTIVE COMMANDS

You Will Enjoy:
An opportunity to be a part of a great culture, an awesome team, a challenging work environment, and some fun along the way!
Apply today to learn more and be part of our Growth story.
All applications will be kept strictly confidential and once shortlisted, our team will be in touch with you for further discussions.","$81,771 /yr (est.)",1 to 50 Employees,Contract,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Oak Ridge National Laboratory
3.6",3.6,"Oak Ridge, TN",Geospatial Data Engineer,"Requisition Id 7635
Overview:
The Geospatial Science and Human Security Division (GSHSD) is seeking a geospatial data engineer within the Location Intelligence group. This position is within the Division’s Human Dynamics section and part of the National Security Science Directorate. The Location Intelligence Group performs ground-breaking research into place-based knowledge extraction, narrative analysis, and geosocial data discovery. Leveraging primarily “non-traditional” geospatial data sources such as volunteered geographic information (VGI), internet of things (IoT) data, and telemetered sensor sources, and harmonizing with commercial and open data sources, the Location Intelligence group delivers novel approaches and technologies for describing places, points of interest, events, activities, and populations, as well as the patterns of interaction among the three. The group’s dynamic content portfolio addresses the timely need to broaden research in spatial data curation, multi-scale land-use modeling, generating higher-level features from ground photo imagery, disinformation detection, network spatialization, narrative analysis, and transformation to describe the landscape and patterns of activity within it, using novel and non-traditional data and theoretical techniques.

This position involves developing and deploying scalable geospatial applications supporting the group’s R&D portfolio. The position’s focus includes architecting geospatial services, working with the CI/CD deployment teams, performing stress testing of deployed services and applications, cloud-to-premise interfacing, and distributed computing to deploy real-time geospatial intelligence applications. The individual will collaborate with other scientists in developing transformative technologies that address these challenges through teaming and collaborative efforts. The work will involve harvesting and processing several hundred disparate data sources from all around the world. These sources contain text, sensors, ground-level imagery, social media, trajectory, and travel data, among others. Enabling this data for purposeful use requires designing and architecting RESTful services and distributed failover backend services. To confidently run them, individuals are required to have a good background in the following initiatives: Architect and design Spring boot and Java Application, open-source exploitation; geospatial OGC services architecture and development, Java and object-oriented concepts, application server failover and load balancing, and geospatial formats such as GeoJson and GeoPackage; and a working knowledge of GIS methodologies.

Our commitment to diversity:
As we strive to become the world’s premier research institution in the sciences and technologies that underpin critical national security missions, we are committed to creating an inclusive environment that highly values a diverse workforce. We recognize that breadth of perspectives, insights, and experiences is necessary to drive innovation and discovery mission-critical to national security sciences. Our commitment extends beyond our workforce to the next generation of researchers with STEM education outreach that seeks to engage a diverse range of students.

Major Duties and Responsibilities:
Architect services on NoSQL data (ElasticSearch), OGC GeoServer configuration, Java and Object-oriented application architecture fluency, Maven built Spring Boot, Docker, Kubernetes, CI/CD pipeline design and deployment.
Design and develop scalable and distributed data and backend services for GEOINT purposes.
Deployment and CI/CD and perform benchmark analysis and examination of results and logs from failover and robust service deployment.
Performing post-deployment logistics.
Examine new and developing technologies and tools to see if they can be used on the job.
Manage and update existing data format and perform regular improvements.
Augment existing research and development activities in location intelligence and association analysis.
Develop and support the plan for project data analysis and management.
As needed, provide technical assistance to other employees and customers.

Basic Requirements
Bachelor’s degree or higher in computer science, mathematics, or a related field and 2+ years of related experience.
Prior experience with J2EE application development, microservices, distributed Java Caching, regular expressions, web-based technologies.
Prior expertise with REST data services, APIs, and microservices as part of a service-oriented architecture (SOA).
Proficiency in open-source or commercial geospatial tools such as ArcGIS, QGIS, and PostGIS.
Prior expertise with relational (MS SQL Server, MySQL, PostgreSQL, and so on), document (JSON, ElasticSearch, and so on), spatial, graph, and other unstructured databases is required.
Experience with Python, R, and other similar languages for data manipulation, exploration, graph analysis, and statistical analysis.
Hands-on experience using cloud-enabled technologies and platforms that includes Google Cloud and AWS.
Excellent written and verbal communication and demonstrated ability to work in interdisciplinary teams.

Preferred Qualifications:
Agile software development approaches are a plus.
Working knowledge of both structured and unstructured data.
Working knowledge of version control systems such as Git.
Working knowledge of automated deployment and testing environments is a plus.
UML, Object-oriented programming, Linux, shell scripting, and other related skills are required.
Containerization technologies such as Docker and Conda have been used to design and manage computing environments.
Experience with geospatial ETL process design and implementation
Experience with database technologies such as MySQL and ElasticSearch to store, analyze, and manipulate data.
Experience in statistics, computational sciences, activity and event data, and working with social media data.

This position will remain open for a minimum of 5 days after which it will close when a qualified candidate is identified and/or hired.
We accept Word (.doc, .docx), Adobe (unsecured .pdf), Rich Text Format (.rtf), and HTML (.htm, .html) up to 5MB in size. Resumes from third party vendors will not be accepted; these resumes will be deleted and the candidates submitted will not be considered for employment.

If you have trouble applying for a position, please email ORNLRecruiting@ornl.gov.

ORNL is an equal opportunity employer. All qualified applicants, including individuals with disabilities and protected veterans, are encouraged to apply. UT-Battelle is an E-Verify employer.","$77,071 /yr (est.)",5001 to 10000 Employees,Government,Government & Public Administration,National Agencies,1943,Unknown / Non-Applicable
"Macy’s
3.4",3.4,"Johns Creek, GA",Lead Data Engineer,"Macy's, Inc is building an Enterprise Data & Analytics team to further grow our capabilities in support of our mission to be a data - led, customer centric company. This team will focus on accelerating impact from analytics, coordinating an enterprise-wide roadmap, and ensuring proper data governance and management. As a member of this team, the engineer will help lead the charge to execute on our vision to build profitable lifetime customer relationships by embedding data & analytics at the heart of everything we do.
Position Overview:
The Lead Data Engineer is responsible for development and support of data products on a modern cloud-based data lake, leveraging expertise and knowledge of multiple technologies & data domains to help build a robust, scalable, and reliable data engineering platform.
The Lead Data Engineer is responsible for providing data services for enterprise-grade analytical environments, utilizing automated data pipelines at scale, and streamlining efficient data transformations for priority use cases, be involved hands on in development of the codebase and partner closely with business units and peer technology groups to support analytics execution.
Essential Functions
Solution Design & Implementation:

Work closely with business stakeholders, implement scalable solutions to meet requirements
Follow and improve existing processes and procedures
Lead a pod of data engineers, providing both technical oversight and supporting their growth
Build, maintain and simplify enterprise data pipelines with emphasis on reusability & data quality
Work with Legal and Privacy teams to adhere to data privacy and security requirements

Culture:
Train and mentor fellow engineers on both technical stack and data domain specifics
Establish a pro-active approach to data management, ensuring business stakeholders & platforms can access required data within the SLA window
Drive change management to increase user adoption of enterprise data repositories and leverage standardized data pipelines across use cases
Increase agility in identifying data issues and taking action to remediate

Qualifications
Education/ Experience:

Data engineering experience with:
3+ years of experience in designing and implementing cloud-based data solutions
3+ years of experience integrating with analytics reporting solutions (e.g. Tablaeu, PowerBI)
3+ experience with big data processing technologies such as Hadoop, Spark, etc.
5+ years of experience building & automating ETL data pipelines using enterprise grade tools
5+ years of experience building enterprise-grade data warehouses (either on-prem or on cloud)
8+ years of overall programming experience, including recent experience with Python & SQL
Ability to effectively share technical information, communicate technical issues and solutions to all levels of business stakeholders
Customer-centric and experienced with cross-functional collaboration
Excellent written and verbal communication skills

What we can offer you:

Exciting, challenging problems to solve - you'll never have a boring day at work
A refreshingly fun work environment where you will collaborate with a smart and talented team
Unique freedom to build and lead a team in next gen thinking
A chance to learn and participate in the growth of NYC’s largest retailer

This job description is not all inclusive. Macy’s Inc. reserves the right to amend this job description at any time. Macy's Inc. is an Equal Opportunity Employer, committed to a diverse and inclusive work environment.","$161,600 /yr (est.)",10000+ Employees,Company - Public,Retail & Wholesale,"Department, Clothing & Shoe Stores",1858,$10+ billion (USD)
"Fathom Management LLC
2.0",2.0,Remote,Sr. Data Engineer Remote Opportunity,"Sr. Data Engineer

seeking a Senior Data Engineer who possesses expert level knowledge of appropriate data sources to address the specific requirements of projects for data modeling. Understand business requirements and translating into technical work. Design and implement features in collaboration with team engineers, product owners, data analysts, business partners using Agile/SCRUM Methodology.

This is a full- time position / 100% Remote.
The salary range of $140,000 - $160,000 will be based on technical experience and technical interview.

Responsibilities:

Ability to build programs or systems that can take data and turn it into meaningful information that can be studied.
Build ETL/ELT jobs and workflows to combine data from disparate sources.
Install continuous pipelines of huge pools of filtered information so that data analyst/scientists can pull relevant data sets for their analyses.
Build data workflows using SQL Server Integration Services (SSIS)
Build data workflows using Microsoft Azure (Azure Data Factory, Storage Accounts, Synapse)
Build data workflows using Databricks
Collaborate with business stakeholders, business operations, and product engineering teams in analyzing business problems, in building and testing solutions and data models.
Experience implementing and operating analytic models and services.
Document the current-state and target-state software architecture and create roadmap plans for success on various software components.
Assist in the design, implementation, and maintenance of complex solutions.
Build systems that collect, manage, and convert raw data into usable information for business analysts to interpret.
Make data accessible for evaluation and optimization
Collaborate with business stakeholders, business operations, and product engineering teams.
Coordinate activities with other technical personnel as appropriate.
Works with back-end data and develops tables using SQL scripts, SSIS, and SSMS.
Experience with Azure cloud platforms and Data bricks

Required Experience and Education:
Master's degree in computer science, systems engineering, or related technical discipline is preferred with 7-10 years of experience as a Data Engineer/Administrator or similar role. OR , B.S. in Computer Science with 15 years of relevant experience.

Benefits Overview: Full-time employees are offered comprehensive and competitive benefits package including paid vacation, sick leave, holidays, health insurance, life insurance, military leave, training, tuition reimbursement, a wellness program, short- and long-term disability, 401(k) retirement plan with company matches/immediate vesting, commuter benefits, and more.
COVID Policy: In accordance with the Federal Executive Order on Ensuring Adequate COVID Safety Protocols for Federal Contractors, this position requires that you are fully vaccinated at least 2 weeks before your start date. You will be required to provide proof of vaccination before you begin employment.
EEO Policy: It is our policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits, and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability, or any other characteristic protected by applicable federal, state or local law.","$150,000 /yr (est.)",1 to 50 Employees,Self-employed,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Farm Credit East
3.9",3.9,"Enfield, CT",Data Engineer,"Be part of a team focused on the success of our customers, the success of our communities, and the success of each other. Farm Credit East is the leading provider of loans and farm advisory services to farm, forest product, fishing, and other agricultural business owners across the northeast. We are One Team Working Together with a focus on our five pillars: Outstanding Customer and Employee Experience, Quality Growth, Operational Excellence, Commitment to our Communities, and Protecting Customer Information.

Position Summary
The Data Engineer is responsible for cleaning, managing, and sharing data that guides business decisions. Using ETL tools you will gather data from a variety of sources, checking for anomalies, automating processes, and generally making it easier for business stakeholders to generate valuable insights. This position will collaborate with internal and external organization to capture requirements, design, create, document, manage, and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.

Duties and Responsibilities
Work with product stakeholders to implement, maintain, and enhance data models and solutions used to define and measure quality of data domains.
Design data models to meet requirements
Perform ETL (Extract, Transform, and Load) on data to meet stakeholder specifications.
Design and develop data access methods, datasets, views etc.
Develops data modeling and is responsible for data acquisition, access analysis, archive, recovery, load design and implementation.
Coordinates new data developments to ensure consistency with existing warehouse structure.
Collaborates with internal customers to capture requirements, design, create, document, manage and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.
Assists with the development, implementation, and maintenance of front-end presentation (dashboards), automated report solutions and other BI solutions to support tactical and strategic reporting needs of the organization.
Assists in identification of data integrity problems and recommends solutions.
Work collaboratively with key stakeholders both internally and externally, including but not limited to Senior Management, Business Unit Leaders, Knowledge Exchange, and Farm Credit Financial Partners (FPI).

Job Qualifications/Requirements
Bachelor’s Degree in Computer Science, Business, Finance, or other related field from an accredited University.
Experience with MSFT SQL Server
Microsoft Azure (Data Bricks, Data Factory, Logic Apps, Functions, etc.)
2 plus years of experience in Finance related informatics, performance measurement, or analysis with strong relational database SQL skills.
1 + years of experience using Microsoft Azure product to perform ETL
Familiar with Databricks Unity catalog

Farm Credit East is an Equal Opportunity Employer. As an Equal Opportunity Employer, we do not discriminate on the basis of race, color, religion, national origin, sex, sexual orientation, gender identity or expression, age, marital status, parental status, political affiliation, disability status, protected veteran status, genetic information or any other status protected by federal, state or local law. It is our goal to make employment decisions that further the principle of equal employment opportunity by utilizing objective standards based upon an individual's qualifications for a specific job opening. In compliance with the Americans with Disabilities Act (“ADA”), if you have a disability and would like a reasonable accommodation in order to apply for a position with Farm Credit East, please call 1-800-562-2235 or e-mail FarmCreditCareers@farmcrediteast.com .","$86,685 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Banking & Lending,1916,$100 to $500 million (USD)
United Digestive,#N/A,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"YSI
3.7",3.7,Remote,Senior Data Engineer,"Position Title: Senior Data Engineer/ Oracle Apex Developer
Job Id: 202301002
Location: Herndon, VA (Remote)
Yakshna Solutions, Inc., (YSI) is a CMMI Level 3 assessed, ISO 9001, 20000:1, 27001 certified, woman-owned small business enterprises, headquartered in Herndon, Virginia, USA. YSI provides professional IT solutions and services to business corporations and government organizations. YSI is committed to serve its business communities as a leading IT vendor providing innovative, quality, and cost-effective IT business solutions and services.
Our benefits are very competitive that include 401(k), health, dental, and vision insurance, Life insurance, short-term and long-term disability insurance, paid time off, training, and professional development assistance.
YSI is seeking a highly qualified Senior Data Engineer. The selected candidate will be able to communicate effectively (written/verbal), possess strong interpersonal skills, be self-motivated, and be innovative in a fast-paced environment.
Responsibilities:
The Data Engineer will be the senior technical expert on work associated with data management, data quality and data structures, coordinating with the ADA as needed to ensure development and data structures are synchronized.
The Data Engineer will design the approach for data tasks and will contribute to completion of data tasks and oversee execution, providing advice and guidance as necessary to junior staff.
Required Qualifications and Skills:
Bachelors or master’s in relevant filed.
Good Data Engineering/Management experience
· Should be familiar with the the Civil Works missions for Hydropower, Recreation, Environmental Stewardship, and Water Supply.
· Extensive technical knowledge of programming in a software stack that includes an Oracle Relational database, SQL, PL/SQL, Oracle Spatial, JavaScript, Oracle REST Data Services (ORDS), and Oracle APEX to make the necessary revisions to the relevant systems. In addition to these general skills and experience, also possess the following:
· Knowledge and experience in developing and maintaining a relational database operated in Amazon Web Services (AWS cloud).
· Knowledge and experience of data entry and options to increase automation and efficiency.
· Knowledge and experience with documenting data systems and processes and creating reports for increased efficiencies.
· Knowledge and experience in optimizing the performance of existing Oracle based applications, including procedures, functions, etc.
· Knowledge and experience with creating Representational State Transfer (RESTful) services utilizing common data dissemination formats.
· Knowledge and experience with making websites 508 compliant.
· Knowledge and experience with authentication and authorization procedures
· Knowledge and experience with maintaining geospatial data in oracle relational database.
· Knowledge and experience with geospatial web services (OGC & ESRI REST)
· Knowledge and experience with cloud native development methodologies
Job Types: Full-time, Contract
Pay: $110,000.00 - $120,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Herndon, VA 20170: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
Oracle Apex: 5 years (Preferred)
AWS CLOUD: 5 years (Preferred)
Erwin: 8 years (Preferred)
Data modeling: 8 years (Preferred)
Metadata: 5 years (Preferred)
Work Location: Remote","$115,000 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,2011,$1 to $5 billion (USD)
"Shutterfly
3.3",3.3,"Eden Prairie, MN",Senior Data Engineer,"Description
At Shutterfly, we make life’s experiences unforgettable. We believe there is extraordinary power in the self-expression. That’s why our family of brands helps customers create products and capture moments that reflect who they uniquely are.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.
What You'll Do as a Sr. Data Engineer II
Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines am improve the operation and performance of the Data Warehouse
The Skills You'll Bring
Expert knowledge Python, Spark, SQL scripting and Java
7+ years of hands-on experience with building data ingress frameworks and experience with dealing with a wide variety of source data formats
Hands-on experience in building data applications, including design, implementation, debugging, and support.
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it’s the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.","$132,365 /yr (est.)",10000+ Employees,Company - Private,Information Technology,Internet & Web Services,1999,$1 to $5 billion (USD)
Seamless Migration LLC,#N/A,Virginia,Data Engineer (Full Scope Poly),"ABOUT US
Seamless Migration is a Service-Disabled Veteran-Owned Small Business (SDVOSB) started in 2021 with the purpose of enabling businesses and organizations through automation. Our goal is to help organizations discover, implement, and maintain solutions which evolve and mature with their ever-changing business needs. We believe in applying agile methodologies in all aspects of our business practices and use these methods to ensure effective results for our clients.
US Citizenship (Required)
CLEARANCE
TS/SCI W/ Full Scope Poly
LOCATION
Northern, VA
OVERVIEW
The Sponsor seeks a Data Engineer who will be responsible for designing, developing, testing, and maintaining data infrastructure projects such as data warehouses, data lakes, ETL pipelines, and data governance frameworks.
The goal of this position is to ensure that our organization processes and manages data efficiently and accurately to support data-driven decision-making.
Key Responsibilities include but are not limited to:
Design, develop and maintain data architectures, pipelines, and data integration solutions.
Collaborate with data analysts, data scientists, and other stakeholders to understand their data requirements.
Implement and manage data quality and data governance initiatives to improve data accuracy, completeness and consistency.
Work with our DevOps and IT teams to ensure sufficient infrastructure and access to required data sources.
Monitor data infrastructure and perform necessary fixes or performance tuning.
Ensure data security and privacy are maintained.
Stay up to date with industry trends and technologies related to data engineering and recommend new approaches to improve data processing capabilities.
REQUIRED SKILLS
Proven experience as a Data Engineer or similar role.
Deep knowledge of data modeling, ETL pipelines, and data warehousing concepts.
Proficiency in big data technologies such as Hadoop or Spark.
Proficiency in programming languages such as Python or Java.
Familiarity with data visualization tools such as Tableau or Power BI.
Experience with building and managing data pipelines using cloud services such as AWS or Azure.
Knowledge of relational database systems and SQL.
DESIRED SKILLS
Bachelor's degree in computer science, computer engineering, or related field.
2+ years of experience in a data engineering or related role.
Experience with data modeling, ETL, and data warehousing.
Excellent analytical and problem-solving skills.
Strong communication and collaboration skills.
BENEFITS
6% 401k match (100% vested)
100% paid Medical, Dental, Vision
$3,000 HSA contribution
28 days' vacation (PTO, Federal Holidays, and Sick Leave)
Flexible Hours
Tuition and certification reimbursement
Growth opportunities within an emerging defense company
All your information will be kept confidential according to EEO guidelines","$187,500 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Apple
4.2",4.2,"Cupertino, CA",Biomedical Data Engineer - Health Technologies,"Summary
Posted: Aug 8, 2022
Weekly Hours: 40
Role Number:200402289
The Health Technologies Team conceives and proves out innovative technology for Apple’s future products and features in health. We are seeking a highly capable Biomedical Data Engineer to join a multi-disciplinary team. Successful candidates will be able to integrate with our research study leads, data scientists and engineers to develop and support effective data analysis and machine learning workflows.
Key Qualifications
Experience with software engineering frameworks
Excellent coding skills in Python (e.g.,Pandas, Spark, Jupyter)
Workflow orchestrations (e.g., Airflow, Luigi)
Designing and maintaining (non-)relational databases (e.g. Postgres, Cassandra, MongoDB) and file systems (e.g. Parquet, CSV, JSON)
Great understanding of infrastructure designs
Linux, MacOS based development frameworks
iOS/ watchOS development (e.g., Swift, Objective-C)
Web Service APIs (e.g., AWS, REDCap, XNAT)
Version control frameworks (Git, virtualenv)
Familiarity with best practices for information security, including safe harbor privacy principles for sensitive data
Experience with biomedical sensors/platforms for measuring physiological signals in the health, wellness and/or fitness realms
Description
- Work closely with team members and study staff to design, build, launch and maintain systems for storing, aggregating and analyzing large amounts of data - Process, troubleshoot, and clean incoming data from human studies - Automate and monitor data ingestion and transformation pipelines, with hooks for QA, auditing, redaction and compliance checks per data management specifications - Create and maintain databases with existing and incoming clinical data - Architect data models and create tools to harmonize disparate data sources - Incorporate and comply with regulations as they pertain to electronic and clinical data and databases
Education & Experience
BS/MS in Computer Science, Engineering, Informatics, or equivalent with relevant 4+ years industry experience with biomedical, health or sensitive data.
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $104,000 and $190,000 annualized, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",#N/A,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1976,$10+ billion (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Atlanta, GA",Lead Software Engineer (Data),"We have an opportunity to impact your career and provide an adventure where you can push the limits of what's possible.

Job summary

As a Lead Software Engineer at JPMorgan Chase, you are an integral part of an agile team that works to enhance, build, and deliver trusted market-leading technology products in a secure, stable, and scalable way. As a core technical contributor, you are responsible for conducting critical technology solutions across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Executes creative data solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems
Develops secure and high-quality production code, and reviews and debugs code written by others
Identifies opportunities to eliminate or automate remediation of recurring issues to improve overall operational stability of software applications and systems
Leads evaluation sessions with external vendors, startups, and internal teams to drive outcomes-oriented probing of architectural designs, technical credentials, and applicability for use within existing systems and information architecture
Leads communities of practice across Software Engineering to drive awareness and use of new and leading-edge technologies
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Formal training or certification on software engineering concepts and 5+ years applied experience
Hands-on practical experience delivering system design, application development, testing, and operational stability
Advanced in both hands on development and solutioning
Proficiency in AWS, Hadoop, Snowflake, Glue, Lake formation, etc.
Proficient in all aspects of data (migration, modernization, building platforms, etc.)
Advanced understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security
In-depth knowledge of the financial services industry and their IT systems
Practical cloud native experience

Preferred qualifications, capabilities, and skills
Experience in financial services industry, specifically Wealth Management
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans","$117,975 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
"Introba Inc.
3.0",3.0,"New Haven, CT","Physical Security Engineer, Data Centers (Embedded)","WHERE PASSION + PURPOSE ALIGN
Introba is the union of Integral Group, a global network of deep green engineers and consultants and Ross & Baruzzini, a technology, consulting, and engineering firm.

Together as Introba, the organization provides world-class building engineering design, analytic, and consulting services at all scales, specializing in net-zero first thinking. Through the cultivation of thought leadership, we deliver sustainable and forward-thinking solutions to the most complex design challenges facing the world’s leading clients and partners.
Job Summary
Introba is the union of Integral Group, a global network of deep green engineers and consultants and Ross & Baruzzini, a technology, consulting, and engineering firm.

Together as Introba, the organization provides world-class building engineering design, analytic, and consulting services at all scales, specializing in net-zero first thinking. Through the cultivation of thought leadership, we deliver sustainable and forward-thinking solutions to the most complex design challenges facing the world’s leading clients and partners.

We are currently seeking an embedded Physical Security Engineer (remote position) to join our team.

Our security engineers are responsible for the design and engineering of electronic security systems, product research, development of design guidelines, and creation of design documents. The embedded security engineer role is a full-time position dedicated to one of our most prestigious clients.

While the successful candidate will be an Introba employee, the entirety of their daily responsibilities will be to provide engineering services for data center related security tasks for our client. These data centers help support the way we work, live, and communicate, and are located all over the world.
Responsibilities & Qualifications
The embedded security engineer will work with our client security team to perform the following:
Review existing security system design standards and develop recommendations for improvements to the current security program
Evaluate new equipment and technologies related to security.
Develop proof-of-concept documents for new equipment and technologies.
Manage master security detail libraries in partnership with outside security consulting firms
Manage system wiring diagrams and develop new standards for security design packages
Coordinate new technology design requirements with existing design document standards
Provide routine updates and improve upon master security specifications (CSI formatted Division 28 specifications)
Minimum of 10 years of experience in the design or installation of electronic security systems
Understanding of construction documentation
Understanding of computer networks and telecommunications systems
Good documentation and technical writing skills
Familiarity with Microsoft Office products, including Microsoft Teams
Proficiency in the use of computer-aided design software platforms (Revit and AutoCAD) considered strongly beneficial
Experience designing, installing, or coordinating security systems specifically for data centers considered strongly beneficial
Education Requirements

BS in electrical engineering, computer engineering, or computer science extremely beneficial, though strong technical experience in the security industry may be considered to meet this requirement

Additional Information
Equal Opportunity Employer

Introba is an equal opportunity employer, and we prohibit discrimination and harassment of any type as protected by federal, state, or local laws. We celebrate diversity and are committed to creating an inclusive environment for all employees.

Introba is an equal opportunity employer, and we prohibit discrimination and harassment of any type as protected by federal, state, or local laws. We celebrate diversity and are committed to creating an inclusive environment for all employees. The Company and its employees are required to comply with all local health authority, legal or lawful client requirements. You should ask your manager about this prior to starting, should you be subject to the current or potential future requirements.
We offer a competitive remuneration package, wellbeing benefits and an Employee Assistance Program to provide mental wellbeing support, guidance and advice. Our hybrid working patterns and flexible hours help our people to achieve a healthy work-life balance.","$111,898 /yr (est.)",1001 to 5000 Employees,Company - Private,"Construction, Repair & Maintenance Services",Architectural & Engineering Services,#N/A,Unknown / Non-Applicable
"Koch Ag & Energy Solutions
3.8",3.8,"Wichita, KS",Project Data Engineer / Analyst,"Description
Koch Ag & Energy Solutions (KAES) is looking for a Project Data Engineer/Analyst who will be a primary point of contact for project cost forecasting, change management, and risk management on small to large capital construction projects and turnarounds. This opportunity utilizes data from multiple sources to extract insights and perform analysis.
Our Team

We support cross site, cross functional teams throughout the project lifecycle on projects and turnarounds across (up to) 6 plants throughout the US and a site in Canada.
This position works a Monday – Friday, 8-hour day, and is based out of Wichita, KS with expected travel about 25% of the time.
What You Will Do
Use critical thinking, analysis, curiosity, and collaboration throughout the following to enable cost competitive project delivery (but not limited to) functions:
Analyze per project actual cost, schedule performance, and estimate details to forecast final cost per project at the cost breakdown structure level.
Track project progress against the baselines (scope, cost, and schedule).
Partner with project teams to identify issues and risks early and develop proactive resolutions and mitigations.
Monitor purchase order commitments, manage change orders, and adjust forecasts to align with reality.
Prepare various charts, tables and reports with insights to understand and communicate the forecast including what has changed and why.
Works with team members in assessing data store solutions that supports dashboards, insights, and other analytical solutions.
Demonstrate initiative to improve project outcomes, data integrations, processes, and forecasting quality.
Share knowledge and solve problems across teams to improve project results across KAES.
Work around the field construction sites without assistance.


Who You Are (Basic Qualifications)
Minimum of 2 years of experience in Projects within construction/oil & gas/manufacturing industry OR Bachelor’s Degree in a Business Administration or Engineering field
Demonstrated ability to analyze trends/variances and determine root causes
Experience creating and maintaining automated reporting processes
What Will Put You Ahead
Experience on projects and/or turnarounds in an industrial or construction environment.
Experience with reporting and analytics software such as Alteryx, Tableau, PowerBI, etc.
Experience with project cost management (including forecasting, change management, WBS/CBS buildup, risk management, reporting, etc.)
Familiarity with Project Management and Project Controls tools (e.g. EcoSys, Maximo, P6 or similar project portfolio or scheduling tools).
Experience performing risk based contingency assessments and identifying key drivers of risks.
Experience influencing change across an organization.


Relocation may apply based on candidate.

At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate’s knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.
Hiring Philosophy
All Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.
Who We Are

As a Koch company, Koch Ag & Energy Solutions (KAES) is a global provider of value-added solutions for the agriculture, turf and ornamental, energy and chemical markets. From agriculture to energy, KAES makes things grow better with plant nutrient and biological technologies. Our team of innovators unleash their potential while developing the technologies that feed and power the world.
At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.
Our Benefits
Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.
Equal Opportunities
Equal Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please visit the following website for additional information: http://www.kochcareers.com/doc/Everify.pdf","$81,180 /yr (est.)",10000+ Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,1940,$10+ billion (USD)
"Capital One
4.2",4.2,"Richmond, VA",Senior Data Engineer - Principal Associate,"West Creek 2 (12072), United States of America, Richmond, Virginia
Senior Data Engineer - Principal Associate
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance

Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Spark, SQL, Python, Scala or Java
2+ years of experience with a public cloud (AWS)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
No agencies please. Capital One is an Equal Opportunity Employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, physical and mental disability, genetic information, marital status, sexual orientation, gender identity/assignment, citizenship, pregnancy or maternity, protected veteran status, or any other status prohibited by applicable national, federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",#N/A,10000+ Employees,Company - Public,Financial Services,Banking & Lending,1994,$10+ billion (USD)
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"CDNetworks Inc.
2.9",2.9,"Monrovia, CA",Jr. Data Center Engineer,"CDNetworks is one of the top leading CDN & Edge Service Providers with global offices in Korea, Japan, Singapore, Malaysia, China, Russia, London, and Canada. We focus on delivering integrated cloud and edge computing solutions with unparalleled speed, ultra-low latency, rigorous security, and reliability so that our clients can focus on what’s most important – growing their business.
The Jr. Data Center Engineer is a Full-Time position based in Monrovia, CA. Job is performed indoors in a Data Center or Warehouse environment. 75% Travel required for this position.
Job Responsibilities
Ensure all incidents are logged and resolved, gather all relevant data, and ensure all incidents and tasks follow the appropriate procedures.
Support data center activities and work closely with our system and network team to complete tasks/projects.
First responder to all alerts and problem reports while managing communications between departments and handling crisis documentation and dissemination after the fact.
Utilize internal systems such as JIRA/Wiki to manage project plans and progress.
Performing general system administration duties including OS patching and upgrades, batch job monitoring, system and hardware diagnostics, and other activities to ensure optimal health and performance of all systems as required.
Resolve complex problems related to Server and H/W areas.
Assisting/working closely with Network, System Engineers to configure customer requirements.
Physical deployment of network devices, servers, cables, etc.
Assembling/dissembling server hardware for deployment and OS installation and network equipment testing.
Maintain existing department and system documentation (update workflow, process, training documentation).
Other duties as assigned.
Abilities Required
Good verbal and written communication skills, and ability to work independently with minimum instruction.
Basic degree of mentorship, training, and direction team members skills.
Knowledge of IDC industry.
Intermediate degree of analytical and project management skills.
Other Features of Job
Job is performed indoors in a Data Center or Warehouse environment.
Language Skills
Excellent communication skills (English) – written and verbal. Bilingual Chinese or Korean is a plus!!
Job Type: Full-time
Salary: $20.00 - $25.00 per hour
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Vision insurance
Schedule:
10 hour shift
8 hour shift
Evening shift
Monday to Friday
Night shift
On call
Overtime
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Monrovia, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Computer networking: 1 year (Preferred)
Shift availability:
Overnight Shift (Required)
Night Shift (Required)
Day Shift (Required)
Willingness to travel:
75% (Preferred)
Work Location: In person",$22.50 /hr (est.),201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$10+ billion (USD)
"Merkle
3.7",3.7,"Norristown, PA",Data Engineer,"Company Description

We are Dentsu International. We innovate the way brands are built. That means we do things differently so they’re better than before. In this way, we make our clients’ most important marketing assets—their brands—win in a changing world.
Dentsu International is a modern marketing solutions company. Our mission is to help clients navigate, progress, and thrive in a world of change. Businesses rely on our integrated network of agencies and specialized practices to champion meaningful progress through creative, media, commerce, data, and technology. Part of Dentsu Group, our global network comprises 66,000 diverse people in 143 countries, who are dedicated to teaming for growth and good. Follow us on Twitter @DentsuUSA and visit dentsu.com/us.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.

Job Description
Develop onboarding process to receive and process information related to onboarding a new subscriber
Convert Snowflake Stored Procedures to Python Scripts directly in Airflow
Develop metadata driven solutions for managing data processing and cadence
End to end automation development and testing
Building and maintaining stored procedures and scripts in Snowflake Data Cloud
Building and maintaining complex SQL queries for ETL/ELT processing
Develop Dashboard for data processed for audit and governance needs
Follow Agile methodologies to deliver iterative feature sets rapidly
Maintain codebase using source control methodologies, Git
Collaborate with business analysts, data analysts, and other developers to deliver powerful new features
Coordinate with teams as part of a larger data-sharing system
Contribute to a collaborative, positive, and enjoyable environment for your operations team
Performing quality assurance and testing
Writing user and technical documentation such as runbooks

Qualifications
Strong communication skills both verbal and written
Strong understanding of conditional programming logic
Knowledge of software engineering and development methodologies, techniques, and tools; includes Issue Tracking (JIRA) Software Development Lifecycle SDLC), and Continuous Integration / Continuous Development (CI/CD)
Experience working with large databases and data warehouses utilizing both relational and dimensional design concepts
Eagerness to learn new technologies, programming languages and platforms to solve complex data challenges
Experience with Python required
Experience with SQL required
Experience with Snowflake Data Cloud is highly desirable
Experience with Amazon Web Services (AWS) a plus
Experience with Airflow (or other orchestration) a plus
Experience in database marketing is a plus

Additional Information

Perks:
Great compensation package
Comprehensive healthcare plans
401(k) with employer match
Flexible time-off
16 weeks paid parental leave
Additional Information
The anticipated salary range for this position is $94,000-$146,000. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. A range of medical, dental, vision, 401(k) matching, paid time off, and/or other benefits also are available. For more information regarding dentsu benefits, please visit dentsubenefitsplus.com
Employees from diverse or underrepresented backgrounds encouraged to apply.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.
About dentsu
Dentsu is the network designed for what’s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com.
We are champions for meaningful progress and we strive to be a force for good—for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all.
Dentsu (the ""Company"") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.","$120,000 /yr (est.)",5001 to 10000 Employees,Company - Private,Media & Communication,Advertising & Public Relations,1971,$500 million to $1 billion (USD)
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"DiNi Communications, Inc.
3.0",3.0,"New York, NY",Data Center Engineer,"DiNi Communications, Inc. is seeking a Data Center Engineer. This person needs to be a team player with exceptional communication skills. Maintaining the confidentiality of the client's information is of utmost importance.
Required Education and Experience
· At least 8 years of experience as a large data center technician
· Knowledge of Networking,
· Ability to lift equipment at least (50lbs) Rack and Stack (Network Switches, Servers, Storage,
Appliances)
· Knowledge of structured Cabling in a Data Center.
· General knowledge of Operating Systems, (Windows, Unix).
· General knowledge of networking in N Tier Environment (App, Web, DMZ,).
· Level 1 and Level 2 troubleshooting in a N Tier Environment.
· Ability to follow and execute commands to start up and shutdown systems.
· Physically travel to and be available at site B to perform technical changes that require a
person at Site B.
· Performing backup and restore activities.
· Asset Management.
· Maintaining document.
· Work with vendor on circuit issues, (WAN and Internet).
· Cable Patch the equipment in the environment.
· Participate in Site B fall-over testing
Preferred Network Engineering Experience
· Knowledge of N Tier networking.
· Certification in Networking (CISCO).
· Certification in various Operating systems (Linux and Windows), Storage, Messaging
(Microsoft).
· Certification in IT Security.
· Knowledge of Databases.
· Experience working with CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee, and other vendors' equipment and software in a networked environment.
Responsibilities include but are not limited to:
Adding equipment to support agency applications in site B Environment,
Coordinating Site B testing with Site A technical resources.
Monitoring the B environment.
Rack and Stack new and replacement equipment
Troubleshooting and resolving incidents and problems in the
Physically travel to and be available at Site B to perform technical changes that require a person at Site B.
Upgrading and expanding the environment
Performing backup and restore activities
Manage Site B assets
Maintaining document
Cable Patch the equipment in the environment
Participate in Site B fall-over testing.
Job Type: Full-time
Pay: $60.00 - $70.00 per hour
Schedule:
Monday to Friday
Experience:
large data center technician: 8 years (Required)
Networking in N Tier Environment (App, Web, DMZ,): 3 years (Required)
Ability to lift equipment at least (50lbs): 1 year (Required)
structured Cabling in a Data Center: 3 years (Required)
Level 1 and 2 troubleshooting in a N Tier Environment: 3 years (Required)
Performing backup and restore activities: 3 years (Required)
Cable Patch the equipment: 3 years (Required)
Certification in Networking (CISCO): 1 year (Preferred)
Certification in IT Security: 3 years (Preferred)
CISCO, EMC, Symantec, HP, Oracle, Bluecoat, MacAfee: 3 years (Preferred)
Work Location: In person",$65.00 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Diamondpick,#N/A,"Alpharetta, GA",Java Software Engineer with Big-Data,"About us
There is someone out there who fits your requirements like a glove! We are an innovative Talent Solutions company with a vision to build talent supply chain models without compromise for the technology industry.
We are a new-age talent solutions company with a vision to build an enterprise recruitment model without compromise. We love to solve complex talent challenges and strive to be an enabler of business success for our clients using data, technology and our team’s vast experience in the technology market.
Senior Java Developer with Big data
Alpharetta, GA (Hybrid Work from Day1)
Job Description:
The ideal candidate must possess strong background on frontend and backend development technologies.
The candidate must possess excellent written and verbal communication skills with the ability to collaborate effectively with domain experts and technical experts in the team.
Responsibilities:
As a Java Senior Developer, you will
Maintain active relationships with Product Owner to understand business requirements, lead requirement gathering meetings and review designs with the product owner
Own his backlog items and coordinate with other team members to develop the features planned for each sprint
Perform technical design reviews and code reviews
Be Responsible for prototyping, developing, and troubleshooting software in the user interface or service layers
Perform peer reviews on source code to ensure reuse, scalability and the use of best practices
Participate in collaborative technical discussions that focus on software user experience, design, architecture, and development
Perform demonstrations for client stakeholders on project features and sub features, which utilizes the latest Front end and Backend development technologies
Requirements:
6+ years of experience in Java/JEE development
Skills in developing applications using multi-tier architecture
Knowledge of google/AWS cloud
Java/JEE, Spring, Spring boot, REST/SOAP web services, Hibernate, SQL, Tomcat, Application servers (WebSphere), SONAR, Agile, AJAX, Jenkins..etc
Skills in UML, application designing/architecture, Design Patterns..etc
Skills in Unit testing application using Junit or similar technologies
Good communication skills
Leadership skills
Provide overlap coverage with onsite/customer teams
Capability to support QA teams with test plans, root cause analysis and defect fixing
Strong experience in Responsive design, cross browser web applications
Strong knowledge of web service models
Strong knowledge in creating and working with APIs
Experience with Cloud services, specifically on Google cloud
Strong exposure in Agile, Scaled Agile based development models
Familiar with Interfaces such as REST web services, swagger profiles, JSON payloads.
Familiar with tools/utilities such as Bitbucket / Jira / Confluence
Job Types: Full-time, Contract, Temporary
Pay: From $60.00 per hour
Benefits:
Flexible schedule
Compensation package:
Hourly pay
Experience level:
8 years
Schedule:
8 hour shift
Day shift
Work Location: Hybrid remote in Alpharetta, GA 30005",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
39U0b7kJu8","$103,213 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
"Longevity Holdings Inc.
3.9",3.9,"Minneapolis, MN",Associate Data Engineer (Temporary),"As a Associate Data Engineer, you will treat data as an asset to design, build, and execute high performance and data centric solutions by using the comprehensive big data capabilities for the company's data platform environment. In this role, you will build and optimize data products to bring data and analytics products and solutions to businesses.
Essential Job Responsibilities:
· Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions
· Leading data collection efforts and performing trend analysis to identify common performance challenges that require further attention
· Partner closely with our data scientists to ensure the right data is made available in a timely manner to deliver compelling and insightful solutions
· Building out scalable data pipelines and choosing the right tools for the right job. Manage, optimize, and monitor data pipelines
· Incorporate core data management competencies including data governance, data security, and data quality
Required Skills:
· Bachelors in a quantitative field
· 1+ years of data engineering or equivalent experience
· Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices
· Demonstrated knowledge of relational data sets, structures, and SQL
· Familiarity with big data platforms such as Apache Spark, Hadoop, Kafka, etc.
· Experience leading data organization, dashboarding, & visualization efforts
· Inquisitive, proactive, and interested in learning new tools and techniques
· Excellent communication skills
Longevity Holdings Inc prohibits discrimination and harassment and will take affirmative action to employ and
advance in employment qualified individuals based on their status as protected veterans or individuals with
disabilities, race, color, religion, sex, national origin, sexual orientation, and gender identity.
Our privacy notice is available at https://longevity.inc/employment-privacy-notice
Job Type: Full-time
Pay: $20.00 - $30.00 per hour
Schedule:
Monday to Friday
Application Question(s):
Will you now or in the future require sponsorship for employment visa status (e.g., H-1B visa status)?
Work Location: Hybrid remote in Minneapolis, MN 55402",$25.00 /hr (est.),51 to 200 Employees,Company - Private,Management & Consulting,Research & Development,1998,$25 to $100 million (USD)
"FreightWaves, Inc.
3.5",3.5,Remote,Senior Data Engineer,"Are you smart, driven, curious, resourceful, and not afraid to fail? Then we want to meet you! Our team of bold, innovative, and creative teammates is what makes us a top startup to work for. FreightWaves delivers news and commentary as well as data and analytics which empower risk management and actionable market insights in the logistics and supply chain industry. If you are ready to join our team, it is time for YOU to apply!
FreightWaves is on the hunt for a curious, tenacious, and team-oriented Senior Data Engineer to join our fast paced engineering team. The ideal candidate is inquisitive, versatile, team oriented, thrives on change, and has a positive attitude. If you are ready to be challenged, learn new and exciting technologies, and have the unique opportunity to work with some of the most talented developers in the country, we want you to apply!
**This position is fully remote.**
**Must RESIDE in the United States and be eligible to work.**
What you will be doing:
Implementing ingestion pipelines, using Airflow as the orchestration platform, for consuming data from a wide variety of sources (API, SFTP, Cloud Storage Bucket, etc.).
Implementing transformation pipelines using software engineering best practices and tools (DBT)
Working closely with Software Engineering and DevOps to maintain reproducible infrastructure and data that serves both API-only customers and in-house SaaS products
Defining and implementing data ingestion/transformation quality control processes using established frameworks (Pytest, DBT)
Building pipelines that use multiple technologies and cloud environments (for example, an Airflow pipeline pulling a file from an S3 bucket and loading the data into BigQuery)
Create and ensure data automation stability with associated monitoring tools.
Review existing and proposed infrastructure for architectural enhancements that follow both software engineering and data analytics best practices.
Working closely with Data Science and facilitating advanced data analysis (like Machine Learning)
What you bring to the table:
Strong working knowledge of Apache Airflow
Experience supporting a SaaS or DaaS product, bonus points if you were creating new data products/features
Strong in Linux environments and experience in scripting languages
Python Expert
Strong understanding of software best practices and associated tools.
Experience in any major RDBMS (MySQL, Postgres, SQL Server, etc.).
Strong SQL Skills, bonus points for having used both T-SQL and Standard SQL
Experience with NoSQL (Elasticsearch, MongoDB, etc.)
Multi-cloud and/or hybrid-cloud experience
Strong interpersonal skills
Comfortable working directly with data providers, including non-technical individuals
Experience with the following (or transitioning from equivalent platform services):
Cloud Storage
Cloud Pubsub
BigQuery
Apache Airflow
dbt
DataFlow
Bonus knowledge/experience:
Experience implementing cloud architecture changes
Working knowledge of how to build and maintain APIs using Python/FastAPI
Transforming similar data from disparate sources to create canonical data structures
Surfacing data to BI platforms such as Looker Studio
Data Migration experience, especially from one cloud platform to another
Certification: Professional Google Cloud Certified Data Engineer
Our Benefits:
An excellent work environment, flat hierarchies, and short decision paths.
Work from home
A generous benefits package including 100% employer-paid health, dental, vision and Life insurance, STD, LTD
Stock options
Appealing 401k matching plan
Career Mentorship Opportunities
Personal Development Credit (Can be used toward Student loans or relevant PD Courses)
Annual life achievement bonus of $2000 for having a baby, buying a house, or getting married (max one per year)
No set days off Vacation policy (our team takes time off as needed with supervisor approval)
Up to $50 for Gym or Virtual Gym membership.
Audible or Kindle Unlimited subscription
Discount on Ford vehicles
oYXkhYiWkU",#N/A,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2017,Unknown / Non-Applicable
"NexTek, LLC",#N/A,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Argo Data
2.7",2.7,"Richardson, TX",Senior Data Engineer,"ARGO is a leading provider of software and analytics solutions for both the financial services and healthcare industries. ARGO transforms business processes for financial service providers and healthcare organizations using proven business models and software innovation informed by real customer challenges, breakthrough technology, and rich analytics.
ARGO- Data Engineer II (Markham, ON; Memphis, TN; ...) (theapplicantmanager.com)
JOB SUMMARY
The Data Engineer II is responsible for designing and implementing data solutions to support business needs. The successful candidate will have a strong background in software engineering and database design, as well as experience with working with cloud-based tools and technologies.
EXPECTED WORK AND PERFORMANCE
This section is intended to provide a more detailed overview of what the expectations of the job are to include: work to be performed, expected performance results and how they will be measured, as well as connection to corporate strategy, values, and mission. Job duties and responsibilities include the following. Other duties may be assigned as needed.
Design, build, and maintain data pipelines for data extraction, transformation, and loading (ETL) from various sources.
Ensure data security and privacy protocols are implemented and enforced.
Monitor and optimize data storage and processing performance to ensure high performance and cost-effectiveness.
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and translate them into technical solutions.
Maintain documentation of data systems, processes, and policies.
Stay current with the latest data tools, technologies, and trends and recommend improvements.
Mentor and train junior data engineers as needed.
QUALIFICATIONS
Bachelor's degree in Computer Science, Software Engineering, or related field.
3-5 years of experience in software engineering and SQL database design.
Proficiency in programming languages such as Python.
Strong understanding of data warehousing, ETL processes, and data modeling.
Experience with cloud infrastructure technologies such as Microsoft Azure.
Excellent problem-solving and analytical skills.
Ability to work well in a team-oriented environment and communicate effectively with both technical and non-technical stakeholders.
PHYSICAL DEMANDS
The candidate will work at our corporate headquarters in Richardson, TX. While performing the duties of this job, the employee is regularly required to stand; walk; sit; use hands to finger, handle, or feel; reach with hands and arms; talk or hear. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.
ADDITIONAL REQUIREMENTS:
Applicants for U.S. or Canadian based positions with ARGO must be legally authorized to work in the United States or Canada. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available at this time for ARGO positions.
Pre-employment background screening will be conducted.
Job Type: Full-time
Pay: $70,745.42 - $141,624.04 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richardson, TX 75081: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Experience:
Informatica: 3 years (Required)
SQL: 5 years (Required)
Data warehouse: 3 years (Required)
Python: 3 years (Required)
Azure: 3 years (Required)
Work Location: In person","$106,185 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,1980,$25 to $100 million (USD)
"Leadstack Inc
4.3",4.3,Remote,Lead Data Engineer with Sql/Python,"LeadStack Inc. is an award winning, one of the nation's fastest growing, certified minority owned (MBE) staffing services provider of contingent workforce. As a recognized industry leader in contingent workforce solutions and Certified as a Great Place to Work, we're proud to partner with some of the most admired Fortune 500 brands in the world.
Title: Lead Data Engineer with Sql/Python
Location: Remote
Duration: 6+months
Direct Client
Immediate interview
Top Skills: Azure, SQL/Python Developer.
Job Description:
Design and develop data system integrations and data models for reporting and alerting.
Essential Job Functions:
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state
Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions
Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Interpreting data, analyzing results using statistical techniques
Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality
Acquiring data from primary or secondary data sources and maintaining databases
Promote the reuse of data assets, including the management of the data catalog for reference
Key Responsibilities:
Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy
Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Mentor team members in data principles, patterns, processes and practices
Promote the reuse of data assets, including the management of the data catalog for referenceDraft and review architectural diagrams, interface specifications and other design documents
To know more about current opportunities at LeadStack , please visit us on https://leadstackinc.com/careers/
Job Type: Contract
Salary: $70.00 - $80.00 per hour
Experience level:
11+ years
Schedule:
8 hour shift
Experience:
Azure: 5 years (Preferred)
Data warehouse: 1 year (Preferred)
SQL developer: 6 years (Preferred)
Work Location: Remote",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,Unknown / Non-Applicable
"Ikigai Labs, Inc.
4.9",4.9,"Cambridge, MA","Software Engineer, Data Engineering","Ikigai Labs is a fast growing startup founded out of MIT to empower data operators. We are building an easy to use AI augmented data processing and analytics platform on the cloud. Our users depend on us to automate, maintain, and enhance day-to-day mission critical operations. We are a team of talented, hardworking and fun-loving engineers, data scientists, and data analysts working towards the goal of building the next generation of data tools.
Job Description
JOB TITLE: Software Engineer, Data Engineer [Full-time]
LOCATION: Cambridge, MA
SUMMARY:
Ikigai Labs is seeking a dynamic and passionate engineer with strong software fundamentals to join a high-performing data platform development team. We are looking for a team player who is a quick learner, performs in a rapid development cycle, has a drive to surpass expectations, and an eagerness to share their work and knowledge.
We encourage applicants from all backgrounds and communities. We are committed to having a team that is made up of diverse skills, experiences, and abilities.
Technologies
Languages: Python3, SQL
Databases: Postgres, Elasticsearch, DynamoDB, RDS
Cloud: Kubernetes, Helm, EKS, Terraform, AWS
Data Engineering: Apache Arrow, Dremio, Ray
Misc.: Apache Superset, Plotly Dash, Metabase, Jupyterhub, Stripe, Fivetran
The Position
Design and develop scalable data integration (ETL/ELT) processes
Design and develop an on-demand predictive modeling platform with gRPC
Utilize Kubernetes to orchestrate the deployment, scaling and management of Docker containers
Utilize and learn various AWS services to solve cloud-native problems
Implement a testing platform which performs sanity check, load test, scale test, heartbeat test, and performance test
Provide periodic support to our customer success team
Qualifications
0-3 years of experience with a bachelor's degree in Computer Science, Math, or Engineering; or a master's degree
Experience with Python, AWS services, and/or ETL/ELT pipeline experiences
Experience with Kubernetes and/or EKS (optional)
Understanding of the fundamentals of design patterns and testing best practices
The ability to learn quickly in a fast-paced environment
Excellent organizational, time management, and communication skills
The desire to work in an AGILE environment with a focus on pair programming
Willingness to discuss obstacles, find creative solutions, and take initiative
The ability to receive and give both constructive and encouragement feedback","$102,419 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Clinical Ink
4.4",4.4,Remote,Data Engineer,"Company Information
Clinical ink is the global life science company that brings data, technology, and patient science together to unlock clinical discovery. Our deep therapeutic-area expertise, coupled with Direct Data Capture, eCOA, eConsent, telehealth, neurocognitive testing, and digital biomarkers advancement, drive the industry standard for data precision and usher in a new generation of clinical trials. With offices in Philadelphia, PA, Winston Salem, NC, and Iowa City, IA, Clinical ink is rewriting the clinical development experience.

Job Description
Clinical ink is seeking a Data Engineer to join our Data Team based remotely across the United States! The Data Engineer will work to develop solutions used in applications for clinical trials. The ideal candidate will be a minimum of two years of experience as a software engineer and prior experience working with a variety of tools and frameworks. The Data Engineer's responsibilities include:
Develop data engineering solutions used in applications for clinical trial data collection that both make data available for further use and generate value out of data
Contribute to the methodology by which advanced analytics projects are delivered to clients and codify the tooling needed to support them
Build and support tools that allow data analysts and data scientists to work in complex projects
Implement quality, availability, and integrity of code, solutions, and respective systems and follow best practices related to data integrity, security, scalability, etc.
Participate in code inspections, reviews, and other activities to ensure quality
Qualifications
Bachelors in Mathematics, Statistics, Computer Engineering, Computer Science, or related field of study
2-5 years of experience in software engineering, working on multi-discipline teams
Experience with a variety of tools and frameworks such as Snowflake, Airflow, Spark, Kafka, RedShift, Sage Maker, Kubernetes, etc., AWS ecosystem (Lambda, Glue, S3, E2C, etc.), programming tools and querying languages (i.e., Python, C++, SQL, Scala, Java, etc.)
At least 2+ years of experience with Python
Data modelling and database development experience required
Data visualization experience preferred in Tableau and/or AWS QuickSight
Nice to have experience with issue tracking tools such as JIRA and Confluence
Ability to think creatively and take initiative; ability to learn new technical topics and develop new technical skills quickly
Willingness to learn and explore bleeding-edge/cutting-edge technologies
Additional Information
Clinical ink is an equal opportunity employer and does not discriminate against otherwise qualified applicants on the basis of race, color, creed, religion, ancestry, age, sex, marital status, national origin, disability or handicap, or veteran status.
www.clinicalink.com",#N/A,201 to 500 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2007,Unknown / Non-Applicable
"CliftonLarsonAllen
3.9",3.9,"Minneapolis, MN",Data Engineer,"The Data Engineer collaborates with internal team members to develop solutions that enable data, insights, analytics and actionable triggers and solutions. S/he utilizes a full range of data technologies including data modeling techniques, data architecture, engineering, data analysis, and reporting for a rapidly growing data analytics platform on Azure.
Essential Job Functions
Pipeline Building: Builds data pipelines utilizing Azure Databricks and Azure Data Factory to build a scalable solutions, following an existing framework and agreed upon design. Operationally monitors and maintains the systems to ensure pipelines continue to run successfully.
Data Solutions: Collaborates with other members of the Data Engineering team to analyze existing software programs and technology processes to ensure effectiveness and efficiency. Troubleshoots data-related problems as needed. Drives strategic initiatives and provides technology solutions for complex business problems. Assists in gathering and analyzing data for business projects, as well as mapping underlying processes and data flows.
New Technology Implementation: Assists with implementing new technology for firm use, as well as creating roadmaps and tools to guide and monitor usage.
Reporting: Routinely provides reports and data extraction as requested by other teams for use within the firm and on behalf of clients.

Requirements
Bachelor’s degree in computer science, mathematics, or related field required. (3 or more years’ experience in data engineering may be considered in lieu of Bachelor’s degree.)
Equal Opportunity Employer /AA Employer/Minorities/Women/Protected Veterans/Individuals with Disabilities.

Wellness at CLA
To support our CLA family members, we focus on their physical, financial, social, and emotional well-being and offer comprehensive benefit options that include health, dental, vision, 401k and much more.
To view a complete list of benefits click
here
.","$92,921 /yr (est.)",5001 to 10000 Employees,Company - Private,Financial Services,Accounting & Tax,1953,$500 million to $1 billion (USD)
"Syngenta Group
4.0",4.0,"Downers Grove, IL",Senior Master Data Engineer,"Company Description

Syngenta Group is one of the world’s leading sustainable agriculture innovation companies, with roots going back more than 250 years. Our 53,000 people across more than 100 countries strive every day to transform agriculture through tailor-made solutions for the benefit of farmers, society and our planet –making us the world’s most local agricultural technology and innovation partner.
Syngenta Group is committed to operating at the highest standards of ethics and integrity. This is a commitment that we are making to investors, customers, society and employees. Syngenta Group is also committed to maintaining a workplace environment free from discrimination and harassment.

Job Description

The Senior Master Data Engineer will be responsible for ensuring Syngenta has the right identity data capabilities to support the current and future Syngenta production and commercial needs.
We have the responsibility to think beyond our past needs and help unlock future opportunities with one of our most valuable data assets. Robust, accurate and trusted identity data will unlock opportunities to improve customer experience, simplify vendor interactions and allow us to explore new ways of marketing our products and services.
The investment in the MDG platform has been the first phase or our journey to support our current and future business needs. The Senior Master Data Engineer will be responsible to build on this first phase and help to define our vision, strategy, operating model, and roadmap for the future of Party data capabilities. This may include supplementing the MDG platform with additional technology and services
Responsibilities
Contributes to creating a breakthrough transformation that shapes the Party data (including Identity and Reference Data) capability in line with Syngenta's strategic vision
Help define & deliver the strategy and roadmap for Identity data and reference data (including operating model, data products, technology, data quality & process analytics/health) ensuring solutions and technologies are maintainable and scalable
Enroll and align with stakeholders to experiment and leverage the Identity data capabilities within relevant domains (Employee, Sales/Customers, Legal Entities, Intercompany, Vendors). Including simplification, automation, rationalization, and harmonization.
Create and advocate Identity data offers (data as a product) that add value and solve business problems that improve integration and adoption.
Provide technical leadership in leveraging and experimenting with appropriate technologies including existing platforms as well as new opportunities (e.g. Microservices, API’s, AI, ML, etc.) to create Identity data products and services to meet business needs.
Seamlessly embed themselves in a cross-functional teams as a subject matter expert and participate in Identity data design authority.
Contribute to the creation and implementation of a reference data capability the compliments Identity master data.
The preferred candidate will be near an established Syngenta location The right candidate will be considered for a remote setting IN THE UNITED STATES.
We are unable to provide Visa Sponsorship for this position at this time.

Qualifications

Required Skills / Experience
Bachelor’s degree with 8 or more years of relevant experience
MUST HAVE hands on experience with a popular MDM tool
Extensive experience in customer master data management
Must have thought leadership and the ability to influence the business with best practices
Experience in reference data principles and practices.
Understand relevant technologies (Master Data Management tools, Microservices, etc)
Stakeholder management / influencing: able to engage with different functions, leadership levels and cultures

Additional Information

What we Offer
A culture that celebrates diversity & inclusion, promotes professional development, and strives for a work-life balance that supports the team members
We offer flexible work options to support your work and personal needs
Full Benefit Package (Medical, Dental & Vision) that starts your first day
401k plan with company match, Profit Sharing & Retirement Savings Contribution
Paid Vacation, 9 Paid Holidays, Maternity and Paternity Leave, Education Assistance, Wellness Programs, Corporate Discounts, among other benefits
Syngenta is an Equal Opportunity Employer and does not discriminate in recruitment, hiring, training, promotion or any other employment practices for reasons of race, color, religion, gender, national origin, age, sexual orientation, marital or veteran status, disability, or any other legally protected status
Family and Medical Leave Act (FMLA)
(http://www.dol.gov/whd/regs/compliance/posters/fmla.htm)
Equal Employment Opportunity Commission's (EEOC)
(http://webapps.dol.gov/elaws/firststep/poster_direct.htm)
Employee Polygraph Protection Act (EPPA)
(http://www.dol.gov/whd/regs/compliance/posters/eppa.htm)

#LI-SB2","$100,820 /yr (est.)",10000+ Employees,Company - Private,Agriculture,Crop Production,2000,$100 to $500 million (USD)
"Axos Bank
3.6",3.6,"San Diego, CA",Junior Business & Technology Analyst - Data Engineer,"Job Summary and Opportunity:
This is an exciting opportunity to join a unique and immersive rotational program as a first step in your career in technology. This full-time rotational program is geared toward providing multi-software platform exposure that focuses on the expansion of knowledge and real-life application within each. We are seeking innovative and energetic individuals who are excited about expanding their skillsets and accelerating their career path with immediate exposure to software applications.
For this position, you will be in the Data Engineer Rotational Program where you will be joining the Axos' Center of ExcellenceTeam. You will get to be a part of a team responsible for the implementation of cutting-edge software driven solutions. As you progress through the program, you will rotate into different complimentary areas within the Data program where roles and responsibilities will change. The final goal of the program is permanent placement within your area of focus. For those looking to make an impact this is where it begins.
In this role you will be focused on SQL related software or software built on direct interactions with SQL. Through the different rotations completed, you will gain the knowledge and skills database development, data quality, and business intelligence reporting to provide enterprise level solutions.
This position is on-site and will be located at our HQ in San Diego, California.
Responsibilities:
Define, prepare, execute and implement data validation and unit and integration testing methods to ensure data quality
Create SSIS packages for data transformation, cleansing, caching, aggregation, staging, and transfer
Analyze and define data flow requirements and prepare applicable system documentation and operation manuals as needed
Code, test and maintain new and existing SQL jobs, stored procedures and functions
Performance tune existing stored procedures, tables and indexes
Troubleshoot problems that may come up with database environments: performance issues, replication issues, or operational issues
Review SQL code written by other developers to ensure compliance to coding standards and best practices as well as maximum performance
Perform data analysis and data profiling tasks to provide support and recommendations for development and design decisions
Develop standardized reporting dashboards to meet the needs of the multiple business units across the Bank
Apply advanced modeling, data mining, machine learning and/or statistical techniques to data and dashboards to generate actionable insights enabling informed decision-making for optimized business and operational performance
Create mock-ups of reporting products, scorecards, dashboards, etc. to provide visualization to the end user
Work with teams within the organization to gather and document reporting requirements
Join client meetings to communicate status, give demos, provide timelines and offer insights
Participate in daily meetings that go over testing, and code reviews
Work with IT, Enterprise Data Management, Project Managers, Business Analysts, stakeholders across multiple business units to systematically plan the launch of new or enhanced dashboards, prepare launch collateral/documentation and work closely with users during through the different phases of a project
Develop deep understanding of the Bank's databases, identify appropriate data sources, relationships and logic needed to produce consistently reliable reports
Contribute to the overall strategy and quality of dashboarding
Document process steps of repetitive tasks performed
Partner with IT and other Infrastructure teams to tackle software upgrades, and coordinate testing
Perform any additional duties as assigned
Requirements:
Bachelor's degree in Information Technology, Computer Science, Business Administration, Mathematics or a related discipline
Customer Obsession: ""Good enough"" isn't good enough for you. You're obsessed with perfecting the customer experience
Leadership: A confident person with the ability to connect and inspire others to achieve success, whether or not they directly report to you
Results Oriented: A driver who possess the ability to take actions and implement effective solutions in a timely manner. Excuses aren’t in your vocabulary because you always find alternative solutions when issues arise
Ethics: Highest level of professional integrity and honesty as well as personal credibility. Your reputation for precedes you in this regard
Innovation: Dedication to maintaining cutting edge talent with the courage to implement new ideas, technology, and aggressively challenge the status quo. You don’t accept responses to new ideas like “That’s the way it’s always been done” because you use facts, data, and people skills to implement meaningful change
Immersion: A propensity to rapidly master the understanding and application of new technology
Excellent verbal and written communication skills, including ability to simplify complex concepts for technical and non-technical audience
Preferred:
Basic to intermediate knowledge in SQL server database development and testing
Working knowledge of Tableau
1+ year's working in an office environment or recent college graduate
APPLY DIRECTLY FOR CONSIDERATION:
Born digital, Axos Bank has reinvented the banking model and grown to over $18.4 billion in assets since our founding in 2000. With a broad and ever-growing range of financial products, Axos Bank is rated among the top 5 digital banks in the country! Axos Financial is our holding company and publicly traded on the New York Stock Exchange under the symbol ""AX"" (NYSE: AX).

We bring together human insight and digital expertise to anticipate the needs of our customers. Our team members are innovative, technologically sophisticated, and motivated to achieve.

Learn more about working here!

A targeted annual base salary range of USD $24/HR - $30/HR, based on the experience, skills, and education/certification required for this position. Eligibility for a discretionary semi-annual incentive compensation plan, based upon performance, payable in cash and/or share grants (RSU’s) that may vest over time. The annual discretionary target bonus percentage is up to 20%.

Axos benefits and perks include:
3 weeks’ Vacation, Sick leave, and Holidays (about 11 a year); Medical, Dental, Vision, Life insurance and more
HSA or FSA account and other voluntary benefits
401(k) Retirement Saving Plan with Employer Match Program and 529 Savings Plan
Employee Mortgage Loan Program and free access to Self-Directed Trading

Pre-Employment Drug Test:

All offers are contingent upon the candidate successfully passing a credit check, criminal background check, and pre-employment drug screening, which includes screening for marijuana. Axos Bank is a federally regulated banking institution. At the federal level, marijuana is an illegal schedule 1 drug; therefore, we will not employ any person who tests positive for marijuana, regardless of state legalization.

Equal Employment Opportunity:

Axos Bank is an Equal Opportunity employer. We are committed to providing equal employment opportunities to all employees and applicants without regard to race, religious creed, color, sex (including pregnancy, breast feeding and related medical conditions), gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship status, military and veteran status, marital status, age, protected medical condition, genetic information, physical disability, mental disability, or any other protected status in accordance with all applicable federal, state and local laws.

Job Functions and Work Environment:

While performing the duties of this position, the employee is required to sit for extended periods of time. Manual dexterity and coordination are required while operating standard office equipment such as computer keyboard and mouse, calculator, telephone, copiers, etc.

The work environment characteristics described here are representative of those an employee may encounter while performing the essential functions of this position. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this position.

#LI-Onsite",$27.00 /hr (est.),1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,2000,$500 million to $1 billion (USD)
"Opensignal
4.5",4.5,"Boston, MA",Senior Data Engineer,"Department: Product/Technology
Location: Boston / US East Coast or Canada

Purpose of Role

We’re looking for a Senior Data Engineer to join our Marketing Performance Group in
transforming our real-world raw data into valuable and credible industry-leading metrics that
provide insights to our analysts and our customers.

What you will be doing
The creation and implementation of a framework to assist in building complex statistical
models. Working closely with our data scientists and our data engineers to create and
evolve products that measure market dynamics in the Telecommunication space that drive
our customers short-term marketing campaign tactics and their longer-term customer
acquisition and retention strategies. This role reports to our Engineering Manager.

We expect our Lead Data Engineer to do:
Own and improve our data pipeline. Assemble large, complex data sets that
meet business requirements, with engineering best practices in mind.
Champion building scalable and resilient data infrastructure, as well as tools to
extract and transform data used by stakeholders and customers.
Be security conscious and sensitive to privacy concerns and legislation related
to the data within the platform.
Have a continuous improvement mindset when it comes to both the platform
and the process.
Work efficiently, automate manual processes where possible, and take a test-
driven approach to engineering.
Take a keen interest in improving the platform’s scalability while understanding
the cost.
Be a good team player, with an agile approach and a can-do attitude.
Keep yourself current and make sure we follow best practices and engineering
standards.
Be an advocate for the platform and its health. Take ownership of your work
from conception through to support.
Be detail orientated and understand the importance of the credibility of our
metrics. Document and communicate with stakeholders in a language
understood by all.
Can work in a fast-paced environment with an ability to shift priorities and focus
on changing requirements and market demands.
Able to coach and mentor Data Engineers in best practices.
Cross-collaborate with the wider team to drive and maintain high standards in
our data pipeline builds.
Comfortable and effective at working in a remote capacity, collaborating with
team members across different locations through digital channels.

What we need from you:
As a Senior Data Engineer, we would expect you to have previous experience in
manipulating, processing, storing, and extracting value from big data.
Advanced with hands-on experience in architecting, crafting, documenting, and
developing highly scalable distributed data processing systems.
Advanced with big data tools, specifically Apache Spark.
Advanced with relational SQL databases. Prior experience with MSSQL,
Postgres, AWS Athena (Presto).
Advanced with SQL query authoring including DBT.
Experience with data pipeline / workflow tools i.e. Apache Airflow.
Experience with AWS cloud services like EC2, S3, managed Kubernetes, AWS
ECS, and Aurora.
Experience with object-oriented/object function scripting languages: Python and
Scala.
Experience in implementing complex clustering and classification models on
large datasets to support new product development.
Experience in writing tests, especially in BDD style and working with Git.
Strong analytic skills in working with unstructured datasets.
Experience in root cause analysis of data when asked to answer specific
business questions.
Experience building and optimizing & ""big data"" data pipelines.
Experience supporting and working with cross-functional teams.
Strong self-organizational skills
Experience being part of a cross-functional team, using agile methodologies.
Bachelor’s degree

For US applicants only

At this time, the company will not sponsor a new applicant for employment sponsorship for this position.
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

About Us
Opensignal is the leading global provider of independent insight and data into network
experience and market performance. Our user-centric approach allows communication
providers to constantly improve their network and maximize commercial performance.
Leading analysts, investors and financial institutions place a high value on our independent
analysis and we are regular contributors to their reports.
Real network experience is our focus and ultimately that’s what influences customer choice.
Our mission is to advance connectivity for all and here at Opensignal, the team is leading
the industry in enabling operators to link their network experience and market performance
in a way that has never before been possible.
With offices in London, Boston and Victoria, British Columbia, we are truly global, with
employees working across four continents and representing over 25 nationalities. We are
an equal opportunity employer dedicated to building an inclusive and diverse workforce.

Benefits:
We believe we are stronger when we not only celebrate our many differences, values, and
voices but include them in everyday practice. Having a diverse and inclusive culture is
essential, which is why we offer a flexible approach to work-life balance, operating in a
remote-hybrid way. We’ll help you get set up with the essentials you need to work from
home or the office. We also offer an attractive range of additional benefits, including:
Competitive compensation packages including a long-term equity program.
Comprehensive group benefits package and company-sponsored retirement
savings plan (details depend on your country of work).
Professional development opportunities: education reimbursement, learning
allowance, company-sponsored workshops, and more!
Generous holiday allowance, sick leave, parental leave, flexibility including Flex
Fridays, and the opportunity to work from abroad.
Charity matching and time off for community volunteering and DE&I
program/committees.
Regular virtual and in-person events and socials.","$148,405 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2010,Unknown / Non-Applicable
"Johns Hopkins University
4.0",4.0,"Baltimore, MD",PARADIM Data Engineer,"The Platform for the Accelerated Realization, Analysis, and Discovery of Interface Materials (PARADIM), a National Science Foundation Materials Innovation Platform developing the next generation of electronic and quantum materials, is looking for a full-time PARADIM Data Engineer to join the platforms materials data science team.

The successful candidate must be capable of designing and implementing software solutions related to data acquisition, streaming, processing, and storage, and should have a keen desire to partner with scientists developing a versatile, real-time, streaming data infrastructure to enable discovery and development of novel interface materials that will drive the quantum computing revolution.

The ideal candidate will also have technical skills that complement a vision and creativity to find solutions to connecting Big Data from diverse experimental and computational laboratories as well as the rigor and experience to develop high-quality, production software and data pipelines to address these challenges. The ability to travel by car to the campuses of both Cornell University in Ithaca, NY, and the Johns Hopkins University in Baltimore, MD, is strongly preferred.

Specific Duties & Responsibilities

With the chief data officer and PARADIM leadership team, design and implement software solutions related to data acquisition, streaming, processing, and storage, both centrally and at end stations at both PARADIM Institutions (JHU and Cornell).
Work with PARADIM scientists to design and implement a versatile, real-time, streaming data infrastructure to enable discovery and development of novel interface materials that will drive the quantum computing revolution.
Implement software tools for data analysis and interpretation as needed by the PARADIM platform.
Participate in the design of back-end and front-end systems to implement PARADIMs data vision.
Research and implement new technologies that could be beneficial to PARADIM.
Evaluate, test and vet new technology in support of PARADIM efforts.
Work with vendors to procure prototypes and demo units.
Attend department and University-sponsored training to increase knowledge, improve skills, and learn new skills.
May substitute University training for supervisor approved commercial job-related course offerings.

This position may be primarily remote (90%). Occasional in-person trips to Cornell University and/or Johns Hopkins University will be needed for work directly involving software on new hardware. This requirement may best suit candidates located between Baltimore, MD and Ithaca, NY.
Minimum Qualifications

Bachelor's Degree
Five years related experience
Additional education may substitute for required experience and additional related experience may substitute for required education, to the extent permitted by the JHU equivalency formula

Preferred Qualifications

The ideal candidate will also have technical skills that complement a vision and creativity to find solutions to connecting Big Data from diverse experimental and computational laboratories as well as the rigor and experience to develop high-quality, production software and data pipelines to address these challenges. The ability to travel by car to the campuses of both Cornell University in Ithaca, NY, and the Johns Hopkins University in Baltimore, MD, is strongly preferred.

Proficiency in at least one major object-oriented language such as Java, C++, or C#
Proficiency in at least one major software versioning and tracking platform (e.g. git, github, gitlab, svn)
Experience with python and pydata packages such as jupyterlab, cython, numpy, tensorflow
Experience working with instrumental data
Contributions towards open-source software and commitment to open-source development
Experience with Apache Kafka or Confluent Platform
Level of Independent Decision Making
Works independently

Classified Title: Systems Engineer
Role/Level/Range: ATP/04/PE
Starting Salary Range: $71,230 - $97,880 - $124,510 annually (Commensurate with experience)
Employee group: Full Time
Schedule: Monday-Friday, 8:30 am - 5:00pm
Exempt Status: Exempt
Location: Homewood Campus (Hybrid)
Department name: Chemistry
Personnel area: School of Arts & Sciences

Total Rewards
The referenced salary range is based on Johns Hopkins University's good faith belief at the time of posting. Actual compensation may vary based on factors such as geographic location, work experience, market conditions, education/training and skill level. Johns Hopkins offers a total rewards package that supports our employees' health, life, career and retirement. More information can be found here: https://hr.jhu.edu/benefits-worklife/

Please refer to the job description above to see which forms of equivalency are permitted for this position. If permitted, equivalencies will follow these guidelines:

JHU Equivalency Formula: 30 undergraduate degree credits (semester hours) or 18 graduate degree credits may substitute for one year of experience. Additional related experience may substitute for required education on the same basis. For jobs where equivalency is permitted, up to two years of non-related college course work may be applied towards the total minimum education/experience required for the respective job.

**Applicants who do not meet the posted requirements but are completing their final academic semester/quarter will be considered eligible for employment and may be asked to provide additional information confirming their academic completion date.

The successful candidate(s) for this position will be subject to a pre-employment background check. Johns Hopkins is committed to hiring individuals with a justice-involved background, consistent with applicable policies and current practice. A prior criminal history does not automatically preclude candidates from employment at Johns Hopkins University. In accordance with applicable law, the university will review, on an individual basis, the date of a candidate's conviction, the nature of the conviction and how the conviction relates to an essential job-related qualification or function.

The Johns Hopkins University values diversity, equity and inclusion and advances these through our key strategic framework, the JHU Roadmap on Diversity and Inclusion .

Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

EEO is the Law

Learn more:
https://www.eeoc.gov/sites/default/files/migrated_files/employers/poster_screen_reader_optimized.pdf

Accommodation Information

If you are interested in applying for employment with The Johns Hopkins University and require special assistance or accommodation during any part of the pre-employment process, please contact the Talent Acquisition Office at jhurecruitment@jhu.edu . For TTY users, call via Maryland Relay or dial 711. For more information about workplace accommodations or accessibility at Johns Hopkins University, please visit accessibility.jhu.edu .

Johns Hopkins has mandated COVID-19 and influenza vaccines, as applicable. Exceptions to the COVID and flu vaccine requirements may be provided to individuals for religious beliefs or medical reasons. Requests for an exception must be submitted to the JHU vaccination registry. For additional information, applicants for SOM positions should visit https://www.hopkinsmedicine.org/coronavirus/covid-19-vaccine/ and all other JHU applicants should visit https://covidinfo.jhu.edu/health-safety/covid-vaccination-information/ .

The following additional provisions may apply, depending on campus. Your recruiter will advise accordingly.

The pre-employment physical for positions in clinical areas, laboratories, working with research subjects, or involving community contact requires documentation of immune status against Rubella (German measles), Rubeola (Measles), Mumps, Varicella (chickenpox), Hepatitis B and documentation of having received the Tdap (Tetanus, diphtheria, pertussis) vaccination. This may include documentation of having two (2) MMR vaccines; two (2) Varicella vaccines; or antibody status to these diseases from laboratory testing. Blood tests for immunities to these diseases are ordinarily included in the pre-employment physical exam except for those employees who provide results of blood tests or immunization documentation from their own health care providers. Any vaccinations required for these diseases will be given at no cost in our Occupational Health office.

Note: Job Postings are updated daily and remain online until filled.

To apply, visit https://jobs.jhu.edu/job/Baltimore-PARADIM-Data-Engineer-MD-21218/1027795600/

jeid-8befc9e383ba7d4394f407df6f0f70f2","$124,510 /yr (est.)",10000+ Employees,College / University,Education,Colleges & Universities,1876,$1 to $5 billion (USD)
"Boston Dynamics AI Institute
4.7",4.7,"Cambridge, MA",Data Engineer,"Our Mission
Our mission is to solve the most important and fundamental challenges in AI and Robotics to enable future generations of intelligent machines that will help us all live better lives.

Data Engineers will work cross-functionally, creating new technology to improve software development for robots. If you have a passion for developing technology for robots and use it to advance their capabilities and usefulness, you will want to join us! We are onsite in our new Cambridge, MA office where we are building a collaborative and exciting new organization.
Responsibilities
Work collaboratively with research scientists and software engineers on software development for a range of different robotic platforms
Develop and maintain our data warehouses and data pipelines in cloud and on-premise infrastructureBuild event and batch driven ingestion systems for machine learning and R&D as needed
Develop and administer databases, knowledge bases, and distributed data stores
Create and use systems to clean, integrate, or fuse datasets to produce data products
Establish and monitor data integrity and value through visualization, profiling, and statistical tools
Perform updates, migrations, and administration tasks for data systems
Develop and implement a data governance, data retention strategyUse Python and SQL to develop, maintain and scale our data stores
Requirements
BS/MS in computer science, robotics, or a related field
5+ years of experience in a data engineering or similar role
Demonstrated experience with a variety of relational database and data warehousing technology such as AWS Redshift, Athena, RDS, BigQuery
Demonstrated experience with big data processing systems and distributed computing technology such as Databricks, Spark, Sagemaker, Kafka, etc
Strong experience with ETL design and implementations in the context of large, multimodal, and distributed datasets
Bonus (Not Required)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)2+ years of experience with Airflow
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.","$112,287 /yr (est.)",201 to 500 Employees,Subsidiary or Business Segment,Information Technology,Computer Hardware Development,1992,$5 to $25 million (USD)
"TECKpert
4.9",4.9,"Miami, FL",Data Engineer,"We are looking for a Data Engineer to support our client based in Miami, Florida.
US BASED CANDIDATES ONLY.
This is an hybrid position. Candidates must be located in or near Miami, Florida.
Who we are
Founded in 2009 and headquartered in beautiful Miami, FL, TECKpert is a tech consulting and staff augmentation firm. At TECKpert, we offer a contingent workforce built for any size digital transformation project. Experts in design, development, IT, analytics and marketing, provide innovative digital solutions to achieve success in our new economy. Our leaders identify the technical talent best suited to bolster our client’s capabilities, across all industries, including, healthcare, government, finance, legal, real estate, and startups.
The project
TECKpert seeks to hire a Data Engineer based in Miami, FL to support our client, a large healthcare system based in Miami, FL with centers throughout the United States.
The Data Engineer leads data integration and analytics projects that support data collection, automation, transformation, storage, delivery, and reporting processes. Optimizes data retrieval and processing, including performance tuning, delivery design for down-stream analytics, machine learning modeling (including feature engineering), and reporting.
Responsibilities
Lead data engineering projects and collaborate with stakeholders to develop end-to-end solutions, including designing data structures for downstream analytics, machine learning modeling, feature engineering, prototype development, and reporting.
Assist in all stages of data orchestration, including working with diverse data sources, data cleaning, data transformation, ETL/ELT processes, and data visualization.
Create analytics solutions using Azure Cloud tools, leveraging the capabilities of the platform.
Design and engineer efficient data pipelines for data collection, processing, and distribution using appropriate data platform infrastructure.
Build visualizations to extract meaningful business insights and construct cloud data warehouses/data marts utilizing Azure Data Lake.
Extract data from relational and structured/unstructured sources, perform data analysis, identify correlations, patterns, and other relevant insights.
Identify gaps in master data and transactional data through data analysis.
Assist in defining and maintaining reporting and dashboard standards, guidelines, and processes to ensure high-quality data.
Compensation and Term
This opportunity is for a full-time, ongoing need and pay commensurate with experience up to $96,000 to $122,000 per year. Medical, dental & vision insurance, employee mental health program, paid time off, paid holidays, 401(k) with employer match, employee stock purchase program, tuition reimbursement and much more.
Qualifications
Bachelor's degree in Computer Science, Math, Statistics, Economics, Accounting, Business, or a related field.
Minimum of 2 years of hands-on development experience in building analytics solutions using Microsoft Azure Cloud, with knowledge of Cloud Security, DevOps, Governance, and Data privacy.
Proficiency in programming languages such as Python, Java, Scala, and SQL. Familiarity with database systems, distributed computing systems, and big data technologies (e.g., Hadoop, Spark, Kafka).
Experience in developing and supporting database systems for medium to large organizations, including database structure systems, data management resources, data mining, and data modeling.
Implement data pipelines for Azure Analysis Services reporting data models.
Conduct complex analyses of business data and processes.
Provide strategic and analytic models to address key business questions.
Collect, organize, manipulate, and analyze diverse datasets.
Track and report on the performance of deployed models.
Assist in developing dashboards to facilitate strategic decision-making by executives.
Perform data studies and product experiments related to new data sources or novel applications of existing data sources, interpreting the results effectively.

crqHwoeq0Q","$109,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Internet & Web Services,2009,$1 to $5 million (USD)
"Abile Group, Inc.
5.0",5.0,"Saint Louis, MO",Data Cloud Engineer - Master,"Overview:
Abile Group has an exciting and challenging opportunity for a Data Cloud Engineer-Master on a 10 year contract providing User Facing and Data Center Services supporting an Intelligence Community customer. All the personnel on the team will work together to support innovative design, engineering, procurement, implementation, operations, sustainment and disposal of user facing and data center information technology (IT) services on multiple networks and security domains, at multiple locations worldwide, to support the IC mission.

The right candidate will possess the below skills and qualifications and be ready to handle all responsibilities independently and professionally.
Responsibilities:
Provides technical/management leadership on major tasks or technology assignments.
Establishes goals and plans that meet project objectives. Has domain and expert technical knowledge.
Directs and controls activities for a client, having overall responsibility for financial management, methods, and staffing to ensure that technical requirements are met.
Interactions involve client negotiations and interfacing with senior management.
Decision making and domain knowledge may have a critical impact on overall project implementation.
May supervise others.
Qualifications:
Clearance Required: TS/SCI

Degree and Years of Experience: BS/BA and 10 -15 years of relevant experience

Required Skills:
Experience in the various aspects of hybrid cloud activities.
Supports procurement and deployment of Platform Services to enable application portability across the private and public cloud environments offered by NGA.
Readies NGA's Hybrid Cloud Environment for system migration to IC ITE and oversee the future expansion of NGA Hybrid Cloud to additional public clouds.
In concert with DCS Government, supports standardization of DCS operations in a NGA Hybrid Cloud Management environment.
Transforms Government cloud requirements into appropriate technological alternatives and provides expertise in hybrid virtualization and cloud environments.
Experience developing systems, products, and/or processes based on a total systems perspective.
Consults, plans, analyzes designs, develops tests, assures quality, configures, installs, implements, integrates, maintains, and manages systems.
Has and maintains a diverse set of skills across multiple technical disciplines with recognized expertise in multiple disciplines and possess advanced knowledge of multiple mature and emerging technologies.
Works across organizational boundaries, both internally and externally and helps to drive the relationship between technical solutions and business needs of customers. Analyzes, defines and documents customer needs and required functionality.
Designs, develops and tests theoretical and/or physical models and develops the system design, considering operational impacts, performance, testing, manufacturing, cost and schedule, training, maintenance, and support.
Performs system level design trade analysis, reviews and approves system specifications and description documents, determines how a system is to be built, tested, and implemented, plans the system development execution and ensures adherence to appropriate standards, policies, principles, and practices.
Analyzes system capacity and performance to support problem resolution and system enhancements and monitors systems tests.
Responds to inquiries from a variety of sources for the purpose of providing technical assistance, consultation, advice and support, and regularly provides advice and recommends actions and solutions involving complex issues.

About Abile Group, Inc.:
Abile Group, Inc. was formed in July 2004 to partner with the Intelligence Community and their Contractors in the areas of Enterprise Analytics & Performance Management, IT & Systems Engineering and Program & Project Management. We have significant experience with the Federal Government and are an EDWOSB dedicated to our employees and clients. We are looking for high performing employees who enjoy providing advice and guidance along with solutions development and implementation support, crafted by combining industry best practices with the clients’ subject matter experience and Abile’s breadth of expertise.
EEO Statement:
Abile Group, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. Anyone requiring reasonable accommodations should email careers@abilegroup.com with requested details. A member of the HR team will respond to your request within 2 business days.

Please review our current job openings and apply for the positions you believe may be a fit. If you are not an immediate fit, we will also keep your resume in our database for future opportunities.",#N/A,51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Financial Information Technologies LLC
3.7",3.7,"Tampa, FL",Senior Data Engineer,"Join Fintech as a Senior Data Engineer!
Fintech is the leading business solutions provider for the beverage alcohol industry, empowering alcohol suppliers, distributors, and retailers with smart solutions that simplify beverage alcohol management. Our unique, thriving company culture promotes collaboration and growth at every level, and our comprehensive employee benefits have earned Fintech the title of a Tampa Bay Times Top 100 Workplaces for 2020 and 2021.
Fintech’s Senior Data Engineer brings a depth of relational database modeling and an understanding of transactional processing across a myriad of database types. They can analyze and assess new data sets to understand nuances of content in the context of purpose with an ability to conceptualize cleansing, harmonization, and modeling efforts. Working under the direction of the principal process architect the senior data engineer will lead a small team of experienced data wranglers to tackle a myriad of ad-hoc custom projects as well as service the development needs within our warehouse and app abstraction layers.
Essential Functions:
Collaborates with ELT/process automation, data insights, and data science teams
Builds data models in accordance with prescribed methodologies
Serves as knowledgeable backstop for level III ticket resolution
Guides and instructs junior developers and engineers on how to implement directives in accordance with project needs within adopted framework
Gains a familiarity with and contributes to the core meta-data driven data processing engine
Advising on data model consumption in analytics layers
Contributes to knowledge base
Qualifications:
8 + years of experience with SQL in multiple database flavors (SQL Server, Oracle, Snowflake, Postgres, Greenplum)
5 + years of experience with data ingest transformations and harmonization
5 + years of experience with database object creation and modeling
Analytical thinker that can adapt and problem solve in a fast-paced environment
Team oriented
Must be able to consume, understand, and implement a complicated but flexible processing back-end in a short time frame
Our Benefits:
Employer Matched 401K (Up to 10% of Employee Salary)
Company Paid Medical Insurance Option (Employee and Dependent Children)
Company Paid Dental Insurance Option (Employee only)
Company Paid Vision Insurance Option (Employee only)
Company Paid Long and Short-Term Disability
Company Paid Life and AD&D Insurance
Employee Recognition Program
18 PTO Days a Year
Six Paid Holidays
Business Casual Dress Code
Check out www.fintech.com for more information!
We E-Verify.
Fintech is an Equal Opportunity Employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth and pregnancy-related conditions), gender identity or expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, genetic information, or any other characteristic protected by applicable federal, state, or local laws and ordinances. Fintech’s management team is dedicated to this policy with respect to recruitment, hiring, placement, promotion, transfer, training, compensation, benefits, employee activities, access to facilities and programs and general treatment during employment.
Fintech is a Drug-Free Workplace.","$109,213 /yr (est.)",51 to 200 Employees,Company - Private,Financial Services,Financial Transaction Processing,1991,$25 to $100 million (USD)
"Chmura Economics and Analytics
3.6",3.6,"Cleveland, OH",Data Engineer,"Description

Founded in 1998, Chmura Economics & Analytics is headquartered in Richmond, Virginia’s historic Shockoe Slip with a regional office in Cleveland, Ohio. We provide labor market software, consulting, and data so our clients can make informed decisions that grow their communities and organizations.

For example, our technology:
Helps economic developers understand their local industries and labor market
Allows workforce development practitioners to guide workers to high-wage, high-demand jobs
Assists educators in training their students for well-paying, in-demand careers
Helps site selectors choose the best site for their clients’ expansion or relocation by understanding the talent availability in competing locations
Allows staffing and recruitment firms to determine competitive wages
We’re more than a technology company – we help our clients win.
Our employees are encouraged to think differently, ask challenging questions, and pursue what’s best for our clients.

We want our clients to make confident decisions. If you want to help communities and organizations thrive, you’ve come to the right place.

Responsibilities
Implement, maintain, and continuously optimize data processing solutions for a wide variety of complex big data sets.
Architect, design, develop, and maintain data integration solutions as they relate to all stages of extract, transform, and load (ETL) pipelines.
Build internal tooling that serves to improve upon the ability to create, test, build, serve, compare, and optimize complex data models and their related data sets.
Troubleshoot data issues and effectively triage timely solutions.
Contribute to and maintain documentation as it relates to new and existing data models, processes, storage, and optimization techniques.
Requirements
Experience in a relevant programming language: C#, Python, Java, etc.
Experience with Elasticsearch
Experience with containerization platforms (Docker, K8S, etc)
Experience with schema design and writing queries for SQL Server, Postgres or similar
Azure experience
Kanban/Agile experience
Familiarity with machine learning and NLP is nice to have but not required
At least 2 years. This is not an “junior” position.

We offer a comprehensive compensation and benefits package.
Salary is commensurate with experience.
Chmura is not able to provide sponsorship for this role.

Chmura is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, physical and mental disability, genetic information, marital status, sexual orientation, gender identity/assignment, citizenship, pregnancy or maternity, protected veteran status, or any other status prohibited by applicable national, federal, state or local law. Chmura promotes a drug-free workplace. Chmura will consider for employment, qualified applicants with a criminal history in a manner consistent with the requirements of applicable federal, state, and local laws and regulations regarding criminal background inquiries, including, to the extent applicable, following applicable federal, state, and local laws and regulations regarding criminal background inquiries.","$90,299 /yr (est.)",1 to 50 Employees,Company - Public,Management & Consulting,Research & Development,1998,Unknown / Non-Applicable
"VISUAL SOFT, INC
4.1",4.1,"Washington, DC",Data Engineer - Active TOP SECRET - REMOTE-ONSITE,"Visual Soft, Inc is seeking qualified candidates to work on our efforts with a Prime for their end customer, a federal agency.

Position: Data Engineer - (50% REMOTE and 50% ONSITE)
Location: Washington, DC or Crystal City, Arlington, VA
Shift time: 8 am to 5 pm

JOB DESCRIPTION:
As a Data Engineer, you’ll implement data engineering activities on some of the most mission-driven projects in the industry. You’ll deploy and develop pipelines and platforms that organize and make disparate data meaningful. You will collaborate and work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You’ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients.
**Desirable skills include, Spark, Databricks, Data Lakes, Bigdata Tools and Technologies and AWS
Years of Experience:: 5+ years of experience
Education Requirement: BS degree preferred
Clearance requirement: Top SECRET is a MUST

Standard Benefits:
Our standard benefits include: Our standard benefits include 3 weeks of Paid time off (PTO that includes sick leave). Any unused PTO will be issued as a check at the end of an employee's anniversary with us. we also provide 2 floating and 8 public holidays. Floating and holidays expire at the end of every year of service of an employee. In addition, company will cover 50% of health and dental insurances only for all full time employees, however, dependents can be added at extra cost. Employee's health and dental coverage becomes effective after 30 days or first of the month after an employee completes initial 30 working days, we cover 50% for the employee's health and dental insurances. Dependents coverage for health and dental insurances is available as an out of pocket expense for employees. An employee has to finish all of your paper work for health and dental in the first 30 days of your employment with us. We provide STD, LTD and one time salary equivalent of life insurance at NO cost to all full time employees. All full time employees or w-2 employees with no benefits will be eligible to participate in company's 401k program after 90 days of employment with a company match of 4%, immediate vesting. In addition, all w-2 employees are eligible to be part of company's profit sharing, no employee contributions required. No commuting and/or parking expenses provided.","$98,574 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,$1 to $5 million (USD)
Wingsoft Consulting LLC,#N/A,Remote,AWS Data Engineer – W2 Role,"Greetings,
This is Deepak Sharma Technical recruiter from Wingsoft consulting LLC. I have urgent opening for AWS Data Engineer for Charlotte, NC Locationwith one of our direct client. Please let me know if you or anyone interested and available for this role!!
Title: AWS Data Engineer – W2 Role
Location: Charlotte, NC (onsite would be preferred) will consider remote
Duration:1 year contract +
Job Description
AWS Data Engineer Lead, Core Technical Skills
1) 5+ years of AWS experience
2) Experience in a regulated environment
3) AWS services - S3, EMR, Glue Jobs, Lambda, Athena, CloudTrail, SNS, SQS, CloudWatch, Step Functions
4) Experience with Kafka/Messaging preferably Confluent Kafka
5) Experience with databases such as Document DB, MySQL, Postgres, Glue Catalog, Lake Formation, Redshift, DynamoDB and Aurora and SQL
6) Tools and Languages – Python, Spark, PySpark
7) Experience with Secrets Management Platform like Vault and AWS Secrets manager
8) Experience with Event Driven Architecture
9) Experience with Rest APIs and API gateway
10) Experience with AWS workflow orchestration tool like Airflow or Step Functions
AWS Data Engineer Lead Additional Technical Skills (nice to have, but not required for the role)
11) Experience with native AWS technologies for data and analytics such as Kinesis, OpenSearch
12) Databases - Document DB, MongoDB Atlas
13) Data Lake platform (Hive, Druid, Apachi Hudi/Apache Iceberg/Databricks Delta)
14) Java, Scala, Node JS, Pandas
15) Workflow Automation
16) Experience transitioning on premise big data platforms into cloud-based platforms such as AWS
17) Strong Background in Kubernetes, Distributed Systems, Microservice architecture and containers
18) Day to Day Responsibilities/project specifics:
a. Provides technical direction, guides the team on key technical aspects and responsible for product tech delivery
b. Lead the Design, Build, Test and Deployment of components
i. Where applicable in collaboration with Lead Developers (Data Engineer, Software Engineer, Data Scientist, Technical Test Lead)
c. Understand requirements / use case to outline technical scope and lead delivery of technical solution
d. Confirm required developers and skillsets specific to product
e. Provides leadership, direction, peer review and accountability to developers on the product (key responsibility)
f. Works closely with the Product Owner to align on delivery goals and timing
g. Assists Product Owner with prioritizing and managing team backlog
h. Collaborates with Data and Solution architects on key technical decisions
i. The architecture and design to deliver the requirements and functionality
i. Mentor other developers in development of components and related processes
Job Type: Contract
Pay: Up to $90.00 per hour
Schedule:
8 hour shift
On call
Experience:
AWS: 7 years (Preferred)
Security clearance:
Confidential (Preferred)
Work Location: Remote",$90.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Mass General Brigham
3.8",3.8,"Somerville, MA",Sr. Data Engineer (Data Lakes),"Sr. Data Engineer (Data Lakes)
- (3244480)

About Us:
At Mass General Brigham Digital, we pride ourselves on our ability to create maximum strategic, clinical, and operational value from established and emergent technologies for our patients, care teams, researchers, and employees. Digital health will not only enhance the equity and efficiency of healthcare delivery, but it will also help make medicine more personalized and precise.
We recognize that increasing value and continually improving quality while maintaining an inclusive focus are essential to organizational excellence, and we invite you to join us on this journey. The work we do in Digital is a strategic imperative, and there is a strong and growing understanding of how together we will transform Mass General Brigham in innovative and impactful ways.
General Summary/ Overview:
At Mass General Brigham Digital, we pride ourselves on our ability to create maximum strategic, clinical, and operational value from established and emergent technologies for our patients, care teams, researchers, and employees. Digital health will not only enhance the equity and efficiency of healthcare delivery, but it will also help make medicine more personalized and precise.
We recognize that increasing value and continually improving quality while maintaining an inclusive focus are essential to organizational excellence, and we invite you to join us on this journey. The work we do in Digital is a strategic imperative, and there is a strong and growing understanding of how together we will transform Mass General Brigham in innovative and impactful ways.
Summary:
Reporting to the Engineering Manager, Data Lake, the Senior Data Engineer (Azure Data Lake) will work towards analyzing, designing, developing, and building ADF data pipelines, ELT/ETL frameworks, and Azure data lake platforms, primarily focusing on Epic (EHR) data and other healthcare data; and will thrive as a member of an experienced, high performing and highly motivated team. Role will be responsible for participating in building out our existing EDW and our new Data Lake, expanding our data ecosystem and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Requires advanced experience with data engineering and building Azure Cloud Data Lake, Azure Big Data Analytics technologies and architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures, and data sets. Expert level of experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Azure Data Bricks, Azure ML, SQL Data Warehouse. Advanced Experience with Hadoop based technologies (e.g., hdfs, Spark) and Programming experience in Python, SQL, Spark.
Principal Duties and Responsibilities:
Design, Develop, construct, test and maintain Data Lake architectures and large-scale data processing systems.
Support big data ecosystem related Tool selection and POC analysis.
Gather and process raw data at scale that meet functional / non-functional business requirements (including writing scripts, REST API calls, SQL Queries, etc.).
Develop data set processes for data modeling, mining and production.
Integrate new data management technologies ( Informatica DQ..) and software engineering tools into existing structures.
The candidate will be responsible for participating in building out our Data Lake platform, expanding and optimizing our data ecosystem and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.
The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up.
The Data Engineer will support our Software Developers, Database Architects, Data Analysts and Data Scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements on cloud based data platforms (e.g. Azure) and relational data systems (SQL Server, SSIS).
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.
Build the data infrastructure required for optimal extraction, transformation, and loading of data from traditional/legacy data sources.
Work with stakeholders including the Management team, Product owners, and Architecture teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Use/s the Mass General Brigham values to govern decisions, actions, and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability & Service Commitment, Decisiveness, Innovation & Thoughtful Risk; and how we treat each other: Diversity & Inclusion, Integrity & Respect, Learning, Continuous Improvement & Personal Growth, Teamwork & Collaboration.
Working Conditions:
This is a remote position.
Diversity Statement
As a not-for-profit organization, Mass General Brigham is committed to supporting patient care, research, teaching, and service to the community. We place great value on being a diverse, equitable and inclusive organization as we aim to reflect the diversity of the patients we serve. At Mass General Brigham, we believe in equal access to quality care, employment and advancement opportunities encompassing the full spectrum of human diversity: race, gender, sexual orientation, ability, religion, ethnicity, national origin and all the other forms of human presence and expression that make us better able to provide innovative and cutting-edge healthcare and research.

5+ Years of experience data engineering and building Azure Cloud Data Lake technologies and architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures and data sets.
5-7 Years of Experience with Hadoop based technologies (e.g. hdfs, Spark). Spark Experience desirable
5+ years of Programming experience in Python, SQL, PySpark.
Healthcare experience, most notably in Clinical data, Epic, Clarity, Caboodle, Payer data and reference data is a plus but not mandatory.
Experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Snowflake, Azure Data Bricks, Powershell.
Experience with Design and Architecture of relational SQL and NoSQL databases, including MS SQL Server, Cosmos DB.
Experience with Design and Architecture of data security and Azure security, VM, Vnet.
Experience with building processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience leading and working with cross-functional teams in a dynamic environment.
Experience building Big data pipeline with Spark and/or Data Bricks is a plus.
Leading development of Data Lake Architectures from scratch.
Experience with Azure DevOps/CI-CD, Continuous integration and deployment.
Experience with Real time analytics on Spark, Kafka, Event Hub is a plus.
Experience in petabyte scale data environments and integration of data from multiple diverse sources.
Skills/Abilities/Competencies:
Advanced hands-on SQL, Spark, Python, pySpark (2+ of these) knowledge and experience working with relational databases for data querying and retrieval.
Strong SQL skills on multiple platform (preferred MPP systems).
Data Modeling tools (e.g. Erwin, Visio).
Strong interpersonal and communication skills, both written and verbal.
Strong Scrum/Agile development experience.
Excellent organizational skills and attention to detail, manage multiple tasks and projects, meet deadlines, follow through, and manage to schedule.
Strong innovation capabilities and the ability to think creatively.
Strong collaboration and team building skills within, across and outside of an organization.
Maintain and promote a positive team environment.
Maintains stable performance under pressure, demonstrating sensitivity to diverse organizational culture.
Ability to effectively cope with change, remain flexible and adaptable within a fast-paced environment with rapidly changing requirements, and ability to negotiate situations when the big picture is not clearly defined.

EEO Statement

Mass General Brigham is an Equal Opportunity Employer. By embracing diverse skills, perspectives, and ideas, we choose to lead. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under the law. We will ensure that all individuals with a disability are provided a reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment.

Primary Location MA-Somerville-MGB Assembly Row
Work Locations MGB Assembly Row 399 Revolution Drive Somerville 02145
Job Business and Systems Analyst
Organization Mass General Brigham
Schedule Full-time
Standard Hours 40
Shift Day Job
Employee Status Regular
Recruiting Department MGB Digital
Job Posting May 12, 2023","$118,726 /yr (est.)",1001 to 5000 Employees,Nonprofit Organization,Healthcare,Health Care Services & Hospitals,1994,$10+ billion (USD)
"Orange County's Credit Union
3.9",3.9,"Santa Ana, CA",Data Engineer,"Great Opportunity At Orange County's Credit Union
Must reside in the state of CA, AZ, NV or TX.
Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.
Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.
Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.
Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!
More about Orange County's Credit Union:
Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.
As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.
Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!
Overview:
Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.
Essential Functions:
Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).
Technical Must Haves for this Role:
5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.","$91,788 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1938,$25 to $100 million (USD)
"Yes Energy, LLC
4.6",4.6,"Boulder, CO","Data Operations Engineer II (Hybrid) Boulder - CO, Chicago - IL, Dedham - MA or Houston - TX","Data Operations Engineer II

Join the Market Leader in Electric Power Trading Solutions
The electrical grid is the largest and most complicated machine ever built. Yes Energy’s industry leading electric power trading analytics software provides real time visibility into the massive amount of data that is generated by the North American electrical grid every day. Our unique and innovative view of the data informs real time trading decisions that keep utility prices low and the grid up and running. It’s both challenging work and work with a purpose.
Be a part of our successful, growing business.
We are currently working in a hybrid environment and are seeking to fill one full time Data Operations Engineer II position immediately in Boulder - CO (HQ), Chicago - IL, Dedham - MA or Houston - TX.
About the team
At Yes Energy, our Market Data Operations (MDO) team plays a crucial role in our business. We are the control room for our customers, responsible for ensuring access to reliable and timely data every minute of every day.
We take pride in our responsibility to maintain the accuracy and reliability of our data. We understand that our clients rely on us to provide them with the information they need to make informed decisions, and we take that responsibility very seriously. Like a control room operator who is constantly monitoring the grid and making adjustments to ensure stability, our team is constantly monitoring our data pipelines and making adjustments to ensure data accuracy and reliability.
Our team is passionate about what we do, and we are dedicated to helping clients navigate the complex and dynamic North American Energy Markets. We work together in a collaborative environment, and we look to continuously improve our processes to ensure that we are providing the highest quality data possible to our clients.
About you
You have a passion for working with inherently messy data
You believe that a deep understanding of the data leads to better solutions
You have a competitive attitude, taking ownership and accountability of the work you produce
You have strong problem-solving skills and a curious mindset
You have experience maintaining and designing data pipelines
Like to design, develop, analyze and troubleshoot PL/SQL code

What you will do
Maintain our real-time data pipelines and ensure so that we can provide reliable and accurate information to our clients
Support clients by answering complex data questions, providing timely and effective solutions, so that we can empower our clients to make informed decisions
Ensure highest possible quality and integrity of Yes Energy data; recommend and implement ways to improve data reliability, efficiency, and quality
Participate in weekly on-call rotations to help resolve critical data pipeline failures for our clients

Requirements
4+ years of SQL or equivalent experience
4+ years of Oracle PL/SQL or equivalent experience
Bonus points for
Experience with ETL and complex data pipelines
Energy industry experience or experience in equities/commodities trading
Experience with web scraping, including HTML parsing, HTTP protocols and network logs
Experience with Python and Bash Scripting
Familiarity with Agile development methodologies
Position Details
Full-time
Reports to Data Operation team Lead
Minimal travel may be required (up to 10 days per year)
Keywords:
Oracle, SQL, PL/SQL, REST API, Time Series Data, ETL, CLI tools.
About Yes Energy
Overview
Yes Energy delivers real-time market data and electric power trading decision solutions. Over 1,000 market participants use Yes Energy solutions daily. The business is a leader in all aspects of information content collection and management, as well as in developing and delivering data and market analytics solutions. Since its inception in 2008 Yes Energy has become a trusted and respected supplier of innovative and reliable solutions focused on the needs of power market analysts, traders and trade managers. Yes Energy has a team of amazing professionals located in Boulder, CO (HQ), Dedham, MA and Chicago, IL.
Culture
At Yes Energy we care about saying “Yes” to customers. We like to listen and learn, and develop our solutions in line with our customers’ needs. We think about customers as business partners and when we help them to be more successful … We are more successful too.
Around the office our culture is driven by some pretty fundamental values that we’re proud of:
We love innovation and solving tough challenges;
We are “high standards people” who combine passion and pride with hard work and rewards of all kinds- in an ethic that is consistent across the company;
We’re team-focused with a flat hierarchy- we work in small teams on well-defined projects that directly impact the success of the business;
We play to the strengths and experience of each person, while each of us also works along a continuum of roles adjacent to our focus area. This presents a challenge of maintaining a broad set of skills as well as an opportunity to learn and contribute in many ways;
We are constantly growing. Professional development happens every day and every year.
Compensation and Benefits
Salary Range: $90,000 - $120,000 plus bonus.
We offer strongly competitive salaries and real bonuses that are achievable and that you can impact. Our benefits package is also very competitive and it includes medical insurance, 401K Plan with matching, flexible vacation and flexible work schedules. Investment in both formal and informal professional development is encouraged and funded by Yes Energy.
At Yes Energy we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
Yes Energy provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Yes Energy complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.","$105,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2008,Unknown / Non-Applicable
"Millennium Physician Group
3.4",3.4,"Fort Myers, FL",IT Data Engineer,"Millennium Physician Group is seeking an experienced Data Engineer to join our Technology and Innovation Services Team in Fort Myers, FL. t
The Data Engineer role is responsible for building the foundation for a Datawarehouse using best practice ETL/ELT methodologies. This position will develop and maintain data pipelines, interfaces, and process automation. The Data Engineer will be required to investigate and understand datasets from dozens of discrete sources that may or may not always have available documentation. This position must communicate highly complex data trends to organizational leaders in a way that's easy to understand. The position requires collaborating with the existing BI/Analytics teams, Software and Database developers to create a centralized repository and platform to be used by all data consumers in the organization. The Data Engineer must demonstrate advanced knowledge of SQL, OOP development concepts, and Business Intelligence/Analytics. The position requires the ability to develop code for use in the automation of data pipelines. This position reports to the Data engineering Manager.
Essential Duties and Responsibilities include the following. Other duties may be assigned.
Build scalable data pipelines that clean, transform, and aggregate data from disparate sources
Assemble large, complex data sets that meet functional / non-functional business requirements
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Snowflake technologies
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Implements processes and systems to monitor data quality to ensure production data is always accurate and available
Writes unit/integration tests, contributes to engineering wiki, and documents work
Defines company data assets (data models)
Designs data integrations and data quality framework
Build analytics tools that utilize the data pipeline to provide actionable insights into key business performance metrics
Works closely with a team of frontend and backend engineers, developers, and analysts

Education and Qualifications:
Master's Degree preferred, Bachelor's required in Computer Science, Information Technology, Informatics, Engineering, Statistics or equivalent.
4+ Years Prior experience as a Data Engineer, Data Scientist, or Database Administrator required
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience with object-oriented/object function scripting languages: Python, Java, C#, Scala, etc.
Experience with Cloud Datawarehouse technologies (Snowflake/Redshift) and design
Ability to create and maintain optimal data pipeline architecture
Hands-on experience building and maintaining ETL/ELT processes
Experience interacting with and extracting data from RESTful APIs
Experience with commercial ETL/ELT toolsets: Matillion, Talend, Fivetran, etc.
Experience building data visualizations using BI tools
Experience with AWS Cloud services
ABOUT MILLENNIUM PHYSICIAN GROUP
Formed in 2008, Millennium Physician Group has grown into one of the largest comprehensive primary care practices with more than 400 health care providers located throughout Florida. With corporate headquarters in Fort Myers, Florida, Millennium Physician Group consists of primary care offices, Imaging Centers, Lab Services, Physical Therapy, and Wellness Programs. We also have several administrative departments supporting our medical offices, such as Quality Assurance, ACO, Business Services, Coding ACO, IT, Human Resources, and more.

If you are interested in joining an organization that emphasizes teamwork and family, Millennium Physician Group is the right choice.
Millennium's core values summarize how we treat others, patients, and fellow community members. Millennium CARES for every patient every time.

ARE YOU READY TO JOIN OUR TEAM? If you are the right candidate for this position, please click the link to apply today. We look forward to meeting you!","$94,511 /yr (est.)",1001 to 5000 Employees,Private Practice / Firm,Healthcare,Health Care Services & Hospitals,#N/A,$25 to $100 million (USD)
"Acubed
4.0",4.0,"Sunnyvale, CA",Staff ML Engineer (Perception / Data),"WAYFINDER
Wayfinder is building scalable, certifiable autonomy systems to power the next generation of commercial aircraft. Our team of experts is driving the maturation of machine learning and other core technologies for autonomous flight; we are creating a reference architecture that includes hardware, software, and a data-driven development process to allow aircraft to perceive and react to their environment. Autonomous flight is transforming the transportation industry, and our team is at the heart of this revolution.
The Opportunity
As a Staff ML Engineer, you will lead the technical execution of our data and ML pipelines with an overall focus into improving the overall end to end perception pipeline (labeling, training and deployment).
This is a hands-on role, so you will be responsible for implementing things yourself, while also acting as a tech lead for the ML and Data engineering teams.
Responsibilities
Be an architect for our overall perception pipeline (training, testing, monitoring and labeling)
Explore, prototype and validate new algorithms/solutions
Be a mentor for other team members within the ML and Data teams
Champion highest quality of engineering excellence
Be able to move from research to productize ML models (fast iteration)
Negotiate initiatives and deliverables with stakeholders
Have deep understanding of the business and operational impact for different technology tradeoffs
Capable of influencing and building consensus in technical debates
Requirements
BS, MS, or higher degree, in CS/CE/EE, or equivalent industry experience
Extensive experience with ML frameworks such as Tensorflow, Caffe, and PyTorch
Strong programming skills in Python and C++
Growing expertise with state-of-the-art perception related ML models
Excellent mathematical reasoning skills, especially with probability
8+ years of experience in computer vision and machine learning
Expertise in setting architectures that are scalable, fault-tolerant and extensible for changes.
Ability to design across multiple systems
Ability to wear several hats between coding, technical strategy, mentorship etc.
Proven record of productizing computer vision models
Strongly Preferred Qualifications
PhD in computer science or machine learning
Experience with MCAP, OpenCV
Experience with CUDA
Real-world experience applying machine learning techniques in autonomous systems such as robots, cars, and UAV
Benefits
Exceptional PPO medical, dental and vision benefits with 100% of premiums covered for employee and their family/dependents
Generous PTO of 5 weeks (6 weeks after two years) in addition to 11 national holidays and unlimited paid sick days
Professional development reimbursement or $15,750 for flight training
3 months paid parental leave from Day 1
Pay Transparency Notice: Depending on your work location and years of experience, the target annual salary for this position can range from $160,000 to $220,000 + target bonus + benefits (including medical, dental, vision, 401(k), and flight training).
Note that Acubed does not offer sponsorship of employment-based nonimmigrant visa petitions for this role.","$190,000 /yr (est.)",51 to 200 Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,#N/A,Less than $1 million (USD)
"Northwestern University
4.2",4.2,"Evanston, IL",Data Management Engineer,"The Data Management Engineer is responsible for supporting the development and implementation of secure and efficient research data management systems, environments, and workflows. In this position, you will collaborate with research data and cyberinfrastructure colleagues to help create and expand services that allow researchers to meet their research project security, compliance, analysis, and collaboration requirements. This includes working with storage systems, data transfer tools, networking, research computing resources, and cloud computing resources. As an integral member of our team, Northwestern Research Computing Services, your work will enable research across the university. In this position, you will report to the Associate Director of Research Data Services.
Note: Not all aspects of the job are covered by this job description.
Specific Responsibilities:
Aid in the design of sustainable and scalable research support services that align with faculty research goals, technology standards, and best practices.
Install, configure, test, and document applications to support data management and computing services.
Translate service requirements into functional architectural design and effective implementation.
Incorporate security best practices into the design, implementation, and monitoring of services.
Document standard and customized data workflows, including development of technical drawings, schematics, and/or reports as needed.
Automate service allocation processes.
Create proof-of-concepts and unique solutions for individual research projects and lead implementation of projects with fellow IT staff.
Build and support data workflows by identifying requirements, testing new iterations, installing and configuring applications, and adapting the process with incremental feedback
Facilitate use of cloud resources for the purposes of storing data, including providing guidance on appropriate product, setting up environment, and providing materials for users to maintain the environment
Clearly communicate project status and updates with the researchers and team in a timely manner.
Minimum Competencies: (Skills, knowledge, and abilities.)
Successful completion of a full 4-year course of study in an accredited college or university leading to a Bachelor's degree in a related field; OR appropriate combination of education and experience.
2 or more years’ experience using Windows, Mac, and Linux operating systems.
2 or more years’ experience with a programming or scripting language, such as Python or bash.
1 or more years’ experience using cloud-based storage resources, such as Amazon S3 or Google Cloud Storage.
Ability to manage and prioritize a wide variety of projects and tasks.
Demonstrated ability to write documentation and communicate effectively with team members with varied technical backgrounds.
Demonstrated interpersonal skills, with the ability to build relationships successfully across teams.
Skilled in using a variety of tools and methods for data storage and transfer, including data transfer to the cloud.
Knowledge of security protocols for data transfer, network, and storage.
Benefits:
At Northwestern, we are proud to provide meaningful, competitive, high-quality health care plans, retirement benefits, tuition discounts and more! Visit us at https://www.northwestern.edu/hr/benefits/index.html to learn more.
Work-Life and Wellness:
Northwestern offers comprehensive programs and services to help you and your family navigate life’s challenges and opportunities, and adopt and maintain healthy lifestyles.
We support flexible work arrangements where possible and programs to help you locate and pay for quality, affordable childcare and senior/adult care. Visit us at https://www.northwestern.edu/hr/benefits/work-life/index.html to learn more.
Professional Growth & Development:
Northwestern supports employee career development in all circumstances whether your workspace is on campus or at home. If you’re interested in developing your professional potential or continuing your formal education, we offer a variety of tools and resources. Visit us at https://www.northwestern.edu/hr/learning/index.html to learn more.

Northwestern strongly recommends COVID-19 vaccinations and boosters for people who can obtain them as a critical tool for minimizing severe illness. More information can be found on the COVID-19 and Campus Updates webpage.
The Northwestern campus sits on the traditional homelands of the people of the Council of Three Fires, the Ojibwe, Potawatomi, and Odawa as well as the Menominee, Miami and Ho-Chunk nations. We acknowledge and honor the original people of the land upon which Northwestern University stands, and the Native people who remain on this land today.",#N/A,5001 to 10000 Employees,College / University,Education,Colleges & Universities,1851,$1 to $5 billion (USD)
Cypress Consulting,#N/A,"Palo Alto, CA",Sr. Automation Data Center Design Engineer/Consultant,"Sr. Automation Data Center Engineer/Network Security Consultant
The Sr Consultant will provide expert automation support, analysis and research into complex problems and processes relating to deployed Security network equipment. The Consultant will function as the Automation Subject Matter Expert (SME) and will interact directly with the customer's personnel and will also serve as the technical expert on executive-level project teams within the customer providing technical direction, interpretation, and alternatives. The ideal candidate contributes to the development of new principles and concepts, works on unusually complex technical problems and provides solutions which are highly innovative and ingenious. This is a highly technical, hands-on role and this person will be required to develop and maintain an expertise on the products and solutions deployed within the Customer's network/Data Centers.
Technical Skills needed:
Ansible, CI/CD tools, DevOps, Git, Python, Strong Automation experience for deployment, Terraform
Job Type: Full-time
Pay: $115.00 - $125.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Professional development assistance
Referral program
Retirement plan
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Palo Alto, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Consultative/Customer-facing: 3 years (Required)
Terraform: 2 years (Required)
Data Center Design: 3 years (Preferred)
Ansible: 3 years (Required)
Python: 2 years (Required)
Work Location: In person",$120.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Synergy technologies
4.7",4.7,Remote,Data Engineer (W2),"Data Engineer
** Remote Opportunity **
JOB DESCRIPTION:
Data Engineer -
Within this role, you would be a hands-on leader in data engineering functions including schema design, data movement, data transformation, encryption, and monitoring: all the activities needed to build, sustain and govern big data pipelines.
Mandatory Skills:
SCALA
SPARK
Java, SQL
AWS
Glue
S3
Responsibilities
Own development of large-scale data platform including operational data store, real time metrics store and attribution platform, data warehouses and data marts for advertising planning, operation, reporting and optimization
Wider team collaboration and system documentation
Maintain next-gen cloud based big data infrastructure for batch and streaming data applications, and continuously improve performance, scalability and availability
Advocate the best engineering practices, including the use of design patterns, CI/CD, code review and automated integration testing.
Required Education, Experience, Skills and Training
Bachelor or above in computer science or EE
5+ years of professional programming in Scala, Java and SQL
5+ years of experience developing in Amazon Cloud technologies including S3, Glue, EC2, and Kinesis
5+ years of big data design experience with technical stacks like Spark, Flink, Druid, Clickhouse, Single Store, Snowflake, Kafka, Nifi and AWS big data technologies
Proven track record with cloud infrastructure technologies, at least two of Terraform, K8S, Spinnaker, IAM, ALB, and etc.
Experience building highly available and scalable services for public consumption
Experience with processing large amount of data at petabyte level
Strong knowledge of system design, application design and architecture
Proficiency in both written and oral English
Job Type: Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Application Question(s):
What is your Work Authorization/ Visa status?
Experience:
Data Engineering: 5 years (Preferred)
Programming in Scala, Java and SQL: 5 years (Preferred)
Big data design: 5 years (Preferred)
Cloud infrastructure: 5 years (Preferred)
Work Location: Remote
Speak with the employer
+91 9549530709",$42.50 /hr (est.),Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Delta
4.3",4.3,"Atlanta, GA","Data Engineer, Operations Analysis, and Performance","United States, Georgia, Atlanta
Operations Anlys & Performance
12-May-2023
Ref #: 19868
How you'll help us Keep Climbing (overview & key responsibilities)
Deltas brand is based on best-in-class operational performance, the foundation of which is providing safe and reliable operations for our customers travel experience. The role of Operations Analytics (OA) is to support this mission by providing strategic insights by first understanding business processes and then leveraging data and analytics to drive continuous improvement efforts.

Data has transformed the way Delta operates.The role of a data engineer is to further harness the power of data by making it accessible, available, and curated for reporting and analysis. Data engineering teams within Deltas Operations Analysis and Performance (OAP) division are typically responsible for: curating Single Source of Truth data tables to be used by analytics teams, producing and distributing automated reporting in a modular and scalable manner, leading efforts to transition to modern data and reporting tools, and acting as a center of expertise for efficient data processing and management.

Responsibilities:
Locate and extract data from a variety of sources for use in analysis, models and project development.
Clean and curate datasetsby researching new data sources and collaborating with other business units to determine the best source for the data, aggregating views into meaningful hierarchies
Be familiar with data environments and collaborate to improve automated reports and analyses in support of divisional leaders and business units
Leverage emerging technologies and proactively identify efficient and meaningful ways to communicate data and analysis in order to satisfy divisional needs.
Support process improvement and project management engagements for both individual business units and cross-divisional initiatives
Train and mentor other team members in various skillsets and subjects
Practice safety-conscious behaviors in all operational processes and procedures
Have a team first attitude with the success of our team and business partners as the top priority
Be intellectually curious, ask questions, and speak up when they have an idea
Enjoy working in a high-profile environment with fluid priorities, ambiguity and aggressive deadlines.

Benefits and Perks to Help You Keep Climbing
A career at Delta not only gives you a chance to see the world, but we also provide excellent benefits to help you keep climbing along the way!
Competitive salary, industry leading profit sharing and 401(k) with generous direct contribution and company match
Comprehensive health benefits including medical, dental, vision, short/long term disability and life benefits
A detailed wellness plan that recognizes the importance physical, emotional, financial, and social wellbeing
Domestic and International flight privileges


What you need to succeed (minimum qualifications)
3+ years of related experience
Proficiency in SQL and ETL/ELT patterns
Proficiency with Python, SAS or other similar tools and programming languages
Ability to troubleshoot a reporting database environment
Strong attention to detail and ability to work autonomously and manage multiple requests with varying timelines
Self-starter with a resilient, solution-minded approach to complex problems working individually or in a group
Demonstrates that privacy is a priority when handling personal data.
Embraces a diverse set of people, thinking and styles.
Consistently makes safety and security, of self and others, the priority.
What will give you a competitive edge (preferred qualifications)
Bachelor's degree or certificate in Computer Science, Engineering, Information Science, or other relevant quantitative field
Previous airline experience
Working knowledge of and/or experience with cloud-based solutioning (i.e. Azure, AWS, GPC, etc.)

< Go back","$102,659 /yr (est.)",10000+ Employees,Company - Public,Transportation & Logistics,"Airlines, Airports & Air Transportation",1928,$10+ billion (USD)
"NexTek, LLC",#N/A,"Shoreview, MN",Sr. Data Engineer,"Nextek, LLC is a software development company in the financial services industry that creates web applications and solutions for financial services professionals. We are seeking a motivated Senior Data Engineer to join the team.
What we provide:
A laptop and other equipment/software necessary for work
Health Insurance, Dental Insurance, Vision Insurance
Paid time off (PTO), Volunteer time off (VTO)
Hybrid remote work options
401K
Life Insurance
Flexible schedules
Monthly Social Activities
Casual dress code
Professional development
Here is what we are looking for:
Experience in database administration, security, backup, and recovery
Preferred Knowledge:4+ years of experience in data/software engineering
Experience in the financial services industry
Experience with a variety of relational databases (particularly MySQL & MSSQL)
Experience with collecting and presenting analytics
Here is what you will be doing:
Display proficiency in relevant technical skills.
Maintain a deep understanding of the data structure.
Provide evaluations and recommendations for our data environment based on potential issues, tradeoffs, and risks.
Scope and implement solutions.
Make good estimates and set realistic expectations for delivery.
Ask for guidance when necessary.
Maintain and monitor the health of databases and associated solutions.
Make use of the same environments setup by the Software Engineering team
Thoroughly test code for bugs and security issues before deploying to production environments
Maintain and monitor actively deployed solutions
Follow and help define best practices
Peer reviews
Maintain awareness of industry trends
Share experience and insights to help your team grow
Increase understanding of impact on other teams
Increase understanding of the customer
All other duties as assigned
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Shoreview, MN: Reliably commute or planning to relocate before starting work (Required)
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Yes Energy, LLC
4.6",4.6,"Boulder, CO","Data Engineer (PL/SQL) Hybrid - Boulder, CO / Chicago, IL / Dedham, MA / Houston, TX","Data Engineer (PL/SQL)

Join the Market Leader in Electric Power Trading Solutions
The electrical grid is the largest and most complicated machine ever built. Yes Energy’s industry leading electric power trading analytics software provides real time visibility into the massive amount of data that is generated by the North American electrical grid every day. Our unique and innovative view of the data informs real time trading decisions that keep utility prices low and the grid up and running. It’s both challenging work and work with a purpose.
Be a part of our successful, growing business.
We are currently working in a hybrid environment and are seeking to fill one full time Data Engineer (PL/SQL) position immediately in Boulder - CO, Chicago - IL, Dedham - MA or Houston - TX.
About the team
At Yes Energy, our Market Data Operations (MDO) team plays a crucial role in our business. We are the control room for our customers, responsible for ensuring access to reliable and timely data every minute of every day.
We take pride in our responsibility to maintain the accuracy and reliability of our data. We understand that our clients rely on us to provide them with the information they need to make informed decisions, and we take that responsibility very seriously. Like a control room operator who is constantly monitoring the grid and making adjustments to ensure stability, our team is constantly monitoring our data pipelines and making adjustments to ensure data accuracy and reliability.
Our team is passionate about what we do, and we are dedicated to helping clients navigate the complex and dynamic North American Energy Markets. We work together in a collaborative environment, and we look to continuously improve our processes to ensure that we are providing the highest quality data possible to our clients.
About you
You have a passion for working with inherently messy data
You believe that a deep understanding of the data leads to better solutions
You have a competitive attitude, taking ownership and accountability of the work you produce
You have strong problem-solving skills and a curious mindset
You have experience maintaining and designing data pipelines
Like to design, develop, analyze and troubleshoot PL/SQL code

What you will do
Maintain our real-time data pipelines and ensure so that we can provide reliable and accurate information to our clients
Support clients by answering complex data questions, providing timely and effective solutions, so that we can empower our clients to make informed decisions
Ensure highest possible quality and integrity of Yes Energy data; recommend and implement ways to improve data reliability, efficiency, and quality
Participate in weekly on-call rotations to help resolve critical data pipeline failures for our clients

Requirements
4+ years of SQL or equivalent experience
4+ years of Oracle PL/SQL or equivalent experience
Bonus points for
Experience with ETL and complex data pipelines
Energy industry experience or experience in equities/commodities trading
Experience with web scraping, including HTML parsing, HTTP protocols and network logs
Experience with Python and Bash Scripting
Familiarity with Agile development methodologies
Position Details
Full time
Reports to Data Operation team Lead
Minimal travel may be required (up to 10 days per year)
Keywords:
Oracle, SQL, PL/SQL, REST API, Time Series Data, ETL, CLI tools.
About Yes Energy
Overview
Yes Energy delivers real-time market data and electric power trading decision solutions. Over 1,000 market participants use Yes Energy solutions daily. The business is a leader in all aspects of information content collection and management, as well as in developing and delivering data and market analytics solutions. Since its inception in 2008 Yes Energy has become a trusted and respected supplier of innovative and reliable solutions focused on the needs of power market analysts, traders and trade managers. Yes Energy has a team of amazing professionals located in Boulder, CO (HQ), Dedham, MA and Chicago, IL.
Culture
At Yes Energy we care about saying “Yes” to customers. We like to listen and learn, and develop our solutions in line with our customers’ needs. We think about customers as business partners and when we help them to be more successful … We are more successful too.
Around the office our culture is driven by some pretty fundamental values that we’re proud of:
We love innovation and solving tough challenges.
We are “high standards people” who combine passion and pride with hard work and rewards of all kinds- in an ethic that is consistent across the company.
We’re team-focused with a flat hierarchy- we work in small teams on well-defined projects that directly impact the success of the business.
We play to the strengths and experience of each person, while each of us also works along a continuum of roles adjacent to our focus area. This presents a challenge of maintaining a broad set of skills as well as an opportunity to learn and contribute in many ways.
We are constantly growing. Professional development happens every day and every year.
Compensation and Benefits
Salary Range: $90,000 - $120,000, plus bonus.
We offer strongly competitive salaries and real bonuses that are achievable and that you can impact. Our benefits package is also very competitive, and it includes medical insurance, 401K Plan with matching, flexible vacation and flexible work schedules. Investment in both formal and informal professional development is encouraged and funded by Yes Energy.
At Yes Energy we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
Yes Energy provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Yes Energy complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.","$105,000 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2008,Unknown / Non-Applicable
"Chewy
3.5",3.5,"Richardson, TX",Data Engineer II,"Our Opportunity:
Chewy’s Data Analytics team has an exciting opportunity for a Data Engineer III to join the pack. Leveraging your strong expertise and background in data engineering and data analysis, you will be a part of a team responsible for operational and tactical reporting generating insights to grow Customer Service operations and planning. This includes building high quality data pipelines that drives analytic solutions and creating data products for analytics and data scientist team members to improve their productivity. Our organization is a fast-paced environment with new challenges and new opportunities each day. You will be responsible for building and implementing data products and technologies which will handle the growing business needs and play a key role in redefining what it means to be a world-class customer service organization.
What You'll Do:
Design, develop, optimize, and maintain data architecture and pipelines using design and programming patterns that follow best-in-class practices and principles.
Manage, maintain, and improve our SSOT tables and data marts, which drive critical business decisions every day.
Work closely with analytics teams and business partners, serving as a trusted partner who can advise, consult, and communicate data solutions.
Mentor and coach other data practitioners on data standards and practices.
Lead the evaluation, implementation and deployment of emerging tools and process for data engineering to improve overall productivity for the organization.
Partner with leaders, vendors, and other data practitioners across Chewy to develop technical architectures for strategic enterprise projects and initiatives.
Document technical details of work and follow agile sprint methodology, using tools like Jira, Confluence etc.

What You'll Need:
Bachelor of Science or Master’s degree in Computer Science, Engineering, Information Systems, Mathematics or related field
3+ years of enterprise experience as a data engineer and/or software engineer
3+ years applying and implementing database and data modeling techniques
3+ years working with enterprise data warehouse (ex. Snowflake, Vertica) and cloud environments (ex. AWS)
3+ years of experience building data integrations and pipelines from data lake, APIs, relational databases, and third-party systems
Strong software development skills in SQL
Self-motivated with strong problem-solving and self-learning skills.
Bonus:
Strong working knowledge of Python programming
Excellent communication and collaboration skills with ability to influence and guide stakeholders
Experience building dimensional models in data warehouses
Experience with data streaming tools and technologies like Kafka, Kinesis, or similar technologies
AWS Developer Certifications
E-commerce, Retail or startup experience
Experience in BI tools such as Tableau, Plotly, Power BI, etc.
Chewy is committed to equal opportunity. We value and embrace diversity and inclusion of all Team Members. If you have a disability under the Americans with Disabilities Act or similar law, and you need an accommodation during the application process or to perform these job requirements, or if you need a religious accommodation, please contact CAAR@chewy.com.

If you have a question regarding your application, please contact HR@chewy.com.

To access Chewy's Customer Privacy Policy, please click here. To access Chewy's California CPRA Job Applicant Privacy Policy, please click here.",#N/A,10000+ Employees,Company - Public,Retail & Wholesale,Pet & Pet Supplies Stores,2011,$5 to $25 million (USD)
"McKinstry
4.1",4.1,"Portland, OR",Data Focused Software Engineer,"Build the future, spark innovation and align your career with purpose.
McKinstry is innovating the waste and climate harm out of the built environment and creating lasting impact. Together, we’re building a thriving planet.
Buildings are a leading contributor to the climate crisis, generating nearly 40% of total global energy-related carbon emissions. We’re making a lasting impact on our industry and within our communities by addressing the climate, affordability and equity crises through:
renewables and energy services
engineering and design
construction and facility services
To get where we’re going, we need big thinkers, problem solvers and collaborative mindsets. Does that sound like you?
The Opportunity with McKinstry
McKinstry is seeking a Data Focused Software Engineer to join our evolving Technology division in Portland, OR. In this role, you will work on key enterprise systems providing analysis, design, development, and configuration support for internal business partners and external clients. You will support the development and implementation of technical business intelligence and data warehousing solutions using a combination of on-premise and hosted technologies. To succeed in this role, you must have experience with SSIS and ETL processing, as well as a strong understanding of Microsoft data technologies. Additional responsibilities you will have include:
Develop complex data extracts, applications, and ad-hoc queries as requested by internal and external customers using the Microsoft suite of data tools (SQL, SSIS, Azure SQL, Azure Data Factory, etc.)
Design, develop, test, implement and manage ETL processes sourcing data from internal and external systems (SQL server, Excel/CSV, SOAP, API, SharePoint, SFTP, etc.).
Participate in the production operations of Business Intelligence products and solutions (Datawarehouse, Data Integration, Power BI) including resolving production issues and responding quickly to priority problems
Participate in the maintenance and publication of logical and physical data models, entity relationship diagrams, and a common data dictionary
Participate in maintenance and publication of data flow diagrams depicting source to target mappings supporting a tiered data architecture compromised of source systems, an operational data store (ODS), a data warehouse (DW) and data marts (DM)
Research, troubleshoot, and resolve data issues impacting extract delivery
Work with Business System Analysts to understand the requirements regarding solutions and ensuring data quality to match back to transaction results
Work with key business users to understand their information needs while providing intuitive data solutions
Collaborate on the definition, development, and maintenance of standards and processes
Drive improvement and automation in BI solutions and processes
What You Need to Succeed at McKinstry
BA/BS degree in Information Technology or related field, or equivalent work experience
3 years of experience using Microsoft data technologies in support of business reporting and analytics objectives such as writing T-SQL scripts
Expereince using different ETL tools
Designing and building data pipelines using SSIS is ideal
1 year of building datasets, reports and dashboards using Power BI
Experience with C# coding, performance tuning and optimizing T-SQL scripts, data modeling techniques
2 years data integration experiences in a data warehousing environment
Demonstrated ability to influence direction within a team and persuade others in researched areas and growing consensus building and facilitation.
Nice to Have
Experience with using Azure Cloud products such as Data Factory, Analysis Services, Data Lake, Data Catalog, Azure Databricks etc. preferred
Experience with Azure DevOps, Agile scrum development practice
Experience with CI/CD
PeopleFirst Benefits
When it comes to the basics, we have you covered:
Competitive pay 401(k) with employer match and profit sharing plan
Paid time off and holidays
Comprehensive medical, prescription, dental, and vision with low or zero deductible options and low out of pocket maximum.
People come first at McKinstry and we go beyond the basic benefits with:
Family formation benefits, including adoption and IVF assistance
Up to 16 weeks paid parental leave
Transgender inclusive benefits
Commuter benefits
Pet insurance
“Building Good” paid community service time
Learning and advancement opportunities via McKinstry University
McKinstry Moves onsite gyms or reimbursement for remote workers
See benefit plan documents for complete details.
If you’re driven by our vision to build a thriving planet together, McKinstry is the place to build your career.
The pay range for this position is $80,100 - $127,680 per year; however, base pay offered may vary depending on job-related knowledge, skills, and experience. A bonus may be provided as part of the compensation package, in addition to a full range of medical, financial, and/or other benefits, dependent on the position offered. Base pay information is based on market location.
The McKinstry group of companies are equal opportunity employers. We are committed to providing equal employment opportunities to all employees and qualified applicants without regard to sex, gender identity, sexual orientation, age, race, color, creed, marital status, national origin, disability, veteran status or any other basis protected by law. This policy applies to all terms and conditions of employment including, but not limited to employment, advancement, assignment, and training.
McKinstry is committed to strengthening our diversity through recruiting and retaining minority and women professionals from all backgrounds. Our commitment is consistent with our recognition that it is the outstanding people within McKinstry who have always been the source of our strength. We recognize that promoting diversity is an integral component of our continuing quest for organizational excellence.
This commitment to Equal Employment Opportunity is made equally as a social responsibility and as an economic and business necessity.
Anyone with questions or concerns regarding Equal Employment Opportunity should contact their direct supervisor or the Human Resources Department without fear of retaliation of any kind.
#LI-AK1","$103,890 /yr (est.)",1001 to 5000 Employees,Company - Private,"Construction, Repair & Maintenance Services",Construction,1960,$500 million to $1 billion (USD)
"Abile Group, Inc.
5.0",5.0,"Saint Louis, MO",Data Cloud Engineer - Master,"Overview:
Abile Group has an exciting and challenging opportunity for a Data Cloud Engineer-Master on a 10 year contract providing User Facing and Data Center Services supporting an Intelligence Community customer. All the personnel on the team will work together to support innovative design, engineering, procurement, implementation, operations, sustainment and disposal of user facing and data center information technology (IT) services on multiple networks and security domains, at multiple locations worldwide, to support the IC mission.

The right candidate will possess the below skills and qualifications and be ready to handle all responsibilities independently and professionally.
Responsibilities:
Provides technical/management leadership on major tasks or technology assignments.
Establishes goals and plans that meet project objectives. Has domain and expert technical knowledge.
Directs and controls activities for a client, having overall responsibility for financial management, methods, and staffing to ensure that technical requirements are met.
Interactions involve client negotiations and interfacing with senior management.
Decision making and domain knowledge may have a critical impact on overall project implementation.
May supervise others.
Qualifications:
Clearance Required: TS/SCI

Degree and Years of Experience: BS/BA and 10 -15 years of relevant experience

Required Skills:
Experience in the various aspects of hybrid cloud activities.
Supports procurement and deployment of Platform Services to enable application portability across the private and public cloud environments offered by NGA.
Readies NGA's Hybrid Cloud Environment for system migration to IC ITE and oversee the future expansion of NGA Hybrid Cloud to additional public clouds.
In concert with DCS Government, supports standardization of DCS operations in a NGA Hybrid Cloud Management environment.
Transforms Government cloud requirements into appropriate technological alternatives and provides expertise in hybrid virtualization and cloud environments.
Experience developing systems, products, and/or processes based on a total systems perspective.
Consults, plans, analyzes designs, develops tests, assures quality, configures, installs, implements, integrates, maintains, and manages systems.
Has and maintains a diverse set of skills across multiple technical disciplines with recognized expertise in multiple disciplines and possess advanced knowledge of multiple mature and emerging technologies.
Works across organizational boundaries, both internally and externally and helps to drive the relationship between technical solutions and business needs of customers. Analyzes, defines and documents customer needs and required functionality.
Designs, develops and tests theoretical and/or physical models and develops the system design, considering operational impacts, performance, testing, manufacturing, cost and schedule, training, maintenance, and support.
Performs system level design trade analysis, reviews and approves system specifications and description documents, determines how a system is to be built, tested, and implemented, plans the system development execution and ensures adherence to appropriate standards, policies, principles, and practices.
Analyzes system capacity and performance to support problem resolution and system enhancements and monitors systems tests.
Responds to inquiries from a variety of sources for the purpose of providing technical assistance, consultation, advice and support, and regularly provides advice and recommends actions and solutions involving complex issues.

About Abile Group, Inc.:
Abile Group, Inc. was formed in July 2004 to partner with the Intelligence Community and their Contractors in the areas of Enterprise Analytics & Performance Management, IT & Systems Engineering and Program & Project Management. We have significant experience with the Federal Government and are an EDWOSB dedicated to our employees and clients. We are looking for high performing employees who enjoy providing advice and guidance along with solutions development and implementation support, crafted by combining industry best practices with the clients’ subject matter experience and Abile’s breadth of expertise.
EEO Statement:
Abile Group, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. Anyone requiring reasonable accommodations should email careers@abilegroup.com with requested details. A member of the HR team will respond to your request within 2 business days.

Please review our current job openings and apply for the positions you believe may be a fit. If you are not an immediate fit, we will also keep your resume in our database for future opportunities.",#N/A,51 to 200 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"AMERICAN SYSTEMS
4.0",4.0,United States,Data Analytics Engineer - Public Trust,"Wanted: An experienced Data Analytics Engineer with demonstrated expertise and experience in design, development, architecture and implementation of large-scale enterprise dataflow platforms as well as analysis of complex, legacy disparate non-conforming datasets.
As a Data Analytics Engineer, you will:
Work with the latest, cutting edge technology.
Work with a team of driven, supportive and highly skilled professionals.
Receive a robust benefits package that includes Employee Stock Ownership Plan!
Enjoy flexibility managing your work hours and personal needs with a single accrual leave plan.

A week in the life of a Data Analytics Engineer:
Provide the management, maintenance, and support of the Data Analytics’ platform hardware and software. Data Analytics shall include deployment and ongoing support for host, application, appliance and log monitoring and alerting. The Data Analytics System will be highly-available with geographically diverse operating locations.
Demonstrate expert-level knowledge/experience in Big data and Cloud technologies: Spark ML, Splunk, Elastic Search, Apache NiFi, AWS Glue, Cribl.io Logstream, and Hadoop.
Architect and deploy Person-Centric Services, including use of AI/ML technologies.
Lead efforts to modernize a portfolio of major legacy applications leveraging the cloud, DevSecOps, and industry best practices.
Architect and implement change-data-capture (CDC) replication for unique disparate datasets of records into Spark ML for deduplication and graph theory entity/identity grouping Architect a scalable, automated enterprise-wide Security Event Management framework—Splunk—to collect and analyze “events”.
Develop risk patterns through the Security Event Management framework to enable Agency ability to perform predictive analysis using billions of events per month.

AMERICAN SYSTEMS is committed to pay transparency for our applicants and employee-owners. The salary range for this position is $110,000 - $150,000. Actual compensation will be determined based on several factors permitted by law. AMERICAN SYSTEMS provides for the welfare of its employees and their dependents through a comprehensive benefits program by offering healthcare benefits, paid leave, retirement plans (including ESOP and 401k), insurance programs, and education and training assistance.
Founded in 1975, AMERICAN SYSTEMS is one of the largest employee-owned companies in the United States. We are a government services contractor focused on delivering Strategic Solutions to complex national priority programs with 100+ locations worldwide. Through our focus on quality, strong cultural beliefs and innovation we deliver excellence every day.
Company Awards:
Forbes National Best Midsize Companies 2021
Energage National Best Workplaces, National 2021
Washington Post Best Workplaces 2021 Veteran
Hiring Awards:
U.S. Department of Labor Hire Vets Medallion
BEST FOR VETS by Military Times
TOP 10 MILITARY FRIENDLY COMPANY by MilitaryFriendly.com

#LI-IG1
Job Requirements
Required: a minimum of 7 years of recent and relevant experience.
Preferred education: A Bachelor’s degree from an accredited college or university in business management, information technology management, or a related field.
Domain expertise: programming languages, data analytics, machine learning (ML/AI), DevSecOps, Cloud, Big Data.
Technical skills preferred: JavaScript, Ansible, SQL, Java, .Net, Python; Data Technologies: Oracle, Cassandra, Elastic Search, Aurora, NiFi, Cribl.io LogStream, Splunk, Hadoop, Tensorflow, Jupyter.; Software/Tools: AWS, Docker, Jira, VMware vCenter/vRealize, Kubernetes, Jenkins, Ansible, Prometheus, and Grafana; Cloud Native: AWS EC2, AWS CDK, S3, CloudWatch, CloudFront, CloudConfig, Elastic Container Services (ECS), DynamoDB, RDS, ElasticSearch, PowerBI, Tableau.
Preferred certification: AWS Solutions Architect Associate.
Previous Transportation Security Administration (TSA) personnel clearance preferred but not required.
Prior experience with and knowledge of TSA’s mission priorities, systems, and applications preferred but not required.
Strong written and oral communication skills. Ability to coordinate across large groups of people at multiple levels.
EOE Minorities/Women/Disabled/Veterans/Gender Identity/Sexual Orientation or EEO M/W/D/V/GI/SO","$130,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,1975,$100 to $500 million (USD)
"Charles Schwab
4.1",4.1,"Lone Tree, CO",Senior Big Data Engineer,"Your Opportunity

At Schwab, the Data and Rep Technology (DaRT) organization governs the strategy and implementation of the enterprise data warehouse, Data Lake, and emerging data platforms. Our mission is to drive activation of data solutions, rep engagement technology (Sales, Marketing and Service) and client intelligence to achieve targeted business outcomes, address data risk and safeguard competitive edge. We help Marketing, Finance, Risk and executive leadership make fact-based decisions by integrating and analyzing data.

As part of the Business Data Delivery team, you will partner with our Business stakeholders and Data Engineering team to design and develop data solutions for data science, analytics and reporting. We are a team of passionate data engineers and SMEs who bring a lot of energy, focus and fresh ideas that support our mission to contribute by seeing the world “Through Clients' Eyes”. ETL Developers work with large teams, including onshore and offshore developers, using best-in-class technologies including BigQuery, Teradata, Informatica, and Hadoop. You will design, development and implement enterprise data integration solutions with opportunities to grow in responsibility, work on exciting and meaningful projects, train on new technologies and lead other Developers to set the future of the Data Warehouse.
What you are good at

Designing, Developing and implementing new data ingestion workflows by practical application of existing and new data engineering techniques
Leading large complex projects for successful delivery
Developing data ingestion workflows across wide variety of data sources and data ingestion patterns such as batch, near real-time and real time
Working with business analysts to understand business/new data requirements and use cases
Crafting and updating ETL specifications and supporting documentation
Developing solution design by working with technical directors, Data Modelers and cross-functional teams to ensure an accurate and efficient implementation of requirements and following standards Defining and executing quality assurance and test scripts
Guiding the ETL delivery team with technical expertise
Reviewing ETL delivery from 3rd party vendor teams
Advocating for agile practices to increase delivery efficiency
Ensuring consistency with published development, coding and testing standards
Applying data integration best practices for data quality and automation
Working with product vendors to identify and manage open product issues.
What you have

Demonstrated ability as an ETL lead with a track record of delivering projects with minimal defects
7+ years of hands-on experience with data integration tools such as Informatica Power Center and Talend
7+ years in Data Warehouse platforms such as Teradata and BigData/Hadoop
At least 5 years of experience in data modeling (logical and/or physical)
At least 5 years of hands-on experience working with near realtime and/or real-time data ingestion techniques
Expertise in schema design and demonstrable ability to work with complex data is required
Strong SQL experience with the ability to develop, tune and debug complex SQL applications is required
Experience with Google Cloud Platform, BigQuery and Informatica Intelligent Cloud Services (IICS) helpful. Experience with scheduling tools (eg. Control M, ESP)
Demonstrable experience in working in large environments such as RDBMS, EDW, NoSQL, BigData etc. is preferred
Prior experience collaborating with various partners, including vendors, offshore development teams and internal groups
Ability to quickly learn & become proficient with new technologies
Strong analytical, problem-solving, influencing, prioritization, decision making and conflict resolution skills
Outstanding interpersonal skills, including collaboration, communication, and negotiation
Ability to help drive processes, run projects and solve highly complex problems using innovative solutions
Ability to work independently with little instruction on day-to-day work and lead multiple projects requiring cross-team and external collaboration
Ability to coach and mentor individuals on technical matters by sharing knowledge Ability to train and handle delivery with a team of developers.","$140,000 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Investment & Asset Management,1973,$10+ billion (USD)
"Mars
4.3",4.3,"Newark, NJ",Data Engineer,"Job Purpose/Overview
One Demand Data & Analytics (ODDA) is a Mars Wrigley program that harnesses the power of data and insights to solve some of the critical business-wide problems we face – unlocking quality growth and operational excellence.
Through ODDA, we deliver connected insights across the entire demand ecosystem. We empower our Associates with the right data, tools and capabilities so they can take decisive action, maximizing value and making a meaningful impact on our consumers, our customers and our business.
The Portfolio & Innovation Analytics vertical within ODDA seeks to equip Mars Associates with the capabilities needed to address portfolio health and innovation from a holistic and analytics-driven viewpoint.

Key Responsibilities
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals
Solve complex data problems to deliver insights that helps business to achieve goals
Create data products for engineer, analyst, and data scientist team members to improve their productivity
Advise, consult, mentor and coach other data and analytic professionals on data standards and practices
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Define and execute the Analytics Data Engineering roadmap (and work with enterprise BI to enhance the data lake and a real-time reporting environment for operations)
Lead complex process improvement and project management engagements for both individual business units and cross-divisional initiatives
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering in order to improve productivity as a team
Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes
Partner with machine learning engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Mentor junior members in technical proficiency and business acumen
Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics
Interface with business unit leaders to develop and maintain internal customer relationships
Job Specifications/Qualifications
Master’s degree in computer science, application programming, software development, information systems, database administration, mathematics, engineering, or other related field
6+ years in a rapid development environment, preferably within an analytics environment
Demonstrated ability to be work with internal (Operations) and external (IT) stakeholders
Must be able to interact and collaborate at all levels within Operations Analysis & Performance, OCC, cross-divisional working groups, and outside entities
Experience with increasing code quality and implementing best practices across teams
Advanced technical skills in the following areas:
Proficiency in SQL (CTE, window functions, temporal data), SAP HANA experience is a large plus
Proficiency in a scripting language (Python preferred)
Proficiency of API Consumption
Proficiency in ETL tooling (such as Informatica)
Proven expertise in SAP ECC and SAP APO is a big plus
Excellent communication skills and ability to present concepts to non-technical audience
Must be able to interact and collaborate at all levels within Operations Analysis & Performance, OCC, cross-divisional working groups, and outside entities
Strong project management, organizational, and prioritizations skills
2 to 4 years' experience in applied data science role or equivalent; ideally in a CPG, Retail
Knowledge and experience in modelling techniques and advanced applied skills (e.g. significance testing, GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.) using tools like Spark, Scala, SAS, R, Python, Bayesia, H2O, Storm, Yarn, and Kafka
Experience querying databases (SQL, Hive)
Experience working with big data platforms such as Hadoop ecosystem (Azure), including in-memory solutions (SAP HANA and Apache Spark)
Working knowledge of data visualization tools such as Tableau, Power BI, D3, ggplot, to deliver output to the broader business community to improve decision making and productivity
Strong communication and presentation skills
What can you expect from Mars?
Work with over 130,000 like-minded and talented Associates, all guided by The Five Principles.
Join a purpose driven company, where we’re striving to build the world we want tomorrow, today.
Best-in-class learning and development support from day one, including access to our in-house Mars University.
An industry competitive salary and benefits package, including company bonus.
#LI-Hybrid","$105,850 /yr (est.)",10000+ Employees,Company - Private,Manufacturing,Food & Beverage Manufacturing,1911,$10+ billion (USD)
"CrowdStrike
4.2",4.2,Remote,Sr. Engineer II - FedRAMP Data Centers (Remote),"#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to the customer to join us in shaping the future of cybersecurity. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
About the Role:
This senior role will be responsible for FedRAMP related needs in data centers. Segmented responsibilities may include:
FedRAMP Data Center Environments
Design Interconnectivity for connecting FedRAMP GovCloud environments (AWS Govcloud)
Site Deployments
Maintaining Servers and Infrastructure in Data Centers
Data Center Development
Site Reliability and Operation
This role will be the person responsible for FedRAMP process related to data centers.
This role will require the candidate to periodically undergo and pass additional background and fingerprint check(s) consistent with government customer requirements.
Mentoring and developing engineers, and technicians such that they can run daily operations with minimal supervision
Build and lead a diverse, data center operations team, developing both the technical capabilities and leadership qualities of individual contributors.
Support and contribute thought leadership to the development and implementation of business practices which support the growth and ongoing management of our global data center footprint
Ability to travel as needed
What You'll Need:
10+ year’s experience working in critical environments
Experience with FedRAMP certification and operations
Experience working in a cutting edge, technical, hands-on environment and leveraging technology to manage and grow environments.
Experience in personnel management, organizational leadership, people development and team growth a plus
Experience in influencing and leading a diverse group of people, partners suppliers across multiple disciplines a plus
Knowledge of data center power, cooling, network, structured cabling, server and storage infrastructure
Leadership experience making decisions with minimal direction and prioritizing across multiple competing demands
Communication and collaboration experience
#LI-Remote
#LI-LY1
#LI-DG1
This role will require the candidate to periodically undergo and pass additional background and fingerprint check(s) consistent with government customer requirements.
Benefits of Working at CrowdStrike:
Remote-first culture
Market leader in compensation and equity awards
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
Physical and mental wellness programs
Paid parental leave, including adoption
A variety of professional development and mentorship opportunities
Offices with stocked kitchens when you need to fuel innovation and collaboration
We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact
Recruiting@crowdstrike.com
, for further assistance.
CrowdStrike, Inc. is committed to fair and equitable compensation practices. The salary range for this position in the U.S. is $135,000 - $220,000 per year + bonus + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
CrowdStrike participates in the E-Verify program.
Notice of E-Verify Participation
Right to Work","$177,500 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2011,$500 million to $1 billion (USD)
"Delta
4.3",4.3,"Atlanta, GA","Data Engineer, Operations Analysis, and Performance","United States, Georgia, Atlanta
Operations Anlys & Performance
12-May-2023
Ref #: 19868
How you'll help us Keep Climbing (overview & key responsibilities)
Deltas brand is based on best-in-class operational performance, the foundation of which is providing safe and reliable operations for our customers travel experience. The role of Operations Analytics (OA) is to support this mission by providing strategic insights by first understanding business processes and then leveraging data and analytics to drive continuous improvement efforts.

Data has transformed the way Delta operates.The role of a data engineer is to further harness the power of data by making it accessible, available, and curated for reporting and analysis. Data engineering teams within Deltas Operations Analysis and Performance (OAP) division are typically responsible for: curating Single Source of Truth data tables to be used by analytics teams, producing and distributing automated reporting in a modular and scalable manner, leading efforts to transition to modern data and reporting tools, and acting as a center of expertise for efficient data processing and management.

Responsibilities:
Locate and extract data from a variety of sources for use in analysis, models and project development.
Clean and curate datasetsby researching new data sources and collaborating with other business units to determine the best source for the data, aggregating views into meaningful hierarchies
Be familiar with data environments and collaborate to improve automated reports and analyses in support of divisional leaders and business units
Leverage emerging technologies and proactively identify efficient and meaningful ways to communicate data and analysis in order to satisfy divisional needs.
Support process improvement and project management engagements for both individual business units and cross-divisional initiatives
Train and mentor other team members in various skillsets and subjects
Practice safety-conscious behaviors in all operational processes and procedures
Have a team first attitude with the success of our team and business partners as the top priority
Be intellectually curious, ask questions, and speak up when they have an idea
Enjoy working in a high-profile environment with fluid priorities, ambiguity and aggressive deadlines.

Benefits and Perks to Help You Keep Climbing
A career at Delta not only gives you a chance to see the world, but we also provide excellent benefits to help you keep climbing along the way!
Competitive salary, industry leading profit sharing and 401(k) with generous direct contribution and company match
Comprehensive health benefits including medical, dental, vision, short/long term disability and life benefits
A detailed wellness plan that recognizes the importance physical, emotional, financial, and social wellbeing
Domestic and International flight privileges


What you need to succeed (minimum qualifications)
3+ years of related experience
Proficiency in SQL and ETL/ELT patterns
Proficiency with Python, SAS or other similar tools and programming languages
Ability to troubleshoot a reporting database environment
Strong attention to detail and ability to work autonomously and manage multiple requests with varying timelines
Self-starter with a resilient, solution-minded approach to complex problems working individually or in a group
Demonstrates that privacy is a priority when handling personal data.
Embraces a diverse set of people, thinking and styles.
Consistently makes safety and security, of self and others, the priority.
What will give you a competitive edge (preferred qualifications)
Bachelor's degree or certificate in Computer Science, Engineering, Information Science, or other relevant quantitative field
Previous airline experience
Working knowledge of and/or experience with cloud-based solutioning (i.e. Azure, AWS, GPC, etc.)

< Go back","$102,659 /yr (est.)",10000+ Employees,Company - Public,Transportation & Logistics,"Airlines, Airports & Air Transportation",1928,$10+ billion (USD)
"Anywhere Real Estate
3.5",3.5,"Madison, NJ",Data Engineer,"Anywhere Real Estate Inc is seeking a remote data engineer to join the Database Service Team, DNA (Data and Analytics Division)! Reporting to Database Team Manager, the Data Engineer will be involved with all phases of IT development projects, database and data mart administration and 24x7 production support.
We expect the joining Data Engineer should have 3-5 years of significant experiences with excellent SQL server and AWS skills.
Job Responsibilities
Development, solving, and performance tuning of complicated SQL server stored procedures and SQL Server Integration Services (SSIS) packages.
Development of ETL pipelines in and out of data warehouse using combination of AWS tools/Python (or Scala) and Snowflakes stored procedures and views.
Conducting data investigations and assisting business partners with sophisticated data analysis and ad-hoc queries.
Must be available 24x7 for Production Support with a rotating on-call schedule.
Required Skills
Bachelor’s degree in Computer Science, Engineering, Information Systems, or related field or equivalent experience
Confirmed experience with SQL Server in developing, implementing, and supporting SQL server databases for web-based applications.
Must have strong Transact-SQL skill and be able to work on complicated stored procedures.
3 years working experience with AWS Cloud Services including Apache Airflow and AWS Glue, Athena, EMR, EC2, S3, Lambda, etc.
Strong Technical hands-on experience in programming languages – T-SQL, Python, Lambda, JavaScript.
In addition to Microsoft SQL Server database, Previous experience in MongoDB, DynamoDB and Snowflake databases will be a huge plus.
Good understanding of SDLC.
Good to know Agile methodology including using Jira boards and Confluence pages.
Understanding of ETL design and development.
Excellent problem solving and root cause analysis skills.
Excellent written and verbal communication skills.

#LI-JC1
#LI-Remote


Exciting News: We are excited to announce that Realogy is now Anywhere Real Estate Inc. It will take a few months for us to transition to our new brand. For more information about this change, please click here .

EEO Statement: EOE AA M/F/Vet/Disability

Compensation Range:
$85,500 - $182,200 ; At Anywhere, actual compensation within that range will be dependent upon the individual’s skills, experience, and qualifications.","$133,850 /yr (est.)",Unknown,Company - Public,Real Estate,Real Estate,#N/A,Unknown / Non-Applicable
"Lithia Motors, Inc.
3.3",3.3,Oregon,Senior Data Engineer,"Dealership:
L0105 Lithia Home Office
Senior Data Engineer
The Senior Data Engineer is responsible for developing and supporting the cutting-edge data solutions by using the Azure stake (Data Lake, Data Warehouse, Data Factory, Functions) SQL script design/dev, and stored procedures.
The Senior Data Engineer reports to a Lead Data Engineer. This role will be Remote.
Responsibilities
Design and implement data load processes from disparate data sources into Azure Data Lake and subsequent Azure SQL & SQL Data Warehouse
Migrate existing processes and data from our On Premises SQL Server and other environments to Azure Data Lake
Explore and learn the latest Azure technologies to provide new capabilities and increase efficiency
Ensure all existing data is created in the right way, and that new data is created according to appropriate standards and with proper documentation
Read, write, and configure code for end-to-end service telemetry, alerting and self-healing capabilities
Strive for continuous improvement of code quality and development practices
Work closely with the Lead Data Engineer and other Data Engineers to develop and document solutions for providing data to the enterprise
Mentor and teach more junior developers
Skills and Qualifications
3+ years of experience in working as an analytics or data engineering member working with cross functional teams
3+ years of SQL Server development or equivalent
Azure SQL DB, SQL Data Warehouse, Azure Data Factory a plus
Version control using Git or TFS
Bachelor’s Degree in computer sciences, Analytics, Systems Eng., Statistics or related field
Strong attention to detail and sense of urgency
Competencies
Does the right thing, takes action and adapts to change
Self-motivates, believes in accountability, focuses on results, makes plans and follows through
Believes in humility, shares best practices, desires to keep learning, measures performance and adapts to improve results
Thrives on a team, stays positive, lives our values
Physical Demands
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job.*
Up to 1/3 of time: standing, walking, lifting up to 25 pounds
Up to 2/3 of time: sitting, kneeling, reaching, talking, hearing
*Reasonable accommodations may be made to enable individuals to perform the essential functions.
NOTE: This is not necessarily an exhaustive list of responsibilities, skills, or working conditions associated with the job. While this list is intended to be an accurate reflection of the current job, the company reserves the right to revise the functions and duties of the job or to require that additional or different tasks be performed.
We offer best in class industry benefits:
Competitive pay
Medical, Dental and Vision Plans
Paid Holidays & PTO
Short and Long-Term Disability
Paid Life Insurance
401(k) Retirement Plan
Employee Stock Purchase Plan
Lithia Learning Center
Vehicle Purchase Discounts
Wellness Programs
High School graduate or equivalent, 18 years or older required. Acceptable driving record and a valid driver's license in your state of residence necessary for select roles. We are a drug free workplace. We are committed to equal employment opportunity (regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status). We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.",#N/A,10000+ Employees,Company - Public,Retail & Wholesale,Vehicle Dealers,1946,$10+ billion (USD)
"HaystackID
3.3",3.3,Remote,Automation Engineer (Data Automation Services),"Position Summary:
The Automation Engineer (AE) assist the Senior Director of Data Automation (SDDA) in developing, implementing, providing quality assurances, and supporting and all aspects of data automation processes and systems across HaystackID departments that handle, manipulate, or create client facing or client owned data. The AE assists in the development of automation planning, change management documentation, approval processes, as well as test and validation cycles. The AE ensures all automation processes have fully developed and documented standard procedures that are regularly audited, updated, and distributed to the relevant staff. The AE provides support of all automation workflows and systems.

Essential Job Duties:
Project planning and development – Developing workflows to accomplish HaystackID’s data automation needs. This includes working with the Automation team to determine the specifications and developing and implementing workflows for particular business needs. This may also include performing proof of concept on new or emerging technologies to advance automation efforts.
Automation management – Monitoring and analyzing automation workflows, operations, and computing system resources using analytic measures and models to ensure operational throughputs are meeting client needs. This also includes recommending and implementing system resource allocations to improve effectiveness and attain project milestones.
Good communication skills. – Communicating with the Automation team will be essential for both internal and external client updates and project tracking.
Back-end Development – Proficient in back-end languages including Java, Python, Rails, Ruby, .NET, and PHP. The focus of the role will be deployment of self-contained packages that will leverage APIs from cloud platforms, internal applications and other business data sources to support the aforementioned workflows and business needs.
Database Development – Successful candidates will be able to communicate with SQL, Lucene, and other databases systems.
Writing Code – Testing, reporting, bug-fixing and documenting code to a high standard.

Nice to Have:
Knowledge of the following platforms:
Nuix
Rampiva
Relativity
QuickBase
Dynamics 365
Business Central
Secure Transfer systems

Qualifications:
Must exhibit strong technical writing skills, have ability to analyze information, communicate processes, and exhibit excellent follow-up skills.
Ability to work under pressure and able to multi-task and prioritize work assignments to meet deadlines.
Have a successful tracking record of managing engagements of all sizes and durations.
Deep technical knowledge of the following:
Rampiva Automate
Nuix eDiscovery
Relativity
AccessData Forensics ToolKit
EDRM
Visual Studio
SQL Server
Python

Work Environment:
Remote

Education & Experience Preferred:
Bachelor’s degree in Computer Science, information systems, mathematics, statistics or other relevant degree.

Company Benefits:
Medical, Dental and Vision insurances
Life, Short and Long-term Disability Insurances
401(k) after 90-days
PTO and Paid Holidays",#N/A,201 to 500 Employees,Company - Private,Legal,Legal,2011,$100 to $500 million (USD)
"Chicago Transit Authority
3.3",3.3,"Chicago, IL",Senior Data Engineer,"Under general supervision, takes a broader-scale focus to building, developing and maintaining data architectures, including back-end infrastructures, complex data integrations and associated processes. Participates in the design, evaluation, selection, implementation and support of new data engineering techniques related to processing data in both structured and unstructured formats. Makes use of a range of industry standard programming languages and data frameworks. Recommends ways to improve the reliability, quality and usefulness of agency data as it relates to the needs of enterprise Data Scientists, Analysts, related stakeholders and departmental direction.


Qualifications
PRIMARY RESPONSIBILITIES

% time

1

45%

Develops, constructs, tests and maintains data architecture, including databases, data processing systems and related applications. Develops custom applications and infrastructure as necessary. Researches and deploys data infrastructure best practices. Builds and optimizes data pipelines to support cross-functional analytics, automation and programming needs.

2

25%

Develops and maintains best practices for marrying disparate CTA data systems. Liaises with Data Scientists, Programmers, Developers and Analysts to support integration of new data processing and warehousing technologies.

3

15%

Maintains existing data analytics infrastructure and architectures. Leads new efforts to acquire and centralize data from disparate systems. Works as directed to assist department Director in data related project oversight and execution.

4

5%

Researches best practices, recommends changes to improve efficiencies, assists in revising processes and procedures and establishes enterprise data architectures. Assesses existing business needs and anticipates future business needs in order to support the growth of department and agency infrastructure and applications.

5

5%

Hires, trains, develops, monitors and evaluates staff. Reviews and recommends personnel actions for approval.

6

5%

Performs other duties as assigned.

MANAGEMENT RESPONSIBILITIES
Reporting to this position are the following jobs:
Job Title

Senior Analyst, Data Analytics
Data Scientist
Programmer Analyst, Data Analytics
Business Intelligence Developer

CHALLENGES
Working with and managing massive amounts of data sourced from several disparate data systems and applications.
Implementing effective time and project management processes to deliver data services on time and on budget.
Balancing multiple high-priority requests simultaneously while adapting to rapidly-changing demands and meeting project deadlines.
Providing consistency in data processing efforts to help shape the utilization and actionability of information.

EDUCATION/EXPERIENCE REQUIREMENTS
Bachelor’s degree in Statistics, Computer Science, Mathematics, Operations Research, Industrial Engineering, Quantitative Analysis, Economics or a related field, plus four (4) years of experience in data engineering, development, and/or a combination of education and experience.

PHYSICAL REQUIREMENTS
Requires sitting for extended periods of time, standing, visual acumen, manual dexterity and fingering for working with computer keyboards.
Chicago Transit Authority requires all employees to be COVID-19 vaccinated. If you are offered employment, you must provide proof of full COVID-19 vaccination or proof that you are in the process of becoming fully vaccinated as part of the hiring process and as a condition of employment. Visit www.transitchicago.com/careers
Service Area Requirement: Exempt (Non-Union) employees must live within the boundaries of the CTA Statutory Service Area either at the time of employment or within 6 months of beginning employment at CTA.


KNOWLEDGE, SKILLS, AND ABILITIES
Detailed knowledge of various data programming languages and frameworks (i.e. Python, SQL, Java, JavaScript, PHP, C#/.NET, AngularJS, etc.).
Detailed experience in custom and/or industry-standard data migration, integration, and ETL tools and techniques.
Detailed knowledge of big data frameworks and tools (Hadoop, Spark, MongoDB, Cassandra, etc.)
Detailed knowledge of programming development requirements and use of source/revision control systems (i.e. Git, Mercurial etc.)
Strong knowledge of cloud-based data infrastructure (AWS, Azure, Google, Vertica)
Strong knowledge of a range of RDBMS platforms (Postgres, Oracle, SQL Server, MySQL, DB2, etc.)
Strong knowledge of microservice architectures.
Strong knowledge of continuous delivery and deployment pipelines.
Strong analytical, problem-solving, and decision-making skills.
Strong report preparation and presenting skills.
Strong oral and written communication skills.
Strong interpersonal and team skills across a variety of fields and management levels.
Strong project management skills.
Strong organization and time management skills.
Ability to effectively analyze and translate data engineering challenges to the business.
Ability to manage large amounts of data and attention to detail.
Ability to multitask competing projects and deadlines for completion.


WORKING CONDITIONS
General office environment.
Required to occasionally travel to locations throughout the CTA system and Chicago area as needed.

EQUIPMENT, TOOLS, AND MATERIALS UTILIZED
Standard office equipment.
Modern data engineering tools, platforms and processes.


Additional Details
Please note, employees and/or union members will be given priority consideration in the hiring process, per the applicable labor contracts.

Final salary will be determined in part by the qualifications of the selected candidate and may be higher or lower than target.

Applicants, if hired,must comply with CTA's residency ordinance.

CTA IS AN EQUAL OPPORTUNITY EMPLOYER

No employee or applicant for employment will be discriminated against because of race, color, creed, religion, sex, marital status, national origin, sexual orientation, ancestry, age, unfavorable military discharge, disability or any other status protected by federal, state, or local laws; except where a bona fide occupational qualification exists We are committed to providing an inclusive environment for our workforce and supporting the communities we serve. CTA will make reasonable accommodations for the known disabilities of otherwise qualified applicants for employment as well as its employees, unless undue hardship would result. If you require an accommodation in the application or hiring process, please contact arc@transitchicago.com prior to the submission of your application or upon notification of your actual test date. CTA will work with you to determine if an accommodation can be provided.

Primary Location: USA-Illinois-Chicago
Job: Data Analytics
Job Posting: May 12, 2023, 12:19:19 PM
Position Type: Full-time Permanent (FTP)","$121,471 /yr (est.)",10000+ Employees,Government,Transportation & Logistics,Taxi & Car Services,1947,$500 million to $1 billion (USD)
"Acubed
4.0",4.0,"Sunnyvale, CA",Staff ML Engineer (Perception / Data),"WAYFINDER
Wayfinder is building scalable, certifiable autonomy systems to power the next generation of commercial aircraft. Our team of experts is driving the maturation of machine learning and other core technologies for autonomous flight; we are creating a reference architecture that includes hardware, software, and a data-driven development process to allow aircraft to perceive and react to their environment. Autonomous flight is transforming the transportation industry, and our team is at the heart of this revolution.
The Opportunity
As a Staff ML Engineer, you will lead the technical execution of our data and ML pipelines with an overall focus into improving the overall end to end perception pipeline (labeling, training and deployment).
This is a hands-on role, so you will be responsible for implementing things yourself, while also acting as a tech lead for the ML and Data engineering teams.
Responsibilities
Be an architect for our overall perception pipeline (training, testing, monitoring and labeling)
Explore, prototype and validate new algorithms/solutions
Be a mentor for other team members within the ML and Data teams
Champion highest quality of engineering excellence
Be able to move from research to productize ML models (fast iteration)
Negotiate initiatives and deliverables with stakeholders
Have deep understanding of the business and operational impact for different technology tradeoffs
Capable of influencing and building consensus in technical debates
Requirements
BS, MS, or higher degree, in CS/CE/EE, or equivalent industry experience
Extensive experience with ML frameworks such as Tensorflow, Caffe, and PyTorch
Strong programming skills in Python and C++
Growing expertise with state-of-the-art perception related ML models
Excellent mathematical reasoning skills, especially with probability
8+ years of experience in computer vision and machine learning
Expertise in setting architectures that are scalable, fault-tolerant and extensible for changes.
Ability to design across multiple systems
Ability to wear several hats between coding, technical strategy, mentorship etc.
Proven record of productizing computer vision models
Strongly Preferred Qualifications
PhD in computer science or machine learning
Experience with MCAP, OpenCV
Experience with CUDA
Real-world experience applying machine learning techniques in autonomous systems such as robots, cars, and UAV
Benefits
Exceptional PPO medical, dental and vision benefits with 100% of premiums covered for employee and their family/dependents
Generous PTO of 5 weeks (6 weeks after two years) in addition to 11 national holidays and unlimited paid sick days
Professional development reimbursement or $15,750 for flight training
3 months paid parental leave from Day 1
Pay Transparency Notice: Depending on your work location and years of experience, the target annual salary for this position can range from $160,000 to $220,000 + target bonus + benefits (including medical, dental, vision, 401(k), and flight training).
Note that Acubed does not offer sponsorship of employment-based nonimmigrant visa petitions for this role.","$190,000 /yr (est.)",51 to 200 Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,#N/A,Less than $1 million (USD)
"Verily
3.8",3.8,"Waterloo, IA","Software Engineer, Data Ingestion","Waterloo, ON

Who We Are
Verily is a subsidiary of Alphabet that is using a data-driven approach to change the way people manage their health and the way healthcare is delivered. Launched from Google X in 2015, Our purpose is to bring the promise of precision health to everyone, every day. We are focused on generating and activating data from a variety of sources, including clinical, social, behavioral and the real world, to arrive at the best solutions for a person based on a comprehensive view of the evidence. Our unique expertise and capabilities in technology, data science and healthcare enable the entire healthcare ecosystem to drive better health outcomes.
DESCRIPTION
As a member of the Precision Health Platform engineering organization, you will build modular, composable, and interoperable platform components, including storage and processing of precision health data, and other product framework components across Verily products.
RESPONSIBILITIES
Work closely with the development team to design, develop, and deliver new software features across the Verily tech stack.
Come up with ideas to technical design problems, compare options, and propose solutions.
Develop using industry-standard tools including GitHub, Google Cloud, Go, Docker, and Terraform to name a few.
QUALIFICATIONS
Minimum qualifications:
BA/BS degree in Computer Science, Electrical Engineering, or equivalent practical experience in software engineering.
At least 5 years experience as a software engineer in an industry setting.
Ability to work independently and collaborate effectively.
Expertise in building software and systems on any of: GCP, AWS or Azure (GCP preferred).
Experience with one or more general purpose programming languages including but not limited to: Java, C/C++, C#, Objective-C, Go, Python, or JavaScript.
Preferred qualifications:
Demonstrated experience with Go, Java, Python, and/or SQL.
Proficiency in Apache Beam and its associated technologies for data ingestion and processing.
Solid understanding of ETL principles and best practices.
Experience with designing and implementing scalable data processing pipelines with Bioquery or other analytics-oriented databases.
Education or exposure to healthcare or life sciences, with emphasis on HL7 FHIR.
LI-TB
Why Join Us
Build What’s Vital.
At Verily, you are a part of something bigger. We are a diverse team of builders innovating at the intersection of health and technology—united by a shared spirit of curiosity, resilience and determination to make better health possible for all. This builder mindset means your fingerprints will be on the work that shapes the future of health. Fulfilling our precision health purpose starts with the health of our Veeps (what we call our employees), which is why we offer flexibility, resources, and competitive benefits to support you in your whole-person well being. We believe diversity of thought drives innovation—we unite the brightest minds, and encourage all Veeps to bring their lived experience to work with them.
If this sounds exciting to you, we would love to hear from you.
You can find out more about our company culture on our LinkedIn Company Page and Verily Careers page.","$96,116 /yr (est.)",1001 to 5000 Employees,Subsidiary or Business Segment,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2015,Unknown / Non-Applicable
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Aurora, CO",Data Center Operations Engineer II (Front End Day Shift),"Launch your career in Technology Operations and put your creative problem solving into action, delivering solutions that shape the future of global business. You'll work directly with clients to build strong customer relationships and problem-solve technical issues to make businesses more productive. Alongside a motivated team of fellow analysts, supervisors, and stakeholders, you'll develop innovative solutions to troubleshoot and resolve issues while accurately diagnosing problems and providing effective user support. Finally, your strong technology background will ensure that the security and standards of our commitment to excellence are met. And because professional development is a key component of our culture, you'll receive coaching, mentoring - and a host of other development opportunities - alongside your invaluable on-the-job experience.

This role requires a wide variety of strengths and capabilities, including:
Ability to identify problems and clearly communicate strategic solutions to clients
Desire to develop a working knowledge of change management, corporate IT audit processes, IT risk management, technical problem resolution, operations systems, and data sources knowledge
Strong initiative and desire to learn
Ability to effectively collaborate with team members and clients to achieve common goals
Good knowledge of Windows/MAC OS with the ability to carry out root cause analysis
Working knowledge of Microsoft Office products
Strong analytical and problem resolution skills
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans
Base Pay/Salary
Aurora,CO $31.98 - $52.16 / hour",$42.07 /hr (est.),10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
IntellectFaces Technology Solutions Pvt Ltd,#N/A,"Washington, DC","Network and Data Center Engineer (Ashburn,VA/ Washington, DC/ Suitland, MD)","Network Engineer and Data Center Engineer
Fulltime
Ashburn, VA / Washington ,DC / Suitland, MD
Onsite - 5 days
Ability to obtain Public trust clearance
Responsibilities:
The “Need-to-Have” Skills & Qualifications:
· Must have a College Degree
· Eight (8) years of experience in a large government organization responsible for administering, operating, maintaining, and securing a large-scale internetworking environment, including one year in a technical leadership or supervisory role
· Three (3) years of experience directly supporting a complex infrastructure covering the major aspects of network administration with expertise in areas of troubleshooting IPv4 issues across varies protocols and technologies such as OSPF, static routing, MPLS, HSRP, SSH, ACL, VLAN, VSAN, VTP, STP, Trunking, SNMP, LACP, vPC, VoIP, Port Channels, Wireshark and TCPdumps
· Three (3) years of a very strong hands on experience understanding and having the ability to implement and troubleshoot Cisco and HP routers and switches (i.e., ISR, ASR, 9500, 9300, 6513, 3850, 3750, 2960, 2920, 2910, Nexus 2k, 5k and 7k)
· Two (2) year experience configuring, administering and maintaining network IT monitoring and management software such as ForeScout CounterACT, Netscout nGenius and Solarwinds (i.e., NetFlow traffic analysis, Bandwidth monitoring, PerfStack, Network performance baselines, Traffic Analyzer, IPAM and NetPath)
· One (1) year of experience having a role in data center operations. Thorough knowledge of data center hardware specifications, lights-out-management solution practices, familiar with power requirements, racking and decommissioning of hardware
· Thorough knowledge of telecommunications, network security issues and best practices
· Exceptional customer service orientation, written and oral communications skills
Certifications:
· Technical certification (e.g., Cisco Certified Network Associate, Cisco Certified Design Associate, Cisco Certified Network Professional, Cisco Certified Design Professional, etc.)
Required Technical / Business Toolset Experience:
· Cisco management and support tools
· Solarwinds, Verizon Concord Circuit Monitoring Application, or other similar network monitoring tools
· Microsoft Visio or similar tools
The “Nice-to-Have” Skills:
· Ability to present complex network solutions to new and ongoing projects and stakeholders
· Ability to write solid, clear, detailed technical solutions, implementation plans, schedules, action items, document incidents and complete root cause analysis statements
· Self-motivated, minimally supervised with the willingness of learning and training others
· High-energy, resourceful individual, can-do attitude seeking awareness to solve problems and identify workarounds and solutions
· Exhibit work flexibility and has the capabilities to adapt to new changes or directions
· Support management and customer’s objectives, scopes, goals, and visions
Job Type: Full-time
Pay: $100,000.00 - $120,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Washington, DC: Reliably commute or planning to relocate before starting work (Required)
Experience:
maintaining, and securing a large-scale internetworking: 8 years (Required)
Data center: 4 years (Required)
Work Location: In person","$110,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Analytica
3.4",3.4,Remote,Senior Data Engineer (Remote),"Analytica is seeking a Senior Data Engineer with Microsoft SQL experience to support a long term data science project for a federal government client. The ideal candidate will be comfortable being a lead data engineer working with data scientists to understand data requirements, develop SQL queries and stored procedures for feeding the database and data models.

Analytica has been recognized by Inc. Magazine as the fastest-growing private US small business. We work with U.S. government customers in health, civilian, and national security missions. As a core member you’ll work with a diverse team of professionals to solution matters, architect nuisances, and come up with alternatives. We offer competitive compensation with opportunities for bonuses, employer paid health care, training and development funds, and 401k match.

Responsibilities include (But Are Not Necessarily Limited To):
Conduct database development on Microsoft SQL Server
Develop business solution logic using SQL and T-SQL or similar query/scripting languages to execute on the reporting and data validation needs related to the data platform.
Create and maintain SQL scripts, stored procedures, and other program logic to create, update, and delete data.
Design and implement schema changes, manage indexes, and alter data objects to optimize performance.
Design efficient data models from a logical design based on business requirements and available use patterns.
Identify entity relationships, referential integrity constraints, and primary key structures. Perform analysis of tradeoffs between alternative schemas.
Extend data templates to include relevant fields, publish data standards, document and publish data taxonomy and hierarchies.
Support data science team to compile, analyze and extract data for use in advanced machine learning and modeling.
Create and maintain documentation that includes file specifications, schema, core record layouts, programs, business requirements, test plans, and other artifacts used in the administration, creation, and execution of database operations.
Use GitHub for code deployment and version control in a collaborative development environment.



Minimum Qualifications:
Bachelor’s Degree in Computer Science, IT, Computer Engineering, or related field
5+ years Structured Query Language (SQL) programming
5+ years’ experience in Microsoft SQL database programming / development / administration
Experience working in an Agile environment or Scrum teams
Expertise in designing and building enterprise-grade applications
Strong communication, leadership, and problem-solving skills
Experience developing data streams in AWS a plus
Familiarity with tax or financial related data a plus

About ANALYTICA: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD, the company is an established 8(a) small business that has been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider.

As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation.",#N/A,51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2009,$5 to $25 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE",Lead Data Engineer,"As a Lead Data Engineer within Consumer and Community Banking, in Home Lending, you are an integral part of an agile team that works to enhance, build, and deliver data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. As a core technical contributor, you are responsible for maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job Responsibilities
Design and implement end-to-end data pipelines supporting analytical and operational needs accounting for data management practices focused on data quality, metadata management etc.
Architect, design, and implement cloud native solutions on AWS.
Define and implement event driven architecture patterns leveraging messaging / streaming solutions like Kafka, Kinesis, Flink, and Spark
Ability to decompose large initiatives / designs into manageable smaller bodies of work to demonstrate continuous progress
Collaborate with business stakeholders, product owners, architects, data domain owners to understand current landscape and develop solutions in alignment with business & technology strategy. Assist in refining /evolving data strategy highlighting clear outcomes.
Deep understanding or desire to continue to learn new database technologies, cloud computing & storage services
Understanding of the pros / cons associated with various technology choices and ability to pick the right technology based on the use case

Required qualifications, capabilities, and skills
Formal training, or certification on data engineering concepts, and 5+ years of experience. In addition, demonstrated coaching and mentoring experience
Programming experience in Java, Python, Scala etc.
Experience in using distributed frameworks like Spark, Hadoop etc.
Experience with AWS services like Lambda, EC2, EMR, Redshift, Glue, S3, IAM, RDS, Aurora, DynamoDB etc.
Knowledge of cloud networking, security, storage, and compute services
Infrastructure provisioning experience using Cloud Formation, Terraform etc.
Experience implementing solutions leveraging CI / CD etc.

Preferred qualifications, capabilities, and skills
AWS Solutions Architect / Developer or any advanced level certification preferred
Experience and proficiency across the data lifecycle
Experience with database back-up, recovery, and archiving strategy
Proficient knowledge of linear algebra, statistics, and geometrical algorithms
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans","$116,853 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
"ABBVIE
3.9",3.9,"Crystal Lake, IL",Senior Data Engineer,"AbbVie Information Research is seeking a Senior Data Engineer who would contribute to the architecture, design, and development of the Data & Analytics Platform supporting world-class research and development at AbbVie. As a Senior Data Engineer, you will be a core member of a high-performance team of data engineers and architects focusing on driving technology innovation and continuous improvement. This role collaborates with solution architects, product owners, program managers, business analysts, infrastructure teams, and service providers to deliver the solutions.
Responsibilities:
Demonstrate mastery across a wide variety of data engineering activities, including data warehousing, master data management, data cataloging, system integration, data streaming, data visualization, data analysis, and data ops.
Demonstrate knowledge of pharmaceutical R&D/Life Science centric datasets and utilize this knowledge to advance agile, impactful, and cost-effective solutions rapidly.
Collaborate & contribute to the architecture, design, development, and maintenance of large-scale data & analytics platforms, system integrations, data pipelines, data models & API integrations to support evolving business strategy.
Contribute and maintain the team's methodology to conform and curate data, benchmarking against industry standards. Ensure that data are optimally standardized and analysis-ready.
Prototype emerging business use cases to validate technology approaches and propose potential solutions.
Research and recommend opportunities to adopt new technologies for continuous improvement.
Ensure compliance with applicable AbbVie software development lifecycle policies and procedures.

Bachelor's degree with 7 years of IT experience
Must have experience with software development life cycle; Experience with DevOps is preferred.
Must have experience with data analysis programming languages (e.g., SQL, Python & Apache Spark, SAS & R)
Must have experience with database technologies (e.g., Oracle, Postgres, Hive, and HBase)
Must have experience with ETL/Orchestration tools (e.g., Informatica, Autosys, and Airflow, etc.)
Experience with AWS and Cloudera Public Cloud architecture is preferred.
Experience working with Pharmaceutical R&D industry-centric datasets is preferred.


AbbVie is an equal opportunity employer including disability/vets. It is AbbVie’s policy to employ qualified persons of the greatest ability without discrimination against any employee or applicant for employment because of race, color, religion, national origin, age, sex (including pregnancy), physical or mental disability, medical condition, genetic information, gender identity or expression, sexual orientation, marital status, status as a disabled veteran, recently separated veteran, Armed Forces service medal veteran or active duty wartime or campaign badge veteran or a person’s relationship or association with a protected veteran, including spouses and other family members, or any other protected group status. We will take affirmative action to employ and advance in employment qualified minorities, women, individuals with a disability, disabled veterans, recently separated veterans, Armed Forces service medal veterans or active-duty wartime or campaign badge veterans. The Affirmative Action Plan is available for viewing in the Human Resources office during regular business hours.","$119,356 /yr (est.)",10000+ Employees,Company - Public,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2013,$10+ billion (USD)
"GE Healthcare
4.2",4.2,Illinois,Sr Data Engineer,"Job Description Summary
Responsible for managing business critical data engineering processes and data architecture solutions in order to enable analytical and reporting solutions. Responsible for analyzing and preparing the data needed for data science based outcomes. Also responsible for managing and maintaining metadata data structures besides providing necessary support for post-deployment related activities. Accountable to deliver results in a timely manner using agile methodologies.
Job Description
Roles and Responsibilities
In this role, you will:
Design & build technical data dictionaries and support business glossaries to analyze the datasets
Perform data profiling and data analysis for source systems, manually maintained data, healthcare industry standard messages
Design & build both logical and physical data models for both Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP) solutions
Develop and maintain data mapping specifications based on the results of data analysis and functional requirements
Build a variety of data loading & data transformation methods using multiple tools and technologies.
Design & build automated Extract, Transform & Load (ETL) jobs based on data mapping specifications
Manage metadata structures needed for building reusable Extract, Transform & Load (ETL) components.
Analyze the impact of changes to downstream systems/products and recommend alternatives to minimize the impact.
Design and build data warehouses and data marts
Minimum Qualifications
Bachelor's Degree in Computer Science or “STEM” Majors (Science, Technology, Engineering and Math) with minimum 6 years of relevant experience
Exposure to industry standard data modeling tools
Exposure to Extract, Transform & Load (ETL) tools like SSIS or Azure Data Factory
Exposure to industry standard BI tools like Power BI and Tableau
Hands-on experience in writing SQL scripts for SQL Server, MySQL, PostgreSQL or HiveQL
Exposure to unstructured datasets and ability to handle Avro, Parqueet, JSON file formats
Conduct exploratory data analysis and generate visual summaries of data. Identify data quality issues proactively.

Desired Qualifications:
Knowledge of for industrial applications in healthcare settings.
A good team player with self-driven execution capabilities.
Ability to communicate ideas clearly with cross teams.
Ability to showcase teamwork skills to achieve common goals, provide resolutions and share ideas.
Demonstrate the presentation and influencing skills
Eligibility Requirements
GE HealthCare may choose to sponsor visas as business needs dictate.
GE HealthCare will only employ those who are legally authorized to work in the United States for this opening.
Work/Life Balance
Our team puts a significant value on work-life balance. Having a healthy balance between your personal and professional life is crucial to your happiness and success here. We don’t focus on how many hours you spend at work or online. Instead, we’re happy to offer a flexible schedule so you can have a more productive and well-balanced life—both in and outside of work.
Mentorship & Career Growth
We maintain diverse engineering, and leadership perspectives and backgrounds across technology and beyond. Our employees are excited to share their experiences and mentor more junior engineers. Team members are highly encouraged to set up mentorship relationships with seasoned engineers, not only in our team, but also across the broader GE Healthcare population.

Inclusive Team Culture
Here at GE HealthCare, we embrace our differences. We are committed to furthering our culture of inclusion. We have many employee-led affinity groups, innovative benefit offerings, and encourage ongoing learning experiences.
While GE HealthCare does not currently require U.S. employees to be vaccinated against COVID-19, some GE HealthCare customers have vaccination mandates that may apply to certain GE HealthCare employees.

#LI-RW1
Additional Information
GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.
GE will only employ those who are legally authorized to work in the United States for this opening. Any offer of employment is conditioned upon the successful completion of a drug screen (as applicable).
Relocation Assistance Provided: No",#N/A,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1892,$10+ billion (USD)
"GSK
4.1",4.1,"Collegeville, PA",Senior Data Platform Engineer,"Site Name: San Francisco, Cambridge 300 Technology Square, London The Stanley Building, Upper Providence
Posted Date: May 12 2023

At GSK, we want to supercharge our data capability to better understand our patients and accelerate our ability to discover vaccines and medicines. The Onyx Research Data Platform organization represents a major investment by GSK R&D and Digital & Tech, designed to deliver a step-change in our ability to leverage data, knowledge, and prediction to find new medicines.
We are a full-stack shop consisting of product and portfolio leadership, data engineering, infrastructure and DevOps, data / metadata / knowledge platforms, and AI/ML and analysis platforms, all geared toward:
Building a next-generation, metadata- and automation-driven data experience for GSK’s scientists, engineers, and decision-makers, increasing productivity and reducing time spent on “data mechanics”
Providing best-in-class AI/ML and data analysis environments to accelerate our predictive capabilities and attract top-tier talent.
Aggressively engineering our data at scale, as one unified asset, to unlock the value of our unique collection of data and predictions in real-time.
Automation of end-to-end data flows: Faster and reliable ingestion of high throughput data in genetics, genomics and multi-omics, to extract value of investments in new technology (instrument to analysis-ready data in <12h)
Enabling governance by design of external and internal data: with engineered practical solutions for controlled use and monitoring
Innovative disease-specific and domain-expert specific data products: to enable computational scientists and their research unit collaborators to get faster to key insights leading to faster biopharmaceutical development cycles.
Supporting e2e code traceability and data provenance: Increasing assurance of data integrity through automation, integration
Improving engineering efficiency: Extensible, reusable, scalable, updateable, maintainable, virtualized traceable data and code would be driven by data engineering innovation and better resource utilization.
We are looking for a skilled and experienced Sr. Data Platform Engineer to join our growing team. Sr. Data Platform Engineers take full ownership of delivering high-performing, high-impact data framework products, and services, from a description of a problem customer Data Engineers are trying to solve all the way through to final delivery (and ongoing monitoring and operations). They are standard bearers for software engineering and quality coding practices within the team and are expected to mentor more junior engineers; they may even coordinate the work of more junior engineers on a large project. They devise useful metrics ensuring their services are meeting customer demand, having an impact, and iterate to deliver and improve on those metrics in an agile fashion.
The Data Platform team builds and manages reusable components and architectures designed to make it both fast and easy to build robust, scalable, production-grade data products and services in the challenging biomedical data space.
A Sr. Data Platform Engineer is a highly technical individual contributor, building modern, cloud-native systems for standardizing and templatizing data engineering process, such as:
Standardized data pipeline orchestration and processing
Standardized physical storage and search / indexing systems
Metadata management (data + metadata + versioning + provenance + governance)
API semantics and ontology management
Standard API architectures
Standard streaming semantics
Standard components for publishing data to file-based, relational, and other sorts of data stores
Tooling for QA / evaluation
Etc.
A Sr. Data Platform Engineer knows the metrics desired for their tools and services and iterates to deliver and improve on those metrics in an agile fashion.
Additional responsibilities also include:
Own architecture design of data platform and integration patterns to other internal systems
Mentor junior team members for better engineering standard and process
Partner with Infra and DevOps team where modifications to underlying tools (e.g. infrastructure as code, Cloud Ops, DevOps, logging / alerting) are needed to serve new use-cases, and to ensure operations are planned
Write fantastic code along with the proper unit, functional, and integration tests for code and services to ensure quality. Mentor more junior engineers in these skills
Stay up to date with developments in the open-source community around data engineering, data science, and similar tooling.
Spot opportunities to test out new tooling for internal use cases, as well as opportunities to contribute back to the community.
Why you?
Basic Qualifications:
We are looking for professionals with these required skills to achieve our goals:
Master's in computer science with a focus in Data Engineering, DataOps, DevOps, MLOps, Software Engineering, etc., plus 5 years job experience, (or PhD or bachelor’s degree in computer science plus 3-8 years job experience)
Experience with common distributed data tools in a production setting (Spark, Kafka, Hive, Presto, etc.)
Experience with specialized data architecture (e.g., data lake, lake house, data fabric, data mesh, optimizing physical layout for access patterns)
Experience with public cloud providers like AWS, Azure and GCP
Experience with search / indexing systems (e.g., Elasticsearch)
Preferred Qualifications:
If you have the following characteristics, it would be a plus:
Experience building and designing a DevOps first way of working.
Demonstrated excellence writing production Python, Java, Scala, Go, and/or C#/C++
Practical experience with agile software development and DevOsps-forward ways of working
Demonstrated experience building reusable components on top of the CNCF ecosystem including platforms like Kubernetes (or similar ecosystem)
Metrics-first mindset
#LI-GSK
#GSKOnyx
#GSKDSDE2022
#GSKDEN2022

Why GSK?
Our values and expectations are at the heart of everything we do and form an important part of our culture. These include Patient focus, Transparency, Respect, Integrity along with Courage, Accountability, Development, and Teamwork. As GSK focuses on our values and expectations and a culture of innovation, performance, and trust, the successful candidate will demonstrate the following capabilities:
Operating at pace and agile decision making – using evidence and applying judgement to balance pace, rigor, and risk.
Committed to delivering high-quality results, overcoming challenges, focusing on what matters, execution.
Continuously looking for opportunities to learn, build skills and share learning.
Sustaining energy and wellbeing
Building strong relationships and collaboration, honest and open conversations.
Budgeting and cost consciousness
GSK offers a competitive compensation package inclusive of the following: Competitive base salary, annual bonus based on company performance, access to healthcare and wellbeing programs, retirement savings program, paid time off, and employee recognition programs which reward exceptional achievements. The salary range for this role is: $145,877 to $197,363
GSK is a global biopharma company with a special purpose – to unite science, technology and talent to get ahead of disease together – so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns – as an organisation where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to positively impact the health of 2.5 billion people by the end of 2030.
Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it’s also about making GSK a place where people can thrive. We want GSK to be a workplace where everyone can feel a sense of belonging and thrive as set out in our Equal and Inclusive Treatment of Employees policy. We’re committed to being more proactive at all levels so that our workforce reflects the communities we work and hire in, and our GSK leadership reflects our GSK workforce.
If you require an accommodation or other assistance to apply for a job at GSK, please contact the GSK Service Centre at 1-877-694-7547 (US Toll Free) or +1 801 567 5155 (outside US).
GSK is an Equal Opportunity Employer and, in the US, we adhere to Affirmative Action principles. This ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, national origin, religion, sex, pregnancy, marital status, sexual orientation, gender identity/expression, age, disability, genetic information, military service, covered/protected veteran status or any other federal, state or local protected class.
Important notice to Employment businesses/ Agencies
GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.
Please note that if you are a US Licensed Healthcare Professional or Healthcare Professional as defined by the laws of the state issuing your license, GSK may be required to capture and report expenses GSK incurs, on your behalf, in the event you are afforded an interview for employment. This capture of applicable transfers of value is necessary to ensure GSK’s compliance to all federal and state US Transparency requirements. For more information, please visit GSK’s Transparency Reporting For the Record site.","$171,620 /yr (est.)",10000+ Employees,Company - Public,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,1830,$10+ billion (USD)
"MAQ Software
3.3",3.3,"Redmond, WA",Software Data Operations Engineer,"About MAQ Software
As 2021 Microsoft Power BI Partner of the Year, we enable leading companies to accelerate their business intelligence and analytics initiatives. Our solutions enable our clients to improve their operations, reduce costs, increase sales, and build stronger customer relationships.
Our clients consistently recognize us for providing architecture and governance frameworks, implementing best practices to optimize reports, and building team capability through training programs. Our innovative tools and 33 certified visuals expand Power BI capabilities to save time for decision makers.
As a premier supplier to Microsoft for two decades, our clients benefit from our extensive insights on the platform and engineering practices. As a Microsoft Partner with 10 Gold competencies, our clients improve their implementations with our breadth and depth of expertise.
With globally integrated teams in Redmond, Washington, and Mumbai, Hyderabad, and NOIDA India, we deliver solutions with increased velocity and tech intensity.
Inc. magazine has recognized us for sustained growth by listing us on the Inc. 5000 list ten times – a rare honor.

Engineering culture
We foster a strong engineering culture with a can-do attitude. All our key managers come from excellent educational backgrounds and have significant experience growing a company and mentoring software engineers. Due to our smaller size, we adopt the latest technologies and computing trends ahead of the larger industry players. As a part of the company’s globally distributed engineering team, our engineers gain exposure to the latest software engineering practices and fast development cycles.
Our developers routinely work on challenging technical problems that utilize the latest technologies for fast-paced software delivery.

Examples of some of our projects:
We built a supervised machine learning model that forecasts the impact of retail sales on our client’s overall revenue. We collected data from existing customer relationship management (CRM) and sales systems. We created a forecasting model in Azure Databricks using existing and custom linear regression to process the collected data. To reduce forecast runtime and achieve near real-time analysis, we modified the existing R libraries to SparkR. The improved insight helped our client proactively focus on retailers with the highest sales impact.
We built a check-in app for one of our client’s most attended event. A multinational technology company organizes an annual multi-event internal expo attended by thousands of their employees. The manual process of tracking attendance, sending acknowledgments, and receiving feedback was time consuming. To automate the process, we built a check-in app that uses mobile devices’ camera to capture the identification badge of each participant. The captured images are stored in an Azure Blob. An Azure Logic App reads the image content utilizing Optical Character Recognition (OCR) API to update attendance records. After the event, notifications are sent to attendees via Microsoft Teams to complete a feedback survey using a Microsoft Power Automate Bot. The Feedback App reports the survey responses to determine the Customer Satisfaction (CSAT) score of the event.
For another client with high volume data, we developed and implemented a hybrid data processing solution using Azure Stream Analytics and Azure Databricks to reduce data refresh time from 3 hours to less than 30 minutes. We sourced data from the Azure Event Hub, where refreshes originate. Refreshes are captured through stream analytics and the updated data is pushed to Azure Data Lake Storage (ADLS). The data is processed in ADLS, then pushed to Power BI for reporting.
To read about some of our recent projects, visit https://maqsoftware.com/case-studies

Responsibilities:
Analyze existing systems (30%)
Collect requirement specifications to analyze business processes and determine the exact nature of user’s system requirements, map process flow, discuss with module leaders and core team members to decide on the architecture.
Analyze existing system structures to provide solutions to improve computer systems to use cloud-based systems and services.
Analyze user requirements to match data available to large computer database source systems to implement solutions at reasonable performance and cost.
Design the processing steps and propose new systems based the user’s requirements. Interact with systems analysts/programmers to develop data migration tools, create processes for the new computer system and attend to ad-hoc issues related to day-to-day activities. Work with software developers in the implementation and testing phase.

Develop specifications and workflow (25%)
Prepare software specifications, flow charts, and process diagrams for software programmers to follow. Develop and maintain systems documentation such as design specifications, user manuals, technical manuals, descriptions of application operations, and methodology documentation.
Analyze feasibility using commercially available software systems (e.g., Microsoft Azure versus Amazon Web Services) and reporting systems (e.g., Power BI versus Tableau).

Analyze and verify implementation (25%)
Interact with systems analysts/programmers to develop data migration tools, create processes for the new computer system and attend to ad-hoc issues related to day-to-day activities.
Work with other software developers in the implementation and testing phase.
Setup test environment and compare data from multiple sources to verify reports for end users.

Review implementation status and reporting (10%)
Participate in technical collaboration meetings and periodical reviews of implementation status.
Report weekly task plan to the project management team for implementation of custom software.

Training and certifications (10%)
Participate in technical trainings and complete relevant industry courses and certifications.

Qualifications:
Undergraduate or graduate degree in Computer Science, Information Systems, Electrical Engineering, Applied Computational Math Sciences or related Engineering discipline.

Benefits & Salary:
Annual pay range $80,000 - $120,000.
Paid time off.
Comprehensive medical, dental and vision insurance with employee premiums paid in full.
401(k) retirement plan with 3% company match and immediate vesting.","$100,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$25 to $100 million (USD)
"Chubb INA Holdings Inc.
3.7",3.7,North Carolina,Principal Engineer Data Pipelines/Analytics,"Job Description (Principal Engineer Data Pipelines/Analytics):

We are looking for an experienced data engineer to join our team. You will use various methods to transform raw data into useful data systems. For example, you’ll create algorithms and conduct statistical analysis. Overall, you’ll strive for efficiency by aligning data systems with business goals. To succeed in this data engineering position, you should have strong analytical skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods. If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you.

Job Duties and Responsibilities:
Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Interpret trends and patterns
Conduct complex data analysis and report on results
Prepare data for prescriptive and predictive modeling
Build algorithms and prototypes
Combine raw information from different sources
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition
Develop analytical tools and programs
Collaborate with data scientists and architects on several projects

Qualifications

Requirements and Skills:
Previous experience as a data engineer or in a similar role
Technical expertise with data models, data mining, and segmentation techniques
Hands-on programming languages (e.g. Python and Java)
Hands-on experience with SQL/No SQL databases. Must have worked with Big Data.
Great numerical and analytical skills
Degree in Computer Science, IT, or similar field; a master’s is a plus
Knowledge of Azure cloud ecosystem

In Jersey City, NJ the pay range for the role is $128,500 to $215,000. The specific offer will depend on an applicant’s skills and other factors. This role may also be eligible to participate in a discretionary annual incentive program. Chubb offers a comprehensive benefits package, more details on which can be found at https://careers.chubb.com/global/en/north-america. This range is specific to Jersey City, NJ and may not be applicable to other locations.
EEO Statement
At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion,and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin,ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law.Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliationagainst any individual who reports discrimination or harassment.","$171,750 /yr (est.)",10000+ Employees,Company - Public,Insurance,Insurance Carriers,1792,$10+ billion (USD)
"BAE Systems
3.9",3.9,"Reston, VA",Data Analyst and Visualization Engineer,"Job Description
What’s it like realizing your potential at an innovative company that takes on some of the world’s most important challenges? Rewarding.

As a member of our Defense and Space Intelligence Technology team, you will join a diverse group of driven professionals who design the products and systems that support enhanced military capabilities, protect national security, and keep critical information and infrastructure secure. With us, you will be able to make an impact while you hone your skills and grow in your career.

BAE Systems is looking for a Data Analyst and Visualization Engineer to add their skills to our team supporting enterprise engineering and operations efforts for DIA IT infrastructure and communications services. This is an exciting and dynamic job opportunity where you will support a dynamic operational customer and help advance the United States military's analytic and operational capabilities.

In this job, the ideal candidate will:
Perform data collection, normalization, and visualization activities associated with Network and UC performance, cost, location, system warranty, and ticket data.
Performs data queries using monitoring systems and custom queries in a variety of programming languages (C#, SQL, Java, Python, and/or R).
Oversees data ingesting into enterprise data mining solutions, and generates visualization and dashboards suited to the purpose of the data query.
Develop data visualization resources and strategy to strategically increase enterprise view of data within customer’s specific context.
Create visual displays for collection system usage statistics from algorithm output, etc. (e.g., dashboards and automated system reports)
Collaborate with domain experts to ensure data interpretability.
Review and analyze collected data to determine the performance and health of a system or network; assists in developing, implementing, monitoring, and analyzing key performance indicators.
Identify new patterns and areas requiring attention or improvement.
Map and perform data transformation (database development, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation)
Perform data analysis interpretation and data management duties.

Required Education, Experience, & Skills
Bachelor's Degree in Statistics or Data Science and 7+ years of data analysis experience
A Statistics or Data Science Master's Degree or PhD can be applied in lieu of a relevant Bachelors
Demonstrated experience utilizing Splunk ITSI, and Tableau data mining solutions.
Familiarity with existing and emerging data mining technology
8570 Information Assurance Technical (IAT) Level II certification (Security+ or other) Training and Certifications
Splunk IT Service Intelligence Certified Admin credential

Preferred Education, Experience, & Skills
Comfortable working with a variety of programming languages (C#, SQL, Java, Python, and/or R).

About BAE Systems Intelligence & Security
BAE Systems, Inc. is the U.S. subsidiary of BAE Systems plc, an international defense, aerospace and security company which delivers a full range of products and services for air, land and naval forces, as well as advanced electronics, security, information technology solutions and customer support services. Improving the future and protecting lives is an ambitious mission, but it’s what we do at BAE Systems. Working here means using your passion and ingenuity where it counts – defending national security with breakthrough technology, superior products, and intelligence solutions. As you develop the latest technology and defend national security, you will continually hone your skills on a team—making a big impact on a global scale. At BAE Systems, you’ll find a rewarding career that truly makes a difference. Intelligence & Security (I&S), based in McLean, Virginia, designs and delivers advanced defense, intelligence, and security solutions that support the important missions of our customers. Our pride and dedication shows in everything we do—from intelligence analysis, cyber operations and IT expertise to systems development, systems integration, and operations and maintenance services. Knowing that our work enables the U.S. military and government to recognize, manage and defeat threats inspires us to push ourselves and our technologies to new levels. At BAE Systems, we celebrate the array of skills, experiences, and perspectives our employees bring to the table. For us, differences are a source of strength. We’re laser-focused on high performance, and we work hard every day to nurture an inclusive culture where all employees can innovate and thrive. Here, you will not only build your career, but you will also enjoy work-life balance, uncover new experiences, and collaborate with passionate colleagues.",#N/A,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1999,$10+ billion (USD)
"Midmark Corporation
4.6",4.6,"Traverse City, MI",RTLS Data Analytics Engineer,"JOB SUMMARY:
Work closely with an Agile team to build, deploy, and maintain a transformative cloud based data infrastructure that supports an analytics solution. This solution will generate insights from our customersâ€™ data, enabling them to advance the data-driven decision-making capabilities of our Real Time Locating System (RTLS). This role requires understanding of cloud computing services and infrastructure, architecture, data engineering, data analysis, data visualization, and a basic understanding of data science techniques and workflows. Adopts best practices to ensure security, privacy, and compliance for all data assets. Oversees CI/CD processes within the team to ensure sustainable and efficient deployment of code. Develops solutions supporting the movement of data and information assets. Experience with the Azure data platform required. Experience with Tableau or other industry leading BI tool is preferred.
ESSENTIAL/PRIMARY DUTIES:
Assess existing analytics infrastructure and business processes; advise on and contribute to the design of best-in-class, modern solutions
Design and develop the analytical layer, building cloud data warehouses, data lakes, ETL/ELT pipelines, and orchestration tools
Write code in SQL and Python, and use software engineering best-practices such as Git and CI/CD; experience with Spark preferred
Communicate clearly, effectively, and proactively with local and remote users, teammates and stakeholders
Install and maintain cloud-based applications, systems, or associated infrastructures in the data and analytics space and support the deployment of analytics solutions across multiple environments
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions
Review security assessment recommendations and remediate issues to ensure that appropriate information security standards are met
SECONDARY DUTIES:
Manage and monitor SLAs for uptime and performance requirements in the Microsoft Azure environment, troubleshooting and resolving errors when they occur
Document and adhere to all critical processes
Stay informed of emerging cloud technologies and evaluates the value to the organization's operations
EDUCATION and/or EXPERIENCE:
Bachelor's degree (B. A.) from four-year college or university in computer science or related engineering field; or equivalent combination of education and experience.
5+ years working with relational databases and query languages
5+ years building data pipelines in production and the ability to work across structured, semi-structured and unstructured data
5+ years data modeling (e.g., star schema, entity-relationship)
3+ years writing clean, maintainable and robust code in Python, Scala, Java or similar coding languages is desirable
Experience working with healthcare data encompassing PHI/PII
Experience in writing queries across large data sets
Expertise in Agile software engineering concepts and best practices and/or DevOps is desirable
Experience working with big data technologies (Spark, Hadoop) is desirable
COMPETENCY/SKILL REQUIREMENTS:
Exceptional knowledge of data analytics, such as dimensional modeling, ETL\ELT, reporting tools, data governance, data warehousing, structured and unstructured data
Must have proven success with developing solutions on cloud computing services and infrastructure in the data and analytics space, with focus on Microsoft Azure data platform and DevOps
Must have experience managing stakeholders and collaborating with customers
Use of data visualization through BI tool sets, Tableau preferred
Ability to define and complete work self-directed, under limited supervision
Ability to work remotely to resolve issues
Ability to document and follow procedures
Knowledge and understanding of BI Reporting Tools and Troubleshooting
Knowledge and understanding of cloud scaling and security practices
SUPERVISORY RESPONSIBILITIES:
Mentoring of teammates in similar responsibility","$90,326 /yr (est.)",1001 to 5000 Employees,Company - Private,Manufacturing,Health Care Products Manufacturing,1915,$500 million to $1 billion (USD)
"Kia America, Inc.
3.8",3.8,"Irvine, CA",Field Data Analysis Engineer,"At Kia, we’re creating award-winning products and redefining what value means in the automotive industry. It takes a special group of individuals to do what we do, and we do it together. Our culture is fast-paced, collaborative, and innovative. Our people thrive on thinking differently and challenging the status quo. We are creating something special here, a culture of learning and opportunity, where you can help Kia achieve big things and most importantly, feel passionate and connected to your work every day.
Kia provides team members with competitive benefits including premium paid medical, dental and vision coverage for you and your dependents, 401(k) plan matching of 100% up to 6% of the salary deferral, and time off starting at 14 days per year. Kia also offers company lease and purchase programs, company-wide holiday shutdown, paid volunteer hours, and premium lifestyle amenities at our corporate campus in Irvine, California.
Status
Exempt
Summary
The purpose of this position is to support the Field Data Analysis Manager and Data Evaluation Team (DET) by identifying at an early stage in a model’s life cycle, any emerging Potential Safety Issues (PSIs) through data searches and sophisticated analysis of Kia America (KUS) data from all primary data sources, including warranty, Techline, Consumer Affairs, customer pay, NHTSA Vehicle Owner Questionnaires (VOQs), as well as other data sources, as applicable. This position will also support safety and noncompliance recall decision-making and requests by NHTSA for Defect Petitions, Preliminary Evaluations, investigations, and pre-investigative requests for information. This position will also support KUS Legal Department for defense of class action suits and in-house and outside counsel requests.
Major Responsibilities
1st Priority - 40%
Review data sources as assigned for new PSIs. Work from analytic dashboards and generate reports showing emerging field issues using primary field data sources, as well as other data sources as appropriate. Investigate PSIs across data sources to determine current complaint counts, rates, and any trends from the data, from emerging field issues stage through safety evaluation list stage and to field action decision stage, if applicable. Identify PSIs and potential noncompliance issues and inform Field Data Analysis Manager of them at the earliest point in time.
2nd Priority - 25%
Coordinate with Forensic Engineering and Investigation team to hand over and collaborate on analyses and updates. Provide analysis explanations to management, coordinators, other departments, and Kia HQ staff.
3rd Priority - 20%
Provide data analysis support for NHTSA inquiries, investigations, monthly NHTSA meeting preparation, and KUS Legal in-house and outside counsel.
4th Priority - 15%
Develop sophisticated processes for evaluating data and identifying PSIs by utilizing new technologies and analytic tools. Assist in the continuous improvement and development of Safety Data Analytics Infrastructure (SDAI). Share new data analysis techniques with team.
Education/Certification
BS degree in Engineering or Automotive Technology or equivalent work experience required.
Overall Experience
3-7 years of experience in technical positions such as Safety Analysis, Product Quality, or in a product engineering environment.
Directly Related Experience
Automotive, aerospace, or similar technical work experience required
Working knowledge of statistical analysis concepts and software.
Other Requirements:
Schedule(s) may vary due to needs of the business including but not limited to working outside of normal business hours, travel, weekends and/or holidays.
Perform other duties as assigned.
Skills
Basic knowledge of SAS programing language and Structured Query Language (SQL).
Excellent analytical skills and attention to detail.
Excellent teamwork skills.
Excellent written and oral communication skills.
Intermediate knowledge of automotive systems and components.
Knowledge of PC software such as Microsoft Excel and PowerPoint.
Knowledge of SAS Visual Analytics.
Knowledge of statistics.
Competencies
CHALLENGE - Solving Complex Problems
COLLABORATION - Building and Supporting Teams
CUSTOMER - Serving Customers
GLOBALITY - Showing Community and Social Responsibility
PEOPLE - Interacting with People at Different Levels
Pay Range
$69,547.00 - $90,401.00
Pay will be based on several variables that are unique to each candidate, including but not limited to, job-related skills, experience, relevant education or training, etc.

Equal Employment Opportunities
KUS provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, ancestry, national origin, sex, including pregnancy and childbirth and related medical conditions, gender, gender identity, gender expression, age, legally protected physical disability or mental disability, legally protected medical condition, marital status, sexual orientation, family care or medical leave status, protected veteran or military status, genetic information or any other characteristic protected by applicable law. KUS complies with applicable law governing non-discrimination in employment in every location in which KUS has offices. The KUS EEO policy applies to all areas of employment, including recruitment, hiring, training, promotion, compensation, benefits, discipline, termination and all other privileges, terms and conditions of employment.

Disclaimer: The above information on this job description has been designed to indicate the general nature and level of work performed by employees within this classification and for this position. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities, and qualifications required of employees assigned to this job.","$79,974 /yr (est.)",501 to 1000 Employees,Subsidiary or Business Segment,Manufacturing,Transportation Equipment Manufacturing,1994,$10+ billion (USD)
"AEEC, LLC.
2.8",2.8,"Salt Lake City, UT",Data and Machine Learning Engineer,"AEEC is currently seeking a Data and Machine Learning Engineer to support our customer in a remote work location. The Data and Machine Learning Engineer has demonstrated expertise with IT Platform implementation, Machine Learning model implementation, experience with analytic solutions that drive cost vs. risk analysis. Statistics, statistical modeling, and statistical process control. Developing, documenting and communicating comprehensive business and data understanding to all stakeholders; Solid understanding of relational data structures and business intelligence reporting concepts.
Responsibilities
Develop machine learning models and deploy them as part of a pipeline.
Build data sets that provide answers to key business decisions like evaluating risk and reducing costs.
Use analytics data to create visualizations.
Research, design, develop, and enhance analytics solutions using a range of reporting, data mining, and analytics techniques, methods, and tools.
Perform analysis of data for Extraction, Transformation, and Load (ETL) strategies, pattern recognition, and application of analytical tools.
Contribute to approaches to data quality, metadata, and/or business rules as they apply to data mining and analytics projects.
Create and maintain scripts to manipulate data and automate complex workflows.
Provide guidance on business and technical issues affecting projects, such as data access, data quality, storage capacity, and analytic tools and software.
Independently, or as part of a team, perform comprehensive and thorough data collection, utilizing a variety of data sources to analyze trends, problems, or other related issues of major importance in support of audits and investigations.
Technical Skills
Azure and AWS cloud platform analytics and ML service offerings.
Data visualization software experience using power bi, Tableu or equivalent.
3+ years in a Data Analyst Role
2+ years Data Mining, Modeling and Reporting
Strong attention to detail
Ability to collaborate effectively and work as part of a team
Data query; import/read/export files
Table joins, lookups, conditional processing, loops, arrays, macros
Automation scripting for routine tasks
Languages:
SAS / Enterprise Guide
SQL/Proc SQL
Python
R
Korn Shell/Perl
HTML/CSS/JavaScript

Statistical knowledge:
Regression modeling
Confidence interval
Correlation
Hypothesis testing
Sample size calculation
Education

Bachelor’s Degree in Mathematics, Computer Engineering or Computer Science or equivalent experience in related areas.
Citizenship
US citizenship is not required for this role, but must be located in the United States. All applicants will be subject to a background investigation.
Assessment
Candidates may be required to complete a skills assessment to assess their skills.

Physical Demands: While performing duties of the job, incumbent is occasionally required to stand, walk, sit, use hands and fingers, handle or feel objects, tools, or controls, reach with hands and arms, talk and hear. Employee must occasionally lift and/or move up to 25 pounds. Specific vision abilities required by job include close vision, distance vision, color vision, peripheral vision, depth perception and the ability to adjust and focus.
Work Environment: The noise level in the work environment is usually moderate.

About AEEC
AEEC is an award winning CMMI Level 3 and ISO accredited professional services organization with a proven track record of providing technology and engineering solutions to the commercial and federal market since 1995. AEEC provides leading edge of innovative technology solutions to solve customer’s complex business problems. We build long lasting business relationships based upon integrity, resourcefulness, ingenuity, and fully delivering commitments. AEEC possesses the fundamental IT and engineering skills needed to objectively evaluate problems and develop technically sound, cost effective solutions. AEEC has been featured in a Harvard Business School Case Study on the topic of innovation.

AEEC Values: Our customers and their missions come first. We want to add value to every engagement. We will demonstrate integrity and responsiveness in all of our business practices. Teamwork and respect for everyone.

AEEC Vision: To be the best partner to our customers and a great place to work and grow.

AEEC Mission: Our mission is our customers’ success. We strive to responsively solve our customers’ needs. Our current skills are IT and Engineering services.

Benefits: AEEC offers competitive wages with benefits (Medical Insurance, Life Insurance, Short term/Long term disability dismemberment, dental and vision insurance coverage, Flex Spending, 401K, and a 529 college savings plan). Medical, vision, and dental benefits start the first of the month following start of full time employment.

AEEC is a Federal Contractor and agrees to comply with all provisions set forth in Equal Employment Opportunity, Executive Order 11246, as amended, Section 503 of the Rehabilitation Act of 1973, as amended, 38 U.S.C. 4212 of the Vietnam Veterans’ Readjustment Assistance Act of 1974, as amended, 29 CFR Part 471, Appendix A to Subpart A (Executive Order 13496), and Executive Order 11246. If you are an individual with a disability and would like to request a reasonable accommodation as part of the employment selection process, please contact Kim Hartley at 703-766-4300.

AEEC is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.

AEEC invites any applicant and/or employee to review the Company’s written Affirmative Action Plan. This plan is available for inspection upon request by contacting Kim Hartley at 703-766-4300.","$103,231 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,1995,$25 to $100 million (USD)
"Capgemini
3.8",3.8,"Malvern, PA",data engineer,"Duration: 8+ months

Job Description:

Collaborate with product owners, UX designers, data analysts, scrum masters, and developers to develop a world-class user experience.
Recommend and effectively explain improvements in functionality, and UX elements that enable users to achieve their goals.
Be an expert in building working relationships across divisions.
Research, advocate and implement industry standard coding methodologies to improve workflows through the selection of evolving technologies and tools that your team will use to build expert-level user experiences.

Qualifications
Excellent analytical skills are essential
Successful candidates will have exhibited the ability to begin working independently and have demonstrated ability to apply theory and concept
Must have ability to communicate effectively with team members and others in the work group, as well as with customers
Must be comfortable working in MS Office, and industry standard statistics and data visualization Packages Power BI/Tableau
Experience working with SQL or statistical languages R or python
An accredited 4-year degree, preferably in accounting, finance, statistics, economics, mathematics, or a similar field with quantitative coursework and 1-2 years of experience OR Masters degree in an associated field.

The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.","$92,252 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,1967,$10+ billion (USD)
"R1 RCM, Inc.
3.4",3.4,Remote,Data Engineer III,"The Software Engineer III serves as a Data Engineer focused on building next generation of healthcare products and services. As part of a collaborative, motivated Agile-based team, the Software Engineer III is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
Responsibilities
Create and maintain optimal data pipeline architecture to assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders to optimize the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies.
Work with stakeholders including the Product, Data and Engineering teams to assist with data-related technical issues and support their data infrastructure needs.
Adhere to and ensure engineering best practices using modern SDLC that enables CI/CD and favors automation, auditability, automated testing, infrastructure, and policy as code.
Ability to collaborate effectively with other architects and engineers delivering end to end, high-performing, highly reliable, scalable, and operable solutions.
Required Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical field; or equivalent experience.
3+ years of experience delivering data engineering applications and services.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets .
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience in any one of the big data technologies (Hadoop, MapReduce , Spark, Data Lake etc. )
Experience in any one of the stream-processing systems ( Kafka, Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages : Python, Java, Scala, etc.
Understanding of modern software development including version control, unit testing, and continuous integration and deployment.
Strong communication skills, with proven ability to present complex ideas and to document concisely.
Preferred Qualifications
Working knowledge of Cloud providers and services such as Amazon AWS or Microsoft Azure.
Experience with relational SQL and NoSQL databases, including MongoDB and ElasticSearch.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12.
Knowledge of the healthcare revenue cycle, EMRs, practice management systems.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",#N/A,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2003,Unknown / Non-Applicable
"Shutterfly
3.3",3.3,"Tempe, AZ",Senior Data Engineer,"Description
At Shutterfly, we make life’s experiences unforgettable. We believe there is extraordinary power in the self-expression. That’s why our family of brands helps customers create products and capture moments that reflect who they uniquely are.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.
What You'll Do as a Sr. Data Engineer II
Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines am improve the operation and performance of the Data Warehouse
The Skills You'll Bring
Expert knowledge Python, Spark, SQL scripting and Java
7+ years of hands-on experience with building data ingress frameworks and experience with dealing with a wide variety of source data formats
Hands-on experience in building data applications, including design, implementation, debugging, and support.
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it’s the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.","$135,965 /yr (est.)",10000+ Employees,Company - Private,Information Technology,Internet & Web Services,1999,$1 to $5 billion (USD)
"Chewy
3.5",3.5,"Richardson, TX",Data Engineer II,"Our Opportunity:
Chewy’s Data Analytics team has an exciting opportunity for a Data Engineer III to join the pack. Leveraging your strong expertise and background in data engineering and data analysis, you will be a part of a team responsible for operational and tactical reporting generating insights to grow Customer Service operations and planning. This includes building high quality data pipelines that drives analytic solutions and creating data products for analytics and data scientist team members to improve their productivity. Our organization is a fast-paced environment with new challenges and new opportunities each day. You will be responsible for building and implementing data products and technologies which will handle the growing business needs and play a key role in redefining what it means to be a world-class customer service organization.
What You'll Do:
Design, develop, optimize, and maintain data architecture and pipelines using design and programming patterns that follow best-in-class practices and principles.
Manage, maintain, and improve our SSOT tables and data marts, which drive critical business decisions every day.
Work closely with analytics teams and business partners, serving as a trusted partner who can advise, consult, and communicate data solutions.
Mentor and coach other data practitioners on data standards and practices.
Lead the evaluation, implementation and deployment of emerging tools and process for data engineering to improve overall productivity for the organization.
Partner with leaders, vendors, and other data practitioners across Chewy to develop technical architectures for strategic enterprise projects and initiatives.
Document technical details of work and follow agile sprint methodology, using tools like Jira, Confluence etc.

What You'll Need:
Bachelor of Science or Master’s degree in Computer Science, Engineering, Information Systems, Mathematics or related field
3+ years of enterprise experience as a data engineer and/or software engineer
3+ years applying and implementing database and data modeling techniques
3+ years working with enterprise data warehouse (ex. Snowflake, Vertica) and cloud environments (ex. AWS)
3+ years of experience building data integrations and pipelines from data lake, APIs, relational databases, and third-party systems
Strong software development skills in SQL
Self-motivated with strong problem-solving and self-learning skills.
Bonus:
Strong working knowledge of Python programming
Excellent communication and collaboration skills with ability to influence and guide stakeholders
Experience building dimensional models in data warehouses
Experience with data streaming tools and technologies like Kafka, Kinesis, or similar technologies
AWS Developer Certifications
E-commerce, Retail or startup experience
Experience in BI tools such as Tableau, Plotly, Power BI, etc.
Chewy is committed to equal opportunity. We value and embrace diversity and inclusion of all Team Members. If you have a disability under the Americans with Disabilities Act or similar law, and you need an accommodation during the application process or to perform these job requirements, or if you need a religious accommodation, please contact CAAR@chewy.com.

If you have a question regarding your application, please contact HR@chewy.com.

To access Chewy's Customer Privacy Policy, please click here. To access Chewy's California CPRA Job Applicant Privacy Policy, please click here.",#N/A,10000+ Employees,Company - Public,Retail & Wholesale,Pet & Pet Supplies Stores,2011,$5 to $25 million (USD)
"BAE Systems
3.9",3.9,"Washington, DC",Data Analyst and Visualization Engineer,"Job Description
What’s it like realizing your potential at an innovative company that takes on some of the world’s most important challenges? Rewarding.

As a member of our Defense and Space Intelligence Technology team, you will join a diverse group of driven professionals who design the products and systems that support enhanced military capabilities, protect national security, and keep critical information and infrastructure secure. With us, you will be able to make an impact while you hone your skills and grow in your career.

BAE Systems is looking for a Data Analyst and Visualization Engineer to add their skills to our team supporting enterprise engineering and operations efforts for DIA IT infrastructure and communications services. This is an exciting and dynamic job opportunity where you will support a dynamic operational customer and help advance the United States military's analytic and operational capabilities.

In this job, the ideal candidate will:
Perform data collection, normalization, and visualization activities associated with Network and UC performance, cost, location, system warranty, and ticket data.
Performs data queries using monitoring systems and custom queries in a variety of programming languages (C#, SQL, Java, Python, and/or R).
Oversees data ingesting into enterprise data mining solutions, and generates visualization and dashboards suited to the purpose of the data query.
Develop data visualization resources and strategy to strategically increase enterprise view of data within customer’s specific context.
Create visual displays for collection system usage statistics from algorithm output, etc. (e.g., dashboards and automated system reports)
Collaborate with domain experts to ensure data interpretability.
Review and analyze collected data to determine the performance and health of a system or network; assists in developing, implementing, monitoring, and analyzing key performance indicators.
Identify new patterns and areas requiring attention or improvement.
Map and perform data transformation (database development, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation)
Perform data analysis interpretation and data management duties.

Required Education, Experience, & Skills
Bachelor's Degree in Statistics or Data Science and 7+ years of data analysis experience
A Statistics or Data Science Master's Degree or PhD can be applied in lieu of a relevant Bachelors
Demonstrated experience utilizing Splunk ITSI, and Tableau data mining solutions.
Familiarity with existing and emerging data mining technology
8570 Information Assurance Technical (IAT) Level II certification (Security+ or other) Training and Certifications
Splunk IT Service Intelligence Certified Admin credential

Preferred Education, Experience, & Skills
Comfortable working with a variety of programming languages (C#, SQL, Java, Python, and/or R).

About BAE Systems Intelligence & Security
BAE Systems, Inc. is the U.S. subsidiary of BAE Systems plc, an international defense, aerospace and security company which delivers a full range of products and services for air, land and naval forces, as well as advanced electronics, security, information technology solutions and customer support services. Improving the future and protecting lives is an ambitious mission, but it’s what we do at BAE Systems. Working here means using your passion and ingenuity where it counts – defending national security with breakthrough technology, superior products, and intelligence solutions. As you develop the latest technology and defend national security, you will continually hone your skills on a team—making a big impact on a global scale. At BAE Systems, you’ll find a rewarding career that truly makes a difference. Intelligence & Security (I&S), based in McLean, Virginia, designs and delivers advanced defense, intelligence, and security solutions that support the important missions of our customers. Our pride and dedication shows in everything we do—from intelligence analysis, cyber operations and IT expertise to systems development, systems integration, and operations and maintenance services. Knowing that our work enables the U.S. military and government to recognize, manage and defeat threats inspires us to push ourselves and our technologies to new levels. At BAE Systems, we celebrate the array of skills, experiences, and perspectives our employees bring to the table. For us, differences are a source of strength. We’re laser-focused on high performance, and we work hard every day to nurture an inclusive culture where all employees can innovate and thrive. Here, you will not only build your career, but you will also enjoy work-life balance, uncover new experiences, and collaborate with passionate colleagues.",#N/A,10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1999,$10+ billion (USD)
"Riverside Research
4.2",4.2,"Wright Patterson AFB, OH",Data Engineer (TS/SCI clearance),"Riverside Research is an independent National Security Nonprofit dedicated to research and development in the national interest. We provide high-end technical services, research and development, and prototype solutions to some of the country's most challenging technical problems.

Job Number: 1246
Riverside Research is seeking full-time Data Engineer to support Director, Air Force Chief Data Office (SAF/CO) sponsored activities across the Air Force Enterprise to ensure the visibility, accessibility, understanding, sharing, and trustworthiness of data across air, space, and cyberspace domains. Candidates will provide subject matter expertise in and perform on multidisciplinary teams that support data preparation and architecture, development of agile algorithmic solutions, evaluate and/or execute data governance and data maturity models; and conduct data analytics using state of the art mathematical and machine learning/artificial intelligence techniques and other data analytic lines of research/effort. Positions will be at various CONUS Air Force installations and the National Capital Region.
All Riverside Research opportunities require U.S. Citizenship
Job Duties
Provide expertise on all data concepts for the broader advanced analytics group, and inspire the adoption of advanced analytics, data engineering and data science across the organization.
This will include Installing continuous pipelines of large pools of filtered information so that data analyst/scientists can pull relevant data sets for their analyses.
Required Qualifications:
Top Secret clearance with SCI adjudication
Bachelor's degree in the requisite relevant field. A Master's degree in a relevant field may be substituted for 3 years of general experience.
7 years or more of experience in the data engineering field, at least three of which must have been in a data analytics environment preferably in DoD or the intelligence community.
Familiarity with the manipulation of unstructured data in a data analytics environment, and the use of open-source tools, cloud computing, machine learning and data visualization.
Familiar with specialized languages relevant to the technologies employed such as Apache, Hadoop, etc.

Riverside Research strives to be one of America's premier providers of independent, trusted technical and scientific expertise. We continue to add experienced and technically astute staff who are highly motivated to help our DoD and Intelligence Community (IC) customers deliver world class programs. As a not-for-profit, technology-oriented defense company, we believe service to customers and support of our staff is our mission. Our goal is to serve as a destination company by providing an industry-leading, positive, and rewarding employee experience for all who join us. We aspire to be a valued partner to our customers and to earn their trust through our unwavering commitment to achieve timely, innovative, cost-effective and mission-focused solutions.
All positions at Riverside Research are subject to background investigations. Employment is contingent upon successful completion of a background investigation including criminal history and identity check.
Riverside Research does not mandate COVID vaccination as a condition of employment. However, proof of vaccination or negative test may be required to enter certain government facilities and sites. Vaccination requirements will depend on the status of the federal contractor mandate and customer site-specific requirements. To protect the health and safety of its employees, their families, and to comply with customer requirements, the company requires all employees to disclose vaccination status (upon hire).
Our EEO Policy
Riverside Research is an equal opportunity employer. We recruit, employ, train, compensate and promote without regard to race, religion, sex, color, national origin, age, gender identity, sexual orientation, marital status, disability/veteran, status as a protected veteran, or any other basis protected by applicable federal, state and local law.
If you need assistance at any time in our application or interview process, please contact Recruiting at email Recruiting@RiversideResearch.org. A member of the Recruiting team will be available to assist.
This contractor and subcontractor shall abide by the requirements of 41 CFR 60-741.5(a). This regulation prohibits discrimination against qualified individuals on the basis of disability and requires affirmative action by covered prime contractors and subcontractors to employ and advance in employment qualified individuals with disabilities.
This contractor and subcontractor shall abide by the requirements of 41 CFR 60-300.5(a). This regulation prohibits discrimination against qualified protected veterans and requires affirmative action by covered contractors and subcontractors to employ and advance in employment qualified protected veterans.
For more information on ""EEO is the Law,"" please visit:
http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf
https://www.dol.gov/sites/dolgov/files/ofccp/regs/compliance/posters/pdf/eeopost.pdf","$64,624 /yr (est.)",501 to 1000 Employees,Nonprofit Organization,Government & Public Administration,National Agencies,1967,$25 to $100 million (USD)
"Deloitte
4.1",4.1,"Lake Mary, FL",Senior Cloud Data Migration Engineer,"Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with our US Delivery Center - we are breaking the mold of a typical Delivery Center.

Our US Delivery Centers have been growing since 2014 with significant, continued growth on the horizon. Interested? Read more about our opportunity below ...

Work you'll do/Responsibilities
Work with the team to evaluate business needs and priorities, liaise with key business partners and address team needs related to data systems and management.
Translate business requirements into technical specifications; establish and define details, definitions, and requirements of applications, components and enhancements.
Participate in project planning; identifying milestones, deliverables and resource requirements; tracks activities and task execution.
Generate design, development, test plans, detailed functional specifications documents, user interface design, and process flow charts for execution of programming.
Develop data pipelines / APIs using Python, SQL, potentially Spark and AWS, Azure or GCP Methods.
Use an analytical, data-driven approach to drive a deep understanding of fast changing business.
Build large-scale batch and real-time data pipelines with data processing frameworks in AWS, Azure or GCP cloud platform.
Moving data from on-prem to cloud and cloud data conversions.

The Team

Artificial Intelligence & Data Engineering:

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Artificial Intelligence & Data Engineering team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Artificial Intelligence & Data Engineering will work with our clients to:

Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.

Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.

Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.

Qualifications

Required
6+ years of experience in data engineering with an emphasis on data analytics and reporting.
6+ years of experience with at least one of the following cloud platforms: Microsoft Azure, Amazon Web Services (AWS), Google Cloud Platform (GCP), others.
6+ years of experience in SQL, data transformations, statistical analysis, and troubleshooting across more than one Database Platform (Cassandra, MySQL, Snowflake, PostgreSQL, Redshift, Azure SQL Data Warehouse, Databricks, etc.).
6+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines.
6+ years of experience with one or more of the follow scripting languages: Python, SQL, Kafka and/or other.
6+ years of experience designing and building solutions utilizing various Cloud services such as EC2, S3, EMR, Kinesis, RDS, Redshift/Spectrum, Lambda, Glue, Athena, API gateway, etc.
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or equivalent experience
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future.
Travel up to 10% annually, on average, based on the work you do and the clients and industries/sectors you serve
Co-location expectation 10-30%
Must live within a commutable distance or relocate near a center location:

1.Lake Mary, FL
2.Mechanicsburg, PA
3.Gilbert, AZ

Preferred
AWS, Azure and/or Google Cloud Platform Certification.
Master's degree or higher.
Expertise in one or more programming languages, preferably Scala, PySpark and/or Python.
Experience working with either a Map Reduce or an MPP system on any size/scale.
Experience working with agile development methodologies such as Sprint and Scrum.

LI-FA1",#N/A,10000+ Employees,Company - Private,Financial Services,Accounting & Tax,1850,$10+ billion (USD)
"LER TechForce
4.3",4.3,"Auburn Hills, MI",Battery Systems Data Analyst Engineer,"The Battery Systems Data Analyst Engineer will be responsible for the development of battery data analytics and predictive system engineering tools and application of these tools for design and development of xEV Battery Systems. The candidate will take a leading role in analyzing, mining, processing, and tracking the increasing amount of battery test data generated from battery, vehicle and fleet testing during the course of product development, testing, and validation. The candidate will be involved in multiple vehicle programs and deal with a broad range of data, i.e. cell level, pack-level, vehicle-level, and fleet-level. The ideal candidate is expected to be highly analytical, have experience in data analysis/processing, and have some background in electrified powertrain vehicles.
The responsibilities include but not limited to:
Develop, document the engineering requirements for automotive traction battery systems. Lead other engineers and teams to balance requirements.
Develop battery system usable energy walk system model by taking account of all system factors in usable energy walk. Create initial models, and develop the improvement plans and targets to optimize the system usable energy.
Develop and apply battery data analytics and predictive system engineering tools (1D, simullink/matlab) for prediction and verification of battery system performance and thermal.
Develop battery usage profiles and mission profiles for battery life modeling/ prediction/simulations and component designs for durability and other applications.
Analyze battery and vehicle usage data from Fleet Test Vehicles to compare and validate battery life and performance models.
Support development of appropriate battery usage history statistics and their online recording
Analyze critical battery performance during the entire product lifecycle including: power and energy performance, battery life, safety performance.
Help develop tests as needed for verifying battery system performance and execute product design improvements.
Lead the data analysis activities and interpretation related to battery field performance to guide requirements development and product improvements
Provide technical expertise in investigation of system issues and support root cause analysis and mitigation and corrective action plans.
Requirements:
Bachelor's degree in Electrical, Chemistry, Applied Physics, Aerospace, Industrial, Systems, or Computer Engineering, or related Engineering field from an ABET accredited, or ABET equivalent university.
3 years in product engineering design and development in automotive industry with 2 years of experience in mathematical modeling and data analysis/processing/mining
Good understanding of Li-ion cell electrochemistry, cell/battery life models and battery life behavior.
Excellent Matlab scripting skills or other languages e.g. Octave, Python
Knowledge of design of experiments, parameter fitting, optimization, and data regression methods
General knowledge of xEV Battery/Client systems, xEV Vehicle/powertrain operation
Demonstrate excellent level of analytical ability, communication and interpersonal skills required to build relationships and coach team members, and work with customers to solve problems and resolve issues.
Preferred Requirements:
Master's degree in Engineering, as accredited by ABET or ABET equivalent; other graduate level technical degrees in the specific area of relevant specialty expertise may be considered.
Minimum of 5 years in product engineering design and development in automotive industry with 3 years working experience in xEV technology applications and areas such as development of battery storage systems, high voltage architecture development, and/or electric powertrain components such as electric traction motors & power electronics.
Understanding of thermal system performance modeling as it pertains to energy storage components
Thorough knowledge of electrification technology functionality and features on the system level.
High Voltage training and advance battery design/development experience is desirable.
Experience in performance modeling with Matlab/Simulink, COMSOL, GT-Suite and ANSYS.
Experience using a disciplined system development process based on ASPICE or CMMI model.
Experience with DOORs, Canalyzer and Canape
Knowledge of battery test methods and procedures
Strong proactive and reactive engineering problem solving skills (including DFSS, Kepner Tregoe, 8D, 5Why, etc.)
Job Type: Full-time
Pay: $84,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Vision insurance
Schedule:
Day shift
Monday to Friday
Ability to commute/relocate:
Auburn Hills, MI: Reliably commute or planning to relocate before starting work (Preferred)
Work Location: In person","$84,000 /yr (est.)",201 to 500 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,2001,$5 to $25 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE",Lead Data Engineer,"As a Lead Data Engineer within Consumer and Community Banking, in Home Lending, you are an integral part of an agile team that works to enhance, build, and deliver data collection, storage, access, and analytics solutions in a secure, stable, and scalable way. As a core technical contributor, you are responsible for maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the firm's business objectives.

Job Responsibilities
Design and implement end-to-end data pipelines supporting analytical and operational needs accounting for data management practices focused on data quality, metadata management etc.
Architect, design, and implement cloud native solutions on AWS.
Define and implement event driven architecture patterns leveraging messaging / streaming solutions like Kafka, Kinesis, Flink, and Spark
Ability to decompose large initiatives / designs into manageable smaller bodies of work to demonstrate continuous progress
Collaborate with business stakeholders, product owners, architects, data domain owners to understand current landscape and develop solutions in alignment with business & technology strategy. Assist in refining /evolving data strategy highlighting clear outcomes.
Deep understanding or desire to continue to learn new database technologies, cloud computing & storage services
Understanding of the pros / cons associated with various technology choices and ability to pick the right technology based on the use case

Required qualifications, capabilities, and skills
Formal training, or certification on data engineering concepts, and 5+ years of experience. In addition, demonstrated coaching and mentoring experience
Programming experience in Java, Python, Scala etc.
Experience in using distributed frameworks like Spark, Hadoop etc.
Experience with AWS services like Lambda, EC2, EMR, Redshift, Glue, S3, IAM, RDS, Aurora, DynamoDB etc.
Knowledge of cloud networking, security, storage, and compute services
Infrastructure provisioning experience using Cloud Formation, Terraform etc.
Experience implementing solutions leveraging CI / CD etc.

Preferred qualifications, capabilities, and skills
AWS Solutions Architect / Developer or any advanced level certification preferred
Experience and proficiency across the data lifecycle
Experience with database back-up, recovery, and archiving strategy
Proficient knowledge of linear algebra, statistics, and geometrical algorithms
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans","$116,853 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE","Software Engineer III, Cloud Data Engineer","As a Software Engineer III, Cloud Data Engineer, within Corporate Enterprise Technology, in Finance, Risk, Data, & Controls, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Executes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems
Creates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems
Produces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets both on-prem and on AWS cloud in service of continuous improvement of software applications and systems
Proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture
Contributes to software engineering communities of practice and events that explore new and emerging technologies
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Formal training or certification on software engineering concepts and 3+ years applied experience
3+ years of working on Big Data Platforms and building frameworks for data pipelines
2+ years of building cloud solutions - AWS preferred.
1+ year of working on cloud data lake solution. Experience working with Terraform, Glue DB, Collibra, Athena, Snowflake, Redshift, EMR would be big plus.
Proficient in coding in one or more languages - Java, Scala and Python are mostly used.
Experience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages
Solid understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security

Preferred qualifications, capabilities, and skills
Advanced knowledge of Apache Spark, Scala, Java, Python and Spring
Understanding of integration technologies such as Apache Kafka
Working knowledge of API-Apigee Edge, Swagger
Containers-Docker advanced development, Kubernetes
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans","$104,623 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
"Kia America, Inc.
3.8",3.8,"Irvine, CA",Field Data Analysis Engineer,"At Kia, we’re creating award-winning products and redefining what value means in the automotive industry. It takes a special group of individuals to do what we do, and we do it together. Our culture is fast-paced, collaborative, and innovative. Our people thrive on thinking differently and challenging the status quo. We are creating something special here, a culture of learning and opportunity, where you can help Kia achieve big things and most importantly, feel passionate and connected to your work every day.
Kia provides team members with competitive benefits including premium paid medical, dental and vision coverage for you and your dependents, 401(k) plan matching of 100% up to 6% of the salary deferral, and time off starting at 14 days per year. Kia also offers company lease and purchase programs, company-wide holiday shutdown, paid volunteer hours, and premium lifestyle amenities at our corporate campus in Irvine, California.
Status
Exempt
Summary
The purpose of this position is to support the Field Data Analysis Manager and Data Evaluation Team (DET) by identifying at an early stage in a model’s life cycle, any emerging Potential Safety Issues (PSIs) through data searches and sophisticated analysis of Kia America (KUS) data from all primary data sources, including warranty, Techline, Consumer Affairs, customer pay, NHTSA Vehicle Owner Questionnaires (VOQs), as well as other data sources, as applicable. This position will also support safety and noncompliance recall decision-making and requests by NHTSA for Defect Petitions, Preliminary Evaluations, investigations, and pre-investigative requests for information. This position will also support KUS Legal Department for defense of class action suits and in-house and outside counsel requests.
Major Responsibilities
1st Priority - 40%
Review data sources as assigned for new PSIs. Work from analytic dashboards and generate reports showing emerging field issues using primary field data sources, as well as other data sources as appropriate. Investigate PSIs across data sources to determine current complaint counts, rates, and any trends from the data, from emerging field issues stage through safety evaluation list stage and to field action decision stage, if applicable. Identify PSIs and potential noncompliance issues and inform Field Data Analysis Manager of them at the earliest point in time.
2nd Priority - 25%
Coordinate with Forensic Engineering and Investigation team to hand over and collaborate on analyses and updates. Provide analysis explanations to management, coordinators, other departments, and Kia HQ staff.
3rd Priority - 20%
Provide data analysis support for NHTSA inquiries, investigations, monthly NHTSA meeting preparation, and KUS Legal in-house and outside counsel.
4th Priority - 15%
Develop sophisticated processes for evaluating data and identifying PSIs by utilizing new technologies and analytic tools. Assist in the continuous improvement and development of Safety Data Analytics Infrastructure (SDAI). Share new data analysis techniques with team.
Education/Certification
BS degree in Engineering or Automotive Technology or equivalent work experience required.
Overall Experience
3-7 years of experience in technical positions such as Safety Analysis, Product Quality, or in a product engineering environment.
Directly Related Experience
Automotive, aerospace, or similar technical work experience required
Working knowledge of statistical analysis concepts and software.
Other Requirements:
Schedule(s) may vary due to needs of the business including but not limited to working outside of normal business hours, travel, weekends and/or holidays.
Perform other duties as assigned.
Skills
Basic knowledge of SAS programing language and Structured Query Language (SQL).
Excellent analytical skills and attention to detail.
Excellent teamwork skills.
Excellent written and oral communication skills.
Intermediate knowledge of automotive systems and components.
Knowledge of PC software such as Microsoft Excel and PowerPoint.
Knowledge of SAS Visual Analytics.
Knowledge of statistics.
Competencies
CHALLENGE - Solving Complex Problems
COLLABORATION - Building and Supporting Teams
CUSTOMER - Serving Customers
GLOBALITY - Showing Community and Social Responsibility
PEOPLE - Interacting with People at Different Levels
Pay Range
$69,547.00 - $90,401.00
Pay will be based on several variables that are unique to each candidate, including but not limited to, job-related skills, experience, relevant education or training, etc.

Equal Employment Opportunities
KUS provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, ancestry, national origin, sex, including pregnancy and childbirth and related medical conditions, gender, gender identity, gender expression, age, legally protected physical disability or mental disability, legally protected medical condition, marital status, sexual orientation, family care or medical leave status, protected veteran or military status, genetic information or any other characteristic protected by applicable law. KUS complies with applicable law governing non-discrimination in employment in every location in which KUS has offices. The KUS EEO policy applies to all areas of employment, including recruitment, hiring, training, promotion, compensation, benefits, discipline, termination and all other privileges, terms and conditions of employment.

Disclaimer: The above information on this job description has been designed to indicate the general nature and level of work performed by employees within this classification and for this position. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities, and qualifications required of employees assigned to this job.","$79,974 /yr (est.)",501 to 1000 Employees,Subsidiary or Business Segment,Manufacturing,Transportation Equipment Manufacturing,1994,$10+ billion (USD)
"Chick-fil-A, Inc.
3.9",3.9,"Atlanta, GA","Sr. Lead Software Engineer, Data Ecosystem","Overview:
Chick-fil-A has successfully implemented a modern cloud-native, self-service data ecosystem comprised of AWS S3, Glue, Redshift and Databricks. In this role you will drive the design and implementation of the software components and features required to evolve it into the next-generation architecture meeting the needs of data engineers throughout Chick-fil-A.

You will be responsible for architecting, designing, and leading the implementation of features for metadata management as well as advanced data management components related to our enterprise data lake, data warehouses, Spark platform, and other relational and non-relational data stores. Integration between components to deliver the best possible developer experience is key.

Your daily work will require partnering with fellow engineers, the product owner, data and enterprise architects, stakeholders, vendor teams, and other parties following an agile methodology, while being part of a diverse team that values high performance and excellence as much as work-life balance.

This role is based in the Atlanta, GA area. Relocation available for the selected candidate.

Our Flexible Future model offers a healthy mix of working in person and virtually, strengthening key elements of the Chick-fil-A culture by fostering collaboration and community.
Responsibilities:
Lead, mentor and assess multiple partner engineering teams with minimal supervision
Identify opportunities to improve the developer experience then design and architect revisions to reduce friction in the user experience.
Partner with data scientists and data engineers to fully understand emerging needs and unmet needs. Collaborate and promote value-based adoption.
Review the work of multiple partner led pods – ensuring conformance to standards, adoption of patterns, sound designs and good development practices.
Exercise skills in cloud infrastructure and deployment as well as areas like application security, data analytics, machine learning, and site reliability engineering (SRE)
Define patterns and processes for data transformation, movement, and manipulation using (among others) Hadoop/Spark, SQL, Airflow, Databricks, Amazon Aurora, DynamoDB, Athena, Redshift, ML libraries/tools
Identify & propose emerging technologies, methodologies and/or approaches related to data and analytics
Be a key participant of the team’s Agile process
Address engineering assignments by autonomously deciding which ones to delegate and which ones to execute hands-on
Note - Working in a DevOps model, this opportunity includes both building and running solutions that could require off hours support. This support is shared amongst the team members to cover weekends and weeknights. The goal is to design for failure and, using cloud-native infrastructure patterns, automate responses to issues so they can be worked during normal hours.
Minimum Qualifications:
5+ years or more related work experience
Master’s degree in Computer Science, Analytics Engineering or related technical field or the equivalent combination of education, training and experience from which comparable skills have been acquired
Broad and deep programming experience in Python, JavaScript, Java, Scala, or other comparable languages
Experience with SQL, data modeling, and the Hadoop ecosystem
Experience with source-control systems like Git or Subversion, and CI/CD tools like GitHub Actions or Jenkins
Experience implementing application security, software design patterns, and the SDLC
Good interpersonal and team collaboration skills
Preferred Qualifications:
Experience architecting software solutions on Amazon Web Services (AWS) or other major CSP
Experience working with an Agile development methodology featuring sprints, point-estimation, and daily standups
Proficiency in Spark programming or equivalent big data technology
Experience with Unix/Linux and container technologies such as Docker
Minimum Years of Experience: 5 Travel Requirements: 5% Required Level of Education: Master's Degree Major/Concentration: Computer Science, Analytics Engineering, or related technical field","$116,088 /yr (est.)",5001 to 10000 Employees,Company - Private,Restaurants & Food Service,Restaurants & Cafes,1946,$5 to $10 billion (USD)
"Peraton
3.6",3.6,"Huntsville, AL",Sr Data Engineer - ETL,"Responsibilities:
Technology is constantly changing and our adversaries are digitally exceeding law enforcement’s ability to keep pace. Those charged with protecting the United States are not always able to access the evidence needed to prosecute crime and prevent terrorism. The Government has trusted in Peraton to provide the technical ability, tools, and resources to bring criminals to justice. In response to this challenge, Peraton is seeking a Senior Data Engineer with ETL expertise to provide proven, industry leading capabilities to our customer.

Experience with data exploration techniques and development of quantitative and qualitative data analysis process, design robust ETL pipelines.
Manage team of data engineers and should have experience working with Data Scientists and Data architects. Databricks or equivalent platform using Apache Spark with Scala, Python, Java.
Work in a team environment to design, develop, and support a software system which is undergoing a modernization.
Participate in developing new functionality and migrating the application into the cloud and introducing new technologies into the tech stack.
Participate in Agile Scrum SDLC activities.
Support developing Agile SDLC phase documentation.
Perform unit and integration testing of software/systems prior to release to the users for user acceptance testing.
Qualifications:
Required Qualifications
BS degree and twelve (12) years of experience with programming and software development including analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
Five (5) years of experience architecting software solutions based on customer requirement.
Led a technical team for at least five (5) years.
Three (3) years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
A current Top Secret security clearance with SCI eligibility and the ability to obtain a polygraph.

Desired Qualifications
Knowledge of Data Dictionary
Knowledge of Normalizations
Experience with AWS cloud services including Glue, Kinesis and Container services.
Knowledge of data acquisition and ingestion of structured and unstructured data sources ensuring quality and data integrity
Experience with open source technologies like Docker, ElasticSearch, and NoSQL Databases,
Experience with an Agile environment and have developed User Stories.
Peraton Overview:
Peraton drives missions of consequence spanning the globe and extending to the farthest reaches of the galaxy. As the world’s leading mission capability integrator and transformative enterprise IT provider, we deliver trusted and highly differentiated national security solutions and technologies that keep people safe and secure. Peraton serves as a valued partner to essential government agencies across the intelligence, space, cyber, defense, civilian, health, and state and local markets. Every day, our employees do the can’t be done, solving the most daunting challenges facing our customers.
Target Salary Range: $112,000 - $179,000. This represents the typical salary range for this position based on experience and other factors. EEO Tagline (Text Only): An Equal Opportunity Employer including Disability/Veteran.","$145,500 /yr (est.)",10000+ Employees,Company - Private,Information Technology,Information Technology Support Services,2017,$5 to $10 billion (USD)
"CVS Health
3.1",3.1,Pennsylvania,Lead Data Engineer,"Designs and develops complex and large-scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs

Writes complex ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing

Develop frameworks, standards & reference material for architecture and associated products

Designs data marts and data models to support Data Science and other internal customers. Behaves as mentor to junior team members to provide technical advice

Applies knowledge of Aetna systems and products to consult and advise on additional efforts across multiple domains spanning broader enterprise

Collaborates with data science team to transform data and integrate algorithms and models into highly available, production systems

Uses in-depth knowledge on Hadoop architecture, HDFS commands and experience designing & optimizing queries to build scalable, modular, and efficient data pipelines

Uses advanced programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems

Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards

Experiments with available tools and advice on new tools in order to determine optimal solution given the requirements dictated by the model/use case

Pay Range
The typical pay range for this role is:
Minimum: 115,000
Maximum: 230,000

This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.

In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company's 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (PTO) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.

For more detailed information on available benefits, please visit
jobs.CVSHealth.com/benefits

Required Qualifications
5+ years of progressively complex related experience

5+ years experience with bash shell scripts, UNIX utilities & UNIX Commands

5+ years experience building and implementing data transformation and processing solutions

Advanced knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar

Advanced knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment

Preferred Qualifications
Has in-depth knowledge of large-scale search applications and building high volume data pipelines

Ability to leverage multiple tools and programming languages to analyze and manipulate large data sets from disparate data sources

Ability to understand and build complex systems and solve challenging analytical problems

Education
Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.

Master’s degree or PhD preferred

Business Overview
Bring your heart to CVS Health Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver. Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable. We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities.","$172,500 /yr (est.)",10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,1963,$10+ billion (USD)
"GE Renewable Energy
3.8",3.8,"Charleroi, PA",Customer Order and Data Engineer,"Job Description Summary
Execute the design, analysis, or evaluation of assigned projects using sound engineering principles and adhering to business standards, practices, procedures, and product / program requirements. This work would include mechanical and/or electrical engineering for all products
Job Description
Required Qualifications
Developing in-depth knowledge of low voltage schematics as they pertain to high voltage circuit breakers.
In-depth understanding of key business drivers; uses this understanding to accomplish own work. In-depth understanding of how work of own team integrates with other teams and contributes to the area.
Uses some level of judgment and has ability to propose different solutions outside of set parameters but with guidance. Uses prior experience and on-the-job training to solve straightforward tasks. Has access to technical skills and analytic thinking required to solve problems. May use multiple internal sources outside of own team to arrive at decisions
Utilize business systems to receive a customer specification, translate the specification to schematics and bills of material, and provide this information to production in a timely manner free from errors.
Maintain a digital tool that involves database maintenance as well as programming logic in order to generate a Bill of Materials given user input
Participate in Phase-In Phase-Out reviews and manage product phasing
Investigate and evaluate current state of the product configurator, understand cross-functional needs of product within the factory, define opportunities for improvement, and create a project plan. The goal of the project is to refine, improve, simplify, and evolve the product configurator for Dead Tank Circuit Breakers to meet factory growth demands in products, options, and function of the tool
Develop/incorporate routing in the product configurator for SAP, BOM, and MES systems as required.
Writes rules, guidelines, and operating procedures necessary for product configurator use.
Train, coach, and develop additional users for product configurator in the future.
Performs other duties as assigned
Position Requirements:
Bachelor's degree in Mechanical or Electrical Engineering (or Associate's degree in Electrical or mechanical with a minimum of 6 years of relevant experience)
Minimum of 4 years of relevant experience related to data analysis, bills of materials, manufacturing, project management, and/or product design
Desired Characteristics
Strong SAP experience
Good understanding of product configurations, R&D Engineering, Planning, Sales, Contract engineering, and Manufacturing.
High voltage and Dead tank circuit breaker knowledge
Minimum of 1 year with ProEngineer design and AutoCAD
Strong Microsoft Excel experience
Experience in project management, planning, activity follow-up.
Experience in MS Office software including MS Excel and MS PowerPoint.
Experience working cooperatively and effectively in a team environment.
GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.
GE will only employ those who are legally authorized to work in the United States for this opening. Any offer of employment is conditioned upon the successful completion of a drug screen (as applicable).
Relocation Assistance Provided: Yes","$62,861 /yr (est.)",10000+ Employees,Subsidiary or Business Segment,"Energy, Mining & Utilities",Energy & Utilities,2015,$10+ billion (USD)
Power3 Solutions and Partnering Companies,#N/A,"Annapolis Junction, MD",SOFTWARE ENGINEER Data storage w/ TS/SCI Poly Clearance,"SOFTWARE ENGINEER
Location: Annapolis Junction, MD
Clearance: TS/SCI with polygraph
TELEWORK- up to 2 days/ week

Company Overview
Metrea Algorithmics (MAX) is electronic warfare and cyberspace solutions and engineering services company support the Department of Defense. With deep asset, mission, industry and finance expertise, and a truly global reach, Metrea focuses on de-risking our customer's aerospace requirements across a number of different mission verticals.
Description
The selected candidate will augment a development team supporting a capability that provides a mechanism to
develop, implement, and maintain workflow management. The team develops services and implements new workflow
processes using free and open source workflow tools. The role requires a self-motivated individual that is comfortable
working in a team environment. The position will require working in a fast-paced team and a willingness to take on
challenges. Experience working in an agile environment will benefit the candidate. The tech stack includes React, Node,
Java, and Spring. The position is required to utilize software development and software design methodologies
appropriate to the development environment and consistent with CMS’ DevOps objectives. Telework up to two days a
week is available in this position.

Key Technologies:
Java
Postgres
React
Node
Spring
Camunda

Desired Skills
Experience working in an Agile Development environment
Experience managing the full software development lifecycle (SDLC) to include requirements definition, design, development, test, deployment, and sustainment
Proficient in Linux


Qualifications:
Three (3) years of experience in software development/engineering
Bachelor’s degree from an accredited college or university in Computer Science or related discipline. Four (4) years of additional software development experience may be substituted for a bachelor’s degree.
Experience with Java, Spring, React, Node
Willingness to learn new technologies
Experience developing and updating technical documentation


Benefits:
Comprehensive benefits program
401k with 6% match
Room to grow within organization

Metrea provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"SAP
4.4",4.4,"Herndon, VA",SAP NS2 Data Scientist/Engineer TS/SCI + poly-MD-20701,"We help the world run better
Our company culture is focused on helping our employees enable innovation by building breakthroughs together. How? We focus every day on building the foundation for tomorrow and creating a workplace that embraces differences, values flexibility, and is aligned to our purpose-driven and future-focused work. We offer a highly collaborative, caring team environment with a strong focus on learning and development, recognition for your individual contributions, and a variety of benefit options for you to choose from. Apply now!

SAP NS2 Data Scientist/Engineer TS/SCI + poly-MD-20701
Hiring will be contingent on contract award
NS2 COMPANY DESCRIPTION
SAP is the global market leader for business software and related services. SAP National Security Services Inc.® (SAP NS2®) is an independent U.S. subsidiary of SAP. At SAP NS2, we leverage best-in-breed technologies engineered by SAP to protect the lives, assets and information of Americans. We offer SAP solutions with specialized levels of security and support to meet the requirements of U.S. national security and critical infrastructure customers.
Must be a U.S. Person; NS2 does not offer Visa sponsorships for this role.
All internals must have manager’s approval to transfer.
Responsibilities
Collaborate with subject matters experts (SMEs) to understand source data
Incorporate SME input into feature vectors suitable for analytic development and testing
Translate customer inputs to be developed into enterprise facing software visualizations
Oversee the development of individual analytic efforts and guide team in analytic development process
Partner with software engineers and cloud developers to develop production analytics
Desired Qualifications:
Experience with SQL, Spark, ETL, Tableau, PowerBI, or other data engineering tools
Experience with enterprise relational databases and other data sources
Understanding of enterprise business data
Ability to quickly learn technical concepts and communicate with multiple functional groups Envision and design data visualizations that exceed business requirements
Strong interpersonal, written, and verbal communication skills
Ability to work independently
Education Requirements:
Bachelor degree in Computer Science, Information Systems or related field.
Professional certification(s) desired.
Clearance Requirements:
Active TS/SCI + Polygraph
#LI-onsite

We build breakthroughs together
SAP innovations help more than 400,000 customers worldwide work together more efficiently and use business insight more effectively. Originally known for leadership in enterprise resource planning (ERP) software, SAP has evolved to become a market leader in end-to-end business application software and related services for database, analytics, intelligent technologies, and experience management. As a cloud company with 200 million users and more than 100,000 employees worldwide, we are purpose-driven and future-focused, with a highly collaborative team ethic and commitment to personal development. Whether connecting global industries, people, or platforms, we help ensure every challenge gets the solution it deserves. At SAP, we build breakthroughs, together.
We win with inclusion
SAP’s culture of inclusion, focus on health and well-being, and flexible working models help ensure that everyone – regardless of background – feels included and can run at their best. At SAP, we believe we are made stronger by the unique capabilities and qualities that each person brings to our company, and we invest in our employees to inspire confidence and help everyone realize their full potential. We ultimately believe in unleashing all talent and creating a better and more equitable world.
SAP is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to the values of Equal Employment Opportunity and provide accessibility accommodations to applicants with physical and/or mental disabilities. If you are interested in applying for employment with SAP and are in need of accommodation or special assistance to navigate our website or to complete your application, please send an e-mail with your request to Recruiting Operations Team: Careers@sap.com.
For SAP employees: Only permanent roles are eligible for the SAP Employee Referral Program, according to the eligibility rules set in the SAP Referral Policy. Specific conditions may apply for roles in Vocational Training.
EOE AA M/F/Vet/Disability:
Qualified applicants will receive consideration for employment without regard to their age, race, religion, national origin, ethnicity, age, gender (including pregnancy, childbirth, et al), sexual orientation, gender identity or expression, protected veteran status, or disability.
SAP believes the value of pay transparency contributes towards an honest and supportive culture and is a significant step toward demonstrating SAP’s commitment to pay equity. SAP provides the annualized compensation range inclusive of base salary and variable incentive target for the career level applicable to the posted role. The targeted combined range for this position is 102,100-221,700 USD. The actual amount to be offered to the successful candidate will be within that range, dependent upon the key aspects of each case which may include education, skills, experience, scope of the role, location, etc. as determined through the selection process. Any SAP variable incentive includes a targeted dollar amount and any actual payout amount is dependent on company and personal performance.
Requisition ID: 363265 | Work Area: Consulting and Professional Services | Expected Travel: 0 - 10% | Career Status: Professional | Employment Type: Regular Full Time | Additional Locations: #LI-Onsite","$108,163 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,1972,$10+ billion (USD)
"Chime
3.6",3.6,"San Francisco, CA",Staff Security Engineer - Data Security,"About the role
We are looking for a Security Software Engineer with experience or background working with the Data Engineering and Data Science teams. In this role, you will be part of a hands-on security team focused on securing our data pipelines, cloud infrastructure, data warehouses, and tools utilized by Chime's machine learning and data engineering teams. As a security engineer embedded into an engineering team, you excel in clear communication to articulate problems and present reasonable solutions that reduce our overall security risk. The ideal candidate for this role has the insight to find, passion to drive, and skill to implement optimal security solutions.
We're a small dedicated team that's always thinking of innovative ways to tackle hard security problems. We take on ambitious projects that have a huge impact on our members and build a strong security culture of our company. The team encourages talking about what problems we are solving for, the methods and accomplishments in public blogs and at conferences. If these are things that resonate with the way you work, we'd love to hear from you.
The base salary offered for this role and level of experience will begin at $188,500 and up to $267,100. Full-time employees are also eligible for a bonus, competitive equity package, and benefits. The actual base salary offered may be higher, depending on your location, skills, qualifications, and experience.
In this role, you can expect to
Recommend and implement security solutions and systems for cloud data warehouses, data analytics tools, and machine learning infrastructure
Define data encryption and masking standards and develop procedures and tools to ensure the desired data protection requirements are implemented
Design and build solutions that empower machine learning and data engineering teams to own their data security posture with confidence
Enhance the security posture of our machine learning data infrastructure in Snowflake, AWS, and Looker by introducing preventive controls and detection alerts
Be the security point of contact for any data security questions or concerns and be heavily involved in developing data security and handling policies
To thrive in this role, you have
Proven experience of building and deploying security features and projects
Experience securing data warehouses, and relational databases, particularly AWS RDS and Snowflake
Experience with data analytic tools (Looker experience preferred)
Experience with data ingestion and ML tools in AWS (Glue, Airflow, and Sagemaker)
Proficiency in designing security solutions for data processing pipelines
Terraform hands-on experience
Practical coding knowledge (Python or Golang is preferred)
A self-starter attitude who knows how to gather requirements and hit milestones
A little about us
We created Chime because we believe everyone deserves financial peace of mind. By eliminating unnecessary fees and helping people grow their savings automatically, we've empowered millions of Americans to take control of their finances.
Chime is the largest and fastest-growing U.S. player in the challenger-banking space. Through our banking partners, we offer access to bank accounts with fee-free overdraft, provide members the chance to receive early access to their paychecks, help them improve their credit, and more!
We've built one of the most experienced leadership teams in Fintech and were recently valued at over $25.5B. We've raised over $1.7B in funding from leading investors including Sequoia Capital Global Equities, SoftBank Vision Fund 2, General Atlantic, Tiger Global, Dragoneer, DST, Coatue, Iconiq, Menlo Ventures and others.
What we offer
Competitive salary based on experience
✨ 401k match plus the usual medical, dental, vision, life, and disability benefits
Generous vacation policy and company-wide Take Care of Yourself Days
Virtual events to connect with your fellow Chimers- think cooking classes, music festivals, mixology classes, paint nights, etc., and delicious snack boxes, too!
A challenging and fulfilling opportunity to join one of the most experienced teams in FinTech and help create a completely new kind of banking service
We know that great work comes from great, and inclusive teams. At Chime, we specifically look for individuals of varying strengths, skills, backgrounds, and ideas. We believe this gives us a competitive advantage to better serve our members and helps us all grow as Chimers and individuals.
We hire candidates of any race, color, ancestry, religion, sex, national origin, sexual orientation, gender identity, age, marital or family status, disability, Veteran status, and any other status. Chime is proud to be an Equal Opportunity Employer and will consider qualified applicants with criminal histories in a manner consistent with the San Francisco Fair Chance Ordinance. If you have a disability or special need that requires accommodation, please let us know. To learn more about how Chime collects and uses your personal information during the application process, please see the Chime Applicant Privacy Notice.
#LI-AY1","$128,583 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Banking & Lending,2012,Unknown / Non-Applicable
"School Specialty, LLC
3.0",3.0,"Lombard, IL","Oracle Data Engineer (Mansfield OH, Greenville WI or Lombard IL)","People Passion Purpose
Everything School Specialty offers is designed for one purpose – to help students succeed. We believe every student can flourish in an environment where they feel safe and inspired to explore and grow.

We’re determined to positively impact the future, one child at a time. We need to talk if you share our passion:

Transforming more than classrooms.®

Benefits

School Specialty offers Medical, Dental, & Vision plans (Effective Day 1), Wellness programs, Health Savings Accounts, Flexible Spending Accounts, 401 (k), PTO, Promise Hours dedicated to volunteering, Education Reimbursement, Paid Holidays, Fall & Winter Flexible Hours, Employee Discounts and much more!

Data Engineer
(Hybrid role- in Greenville WI, Lombard IL, or Mansfield OH)

Candidate will be responsible for designing, building, and maintaining scalable and efficient data systems supporting our organization's data-driven decision-making. You will work closely with our data scientists and business analysts to develop ETL processes and pipelines that transform raw data into valuable insights. The Data Engineer will serve as a backup to the DBA role in the event of their absence or unavailability.

The base salary range for this role is $77-100K Annually

Summary of Primary Responsibilities
Design, build, and maintain scalable and efficient data systems and pipelines
Develop and maintain ETL processes that transform raw data into valuable insights, including data cleansing, data mapping, and data transformation
Work closely with data scientists and business analysts to understand their data requirements and develop solutions to meet their needs
Design and implement data storage solutions that are secure, reliable, and accessible
Develop data quality checks and monitoring to ensure data accuracy and completeness
Develop and implement data processing and validation procedures
Develop and maintain documentation on data pipelines, data dictionaries, and data lineage
Perform data profiling, data mapping, and data modeling to support data analysis and reporting
Collaborate with cross-functional teams to integrate data from different sources
Continuously optimize and improve data systems and pipelines for performance, scalability, and reliability
Creating and executing backups, performing database tuning and optimization, monitoring database activity and usage, and providing support to end-users
Stay up-to-date with emerging trends and technologies in data engineering
Minimum Experience Requirements
Proven experience as a Data Engineer or similar role
Strong understanding of data modeling, database design, and data architecture principles
Experience building ETL processes and pipelines, including data cleansing, data mapping, and data transformation
Proficiency in SQL and experience working with Oracle and MS-SQL database technologies
Experience in database administration and be able to troubleshoot issues related to database connectivity, security, and performance
Ability to work independently and collaboratively in a fast-paced environment
Excellent problem-solving and communication skills
Willingness to learn and adapt to new environments and technologies
Self-starter and confident
Preferred Knowledge and Skills
Experience with big data technologies such as Hadoop, Spark, or NoSQL databases
Experience with distributed data processing frameworks like Apache Hadoop, Apache Spark, or Apache Flink
Knowledge of data warehousing concepts and tools such as Redshift, Snowflake, or BigQuery
Familiarity with data visualization and reporting tools such as Tableau or Power BI
Experience working with data streaming and real-time data processing frameworks like Apache Kafka or Apache Storm
Disclaimers
The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. All personnel may be required to perform duties outside of their normal responsibilities from time to time, as needed.
School Specialty, LLC. is a Drug Free Workplace. All applicants are subject to a drug screen and background check as a condition of employment.
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.
If you need a reasonable accommodation for any part of the employment process, please contact us by email at Opportunities@SchoolSpecialty.com and let us know the nature of your request and your contact information.
#LI-Hybrid","$88,500 /yr (est.)",1001 to 5000 Employees,Company - Private,Education,Education & Training Services,1959,$500 million to $1 billion (USD)
"Woongjin, Inc
3.3",3.3,"Ridgefield Park, Bergen, NJ",Data ETL Lead (Software Engineer) - bilingual (Korean/English),"Company Description

Our Mission
WOONGJIN INC. is a rapidly growing team who provides a range of unique, exceptional, and enhanced services to our clients. We have a strong moral code that includes the service of goodness without expectations of reward. We are motivated by the sense of responsibility and servant leadership.

Job Description

Drive design and development of Data Provider Framework
Conducting a thorough analysis of operational, technical, systemic requirements and any required modification to maintain the engineering tools infrastructure
Works on different database management systems efficiently
Extracting company data and transferring it into the new data hub environment
Ensure that the data is scalable and architecture
Work with clients to deliver scalable and maintainable data storage and data flow
Gather essentials and process them for business needs
Has a good command over data hub concepts to load the data effectively
Design, develop, and automate the data processing
Develops data flow models for ETL applications
Testing the storage system after transferring the data
Works on different database management systems efficiently
Prepare documentation for the further reference
Collaborate with other IT teams to modify, and enhance existing IT assets.
Troubleshoot and resolve issues like database performance, database capacity, replication, and other data-related issues
Testing the new storage system once all the data has been transferred.
Troubleshooting any issues that may arise.
Providing maintenance support.
Works on different database management systems efficiently
Understanding Structured Query Language and information management best practices
Designing, implementing, and sustaining networked and multi-sited application instances
Working closely with the customer and other engineers to ensure system requirements are met
Providing timely and accurate information and status updates to leadership, project sponsors, end users, and management

Qualifications

9 to 6 ET. support required
Bachelor’s degree in Business or related discipline with an information technology focus
9+ years of experience as a ETL development with Java Script ,.Net, Spring, C#, python
5+ Application development using JAVA, python
Database modeling with Oracle, MS SQL preferred
Artificial Intelligence development experience using python preferred
Web development framework experience preferred
Working with the company Global Systems such as Next ERP/GSCM preferred
Demonstrated experience and understanding of system development life cycle, dynamic of application development and information technology practices and methods
Full lifecycle ETL implementation with analysis, architecture, design, development, data extraction, staging and storage, transformation, presentation, testing, cutover/go-live planning

Additional Information

Describe your perks and culture","$93,494 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Less than $1 million (USD)
"Titan America
3.8",3.8,"Roanoke, VA",Data Engineer,"Data Engineer
We are seeking a motivated and detail-oriented Data Engineer to join our team based in Roanoke, Virginia. As a Data Engineer, you will play a crucial role in designing, developing and maintaining scalable data pipelines and transformations for business-critical use cases. In addition, you may take on various Software Development and Data Science responsibilities as we frequently share responsibilities and knowledge across our growing team.
Responsibilities:
Design, develop and maintain scalable data pipelines for extracting, transforming, and loading data from various sources
Collaborate with cross-functional teams to identify data needs and determine the best data solutions
Develop and implement data models to support business requirements and ensure data quality
Ensure the security and privacy of sensitive data by implementing appropriate access controls
Monitor and optimize data pipeline performance to ensure timely and accurate data delivery
Document data pipeline processes, data dictionaries, and data storage solutions
Requirements:
Bachelor's degree in Computer Science, Computer Engineering, or a related technical field
Minimum of five years of professional experience working as a Data Engineer or Software Developer
Strong hands-on experience with data warehouse and transformation solutions, i.e. Domo, Snowflake or similar
Proficient in at least one scripting language such as Python, JavaScript, or R
Understanding of data modeling, data integration and data quality processes
Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem solving skills
Full Stack Software Development experience in a professional setting is highly desired, but not required
This is an excellent opportunity for a driven and collaborative individual to make a significant impact in a dynamic and growing team. If you have a passion for data and a desire to work in a fast-paced and dynamic environment, we want to hear from you!
Success begins with hiring the right people to partner with us as we grow and develop our businesses. People are central to everything we do. It is through their efforts and talents that Titan has been successful for over 100 years.
Titan America is an Equal Employment Opportunity (EEO) / Affirmative Action employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Job Type: Full-time
Pay: $75,000.00 - $85,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Roanoke, VA 24019: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
Do you currently or will you in the future need sponsorship to work in the United States?
This position requires the successful candidate to be in the office in Roanoke, VA at least 3 days a week. Can you meet this requirement?
This position requires the successful candidates to be a full time, W2 employee. Can you meet this requirement?
Education:
Bachelor's (Preferred)
Experience:
Data Engineering: 5 years (Preferred)
Work Location: Hybrid remote in Roanoke, VA 24019","$80,000 /yr (est.)",1001 to 5000 Employees,Company - Public,"Energy, Mining & Utilities",Mining & Metals,1902,Unknown / Non-Applicable
"MyCare Medical Group
4.1",4.1,"Lutz, FL",Data Engineer,"Job Summary
The Data Engineer will design, develop, optimize, and maintain data architecture and pipelines that adhere to ELT principles and business goals.
Job Responsibilities:
Designs, develops, optimizes, and maintains data architecture and pipelines that adhere to ELT principles and business goals.
Solves complex data problems to deliver insights that helps business achieve its goals.
Creates data products for engineer, analyst, and data scientist team members to accelerate their productivity.
Engineers effective features for modelling in close collaboration with business analysts and stakeholders.
Leads the evaluation, implementation and deployment of emerging tools and process for analytics data engineering in order to improve productivity as a team and quality.
Partners with engineers, BI, and solutions architects to develop technical architectures for strategic enterprise projects and initiatives.
Fosters a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Advises, consults, mentors, and coaches other data and analytic professionals on data standards and practices.
Develops and delivers communication and education plans on analytic data engineering capabilities, standards, and processes.
Job Qualifications:
Education and Work Experience: Bachelor’s degree in computer science, information systems, engineering, or a related field; Masters preferred.
3+ years of experience in data engineering role.
Expertise in ELT and data analysis, SQL and at least one programming language (e.g., SQL, Python).
Conceptual knowledge of data and analytics, such as dimensional modelling, reporting tools, data governance, and structured and unstructured data.
Skills: Strong understanding of agile methodologies and experience as a Data Engineer on a cross-functional agile team preferred.
Experience with designing and maintaining data warehouses and/or data lakes with technologies, such as Hadoop, Spark, or distributed databases, like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases.
Experience with cloud database technologies (e.g., Azure) and developing solutions on cloud computing services and infrastructure in the data and analytics space.
Experience in building data pipelines and deploying/maintaining them with best-in-class tools (e.g., Git and Jenkins).
Familiarity with the Linux operating system.
Basic familiarity with BI tools (e.g., Tableau, Power BI, Looker).
Experience working in data engineering or data architect role.
Candidate Qualities: Focuses on business agility and agile delivery with a “fail-fast, succeed early” mindset and emphasis on measurable outcomes.
Brings a high-energy and passionate outlook to the job and can influence those around them.
Able to build a sense of trust and rapport that creates a comfortable & effective workplace.
Passion for innovation and “can do” attitude.
BENEFITS
Comprehensive benefits package, including Health, Vision, Dental, and Life insurancesFSA and Life Assistance Program (EAP)
401(k) Retirement Plan
Health Advocacy, Travel Assistance, and My Secure Advantage
PTO Accrual and Holidays
#ZIP
Job Type: Full-time
Pay: $125,000.00 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Employee discount
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Performance bonus
Yearly bonus
Experience level:
3 years
Schedule:
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Lutz, FL 33558: Reliably commute or planning to relocate before starting work (Preferred)
Experience:
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: In person","$137,500 /yr (est.)",501 to 1000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
OpenSecrets.org,#N/A,Remote,Snowflake Data Engineer,"About us
Our goal is to is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy..
Our work environment includes:
Modern office setting
Work-from-home days
Wellness programs
Data Engineer
OpenSecrets is seeking to add a new member to our data team. The Data Engineer will be responsible for helping build and maintain our data systems.
Responsibilities include:
Implementing methods to improve data reliability and quality
Analyzing raw data and implementing that data into our system using data pipelines
Troubleshooting data errors and pipeline issues
REQUIRED QUALIFICATIONS:
Experience working with and analyzing large datasets, equivalent to 2-3 years in a similar position
Proven ability to build complex data pipelines using Snowpipe and SSIS
The ability to work with diverse populations
Experience with MSSQL Server and Snowflake
Advanced knowledge of ANSI SQL
PREFERRED QUALIFICATIONS:
Excellent written and verbal communication skills.
The ability to manage multiple priorities and projects both independently and as a member of a team
Snowflake SnowPro Data Engineer certification
Experience with Apache Spark
Experience with machine learning matching models
Knowledge of and interest in American government and money in politics.
SALARY AND BENEFITS:
We are a fully remote organization and this person can be based anywhere in the U.S.
OpenSecrets offers an excellent benefits package, including full medical, dental, 403(b) match, generous vacation and sick leave, and all federal holidays paid.
TO APPLY:
Please visit this link: https://airtable.com/shrGe2bugeapKVr8a
People of diverse backgrounds are strongly encouraged to apply. OpenSecrets is an equal opportunity employer committed to creating a diverse and inclusive organizational culture. We do not discriminate against candidates and employees because of their disability, sex, race, gender identity, sexual orientation, religion, national origin, age, veteran status, or any other protected status under the law.
ABOUT US: OpenSecrets is the nation's premier research group tracking money in U.S. politics and its effect on elections and public policy. Our mission is to track the flow of money in American politics and provide the data and analysis to strengthen democracy. Our vision is for Americans to use this knowledge to create a more vibrant, representative and accountable democracy.
Job Type: Full-time
Pay: $83,874.82 - $100,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Retirement plan
Vision insurance
Compensation package:
Yearly pay
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Experience:
SQL: 3 years (Required)
Snowflake: 1 year (Preferred)
ETL: 2 years (Required)
Work Location: Remote","$91,937 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Trellis
4.4",4.4,Remote,Data Engineer,"Overview
As a Data Engineer for Trellis, you will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure to support the deployment of machine learning models for educational applications. You will be working with large volumes of text data, and will be responsible for ensuring data quality, performing data cleaning, and implementing data transformation processes.
Responsibilities
Build and maintain data pipelines for processing large volumes of text data
Perform data cleaning and preprocessing to ensure data quality and consistency
Implement data transformation processes to convert raw data into formats suitable for machine learning models
Collaborate with data scientists and other stakeholders to understand project requirements and provide data engineering support as needed
Develop and maintain data infrastructure to support machine learning workflows
Monitor and troubleshoot data pipelines to ensure high availability and reliability
Qualifications
Bachelor's or Master's degree in Computer Science, Data Science, or a related field
Strong programming skills in Python and experience with data processing frameworks such as Spark and Hadoop
Experience with text data processing and natural language processing (NLP) techniques
Familiarity with machine learning workflows and frameworks such as TensorFlow and PyTorch
Experience with cloud-based data storage and processing technologies such as AWS, Google Cloud, or Azure
Strong problem-solving skills and attention to detail
Excellent communication and collaboration skills
Benefits
SF office, but remote-friendly. Come into the office 60% of the time — you’ll want to! It’ll be built as a library in a way that’s anti-fatigue. We will also have offices in NYC and Montreal.
Health insurance with 100% premium covered
Generous PTO / sick leave
401(k) plan with employer match
Free lunch and snacks
Annual company retreat in Montreal
Bring your dog to work",#N/A,51 to 200 Employees,Company - Private,Media & Communication,Advertising & Public Relations,2012,$5 to $25 million (USD)
"Purpose Financial
4.2",4.2,Remote,Data Engineer,"Address : 322 Rhett St, Greenville, South Carolina, United States - 29601
Purpose Financial, Inc. is an innovative consumer financial services company that offers a diverse suite of credit products, promoting financial inclusion and meeting consumers wherever they are. Through its brands, the company is committed to helping customers achieve their version of financial stability in the moment and in the future. Since 1997, Purpose Financial has been a pioneer in the consumer credit and financial services market offering money solutions in over 1,000 storefronts locations and online lending. Providing services in over 20 states, Purpose Financial employs over 2,700 team members.
At Purpose Financial, our people are our greatest asset. We are inclusive and encourage different points of view. We all belong here. We respect our colleagues. We value our customers, and we support our communities.
What do we do? Our sole purpose is to help each customer achieve their version of financial stability in the moment and in the future.
We have a history! Consumers know that when they choose Purpose, they’re backed by a trusted, reliable financial services company with 25 years in the business, 2,700+ employees, 1,000+ store locations, and operating in 24 states.
Benefits and Perks. We believe our employee benefits should make life better. Purpose Financial offers a comprehensive benefits package to everyone on our team.
Health & Welfare Benefits. When you take care of yourself, you’re a better you – which is better for everyone!
Health Savings & Flexible Spending Accounts. Pre-tax money for your family’s out-of-pocket expenses.
401k with Matching Contributions. Think bigger! You can build a better future.
Wellness & Employee Assistance Programs. Take care of yourself in every way that matters.
Generous Vacation Policy & Paid Time Off. Time off to take care of yourself, your family – and your bucket list!
Volunteer Days. We want you to care. Always. Take 2 days off each year to volunteer and make a difference in your community.
Tuition Reimbursement. Pursue your goals without fretting over all the costs.
Employee Discounts. Save on home and auto insurance, cell phone service, entertainment, shopping, pet insurance and even travel.
Performance Incentives & Awards. We want you to feel valued for all your contributions.
Hybrid Workforce Support. We believe in getting it done – in whatever way works best for you and your team.
Want to meet our executive team? Visit our home page at www.havepurpose.com.
Position Summary
The job function is responsible for building a scalable data infrastructure. The role will be part of the effort to enable Purpose Financial to democratize data by developing self-serving and curated data models for our single source of truth. The job function includes understanding of business requirements, defining data requirements, identifying enterprise data sources, planning, designing, and building data acquisition from sources, defining data model, designing data ingestion pipelines, data normalization, data transformation, and consumption using platforms such as Snowflake and AWS.
Job Responsibility
Take ownership of building/optimizing data pipelines to increase efficiency and performance
Develop and maintain data ingestion and integration processes that focus on data quality and monitoring
Use AWS and Snowflake tools such as Glue, Lambda, Snowpark, and Snowpipe to build data ingestion pipelines from various sources
Incorporate standards and best practices into data pipeline solutions
Participate in troubleshooting and problem-solving design discussions
Provide logical and physical data design, and database modeling
Solve complex data issues around data integration, unusable data elements, unstructured data sets, and other data processing incidents
Continuously design the future state of our data & reporting process by collaborating with IT and business partners.
Partner with enterprise teams, data scientist, architects to define development standards and solutions.
Manage cloud infrastructure as code (IaC), using Terraform.
Work closely with other members of the data team to better design and build metrics that enhance our analytical capabilities
Manage workflows and data build processes that help the BI team build key business metrics and design dashboard visualizations
Assess the effectiveness and accuracy of new data sources, data points, and data gathering techniques
Troubleshoot problems with data (missing, corrupted, invalid) and build data recovery strategy
Apply best practices for AWS & Snowflake architecture, ELT and data models
Be a leader for our data journey, act as SME for data and database related topics, and assist with development of data lakes and warehouses
Education Required
Bachelor’s degree (or foreign equivalent degree) in any Computer, Engineering, or other technical field.
Experience Required
5+ years experience in data warehousing
3+ years of experience in data architecture and design, with a good understanding of various data systems and structures (i.e. SQL, NoSQL, Key-Value, Streams)
3+ years of experience with building data pipelines and using data orchestration tools and implementing end to end Monitoring & Alerting solutions
3+ years of experience working with AWS or public cloud provider.
Strong understanding of SQL, data structures, data integrity, and schema design best practices
Programming experience required, preferably Python.
A strong advocate of data governance and data quality
Experience with database internals, database design, SQL and database programming
Experience in financial services (Banking and consumer lending preferred)
Experience of authoring CI/CD pipelines, automation elements related to infrastructure composition, deployment orchestration, and monitoring.
Familiarity and experience with Agile/SCRUM based development methodology.
Knowledge Required
Excellent written and verbal communications skills as would be needed to communicate in person, by phone, and through email; adaptability and flexibility to changing environment; and comfortable working in a dynamic, high volume, fast-paced environment. Ability to read, write, evaluate, and apply information. Ability to interact professionally and exhibit appropriate social skills. Ability to understand and ensure compliance with policies, procedures, and laws governing our industry/business and products. Ability to develop and maintain business relationships.
Physical Requirements
Sitting for long periods of time; standing occasionally; walking; bending; squatting; kneeling; pushing/pulling; reaching; twisting; frequent lifting of less than 10 lbs., occasional lifting of up to 20 lbs.; driving and having access during the workday to an insured and reliable transportation; typing; data entry; grasping; transferring items between hands and/or to another person or receptacle; use of office equipment to include computers; ability to travel to, be physically present at, and complete the physical requirements of the position at any assigned location.
Competencies
Integrity
Customer Advocacy
Interpersonal Skills
Results/Accountability
Inclusiveness
Technical Proficiency/Leadership
Big Data Trends
Data Management
Adaptability
Travel - None
Attire - Business Casual
Other - Must be eligible to work in the USA and able to pass a background check.
Job Type: Full-time
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Day shift
Work Location: Remote",#N/A,Unknown,Company - Private,Financial Services,Financial Transaction Processing,#N/A,Unknown / Non-Applicable
"Integration Developer Network LLC
4.9",4.9,Remote,Data Engineer with PL/SQL Background,"Requirements:
Recent PL/SQL development work experience with Oracle 19.
Must have working experience with tools like Airflow, Git.
Must have python coding experience and REST API development experience.
Plus would be experience with microservices, docker, Kubernetes.
Plus would be experience with accounting and financial data
Experience in event driven and cloud based architectures and working knowledge of containerization and streaming technologies.
Detailed work experience on Oracle, SingleStore or IBM I DB2 database, SQL & PL/SQL, including ANSI SQL.
Experience in database analysis, development and maintenance of business applications specific to oil & gas back-office operations. Expertise in various ETL tools such as SQL*Loader, Equalum, Python scripts.
Familiarity with No-SQL databases and pipelines (Elasticsearch, MongoDB).
2+ years’ experience in the design, implementation and support of full life-cycle data engineering projects.
Bachelor's degree in Computer Science, Engineering or other equivalent degree, with 5+ years data engineering experience or Master's degree in Computer Science, Engineering or other equivalent degree with 2+ years data engineering preferred.
High school diploma or GED is required.
Job Type: Contract
Pay: $60.00 - $65.00 per hour
Experience level:
9 years
Schedule:
8 hour shift
Experience:
Python: 8 years (Required)
PL/SQL: 5 years (Required)
ETL: 5 years (Required)
Work Location: Remote",$62.50 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2011,$1 to $5 million (USD)
Proits Hub LLC,#N/A,Remote,Data Analytics Engineer,"JD and Details
Current location: Santa Clara Valley (Cupertino)
Position: Digital Marketing Data Analyst
Position Type: Hybrid - Long Term Contract
JD:
KEY QUALIFICATIONS
Business Skills
Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems
Experience analyzing the launch of new products, services, or websites.
In-depth knowledge of digital analytics data, measurement, methodologies and industry standards.
Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily.
Curiosity to dive below the surface and identify important strategic implications in the data.
Can take a project from start to finish with minimal supervision.
Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time.
Able to balance multiple projects and assignments with a focus on efficiency.
Detail-oriented, organized, and patient.
Technical Skills
Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred).
Proficiency with Tableau or other Data Visualization tools
Strong working knowledge of SQL, Snowflake experience preferred
Job Type: Contract
Salary: Up to $60.00 per hour
Schedule:
Monday to Friday
Work Location: Remote
Speak with the employer
+91 9417599933",$60.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Trella Health
4.8",4.8,"Philadelphia, PA",Senior Data Engineer,"At Trella Health, we are passionate and committed to our mission – empowering meaningful change in healthcare. Since our founding in 2015, we continue to grow our team, enhance our solution and services offerings, accelerate into new markets, and expand our customer base. We are rapidly growing and are looking for new Trellavators to join our team!
""What is a Trellavator?"" you ask. Innovate and elevate is the name of our game! We go above and beyond to collaborate with and support each other – we believe that when a colleague or customers succeeds, we succeed. By learning from others, building on our successes, and taking risks, we constantly raise the bar – continuous improvement is in our DNA. Our word is paramount, we keep our commitments, and we always follow through. We have a strong, reliable support system that fuels growth, collaboration, and passion – and together, we create a positive environment where everyone at Trella Health, including the customers we support, can thrive. Are you ready to learn more about the opportunities with our team? Trell-yeah you are!
Position Overview:
The Senior Data Engineer will be responsible for designing, implementing, and maintaining an effective data pipeline for Trella Health's applications. This position is responsible for turning data concepts into effective data models that are efficient and scalable. The Senior Data Engineer will ensure a smooth data release and deployment process throughout the data development lifecycle.
Successful candidate will be a highly organized, self-starter that can roll up their sleeves and solve problems efficiently. Ability to document procedures, manage time, and have a knack for optimizing data flows.
Location: Atlanta HQ; Raleigh, NC; Nashville, TN; Philadelphia, PA; Remote considered
Reports to: Manager, Data Engineering
As a member of the Data team at Trella, you will:
Champion a comprehensive data vision and operating principles that improve data quality, consistency, accessibility, decision making, use, and stewardship.
Manage data pipeline by developing processes for data input, transformation, and storage
Act as an interface between the Data Science team and Application Developers
Architect and implement front-end (application layer) and back-end (ETL layer) database technologies in a highly customized data environment
Develop automated processes for moving data between systems
Implement ETL processes that transform source data into functional data stores consumed by front-end applications
Scale data pipeline by utilizing the most efficient tools and technologies
This job might be a fit for you if you have:
Bachelor's degree in Computer Science, Engineering, Mathematics, or other technical discipline (proven professional experience will be considered in lieu of a technical degree)
2+ full years of recent advanced Snowflake development experience
Experience manipulating healthcare data
In-depth knowledge of Amazon Web Services (AWS) and Platform as a Service (PAAS)
Demonstrable experience with Snowflake utilities including SnowSQL & Snowflake Javascript
Analyze all aspects of the Snowflake Query Engine to improve bottlenecks that may exist
Ability to automate data ingestion from Amazon S3, FTP, and other technologies
Experience with scripting languages, extensive Python experience, Node.js experience a plus
Working knowledge of Bitbucket and Git
Experience managing and loading data in AWS RDS (MySQL/MariaDb)
Ability to optimize MySQL query execution plans for robust performance
Technical Job Requirements (Nice to Have)
Experience manipulating healthcare data including claims, ADT data
Foundational knowledge of CI/CD platforms, preferably TeamCity
Development and consumption of API services
Experience with data science tools (Jupyter, SageMaker, R)
Understanding of Atlassian suite of tools, including Jira and Confluence
Soft Skill Job Requirements (Nice to Have)
Strong analytical skills and ability to rapidly switch contexts
Excellent communication and problem-solving skills
Sole contributor that is willing to independently uncover and improve processes that are inefficient or sub-optimal
Track record of implementing creative solutions to complex problems
Experience working in a nimble, fast-paced, start-up environment
Knowledge of HIPPA, PHI, PII policies
About Trella Health:
Trella Health provides unmatched, actionable market intelligence to post-acute care and value-based care providers of all sizes. Our industry leading analytics paired with CRM and EHR integration workflows positions us as the most advanced sales enablement platform for the post-acute care market. One of only a few companies to be deemed both a Qualified Entity by CMS and an Innovator under its Virtual Research Data Center Program, Trella has elite access to billions of performance metrics and referral data to enable competitive positioning across the market. Its standardized insights, representing 90% of all lives 65+ U.S. population, help customers identify, engage, and manage critical relationships and advance their organizations with certainty.
Since launching our first product in 2016, we have experienced tremendous growth and have highlighted a few key data points to get an at-a-glance view of Trella:
Over 700 customers including 10 of the top 10 post-acute providers and 4 of the top 10 health systems
Over 120 team members and growing
Recognized as one of Modern Healthcare's ""Best Places to Work"" in consecutive years and were ranked in the Top 50 startups in the US by LinkedIn
Headquartered in Atlanta, GA with remote hubs in Nashville, Philadelphia, Raleigh, and the Philippines
The Trellavator Experience:
At Trella Health, you can expect to join a welcoming team that truly embodies our core values. Our collaborative culture is anchored by trust, transparency, and inclusion — and we also have a lot of fun. No matter where you work, at Trella Health you can expect an awesome team, frequent virtual gatherings, engaging events, casual attire, and more.
We offer competitive salaries with a comprehensive benefits package to all employees and provide an environment that fosters work-life harmonization with unlimited PTO, company-observed holidays, and Summer Fridays. As we continue to see exponential growth, our goal is to continue to put team members first and strive to offer our team members the best culture and benefits possible. Some of the benefits we provide are:
Health, Dental, Vision & Voluntary Benefits
Competitive Salary & Bonus Plans
401k Retirement Savings
Unlimited PTO & 10 Paid Holidays
Flexible Work Hours
Equity Shares
Paid Leave Programs
Marketplace for discounted retail and entertainment
Equal Opportunity Employer
Trella Health is an equal opportunity employer. All persons will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, handicap, veteran status, genetic information, or any other protected status as recognized by federal, state, or local laws.","$113,846 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,Unknown / Non-Applicable
"iManage
4.5",4.5,Remote,Senior Data Engineer (Azure),"We offer a flexible working policy that supports the health and well-being of our iManage employees. As an organization, we value collaborating and learning from our peers in person, while providing the necessary flexibility for our employees to have a meaningful work-life balance. Please reach out to learn more.
Being a Senior Data Engineer at iManage Means…
You are excited about data and believe in the democratization of data to support data driven decision-making. You will partner with our Information Technology team to implement, support, and extend our Enterprise Data Lake hosted on Azure and built using Azure Synapse. You will gather requirements from iManage business units and craft solutions which provide access to critical business data. You will develop data models and data pipelines for our Enterprise Data Lake, and provide integration with BI platforms and tools such as Totango and Power BI. You are passionate about lakehouse architecture and have experience using Delta Lake and bronze, silver, and gold data lake design.
Here is what one of our leaders, Cloud Services Director (Jacqueline Toepfer), has to say about the role: “As a Senior Data Engineer on our team, you will get the opportunity to showcase your expertise and make a real difference across the organization. You will be part of a truly collaborative team that is passionate about delivering quality solutions. You will be the in-house expert in the data models of multiple, disparate enterprise SaaS systems and utilize your wealth of knowledge to provide recommendations and solutions for consolidation, transformation, and integration of the disparate data sources.”
iM Responsible For…
Modeling, managing, and reporting of data stored in Azure Data Lake.
Gathering data requirements from various business units and translating these requirements into data models.
Using Python, PySpark, and system specific APIs to extract, transform, store and analyze data from a variety of systems.
Data modeling, defining data pipelines, and integrations necessary to present data in BI platforms such as Totango, or BI tools like Power BI.
Identifying and modeling all current disparate data sources and the data flows between these data sources.
Analyzing current repositories and proposing changes to data repositories and data flows to better support company objectives for the measurement of user experience and customer success.
Understanding the business needs of data integration and governance from disparate systems to drive the enhancement of the enterprise data lake.
Applying best practices to ensure the security and privacy of the data repositories.
Ensuring data repositories meet company standards for storage of PII.
Developing proficiency with the iManage product APIs for all iManage Cloud services.
iM Qualified Because I Have...
A Bachelor’s degree or higher in Computer Science or equivalent field.
3-5 years of experience working with data in a business setting.
Proficiency in data extraction, manipulation, and subsequent reporting with Spark and Python.
Experience designing data pipelines with a cloud-native mindset using Azure or AWS.
Knowledge and experience with architecting a data lake with Azure Synapse or adjacent technologies like Databricks.
Experience ingesting data from SaaS solutions and other services via API or other related technologies.
A passion to be a thought leader and work collaboratively within a team.
Commitment to understanding data requirements and delivering scalable, robust solutions that meet those requirements.
A creative mindset with a desire to explore new technologies and create innovative solutions.
Bonus Points If I Have…
Familiarity with Delta Lake.
A background with relational databases and data warehouse design using star schemas.
Experience with cloud-based data models for business solutions like Salesforce, Zendesk, and NetSuite.
Don't meet every qualification listed above? Studies show that women and people of color are less likely to apply to jobs unless they meet all qualifications. At iManage, we are committed to building a diverse and inclusive environment and encourage everyone to show up as their full authentic selves. We welcome those that come with a growth mindset and a hunger for learning; so, if you are excited about this role but your past experience doesn't align perfectly with every qualification, we encourage you to apply anyways!
iM Getting To…
Join a supportive, experienced team with an inclusive, encouraging, and vibrant culture.
Have flexible work hours that allow me to balance my ‘me time’ with my work commitments.
Collaborate in a modern open plan workspace, with a gaming area, free snacks, drinks and regular social events.
Focus on impactful work, solving complex, real challenges utilizing the latest technologies and protocols.
Own my career path with our internal development framework. Ask us more about this!
Learn new skills and earn certifications with access to unlimited courses in LinkedIn Learning.
Join an innovative, industry leading SaaS company that is continuing to grow & scale!
iManage Is Supporting Me By...
Creating an inclusive environment where I can help shape the culture not just by fitting in, but by adding to it.
Providing a market competitive salary that is applied through a consistent process, equitable for all our employees, and regularly reviewed based on industry data.
Rewarding me with an annual performance-based bonus.
Offering comprehensive Health/Vision/Dental/Life Insurance, and a 401k Retirement Savings Plan with a company match up to 4%.
Giving access to HealthJoy, a healthcare concierge service, to help me maximize my health benefits.
Granting enhanced leave for expecting parents; 20 weeks 100% paid for primary leave, and 10 weeks 100% paid for secondary leave.
Providing me with a flexible time off policy to take the time off that I need. Be it for vacation, volunteering, celebrating holidays, spending time with family, or simply taking time to recharge and reset.
Caring for my mental health and well-being with multiple company wellness days and free access to the Healthy Minds app for mindfulness, meditation and more.
About iManage…
iManage is dedicated to Making Knowledge WorkTM. Over one million professionals across 65+ countries rely on our intelligent, cloud-enabled, secure knowledge work platform to uncover and activate the knowledge that exists inside their business content and communications.
We are continuously innovating to solve the most complex professional challenges and enable better business outcomes; Our work is not always easy but it is ambitious and rewarding.
So we’re looking for people who love a challenge. People who are happiest when they’re solving problems and collaborating with the industry’s best and brightest. That’s the iManage way. It’s how we do things that might appear impossible. How we develop our employees’ strengths and unlock their potential. How we find meaning in everything we do.
Whoever you are, whatever you do, however you work. Make it mean something at iManage.
iManage provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Learn more at: www.imanage.com
Please see our privacy statement for more information on how we handle your personal data: https://imanage.com/privacy-policy/
#LI-LM1
#LI-Remote
V436F7WSwa",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,$100 to $500 million (USD)
"Braintrust
4.6",4.6,"San Francisco, CA",Data Engineer,"ABOUT US:
Braintrust is a user-owned talent network that connects you with great jobs with no fees or membership costs—so you keep 100% of what you earn.

JOB TYPE: Freelance, Contract Position (no agencies/C2C - see notes below)
LOCATION: Remote - United States only (TimeZone: EST | Partial overlap)
HOURLY RANGE: Our client is looking to pay $100 – $105/hr
ESTIMATED DURATION: 40h/week - Long term

ABOUT THE HIRING PROCESS:
When you join Braintrust, you will be invited to a screening process for Braintrust to learn more about your previous work experiences. Once completed, you will have access to the employer for this role and other top companies that seek high-quality talent. Apply to this job to kick off the process.
THE OPPORTUNITY
Requirements
Summary:
Advanced analytics SQL - minimum 2 years, preferable 5
Python - minimum 1 year, preferable 2
Ability to work in cloud platform
Qualities that will help you thrive in this role:
You understand that being an effective engineer is about communicating with people as much as it is about writing code.
You are willing to work with and improve code you did not originally write, primarily in SQL and Python.
You are generous with your time and experience and can mentor and learn from other engineers.
You are comfortable with best practices for traditional data warehousing.
You love SQL and writing efficient and optimized ETL pipelines.
You are familiar with building and monitoring cloud services and infrastructure.
What you’ll be working on
What’s the role?
As a member of our client's Data Applications, Data Warehouse team, you’ll help us improve the stability, performance, and usability of their BigQuery data warehouse while advising their stakeholders on best practices and optimizations. Your work will enable other developers, data scientists, and analysts to write the high-performing pipelines that power data science, machine learning, and product development.
In addition to BigQuery SQL, our client's toolset includes Looker, Java, Python, and Spark, as well as Airflow, Terraform, and Kubernetes, and GCP services like Dataproc and Dataflow.
About The Team
They build highly-performant systems and data warehouses that are maintainable and cost effective.
They develop robust, highly available, well-monitored data infrastructure.
They stay in close communication with the internal customers and make strategic improvements to ensure those that depend on us have a great experience using data
What does the day-to-day look like?
You should have experience building data warehouses, data marts, and aggregate tables - supporting them at scale, and collaborating with other teams that depend on them.
Experience building applications and managing infrastructure using one of the major cloud providers is preferred but not required. (Our client uses Google Cloud).
Our client values curiosity, passion, responsibility, and generosity of spirit.
Apply Now!
Braintrust Job ID: 6590


C2C Candidates: This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.

Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.

This is a remote position.",$102.50 /hr (est.),51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2018,$5 to $25 million (USD)
"Dobbs Defense Solutions, LLC",#N/A,"Fort Meade, MD",Data/NiFi Engineer,"Our Mission
At Dobbs Defense, we deliver mission-centric IT, Cyber, and data analytics solutions for our government and commercial clients through the convergence of automation, innovation, training, and education. Delivering high-quality IT, cybersecurity, and data analytics solutions through proven and innovative methods is our vision.

Job Description
Dobbs Defense Solutions is seeking a Senior Data/NiFi Engineer to design, develop, implement, test, secure, and support data transport and brokering for multiple classified networks and between networks.

Duties
Design, develop, implement, test, secure, and support data transport and brokering.
Data flow engineering for one of DISA’s premier boundary defense systems.
Perform system administration of Niagara Files (NiFi) on Linux VMs.
Support backup/failover/load balancing of data brokering to improve availability.
Monitor availability & amp, usage.
Respond to outages, take corrective action, and issue outage reports.
Familiarity with DISA STIGs and SCAP.
Familiarity with Java programing.

Qualifications
Required education and experience:
5+ years of Data Engineering experience.
Significant experience with Niagara Files (NiFi) and MiNiFi or similar data engineering tools.
Excellent written and verbal communication skills.
IAM or IAT Level II Certification (e.g., GSEC, SEC+, SCNP, SSCP, etc.).
Active Secret clearance.

Required Clearance
Secret

Working Environment
Onsite

Our Equal Employment Opportunity Policy
Dobbs Defense Solutions is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or medical conditions related to pregnancy, childbirth or breastfeeding), sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), gender expression and transgender status, national origin, ancestry, age, disability, military or veteran status, marital or domestic partner status, genetic information, citizenship, low-income status or any other status or characteristic protected by applicable law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits, and all other privileges, terms, and conditions of employment.
4Cmw9yg2oT","$83,799 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"R1 RCM, Inc.
3.4",3.4,Remote,Data Engineer III,"The Software Engineer III serves as a Data Engineer focused on building next generation of healthcare products and services. As part of a collaborative, motivated Agile-based team, the Software Engineer III is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
Responsibilities
Create and maintain optimal data pipeline architecture to assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders to optimize the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies.
Work with stakeholders including the Product, Data and Engineering teams to assist with data-related technical issues and support their data infrastructure needs.
Adhere to and ensure engineering best practices using modern SDLC that enables CI/CD and favors automation, auditability, automated testing, infrastructure, and policy as code.
Ability to collaborate effectively with other architects and engineers delivering end to end, high-performing, highly reliable, scalable, and operable solutions.
Required Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical field; or equivalent experience.
3+ years of experience delivering data engineering applications and services.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets .
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience in any one of the big data technologies (Hadoop, MapReduce , Spark, Data Lake etc. )
Experience in any one of the stream-processing systems ( Kafka, Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages : Python, Java, Scala, etc.
Understanding of modern software development including version control, unit testing, and continuous integration and deployment.
Strong communication skills, with proven ability to present complex ideas and to document concisely.
Preferred Qualifications
Working knowledge of Cloud providers and services such as Amazon AWS or Microsoft Azure.
Experience with relational SQL and NoSQL databases, including MongoDB and ElasticSearch.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12.
Knowledge of the healthcare revenue cycle, EMRs, practice management systems.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",#N/A,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2003,Unknown / Non-Applicable
"Shutterfly
3.3",3.3,"Tempe, AZ",Senior Data Engineer,"Description
At Shutterfly, we make life’s experiences unforgettable. We believe there is extraordinary power in the self-expression. That’s why our family of brands helps customers create products and capture moments that reflect who they uniquely are.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.
What You'll Do as a Sr. Data Engineer II
Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines am improve the operation and performance of the Data Warehouse
The Skills You'll Bring
Expert knowledge Python, Spark, SQL scripting and Java
7+ years of hands-on experience with building data ingress frameworks and experience with dealing with a wide variety of source data formats
Hands-on experience in building data applications, including design, implementation, debugging, and support.
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it’s the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.","$135,965 /yr (est.)",10000+ Employees,Company - Private,Information Technology,Internet & Web Services,1999,$1 to $5 billion (USD)
"Chewy
3.5",3.5,"Richardson, TX",Data Engineer II,"Our Opportunity:
Chewy’s Data Analytics team has an exciting opportunity for a Data Engineer III to join the pack. Leveraging your strong expertise and background in data engineering and data analysis, you will be a part of a team responsible for operational and tactical reporting generating insights to grow Customer Service operations and planning. This includes building high quality data pipelines that drives analytic solutions and creating data products for analytics and data scientist team members to improve their productivity. Our organization is a fast-paced environment with new challenges and new opportunities each day. You will be responsible for building and implementing data products and technologies which will handle the growing business needs and play a key role in redefining what it means to be a world-class customer service organization.
What You'll Do:
Design, develop, optimize, and maintain data architecture and pipelines using design and programming patterns that follow best-in-class practices and principles.
Manage, maintain, and improve our SSOT tables and data marts, which drive critical business decisions every day.
Work closely with analytics teams and business partners, serving as a trusted partner who can advise, consult, and communicate data solutions.
Mentor and coach other data practitioners on data standards and practices.
Lead the evaluation, implementation and deployment of emerging tools and process for data engineering to improve overall productivity for the organization.
Partner with leaders, vendors, and other data practitioners across Chewy to develop technical architectures for strategic enterprise projects and initiatives.
Document technical details of work and follow agile sprint methodology, using tools like Jira, Confluence etc.

What You'll Need:
Bachelor of Science or Master’s degree in Computer Science, Engineering, Information Systems, Mathematics or related field
3+ years of enterprise experience as a data engineer and/or software engineer
3+ years applying and implementing database and data modeling techniques
3+ years working with enterprise data warehouse (ex. Snowflake, Vertica) and cloud environments (ex. AWS)
3+ years of experience building data integrations and pipelines from data lake, APIs, relational databases, and third-party systems
Strong software development skills in SQL
Self-motivated with strong problem-solving and self-learning skills.
Bonus:
Strong working knowledge of Python programming
Excellent communication and collaboration skills with ability to influence and guide stakeholders
Experience building dimensional models in data warehouses
Experience with data streaming tools and technologies like Kafka, Kinesis, or similar technologies
AWS Developer Certifications
E-commerce, Retail or startup experience
Experience in BI tools such as Tableau, Plotly, Power BI, etc.
Chewy is committed to equal opportunity. We value and embrace diversity and inclusion of all Team Members. If you have a disability under the Americans with Disabilities Act or similar law, and you need an accommodation during the application process or to perform these job requirements, or if you need a religious accommodation, please contact CAAR@chewy.com.

If you have a question regarding your application, please contact HR@chewy.com.

To access Chewy's Customer Privacy Policy, please click here. To access Chewy's California CPRA Job Applicant Privacy Policy, please click here.",#N/A,10000+ Employees,Company - Public,Retail & Wholesale,Pet & Pet Supplies Stores,2011,$5 to $25 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE","Software Engineer III, Cloud Data Engineer","As a Software Engineer III, Cloud Data Engineer, within Corporate Enterprise Technology, in Finance, Risk, Data, & Controls, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Executes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems
Creates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems
Produces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets both on-prem and on AWS cloud in service of continuous improvement of software applications and systems
Proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture
Contributes to software engineering communities of practice and events that explore new and emerging technologies
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Formal training or certification on software engineering concepts and 3+ years applied experience
3+ years of working on Big Data Platforms and building frameworks for data pipelines
2+ years of building cloud solutions - AWS preferred.
1+ year of working on cloud data lake solution. Experience working with Terraform, Glue DB, Collibra, Athena, Snowflake, Redshift, EMR would be big plus.
Proficient in coding in one or more languages - Java, Scala and Python are mostly used.
Experience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages
Solid understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security

Preferred qualifications, capabilities, and skills
Advanced knowledge of Apache Spark, Scala, Java, Python and Spring
Understanding of integration technologies such as Apache Kafka
Working knowledge of API-Apigee Edge, Swagger
Containers-Docker advanced development, Kubernetes
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans","$104,623 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
"Purpose Financial
4.2",4.2,Remote,Data Engineer,"Address : 322 Rhett St, Greenville, South Carolina, United States - 29601
Purpose Financial, Inc. is an innovative consumer financial services company that offers a diverse suite of credit products, promoting financial inclusion and meeting consumers wherever they are. Through its brands, the company is committed to helping customers achieve their version of financial stability in the moment and in the future. Since 1997, Purpose Financial has been a pioneer in the consumer credit and financial services market offering money solutions in over 1,000 storefronts locations and online lending. Providing services in over 20 states, Purpose Financial employs over 2,700 team members.
At Purpose Financial, our people are our greatest asset. We are inclusive and encourage different points of view. We all belong here. We respect our colleagues. We value our customers, and we support our communities.
What do we do? Our sole purpose is to help each customer achieve their version of financial stability in the moment and in the future.
We have a history! Consumers know that when they choose Purpose, they’re backed by a trusted, reliable financial services company with 25 years in the business, 2,700+ employees, 1,000+ store locations, and operating in 24 states.
Benefits and Perks. We believe our employee benefits should make life better. Purpose Financial offers a comprehensive benefits package to everyone on our team.
Health & Welfare Benefits. When you take care of yourself, you’re a better you – which is better for everyone!
Health Savings & Flexible Spending Accounts. Pre-tax money for your family’s out-of-pocket expenses.
401k with Matching Contributions. Think bigger! You can build a better future.
Wellness & Employee Assistance Programs. Take care of yourself in every way that matters.
Generous Vacation Policy & Paid Time Off. Time off to take care of yourself, your family – and your bucket list!
Volunteer Days. We want you to care. Always. Take 2 days off each year to volunteer and make a difference in your community.
Tuition Reimbursement. Pursue your goals without fretting over all the costs.
Employee Discounts. Save on home and auto insurance, cell phone service, entertainment, shopping, pet insurance and even travel.
Performance Incentives & Awards. We want you to feel valued for all your contributions.
Hybrid Workforce Support. We believe in getting it done – in whatever way works best for you and your team.
Want to meet our executive team? Visit our home page at www.havepurpose.com.
Position Summary
The job function is responsible for building a scalable data infrastructure. The role will be part of the effort to enable Purpose Financial to democratize data by developing self-serving and curated data models for our single source of truth. The job function includes understanding of business requirements, defining data requirements, identifying enterprise data sources, planning, designing, and building data acquisition from sources, defining data model, designing data ingestion pipelines, data normalization, data transformation, and consumption using platforms such as Snowflake and AWS.
Job Responsibility
Take ownership of building/optimizing data pipelines to increase efficiency and performance
Develop and maintain data ingestion and integration processes that focus on data quality and monitoring
Use AWS and Snowflake tools such as Glue, Lambda, Snowpark, and Snowpipe to build data ingestion pipelines from various sources
Incorporate standards and best practices into data pipeline solutions
Participate in troubleshooting and problem-solving design discussions
Provide logical and physical data design, and database modeling
Solve complex data issues around data integration, unusable data elements, unstructured data sets, and other data processing incidents
Continuously design the future state of our data & reporting process by collaborating with IT and business partners.
Partner with enterprise teams, data scientist, architects to define development standards and solutions.
Manage cloud infrastructure as code (IaC), using Terraform.
Work closely with other members of the data team to better design and build metrics that enhance our analytical capabilities
Manage workflows and data build processes that help the BI team build key business metrics and design dashboard visualizations
Assess the effectiveness and accuracy of new data sources, data points, and data gathering techniques
Troubleshoot problems with data (missing, corrupted, invalid) and build data recovery strategy
Apply best practices for AWS & Snowflake architecture, ELT and data models
Be a leader for our data journey, act as SME for data and database related topics, and assist with development of data lakes and warehouses
Education Required
Bachelor’s degree (or foreign equivalent degree) in any Computer, Engineering, or other technical field.
Experience Required
5+ years experience in data warehousing
3+ years of experience in data architecture and design, with a good understanding of various data systems and structures (i.e. SQL, NoSQL, Key-Value, Streams)
3+ years of experience with building data pipelines and using data orchestration tools and implementing end to end Monitoring & Alerting solutions
3+ years of experience working with AWS or public cloud provider.
Strong understanding of SQL, data structures, data integrity, and schema design best practices
Programming experience required, preferably Python.
A strong advocate of data governance and data quality
Experience with database internals, database design, SQL and database programming
Experience in financial services (Banking and consumer lending preferred)
Experience of authoring CI/CD pipelines, automation elements related to infrastructure composition, deployment orchestration, and monitoring.
Familiarity and experience with Agile/SCRUM based development methodology.
Knowledge Required
Excellent written and verbal communications skills as would be needed to communicate in person, by phone, and through email; adaptability and flexibility to changing environment; and comfortable working in a dynamic, high volume, fast-paced environment. Ability to read, write, evaluate, and apply information. Ability to interact professionally and exhibit appropriate social skills. Ability to understand and ensure compliance with policies, procedures, and laws governing our industry/business and products. Ability to develop and maintain business relationships.
Physical Requirements
Sitting for long periods of time; standing occasionally; walking; bending; squatting; kneeling; pushing/pulling; reaching; twisting; frequent lifting of less than 10 lbs., occasional lifting of up to 20 lbs.; driving and having access during the workday to an insured and reliable transportation; typing; data entry; grasping; transferring items between hands and/or to another person or receptacle; use of office equipment to include computers; ability to travel to, be physically present at, and complete the physical requirements of the position at any assigned location.
Competencies
Integrity
Customer Advocacy
Interpersonal Skills
Results/Accountability
Inclusiveness
Technical Proficiency/Leadership
Big Data Trends
Data Management
Adaptability
Travel - None
Attire - Business Casual
Other - Must be eligible to work in the USA and able to pass a background check.
Job Type: Full-time
Compensation package:
Yearly pay
Experience level:
5 years
Schedule:
Day shift
Work Location: Remote",#N/A,Unknown,Company - Private,Financial Services,Financial Transaction Processing,#N/A,Unknown / Non-Applicable
"PSEG
3.9",3.9,"Edison, NJ",AWS Data Engineer,"Requisition: 73755
PSEG Company: PSEG Services Corp.
Salary Range: $ 78,600 - $ 149,400
Incentive: PIP 10%
Work Location Category: Remote Local
PSEG operates under a Flexible Work Model where flexible work is offered when job requirements allow. In support of this model, roles have been categorized into one of four work location categories: onsite roles, hybrid roles that are a blend of onsite and remote work, remote local roles that are primarily home-based but require some level of purpose-driven in-person interaction and living within a commutable distance, and remote non-local roles that can be effectively performed remotely with the ability to work in approved states.
PSEG offers a unique experience to our more than 12,000 employees – we provide the resources and opportunities for career development that come with being a Fortune 500 company, as well as the attention, camaraderie and care for one another you might typically associate with a small business. Our focus on combatting climate change through clean energy technology, our new net zero climate vision for 2030 and enhanced commitment to diversity, equity and inclusion; and supporting the communities we serve make this a particularly exciting time to join PSEG.
Job Summary
The Technology Engineer is a direct report to the Senior Technology Engineer with understanding of business goals, business processes, technology expertise in one or more domains, technology solution design and implementation.
The Technology Engineer is involved in the full technology life cycle and responsible for designing, coding, configuring, testing, implementing and supporting application software. Technology Engineers work closely with Analysts and Product Managers to understand the business requirements that drive the analysis and physical design of technical solutions. Technology Engineers may be assigned to either implementation or support functions. Implementation involves creating new technology based solutions whereas support involves upgrades, maintenance or issue resolution to existing technology solutions.

Job Responsibilities
Primary Responsibilities:
Design, configuration, development, documentation and testing of technology solutions to meet business or technology requirements. Evaluation of existing technology solutions to determine fit for purpose for the new business or technology requirements. Recommendation of technology alternatives. Collaboration with other individuals to ensure proper integration of the new technology solution with the existing technology solutions.
Analysis of end user’s needs, business and technology requirements. Translation of these requirements into technology solution capabilities and design. Alternatively, using the user needs, business and technology knowledge to peer review designs, implementation plans, software code, configuration settings or other artifacts to ensure technology solution being created by others meets the user needs, business and technology requirements.
Provides support toward resolution of escalated support tickets. May perform the needed analysis to identify root cause of reported incidents, identify the short and long term remediation for the identified incidents
Specific responsibilities include:
Analyzes end-user needs and designs, configures, develops and tests technology solutions to satisfy demand. Performs build versus buy analysis for business demand.
Partners with business analysts to translate business needs to technology solution requirements.
Evaluates existing technology solutions and platforms and provides recommendations for improving technology performance by conducting gap analysis, identifying feasible alternative solutions, and assisting in the scope of needed modifications.
Collaborates with all stakeholders such as enterprise architects, software development, operations, cybersecurity and infrastructure to integrate applications and hardware.
Ensures that the design and technology solution implementation meets security and QA standards. Suggests fixes to issues by doing a thorough analysis of root cause and impact of any defect(s).
Provides support toward resolution of escalated support tickets. Applies operation break fixes and performs other proactive maintenance activities until permanent solutions can be implemented. Supports and participates in the solution deployment process for new functionality/ subsystems/modules, upgrades, updates and fixes to the production environment.
Makes solutions production-ready by following the standard change management processes, completing required forms, following procedures, completing version control documents, etc.

Job Specific Qualifications
Bachelor’s degree in Computer Science or a related field 4 years of professional technology solution engineering
Demonstrated technology solution ownership and adoption, projects or other work experiences.
Demonstrated experience in analysis of end-user needs to configure, develop and test low to medium complexity technology solutions to satisfy business demand.
Demonstrated track record of implementing technology solutions using structured methodologies such as agile (SCRUM, Kanban etc.) and waterfall.

Required Competencies:
AWS Data Cloud development experience
Experience developing and sustaining data load jobs using Extract, Transform, Load (ETL/ELT) methodologies and tools such as, Python, SQL, Shell Scripts and AWS Services.
Developing and sustaining a data ingestion pipeline to AWS leveraging services such as Athena, Glue, Lambda, S3, Relational Database Service (RDS) and Redshift.
Integration of AWS Data Lake with reporting tools such as PowerBI.
Experience using databases such as Oracle, MS SQL Server, and developing software code in one or more programming languages (Java, Python, etc.)
Experience with production support of mission critical technology solutions.
Experience documenting technical solutions and system process flow techniques.
Ability to work independently, multi-task effectively, and be flexible to accommodate change in priorities as necessary.
Strong analytical ability, communication skills, excellent problem solving skills, and ability to learn new technical concepts.
Ability to foster working relationships with Client departments, IT Management and Software Service Providers.
Desired Qualifications:
Graduate degree or MBA
AWS Developer/Architect certification
Minimum Years of Experience
4 years of experience
Education
Bachelors in Engineering or Computer Science
Bachelor in Information Technology
Certifications
None Noted
Disclaimer
Certain positions at the Company may require you to have access to Part 810-Controlled Information. Under the law, the Company is limited in who it can share this information with and in certain circumstances it is necessary to obtain specific authorization before the Company can share this information. Accordingly, if the position does require access to this information, you must complete a 10 CFR Part 810 Export Control Compliance Nationality Request Form, a copy of which will be provided to you by Talent Acquisition if an offer is made. If there is a need for specific authorization, due to the time it takes to obtain authorization from the government, we will likely not be able to further proceed with an offer
Candidates must foster an inclusive work environment and respect all aspects of diversity. Successful candidates must demonstrate and value differences in others' strengths, perspectives, approaches, and personal choices.
As an employee of PSE&G or PSEG LI, you should be aware that during storm restoration efforts, you may be required to perform functions outside of your routine duties and on a schedule that may be different from normal operations.
Certain positions at the Company may require you to have access to 10 CFR Part 810 controlled information. If the position does require access to this information, the Talent Acquisition representative will provide further details upon making an offer.
PSEG is an equal opportunity employer, dedicated to a policy of non-discrimination in employment, including the hiring process, based on any legally protected characteristic. Legally protected characteristics include race, color, religion, national origin, sex, age, marital status, sexual orientation, disability or veteran status or any other characteristic protected by federal, state, or local law in locations where PSEG employs individuals.
Business needs may cause PSEG to cancel or delay filling position at any time during the selection process.
This site (http://www.pseg.com) is strictly for candidates who are not currently PSEG employees. PSEG employees must apply for jobs internally through emPower which can be accessed through sharepoint.pseg.com by clicking on the emPower icon, then selecting careers.

PEOPLE WITH DISABILITIES:
PSEG is committed to providing reasonable accommodations to individuals with disabilities. If you have a disability and need assistance applying for a position, please call 973-430-3845 or email accommodations@pseg.com. If you need to request a reasonable accommodation to perform the essential functions of the job, email accommodations@pseg.com. Any information provided regarding a disability will be kept strictly confidential and will not be shared with anyone involved in making a hiring decision.

ADDITIONAL EEO/AA INFORMATION (Click link below)
Know your Rights: Workplace Discrimination is Illegal
Pay Transparency Nondiscrimination Provision","$114,000 /yr (est.)",10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1903,$5 to $10 billion (USD)
"LatentView Analytics
4.0",4.0,"Dallas, TX",Data Engineer,"Role : Data Engineer
Experience : 6 - 8+ Years
Location : Dallas,Tx (Onsite)
Position: FullTime Only
Skills :Python,SQL Server ,Scala, Hadoop, HPCC, Storm, Cloudera, Cassandra,Excel, R,Docker,Kubernetes,Snowflake,Azure,Kafka,Redshift,Hadoop,AWS.
Job Type: Full-time
Pay: $80,000.00 - $1,100,000.00 per year
Compensation package:
Yearly pay
Experience level:
6 years
Schedule:
8 hour shift
Experience:
data engineer: 6 years (Preferred)
Work Location: In person
Speak with the employer
+91 9876543210","$109,012 /yr (est.)",1001 to 5000 Employees,Company - Public,Management & Consulting,Business Consulting,2006,$25 to $100 million (USD)
"Dobbs Defense Solutions, LLC",#N/A,"Fort Meade, MD",Data/NiFi Engineer,"Our Mission
At Dobbs Defense, we deliver mission-centric IT, Cyber, and data analytics solutions for our government and commercial clients through the convergence of automation, innovation, training, and education. Delivering high-quality IT, cybersecurity, and data analytics solutions through proven and innovative methods is our vision.

Job Description
Dobbs Defense Solutions is seeking a Senior Data/NiFi Engineer to design, develop, implement, test, secure, and support data transport and brokering for multiple classified networks and between networks.

Duties
Design, develop, implement, test, secure, and support data transport and brokering.
Data flow engineering for one of DISA’s premier boundary defense systems.
Perform system administration of Niagara Files (NiFi) on Linux VMs.
Support backup/failover/load balancing of data brokering to improve availability.
Monitor availability & amp, usage.
Respond to outages, take corrective action, and issue outage reports.
Familiarity with DISA STIGs and SCAP.
Familiarity with Java programing.

Qualifications
Required education and experience:
5+ years of Data Engineering experience.
Significant experience with Niagara Files (NiFi) and MiNiFi or similar data engineering tools.
Excellent written and verbal communication skills.
IAM or IAT Level II Certification (e.g., GSEC, SEC+, SCNP, SSCP, etc.).
Active Secret clearance.

Required Clearance
Secret

Working Environment
Onsite

Our Equal Employment Opportunity Policy
Dobbs Defense Solutions is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or medical conditions related to pregnancy, childbirth or breastfeeding), sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), gender expression and transgender status, national origin, ancestry, age, disability, military or veteran status, marital or domestic partner status, genetic information, citizenship, low-income status or any other status or characteristic protected by applicable law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits, and all other privileges, terms, and conditions of employment.
4Cmw9yg2oT","$83,799 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Octo
4.2",4.2,"Chantilly, VA",Data Engineer,"You…
As a Data Engineer, you will be joining the team that is deploying and delivering a cloud-based, multi-domain Common Data Fabric (CDF), which provides data sharing services to the entire DoD Intelligence Community (IC). The CDF connects all IC data providers and consumers. It uses fully automated policy-based access controls to create a machine-to-machine data brokerage service, which is enabling the transition away from legacy point-to-point solutions across the IC enterprise.
Us…
We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client’s missions.
Program Mission…
The CDF program is an evolution for the way DoD programs, services, and combat support agencies access data by providing data consumers (e.g., systems, app developers, etc.) with a “one-stop shop” for obtaining ISR data. The CDF significantly increases the DI2E’s ability to meet the ISR needs of joint and combined task force commanders by providing enterprise data at scale. The CDF serves as the scalable, modular, open architecture that enables interoperability for the collection, processing, exploitation, dissemination, and archiving of all forms and formats of intelligence data. Through the CDF, programs can easily share data and access new sources using their existing architecture. The CDF is a network and end-user agnostic capability that enables enterprise intelligence data sharing from sensor tasking to product dissemination.
Responsibilities...
Primary responsibility is to work with data providers within the IC and DoD Enterprise to identify and ingest data sets into the CDF data broker. In this role you will:
Develop, optimize, and maintain data ingest flows using Apache Nifi and Python.
Develop within the components in the cloud platform, such as Apache Kafka, NiFi, and HBase.
Communicate with data owners to set up and ensure CDF streaming and batching components are working (including configuration parameters).
Document SOP related to streaming configuration, batch configuration or API management depending on role requirement.
Document details of each data ingest activity to ensure they can be understood by the rest of the team
What we’d like to see…
A minimum of 3 years of experience with programming and software development including analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems
DoD 8570 IAT Level II Certification (e.g. Security+) or the ability to obtain the certification within 90 days
Demonstrable CentOS command line knowledge
Working knowledge of web services environments, languages, and formats such as RESTful APIs, SOAP, FTP/SFTP, HTML, JavaScript, XML, and JSON
Understanding of foundational ETL concepts
Experience implementing data ignorations with in the IC DoD Enterprise.
Desired Skills:
Experience or expertise using, managing, and/or testing API Gateway tools and Rest APIs (desired)
2+ Experience in Python Development
Experience or expertise configuring an LDAP client to connect to IPA (desired)
Advanced organizational skills with the ability to handle multiple assignments
Strong written and oral communication skills
Years of Experience: Junior Level (0-4 years),Mid Level (5-8 years), Senior Level (9+)
Education: Bachelor's degree in systems engineering, computer engineering, or a related technical field (preferred)
Location: Chantilly, VA
Clearance: Active TS/SCI w/ ability to obtain CI Poly (preferred)","$99,041 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,$100 to $500 million (USD)
"TIAA
4.0",4.0,"Iselin, NJ",Senior Cloud Data Engineer,"The Senior Data Platform Engineer, Cloud role designs datastore systems that are appropriate for applications, customer needs and consistent with the overall design of the organization's information systems architecture. Under limited supervision, this job is responsible for the solution engineering and design, provisioning, delivery, service management, continuous automations of the organization's datastore systems.

Key Responsibilities and Duties
Design, develop and deliver cloud datastore solutions and develop automation pipelines to migrate data sets from On-prem to Cloud platforms. Practice Infrastructure as code to develop automation routines and integration flows to manage state of the datastore platform systems

Provision secures from start datastores and enable them with required security controls including encryption, masking, certificate/keys rotation etc.

Collaborates with developers, analysts, various system administrators to identify business requirements in designing efficient datastore solutions and interfaces.

Identifies and documents all system constraints, implications, and consequences of various proposed system changes.

Reviews technical documentation to guide system users and to assist with the ongoing operation, maintenance, and development of the system. Evaluates the efficiency and effectiveness of application operations and troubleshooting problems.

Provide expert level IT technical lead services, including the direction, evaluation, selection, configuration, implementation, and integration of new and existing technologies and tools in a cloud platform.

Responsible for development of cloud integrations and data migrations to support operations of Cloud infrastructure, provisioning, monitoring with (IaaS) and (PaaS) models

Deploy, automate, maintain, and manage AWS cloud-based production system, to ensure the availability, performance, scalability, and security of productions systems.

Manage the governance framework for DB-Services specific to private, hybrid and public cloud platform adhering to standards and integration with existing tools.

Ability to anticipate technology changes within a rapidly evolving environment.
Educational Requirements
Bachelor's Degree Preferred
Work Experience
3+ Years Required; 5+ Years Preferred
Physical Requirements
Physical Requirements: Sedentary Work

Career Level
7IC

Required Skills:
3 or more years of experience in SQL, ETL and ELT Tools.
Experience working with Data Virtualization Platforms like Starburst, Presto, Denodo, Dremio.
Preferred Skills:
Experience with AWS or GCP, PySpark, CI/CD Pipelines using ElectricFlow.
Base Pay Range: $88,600/yr. - $147,600/yr.
Actual base salary may vary based upon, but not limited to, relevant experience, time in role, base salary of internal peers, prior performance, business sector, and geographic location. In addition to base salary, the competitive compensation package may include, depending on the role, participation in an incentive program linked to performance (for example, annual discretionary incentive programs, non-annual sales incentive plans, or other non-annual incentive plans).
_____________________________________________________________________________________________________
Company Overview
TIAA is the leading provider of financial services in the academic, research, medical, cultural and government fields. We offer a wide range of financial solutions, including investing, banking, advice and education, and retirement services.
Benefits and Total Rewards
The organization is committed to making financial well-being possible for its clients, and is equally committed to the well-being of our associates. That’s why we offer a comprehensive Total Rewards package designed to make a positive difference in the lives of our associates and their loved ones. Our benefits include a superior retirement program and highly competitive health, wellness and work life offerings that can help you achieve and maintain your best possible physical, emotional and financial well-being. To learn more about your benefits, please review our
Benefits Summary
.
Equal Opportunity
We are an Equal Opportunity/Affirmative Action Employer. We consider all qualified applicants for employment regardless of age, race, color, national origin, sex, religion, veteran status, disability, sexual orientation, gender identity, or any other protected status.
Read more about the Equal Opportunity Law
here
.
Accessibility Support
TIAA offers support for those who need assistance with our online application process to provide an equal employment opportunity to all job seekers, including individuals with disabilities.
If you are a U.S. applicant and desire a reasonable accommodation to complete a job application please use one of the below options to contact our accessibility support team:
Phone: (800) 842-2755
Email:
accessibility.support@tiaa.org
Privacy Notices
For Applicants of TIAA, Nuveen and Affiliates residing in US (other than California), click
here
.
For Applicants of TIAA, Nuveen and Affiliates residing in California, please click
here
.
For Applicants of Nuveen residing in Europe and APAC, please click
here
.
For Applicants of Greenwood residing in Brazil (English), click
here
.
For Applicants of Greenwood residing in Brazil (Portuguese), click
here
.
For Applicants of Westchester residing in Brazil (English), click
here
.
For Applicants of Westchester residing in Brazil (Portuguese), click
here
.","$118,100 /yr (est.)",10000+ Employees,Company - Private,Financial Services,Investment & Asset Management,1918,$100 to $500 million (USD)
"Delta
4.3",4.3,"Atlanta, GA",Senior Data Engineer Modeler,"United States, Georgia, Atlanta
Information Technology
11-May-2023
Ref #: 20782
LinkedIn Tag: #LI-JM2
How you'll help us Keep Climbing (overview & key responsibilities)
Location: Atlanta GA - NOT Remote

Delta IT is on a journey of transformation. We are changing the way we do business from top to bottom. As leaders with vision within Delta, we strive to build important and innovative solutions and are looking for team members to help us realize our vision.

Delta employees are problem solvers, doers, innovators.

We are proactive.

We are collaborative.

We deliver impact to our customers.

Join us on our transformation journey in becoming a best-in-class IT organization at the world's best airline!

The Senior Data Engineer, Modeler to join our Enterprise Data team. This position is responsible for building, modifying, modernizing the conceptual, logical, and physical data models used in Enterprise Data databases. The position also requires establishing and maintaining effective partnerships with the Business and IT stakeholders.

We are looking for someone with strong analytical and organizational skills to transform data into insights, distill requirements, and develop processes. The candidate should have excellent communication skills, business maturity, and feel comfortable in a fast-paced environment.

Responsibilities
Understand the data needs of the company for ingestion, migration, storage, and access
Work with business teams to gather requirements for the database design and model
Collaborate with the Enterprise Data team and business teams to define requirements, then design and build database models
Design and build conceptual, logical, and physical data models in accordance with companys data standards
Apply relational and dimensional models for raw ingestion and curated/semantic layers
Create Physical Data Structures (DDLs) and corresponding metadata
Creating and maintaining data reference architecture architectures and integration patterns
Updating knowledge by tracking and understanding emerging large data and modeling practices and standards
Benefits and Perks to Help You Keep Climbing
Our culture is rooted in a shared dedication to living our values Care, Integrity, Resilience, Servant Leadership, and Teamwork every day, in everything we do. At Delta, our people are our success. At the heart of what we offer is our focus on Sharing Success with Delta employees. Exploring a career at Delta gives you a chance to see the world while earning great compensation and benefits to help you keep climbing along the way:
Competitive salary, industry-leading prot sharing program, and performance incentives.
401(k) with generous company contributions up to 9%
Paid time off including vacation, holidays, paid personal time, maternity and parental leave.
Comprehensive health Benefits including medical, dental, vision, short/long term disability and life Benefits.
Family care assistance through fertility support, surrogacy and adoption assistance, lactation support, subsidized back-up care, and programs that help with loved ones in all stages.
Holistic Wellbeing programs to support physical, emotional, social, and financial health, including access to an employee assistance program offering support for you and anyone in your household, free financial coaching, and extensive resources supporting mental health.
Domestic and International space-available flight privileges for employees and eligible family members
Career development programs to achieve your long-term career goals.
World-wide partnerships to engage in community service and innovative goals created to focus on sustainability and reducing our carbon footprint
Business Resource Groups created to connect employees with common interests to promote inclusion, provide perspective and help implement strategies
Recognition rewards and awards through the platform Unstoppable Together
Access to over 500 discounts, specialty savings and voluntary benefits through Deltaperks such as car and hotel rentals and auto, home, and pet insurance, legal services, and childcare
What you need to succeed (minimum qualifications)
7 or more years of experience in Information Technology or related technical capacity
Expert in concepts and principles of data modeling
Knowledge of entity relationship, dimensional modeling, big data, enterprise data, and physical data models
Knowledge of relational databases and data architecture computer systems, including SQL Familiarity
Ability to design, build, and develop a new product, technology, or service from feasibility through to production
Familiarity with data modeling software such as SAP PowerDesigner, Microsoft Visio, E/R Studio or Erwin Data Modeler
Must have hands-on experience with cloud platforms; AWS preferred
Knowledge of big data platforms such as Teradata, Oracle DB, DB2, AWS Aurora, AWS Athena, etc.
Experience using Python and/or PowerShell scripting for data processing
Strong attention to detail
Excellent communicationwith both technical and business stakeholders
Ability to work in a fast-paced environment
Ability to work both independently and as part of a team
Understanding of the business and the ability to assess and address risk without negatively impacting the business
Consistently prioritizes safety and security of self, others, and personal data.
Embraces diverse people, thinking, and styles.
Possesses a high school diploma, GED, or high school equivalency.
Is at least 18 years of age and has authorization to work in the United States.
What will give you a competitive edge (preferred qualifications)
Bachelors or masters degree in Information Technology, Computer Science, Mathematics, Engineering, Information Systems, or equivalent
In-depth understanding of ETL and data ingestion processes used in large scale data warehouses
Knowledge of data architecture principles for on-prem and cloud solutions
Flexibility to adapt and plan for changing business objectives
Solid Understanding of Agile/Scrum development methodologies
Experience translating business outcome requirements into data model requirements
Ability to adapt communication style for technical and business audiences
< Go back","$124,024 /yr (est.)",10000+ Employees,Company - Public,Transportation & Logistics,"Airlines, Airports & Air Transportation",1928,$10+ billion (USD)
"Great Dane
2.9",2.9,"Chicago, IL",Data Engineer,"Data Engineer - (230005R)
Description
With thousands of employees worldwide, teamwork and collaboration are valued here.

We look for employees who are driven, determined and ready to accelerate their future. By joining our team, you will earn competitive pay, benefits, insurance, 401k, pension and more while working in an environment with the highest safety standards in the industry.
The Position:
The Data Engineer is responsible for expanding and optimizing our current data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is experienced in both data pipeline creation and data transformation. The Data Engineer will support our software developers, system architects, data analysts and Business Analysts on all data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s current data architecture to support our next generation of products and data initiatives.
Key Responsibilities:
Create and maintain optimal data pipeline architecture.
Assemble complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep data separated and secure across all platforms used.
Work with data and business experts to strive for greater functionality in our data systems.
Other duties as assigned.
Qualifications
Requirements:
Education: Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Experience: 5+ years of experience in Data Engineer role
Skills: Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing data pipelines architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from disconnected datasets.
Project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with relational SQL databases, including MS SQL server, Oracle, DB2, and Maria.
Experience with Data Cloud platforms like SnowFlake and or Data Bricks.
Experience with data pipeline and workflow management tools: SQDR, Airflow, Fivetran, Airbyte, etc.
Experience with object-oriented/object function scripting languages: Python, Java
Travel: 20% at most
Physical Demands/Work Environment:
The physical demands and work environment characteristics described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Physical demands: While performing duties of job, employee is occasionally required to stand; walk; sit; use hands to finger, handle or feel objects; reach with hands and arms; talk and hear. Specific vision abilities required by the job include close and distance vision.
Work environment: The noise level in the work environment is usually minimal to moderate.
Must be willing to work occasional nights and/or weekends as business commitments dictate.
Great Dane is an Equal Opportunity Employer
Primary Location
: US-IL-Chicago
Work Locations
: Chicago N. LaSalle St. 222 N. LaSalle St. Suite 920 Chicago 60601
Job
: Information Systems
Schedule
: Full-time
Shift
: 1st Shift","$104,652 /yr (est.)",5001 to 10000 Employees,Company - Private,Manufacturing,Transportation Equipment Manufacturing,1900,$500 million to $1 billion (USD)
"Texas Capital Bank
3.0",3.0,"Richardson, TX",Senior Data Engineer,"Overview:
A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses on development and delivery of analytical solutions using various tools including AWS Glue, Lambda, Snowflake and AWS RDS. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.
Responsibilities:
Partner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (onshore and offshore)
Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storage
Advanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plans
Skilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumption
Expert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging, and error handling
Support continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review board
Develop and ensure adherence to published system architectural decisions and development standards
Lead and foster junior data engineers in their careers to produce higher quality solutions at a faster velocity through optimization training and code review
Multi-task across several ongoing projects and daily duties of varying priorities as required
Interact with global technical teams to communicate business requirements and collaboratively build data solutions
Qualifications:
Education
Bachelor’s degree in computer science or MIS related area required or equivalent experience (industry experience substitutable)
Job experience
4-8 years of total experience in data engineering/Cloud development activity.
1+ years of experience in Banking and financial domain

Technical Requirement
Must be extremely proficient in Data Warehouse ETL Design/Architecture, dimensional/relational data modelling.
Experience in atleast one ETL development project, writing/analyzing complex stored procedures.
Should have entry level/intermediate experience in Python/PySpark – working knowledge on spark/pandas dataframe, spark multi-threading, exception handling, familiarity with different boto3 libraries, data transformation and ingestion methods, ability to write UDF.
Snowflake – Familiarity with stages and external tables, commands in snowflake like copy, unload data to/from S3, working knowledge of variant data type, flattening nested structure thru SQL, familiarity with marketplace integrations, role-based masking, pipes, data cloning, logs, user and role management is nice to have.
Familiarity with Coalesce is an added advantage for this job
Collibra integration experience for Data Quality and Governance in ETL pipeline development is nice to have.
AWS – Should have hands-on experience with S3, Glue (jobs, triggers, workflow, catalog, connectors, crawlers), CloudWatch, RDS and secrets manager.
AWS - VPC, IAM, Lambda, SNS, SQS, MWAA and Athena is nice to have.
Should have hands-on experience with version controlling tools like github, working knowledge on configuring, setting up CI/CD pipelines using yaml, pip files.
Streaming Services – Familiarity with Confluent Kafka or spark streaming or Kinesis (or equivalent) is nice to have.
Data Vault 2.0 (hubs satellite links) experience will be a
Highly proficient in Publisher, PowerPoint, SharePoint, Visio, Confluence and Azure DevOps
Working knowledge of best practices in value-driven development (requirements management, prototyping, hypothesis-driven development, usability testing)
Good communicator with problem solving mindset and focus on process improvement
Strong time management skills and a proven track record of meeting various deadlines
Strong executive presentation skills with expertise in PowerPoint and presentation best practices.
Consistently demonstrates clear and concise written and verbal communication skills
Good interpersonal skills, ability to interact with Senior Management
Highly self-motivated with a strong sense of initiative
Excellent multitasking skills and task management strategies
Ability to work well in a team environment, meet deadlines, demonstrate good time management, and multi-task in a fast-paced project environment.","$106,721 /yr (est.)",1001 to 5000 Employees,Company - Public,Financial Services,Banking & Lending,1998,Unknown / Non-Applicable
"Mastery Logistics Systems, Inc
3.9",3.9,"Omaha, NE",Senior Data Engineer (Kafka),"About the Role
In the world of transportation, data is constantly moving, and Kafka is the roadway that keeps that traffic running smoothly to its destination. As a technical expert, you must be comfortable working across teams on multiple, high impact projects. You will be a valued part of a team that is constantly maturing Kafka use and event-driven architecture. Members of this team are responsible for the overall use and implementation of Kafka components including the Confluent platform, observability, governance, best practices, and solution development. An understanding of Kafka principles and enterprise integration patterns is required.
In order to be successful:
You are a self-directed person who can identify priorities.
You are a detail-oriented person who takes pride in keeping data correct and always having a backup plan.
You are a problem-solver who might write a script or find a tool to get things done when there isn't an established solution.
You want to learn and grow in the event-driven world.
You love Kafka! When you hear terms like ""event-driven"" or ""real-time streaming"" you're ready ready to dive in!
Responsibilities
Lead a team of Kafka engineers in an operational capacity
Develop and implement solutions using Kafka.
Administer and improve use of Kafka across the organization including Kafka Connect, ksqlDB, Streams, and custom implementations.
Work with multiple teams to ensure best use of Kafka and data-safe event streaming.
Understand and apply event-driven architecture patterns and Kafka best practices. Enable development teams to do the same.
Assist developers in choosing correct patterns, event modeling, and ensuring data integrity.
Continuous learning to be a Confluent/Kafka subject matter expert.
Work with Kafka and Confluent API's (e.g. metadata, metrics, admin) to provide pro-active insights and automation.
Work with SRE's to ensure Kafka-related metrics are exported to New Relic.
Perform regular reviews of performance data to ensure efficiency and resiliency.
Contribute regularly to event-driven patterns, best practices, and guidance.
Review feature release and change logs for Kafka, Confluent, and other related components to ensure best use of these systems across the organization.
Work with lead to ensure all teams are aware of technology changes and impact.
Acquire a deep understanding of source and sink connector technical details for a variety of platforms including PostgreSQL, MS SQL Server, Snowflake, and others as required.
Requirements
Be able to describe the primary components of Kafka and their function (brokers, zookeeper, topics).
At least two years of experience supporting applications in a production environment.
You will be expected to read and navigate code in multiple languages. Multi-language fluency and writing is not required.
Experience in a microservice architecture
Experience with event driven architecture
Proficiency in at least one programming language and one scripting language.
Proficiency with Docker containers.
Ability to participate in and contribute to code management in Github including actively collaborating in peer-reviews, feature branches, and resolving conflicts and commits.
Excellent written and verbal communication skills.
Strong sense of responsibility with a bias towards action.
Comfortable self-directing and prioritizing your own work.
Microservices experience is a plus.
Distributed tracing experience a plus.
An understanding of any cloud (Azure preferred) infrastructure and components is a plus, but is not required.
Create reference solutions.","$96,493 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"University of Arizona
4.2",4.2,"Tucson, AZ",KMAP Data Engineer,"Posting Number
req16040

Department
Research Innovation & Impact

Department Website Link
https://kmap.arizona.edu/

Location
Main Campus

Address
Tucson, AZ USA

Position Highlights
The Institutional Knowledge Map (KMap) program at the University of Arizona is in search of a KMap Data Engineer (Applications Developer Programmer Analyst I). KMap is a data science system that helps organizations to understand their knowledge landscape. It uses advanced technologies to analyze research information like research papers, patents, grants, biography, CV, and web presence to make an artificial map of the organization's knowledge. This visual map helps to find experts and collaborators. This position is responsible for collecting, cleaning, and aggregating data for research activities of the university. This position will take care data collections and data processing pipelines of the KMap project.
The ideal candidate must possess proficiency in Python and MongoDB for efficient handling, processing, and analysis of research information.
Outstanding UA benefits include health, dental, and vision insurance plans; life insurance and disability programs; paid vacation, sick leave, and holidays; UA/ASU/NAU tuition reduction for the employee and qualified family members; state and optional retirement plans; access to UA recreation and cultural activities; and more!
The University of Arizona has been recognized for our innovative work-life programs.


Duties & Responsibilities
Duties & responsibilities:
Develop and maintain the data collection and processing pipelines.
Collaborate with data partners and other related groups
Technical documentation of the project
Collecting, cleaning, and transforming new data from internal and external sources
Write and execute automatic test cases to test data and application
Additional duties may be assigned
Knowledge, skills & abilities:

Knowledge of data processing techniques and tools
Knowledge of using API, and different data collection methodologies
Strong analytic skills related to working with unstructured and RDMBS datasets
Proficiency in Python and MongoDB for efficient handling, processing, and analysis of research information

Minimum Qualifications
Bachelor's degree in Information Technology or equivalent advanced learning attained through experience required and a minimum of 1 year of relevant work experience required.

Preferred Qualifications
Experience in data science tasks with Python is a plus
Experience working with complex data in a NoSQL-based environment
Experience with Python packages such as pandas, NumPy
Experience working with Linux server environments including shell scripting
Experience with aggregation tasks in MongoDB
Experience working with Neo4j graph database
Experience with graph data analysis and visualize
Experience with Python programming and MongoDB database
Experience with using source-controlling systems

FLSA
Exempt

Full Time/Part Time
Full Time

Number of Hours Worked per Week
40

Job FTE
1.0

Work Calendar
Fiscal

Job Category
Information Technology

Benefits Eligible
Yes - Full Benefits

Rate of Pay
DOE

Compensation Type
salary at 1.0 full-time equivalency (FTE)

Grade
8

Career Stream and Level
PC1

Job Family
Applications Development

Job Function
Information Technology

Type of criminal background check required:
Name-based criminal background check (non-security sensitive)

Number of Vacancies
1

Target Hire Date

Expected End Date

Contact Information for Candidates
Iqbal Hossain
hossain@arizona.edu

Open Date
5/11/2023

Open Until Filled
Yes

Documents Needed to Apply
Resume and Cover Letter

Special Instructions to Applicant

Diversity Statement
At the University of Arizona, we value our inclusive climate because we know that diversity in experiences and perspectives is vital to advancing innovation, critical thinking, solving complex problems, and creating an inclusive academic community. As a Hispanic-serving institution, we translate these values into action by seeking individuals who have experience and expertise working with diverse students, colleagues, and constituencies. Because we seek a workforce with a wide range of perspectives and experiences, we provide equal employment opportunities to applicants and employees without regard to race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, gender identity, or genetic information. As an Employer of National Service, we also welcome alumni of AmeriCorps, Peace Corps, and other national service programs and others who will help us advance our Inclusive Excellence initiative aimed at creating a university that values student, staff and faculty engagement in addressing issues of diversity and inclusiveness.","$79,143 /yr (est.)",10000+ Employees,College / University,Education,Colleges & Universities,1885,$1 to $5 billion (USD)
"Vertex, Inc.
3.9",3.9,Remote,Sr. Data Engineer- Cloud (Remote),"Job Description:
This position is responsible for performing analysis, design, implementation, testing, maintenance, and support tasks for data-intensive software applications programming. Improves system quality by identifying issues and common patterns and developing standard operating procedures. Enhances applications by identifying opportunities for improvement, making recommendations, and designing and implementing systems. ESSENTIAL JOB FUNCTIONS AND RESPONSIBILITIES: · Prepare technical design specifications based on functional requirements and analysis documents. · Implement, test, maintain and support software, based on technical design specifications. · Improve system quality by identifying issues and common patterns, and developing standard operating procedures · Enhance applications by identifying opportunities for improvement, making recommendations, and designing and implementing systems · Maintain and improve existing codebases and peer review code changes · Liaise with colleagues to implement technical designs · Investigating and using new technologies where relevant · Provide written knowledge transfer material · Review functional requirements, analysis, and design documents and provide feedback. · Assist customer support with technical problems and questions. · Assist and mentor other development staff. · Perform special assignments. · Participate in architecture and code reviews. · Lead or participate in other projects or duties. · Occasional travel required. (Up to 5%) · Participate in other projects or duties. SUPERVISORY RESPONSIBILITIES: · N/A KNOWLEDGE, SKILLS, AND ABILITIES: * * * * * * * · Ability to network with key contacts outside own area of expertise. * * · Must possess strong interpersonal, organizational, presentation and facilitation skills. · Must be results oriented and customer focused. · Must possess good organizational skills. * * EDUCATION AND TRAINING: · Bachelor’s degree in computer science, Information Systems, or related field; or equivalent combination of education/experience. Master’s degree is a plus. · 7 years or more of extensive experience developing mission critical and low latency solutions. · At least 4 years of experience with developing and debugging distributed systems and data pipelines in the cloud. AWS is a must. · Extensive experience with SQL cloud databases like Snowflake (a must-have experience), and MS SQLServer. Experience with NoSQL databases like AWS DynamoDB and Azure Cosmos is a plus. · Good understanding of data modeling, ETL, data curation, and big data performance tuning. · Experience with Business Intelligence tools is a plus. · Experience working with AWS and/or Azure DevOps and extensive debugging experience. · Ability to code in one or more languages like Python, Java, Scala. · An understanding of unit testing, test driven development, functional testing, and performance · Knowledge of at least one shell scripting language. Other Qualifications The Winning Way behaviors that all Vertex employees need to meet the expectations of each other, our customers, and our partners. • Communicate with Clarity - Be clear, concise, and actionable. Be relentlessly constructive. Seek and provide meaningful feedback. • Act with Urgency - Adopt an agile mentality - frequent iterations, improved speed, resilience. 80/20 rule - better is the enemy of done. Don’t spend hours when minutes are enough. • Work with Purpose - Exhibit a ""We Can"" mindset. Results outweigh effort. Everyone understands how their role contributes. Set aside personal objectives for team results. • Drive to Decision - Cut the swirl with defined deadlines and decision points. Be clear on individual accountability and decision authority. Guided by a commitment to and accountability for customer outcomes. • Own the Outcome - Defined milestones, commitments and intended results. Assess your work in context, if you’re unsure, ask. Demonstrate unwavering support for decisions. COMMENTS: The above statements are intended to describe the general nature and level of work being performed by individuals in this position. Other functions may be assigned, and management retains the right to add or change the duties at any time.",#N/A,1001 to 5000 Employees,Company - Public,Information Technology,Software Development,1978,$100 to $500 million (USD)
"Delaware North
3.6",3.6,"Buffalo, NY",Data Engineer,"The Opportunity
Delaware North Global Headquarters is hiring a Data Engineer to join our Information Technology team in Buffalo, New York. As a Data Engineer, you will work with programming languages, frameworks, databases, front-end tools, back-end tools, and applications connected via APIs to collect raw data and transform the data into canonical models. The Data Engineering team is tasked with harmonizing and enhancing the data to provide trusted datasets to our consumers. The work our data manager team does forms the foundation of company initiatives to help automate business processes and gather insights, to help the company make more informed decisions.
Minimum - Anticipated Maximum Salary: $70700 - $93700 / year
The advertised pay range represents what we believe at the time of this job posting, that we would be willing to pay for this position. Only in special circumstances, where a candidate has education, training, or experience that far exceeds the requirements for the position, would we consider paying higher than the stated range. Information on our comprehensive benefits package can be found at https://careers.delawarenorth.com/whatweoffer.

At Delaware North, we care about our team member’s personal and professional journeys. These are just some of the benefits we offer:
Health, dental, and vision insurance
401(k) with company match
Performance bonuses
Paid vacation days and holidays
Paid parental bonding leave
Tuition and/or professional certification reimbursement
Generous friends-and-family discounts at many of our hotels and resorts
Responsibilities
Utilize technology stacks such as Apigee, Python, Django, Apache Airflow, MongoDB, PostgreSQL, and Amazon Web Services such as Lambda, EBS, S3, SQS, ECS, RDS, EC2 and Redshift to build datasets.
Design and implement project-based solutions.
Implement data platform improvements and new features.
Assist the support team with the resolution of data platform bug fixes.
Interface with clients, vendors, and internal users of the data platform on understanding the data.
Author documentation for standard operating procedures and knowledge base articles.
Develop integration tests to validate solutions.
Qualifications
Bachelor’s degree or equivalent from an accredited college or university in Computer Science, Information Systems, or similar STEM field preferred.
Minimum of 2 years of experience developing data pipeline ETL processes.
Extensive experience following End to End Agile Development Lifecycle and writing in SQL and Query.
Data persistence methods such as NoSQL and RDBMS, data structures and formats such as JSON, XML, and parquet.
Cloud computing experience as it relates to event-based serverless architecture, AWS preferred.
Extensive experience with handling large data sets, high performance computing, building high performance solutions and data integration projects.
Technical specification and use case documentation, such as UML, Domain and Entity-Relationship Modeling, Business Process Notation.
Must be legally authorized to work in the US without sponsorship.
Who We Are
At Delaware North, you’ll love where you work, who you work with, and how your day unfolds. Whether it’s in sporting venues, casinos, airports, national parks, iconic hotels, or premier restaurants, there’s no telling where your career can ultimately take you. We empower you to do great work in a company with 100 years of success, stability and growth. If you have drive and enjoy the thrill of making things happen - share our vision and grow with us.
Delaware North Companies, Incorporated and its subsidiaries consider applicants for all positions without regard to race, color, religion, creed, gender, national origin, age, disability, marital or veteran status, sexual orientation, or any other legally protected status. Delaware North is an equal opportunity employer.","$82,200 /yr (est.)",10000+ Employees,Company - Private,Restaurants & Food Service,Catering & Food Service Contractors,1915,$1 to $5 billion (USD)
"Equifax
3.6",3.6,"Alpharetta, GA",Data Reliability Engineer - P3,"Equifax is where you can power your possible. If you want to achieve your true potential, chart new paths, develop new skills, collaborate with bright minds, and make a meaningful impact, we want to hear from you.
The Data Reliability Engineer position is a high-performing role in support of data reliability and quality functions for various domains within USIS’s broader data library.
This individual will work within a team of data reliability engineers and data stewards, who serve as experts for the data sources, to ensure the appropriate quality and internal and external support are in accordance with our service level commitments. Candidates should be courageous, tenacious, creative, optimistic and curious, with an appetite for growth in business-minded leadership.
This role is located in our Alpharetta, Georgia office. We have a hybrid work model which requires being in the office 3 days/week.
What you’ll do
Domain Expertise: Understand the lifecycle of the Equifax data sources across all stages from ingestion to production
Drive Data Reliability and Quality across our Data Pipeline
Quality Monitoring and Incident Management: Own threshold-based reports that efficiently and reliably identify areas for further investigation and improvement. Act on the findings of these reports in a timely manner, prioritizing effort based on business impact.
Continuous Improvement: Work with an evergreen roadmap of initiatives, powered by work in Quality Monitoring and Customer Engagement, helping to drive these monitors and incident management process to an automated approach for each data domain/index.
Be an ambassador for our data assets and encourage data awareness which can assist with new opportunities and points of synergy.
What experience you need
5+ years of relevant experience with industry-leading lenders and/or data companies, with a working knowledge of data structure, data handling, the credit lifecycle and data management
5+ years of analysis experience in credit risk, telecommunications, financial services, marketing, fraud, or insurance
3+ years of experience with data handling/quality tools and Big Data ecosystems i.e. SQL, Google Cloud Platform, BigQuery
3+ years of experience with data, business insights and data & data processing audit analysis which identify and drive ongoing excellence in data quality practices
What could set you apart
A strong sense of governance protocols, the need to respect highly sensitive data and proactive attention to regulatory, security and compliance driven policies
Strong aptitude and proven ability to develop data-driven solutions to meet business objectives
Demonstrates leadership with cross-functional efforts; develops effective working relationships with peers, managers, and senior management within and across organizational lines
The ability to manage multiple high-impact projects at the same time
Creativity and drive, coupled with a desire to grow in thought leadership, initiative and strategic thinking
Experience with Collibra, or similar metadata management tool
Experience with Spotfire, or similar data visualization tool
Experience with SQL, Google BigQuery, or equivalent database queries and views
We offer comprehensive compensation and healthcare packages, 401k matching, paid time off, and organizational growth potential through our online learning platform with guided career tracks.
If this sounds like somewhere you want to work, don’t delay, apply today - we’re looking for you!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.","$85,081 /yr (est.)",10000+ Employees,Company - Public,Management & Consulting,Research & Development,1899,$1 to $5 billion (USD)
"Veolia
3.8",3.8,"Paramus, NJ",Senior Data Engineer,"Company Description Veolia Group aims to be the benchmark company for ecological transformation. With nearly 220,000 employees worldwide, the Group designs and provides game-changing solutions that are both useful and practical for water, waste and energy management. Through its three complementary business activities, Veolia helps to develop access to resources, preserve available resources and replenish them. In 2021, the Veolia group provided 79 million inhabitants with drinking water and 61 million with sanitation, produced nearly 48 million megawatt hours and recovered 48 million tonnes of waste. Veolia Environnement (Paris Euronext: VIE) achieved consolidated revenue of 28,508 billion euros in 2021. www.veolia.com
Job Description
Develop and operate data management tools, monitoring data flows, data quality, data cleansing and data processing
Create and document logical data integration strategies for data flows between disparate systems and the enterprise data warehouse/data lakes
Collaborate with different stakeholders (engineers, data stewards) to collect required data from internal and external systems
Work in an Agile environment that focuses on collaboration and teamwor
Improve and extend existing data infrastructure services
Monitor production job schedule and correct job failures in a timely manner

Qualifications
MS degree in Computer Science or computer related field from an accredited institution.
5+ years hands proven experience as a Data Engineer or similar role.
5+ years of strong experience building, running and maintaining datalake(s) and warehouse(s) in a cloud environment.
More than 4 years of experience developing with Python.
4+ years performing with production environments in a DevOps culture managing code composed of multi-developer teams, following industry best practices.
4+ years SQL development experience.
Experience with data modeling
4+ years bash scripting experience.
Strong experience with Git, CI/CD (preferably GitLab) and Docker.
Experience deploying and running services in Cloud Big Data platforms such as BigQuery and Snowflake.
Strong experience with GCP services.
Experience designing and building data pipelines using tools like Apache Beam, CDAP (Data Fusion) or other ETLs.
Knowledge with CDC design patterns and their challenges.
Experience with DAG workflows orchestration such as Apache Airflow.
Experience with NoSQL databases is a plus (i.e Firestore, MongoDB).
Experience designing and developing APIs is a plus (i.e using FastAPI, Flask).

(Nice to have) Google Cloud Data Engineer
Abilities:
Being able to work in a large company with different stakeholders.
Embrace mentorship through design sessions, code reviews, and community building.
Take ownership and support solutions you develop.
Value collaboration with other members of the team.
Have a product mindset.
Good communication.

Additional Information

A subsidiary of Veolia group, Veolia North America (VNA) offers a full spectrum of water, waste and energy management services, including water and wastewater treatment, commercial and hazardous waste collection and disposal, energy consulting and resource recovery. VNA helps commercial, industrial, healthcare, higher education and municipality customers throughout North America. Headquartered in Boston, Mass., Veolia North America has more than 10,000 employees working at more than 350 locations across the continent. www.veolianorthamerica.com As an inclusive company, Veolia is committed to diversity and gives equal consideration to all applications, without discrimination. We are an Equal Opportunity Employer! All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. As an inclusive company, Veolia is committed to diversity and gives equal consideration to all applications, without discrimination. We are an Equal Opportunity Employer! All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.
Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. The Company reserves the right to modify this information at any time, subject to applicable law.","$119,169 /yr (est.)",10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1853,$1 to $5 billion (USD)
"Crisis Prevention Institute
3.0",3.0,"Milwaukee, WI",Senior Data Engineer,"Our Story:
Crisis Prevention Institute Inc. is the worldwide leader in evidence-based de-escalation and crisis prevention training, and dementia care services. Since 1980, we’ve helped train more than 15 million people within service-oriented industries including education, healthcare, behavioral health, long-term care, human services, security, corrections, corporate, and retail.

At CPI, we are dedicated to changing behaviors and reducing conflict for the Care, Welfare, Safety, and SecuritySM of everyone. We believe in the power of empathy, compassion, and meaningful connections. We believe personal safety and security are the antidotes to fear and anxiety. It’s a philosophy that is central to everything we do, and traces back to our beginning. It is what defines and differentiates us, and informs our core beliefs.
As a member of the team, you can expect to:
Make a difference through your work – You’ll be proud to tell your family and friends about what you do.
Gain significant career experience only obtained within a fast-growing organization – Entry-level roles through executive leadership.
Feel fulfilled and have fun – We work hard but make the time to build meaningful relationships and celebrate the wins.

The Role:
The Senior Data Engineer will focus on quality engineering best practices to meet and exceed internal and external client expectations. In this position, you will analyze, design, develop, test and document solutions supporting data integration, performance tuning, and data modeling to drive organization growth objectives. The Senior Data Engineer will define the standards for data architecture, platform architecture, and data quality and governance. This role is responsible for ensuring that the function is aligned with the overall CPI organization and continuously works to meet critical service levels in access, delivery and security.
What You Get To Do Everyday:
Co-architect CPI’s next gen cloud data analytics platform.
Increase operating efficiency and adapt to new requirements.
Monitor and maintain the health of solutions generated.
Support and enhance our data-ops practices.
Provide task breakdowns, identify dependencies, and provide effort estimates.
Model data warehouse entities in Erwin.
Build data transformation pipelines with Data Build Tools (DBT).
Evaluate the latest technology trends and develop proof-of-concept prototypes that align with CPI opportunities.
Develop positive relationships with clients, stakeholders, and internal teams.
Understand business goals, drivers, context, and processes to suggest technology solutions that improve the organization.
Work collaboratively on creative solutions with engineers, product managers, and analysts in an agile like environment.
Perform, design, and code reviews.
Perform other position-related duties as assigned.
You Need to Have:
Bachelor’s degree in computer engineering, computer science, data science, or related field
Two years or more experience designing and implementing data warehouses in Snowflake
Eight years or more experience working with data modeling, architecture and engineering
Experience with all core software development activities, including requirements gathering, design, construction, and testing
Experience performing data transformation using DBT
Experience working with DQ products such as Monte Carlo, BigEye, or Great Expectations
Experience with Azure DevOps (Repos, Pipelines, Boards, Wiki, Test Plans)
Experience with formal software development methodologies including Software Development Life Cycle (SDLC), Agile or SCRUM
Experience building high-performance and highly reliable data pipelines
Experience Knowledge of data warehouse design patterns (star schema, data vault)
Experience building dashboards with business integrations tools
Knowledge of DataOps
with cloud-based compute, storage, integration and security patterns
Knowledge and understanding of RESTful APIs
Knowledge of current data engineering trends, best practices, and standards
Knowledge of SQL and Python
Ability to work in a collaborative environment
Ability to facilitate evaluation of technologies and achieve consensus on technical standards and solutions among a diverse group of information technology professionals
Ability to work in an organization driven by continuous improvement or with equivalent focus on process improvement
Ability to manage multiple, competing priorities and attain the best possible outcomes for the organization
Excellent verbal and written communication and effective listening skills
We'd Love to See:
Experience in delivering an end-to-end data analytics platform using modern data stack components
Experience with AI/ML
SnowPro Advanced Certification
DBT Analytical Engineer Certification
What We Offer:
Competitive salary
Comprehensive benefits package
401k
PTO
Health & Wellness Days
Paid Volunteer Time Off
Continuing education and training
Hybrid work schedule
???????Crisis Prevention Institute is an Equal Opportunity Employer that does not discriminate against any applicant or employee on the basis of age, race, color, ethnicity, national origin, citizenship, religion, creed, sex, sexual orientation, gender, gender identity, or expression (including against any individual that is transitioning, has transitioned, or is perceived to be transitioning), marital status or civil partnership/union status, physical or mental disability, medical condition, pregnancy, childbirth, genetic information, military and veteran status, or any other basis prohibited by applicable federal, state, or local law. The Company will consider for employment qualified applicants with criminal histories in a manner consistent with local and federal requirements. Our management team is dedicated to this policy with respect to recruitment, hiring, placement, promotion, transfer, training, compensation, benefits, employee activities, and general treatment during employment.","$95,913 /yr (est.)",201 to 500 Employees,Company - Private,Education,Education & Training Services,1980,$25 to $100 million (USD)
"Skillable
4.7",4.7,Remote,Senior Data Engineer,"Job Type
Full-time
Description


Skillable is a 100% remote and virtual tech company that’s modernizing the world of training. Come share your professional magic with highly talented, drive and fun colleagues who believe in the power of “skilling.” Experience what a true team focused on doing the right thing feels like!

Our people and talent are what make us great and fun! We work together to create amazing solutions and experiences for our customers and their clients. We utilize our employees’ personal strengths to help our company grow and ensure our team is living their best, authentic life. We don’t just share our appreciation for our team members once a year with a branded mug—it’s shared on a daily basis. Our remote work environment blends the demands of work and life without the added pressure of commuting or feeling guilty about leaving early to visit the dentist.

Come work with us and learn what teamwork and integrity blended with an emphasis on well-being and balance can do for your career!

The Senior Data Engineer is a highly skilled data professional responsible for executing and guiding on the strategic development and maintenance of our data resources. Responsible for a broad range technical and detailed initiatives including being a thought partner to leadership in the architecture and creation of new data products to working with security and infrastructure resources to ensure the health of databases and data pipelines. This role will work closely with cross-functional teams, including data science, software engineering and product management to build, improve and maintain our data infrastructure, and contribute to the overall success of the company. Partner with leadership to give insights to the team of data engineers and database administrators, ensuring project organization around databases and focusing on continuous stability, security, and performance of databases to contribute to the overall success of the company.
Requirements
Provide technical data expertise and guidance to support core business data systems and data-driven processes.
Serve as a key contributor in the design, development, and maintenance of data pipelines for real-time and batch processing with the support and partnership of other technical team members.
Design, develop, execute on the implementation and management of databases.
Build and optimize data models for efficient querying and analysis.
Monitor and troubleshoot data pipeline and database issues.
Design and implement database structures and ensure their stability, reliability, and performance.
Troubleshoot database performance and optimize as necessary.
Assist in planning database backups and maintenance, disaster recovery, and replication.
Continuously improve data infrastructure and processes.
Provide technical knowledge and experience in serving as a lead in the process of creating, delivering, and scaling “Data as a Service” projects or products and data-related features.
Collaborate with data scientists, engineers, and analysts to understand data needs and requirements.
Parter with outside teams, stakeholders and executives to understand business needs and requirements.
Build and develop key relationships across the enterprise to help build internal enthusiasm and momentum for project initiatives.
Regularly assist leadership and key Company stakeholders in the development and implementation of data governance policies to ensure data quality.
Stay current with new database technologies and assist in upgrading systems as necessary.
Regularly review current and new data technologies and industry trends and implement as appropriate.
Assist in guiding, influencing and developing a company culture of data-driven decision making.
Assist leadership in mentoring a growing team of data professionals.
Represent Skillable at industry events and conferences as required.
Support and promote the company values through positive interactions with both internal and external partners and customers on a regular basis.
Other strategic business initiatives or cross-functional project involvement as required.
Qualifications
Bachelor’s degree in related field (product management, marketing, business, finance, product development, project management, etc.) or equivalent work experience.
10+ years of relevant professional experience working intimately in data engineering or database administration.
Experience as a functional leader on key projects or teams of technical talent or providing guidance on cross-functional work groups or project teams successfully preferred, but not required.
Experience mentoring and coaching others within an assigned function or cross-functionally.
Proven track record of strong problem-solving and business analysis skills using data creatively.
Naturally inquisitive with a desire to solve problems and dig into detailed analysis.
Experience working cross-functionally and promoting collaborative partnerships to drive results.
Proven ability to communicate effectively to various audiences/levels including leadership through various mediums.
Ability to take complex data and problems and deconstruct it into a concise, impactful message(s).
Ability to present and convey material both formally and informally to all levels of the organization.
Thorough understanding (or willingness to learn expeditiously) of business operations and processes.
Provide a high-level of confidence, integrity, enthusiasm and professional presence.
Experience with real-time data systems, data warehousing, ETL technologies, data modeling, and data governance.
Experience with cloud computing platforms such as AWS, GCP, or Azure.
Strong knowledge of database management systems, especially SQL Server.
Experience with cloud-based databases, especially Azure SQL.
Experience managing high-availability and -uptime databases.
Strong programming skills in SQL.
Demonstrated ability to prioritize and manage workload and meet project deadlines.
Interest and ability in mentoring other team members as applicable.
Strong MS Office, web conferencing and internal communication software experience.
Detail oriented and organized.
What’s in it for You? Rewards and Perks

We believe in providing a suite of benefits that ensure our employees know we appreciate them as people first. Skillable wants to be a company that promotes physical, emotional and all around well-being through our benefit offerings! Subject to eligibility requirements, the Company offers comprehensive benefits including:
Fully remote with a monthly stipend to pay for office services and supplies
Medical (2 plan options), dental (2 plan options), vision, health savings account with generous employer contributions, healthcare spending accounts, dependent care spending accounts, EAP, group paid life insurance, group paid STD and LTD and voluntary life/AD&D insurance, accident and critical illness options.
401(k) with Company match, tuition reimbursement, healthy lifestyle reimbursements.
Open PTO, Paid holidays, bereavement leave, parental leave, caregiver leave and paid FMLA leave.
Friends and Family Friday to end our standard workweek at 2pm local time; Full company closure during the 4th of July holiday week.
Access to pet insurance; Access for employees and dependents to Skillable learning opportunities through our product and more!
Working Conditions

The job conditions for this position are in a remote home office setting, requiring a space that supports privacy and focus to attend to regular and frequent video and voice calls. Employees in this position use PC and phone on an on-going basis throughout the day. Travel is anticipated up to 10% of the time for critical business meetings and industry events.",#N/A,51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2004,Unknown / Non-Applicable
"(ISC)2
3.3",3.3,Remote,Data Engineer,"Overview:
(ISC)² is an international nonprofit membership association focused on inspiring a safe and secure cyber world. (ISC)² offers a portfolio of credentials that are part of a holistic, pragmatic approach to security. Our association of candidates, associates, and members, nearly 330,000 strong, is made up of certified cyber, information, software, and infrastructure security professionals who are making a difference and helping to advance the industry. Our vision is supported by our commitment to educate and reach the general public through our charitable foundation – The Center for Cyber Safety and Education™. For more information on (ISC)², visit www.isc2.org, follow us on Twitter, or connect with us on Facebook and LinkedIn.

We are committed to an inclusive and equitable environment that values the unique perspectives and experiences of our entire workforce. We strive for a true sense of belonging for all our employees and to foster authenticity, trust, empowerment and connectedness that leads to everyone’s success. For more information, visit www.isc2.org/dei.
Position Summary:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities:
Implement Azure Data Services and tools to ingest , egress and transform data from multiple sources and create and maintain optimal data pipeline architecture.
Responsible for creating ETL pipeline with Azure Ecosysem like Azure Synapse ,Azure Data Factory.
Implement and support ETL related jobs to curate , transform and aggregate data to create models for end user consumption.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Azure from Sales Force ,Pearson Vue etc.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Maintain ownership of a given pipeline or domain and raise flags to leadership when appropriate regarding architectural concerns.
Demonstrates commitment to valuing diversity and contributing to an inclusive working and learning environment.
Miscellaneous duties as assigned.
Qualifications:
Bachelor’s degree in computer science or other equivalent degree
7+ years of experience in a Data Engineer role.
Experience with Cloud Data warehouses such as Azure , AWS and Google BigQuery
Experience with big data tools: Hadoop, Spark and Kafka.
Experience with Azure Dedicated and Azure Synapse
Experience with Analytics one or more of the following reporting tools; Tableau, PowerBI, Looker, Domo and Microstrategy
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++,etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Skills/Competencies:
Ability to demonstrate and support the 5 Company Core Values: Integrity, Excellence, Unity, Accountability, Agility
Ability to build an inclusive culture that encourages, supports and celebrates diversity; serve as a role model to promote DEI best practices.
Strong analytic skills related to working with unstructured datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Physical & Mental:
Up to 5% travel may be required.
Work normal business hours and extended hours when necessary.
Remain in a stationary position, often standing or sitting, for prolonged periods
Regular use of office equipment in a remote environment such as a computer/laptop and monitor computer screens
Dexterity of hands and fingers to operate a computer keyboard, mouse, and other computer components
#LI-remote
Equal Employment Opportunity Statement:
All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic as protected by applicable law.
Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.",#N/A,201 to 500 Employees,Nonprofit Organization,Management & Consulting,Membership Organizations,1989,$25 to $100 million (USD)
"Procore Technologies
4.4",4.4,Oregon,Staff Data Engineer,"Job Description

We’re looking for a Staff Data Engineer to join Procore’s Data Intelligence group. In this role, you’ll be a part of the data platform engineering team focused on building datamarts on the Procore Data Platform. The products and services that you build will support the data driven needs of many existing and forthcoming products of Procore.
As a Staff Data Engineer, you’ll partner with data engineers, platform engineers and product leaders to create and support Procore Risk Advisors Underwriting product portfolio. Use your analytical, data modeling, pipeline development skills to create and enhance the core data driven decision making at Procore. Procore Risk Advisors is a burgeoning and high value organization at Procore that is bringing technology driven experiences for quick and secure experiences in the Insurance industry. Backed by the wealth of data and industry leadership of Procore, PRA is breaking new ground in how the construction industry perceives Insurance.
This position reports into Senior Manager and will be based in remotely. We’re looking for someone to join us immediately.
What you’ll do:
Build the design and development of big data predictive analytics using object-oriented analysis, design and programming skills, and design patterns
Implement ETL workflows for data matching, data cleansing, data integration, and management
Maintain existing data pipelines and develop new data pipeline using big data technologies
Responsible for leading the effort of continuously improving reliability, scalability, and stability of the enterprise data platform
Contribute to and lead the continuous improvement of the software development framework and processes by collaborating with Quality Assurance engineers
Reproduce, troubleshoot and determine the root cause of production issues
Deliver observable, reliable, and secure software, embracing the “you build it, you run it” mentality, and focus on automation and GitOps
Participate in daily standups, team meetings, sprint planning, and demo/retrospectives while working cross-functionality with other teams to drive the innovation of our products
What we’re looking for:
BS degree in Computer Science, a similar technical field of study, or equivalent practical experience; MS or Ph.D. degree in Computer Science or a related field is preferred
8+ years of experience in a Data Engineering position
Strong expertise with 2+ years of experience building enterprise techniques for large scale distributed system design and data processing including:
Building streaming data pipelines using Kafka, Spark, or Flink
Building and maintain data warehouses in support of BI tools
Building data pipeline framework for data workflow to process large data sets and Real-Time & Batch Data Pipeline development
Experience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metric providers ranging from advertising, web analytics, and consumer devices
Desire to be actively hands-on with code using Java and Python (70-90%) along with willingness and passion for mentoring junior engineers and performing code reviews
Possess strong knowledge or familiarity with Apache Beam or AWS managed services for data (Glue, Athena, Data Pipeline, Flink, Spark) and Snowflake
Develop data catalogs and data cleanliness to ensure clarity and correctness of key business metrics

Additional Information

Base Pay Range $169,280-$232,760. Eligible for Bonus Incentive Compensation. Eligible for Equity Compensation. Procore is committed to offering competitive, fair, and commensurate compensation, and has provided an estimated pay range for this role. Actual compensation will be based on a candidate’s job-related skills, experience, education or training, and location.
Perks & Benefits
At Procore, we invest in our employees and provide a full range of benefits and perks to help you grow and thrive. From generous paid time off and healthcare coverage to career enrichment and development programs, learn more details about what we offer and how we empower you to be your best.
About Us
Procore Technologies is building the software that builds the world. We provide cloud-based construction management software that helps clients more efficiently build skyscrapers, hospitals, retail centers, airports, housing complexes, and more. At Procore, we have worked hard to create and maintain a culture where you can own your work and are encouraged and given resources to try new ideas. Check us out on Glassdoor to see what others are saying about working at Procore.
We are an equal-opportunity employer and welcome builders of all backgrounds. We thrive in a diverse, dynamic, and inclusive environment. We do not tolerate discrimination against employees on the basis of age, color, disability, gender, gender identity or expression, marital status, national origin, political affiliation, race, religion, sexual orientation, veteran status, or any other classification protected by law.
If you'd like to stay in touch and be the first to hear about new roles at Procore, join our Talent Community.","$201,020 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2002,Unknown / Non-Applicable
"University of California San Francisco
4.0",4.0,California,Software and Data Engineer,"The software and data engineer role includes the design, build, configuration, and support of research projects within UCSF’s APeX Enabled Research (AER) team. Most projects will be in partnership with other UCSF technical teams and involve highly customized research solutions. Communication skills and inventive technical solutioning are crucial.

The AER team provides a large array of services to the UCSF Research community, including project consultation, grant support, budget estimations, and project implementation and support. Project examples include:
Development of EHR-based interventions via clinical trials embedded within healthcare delivery systems to generate scientific evidence while delivering healthcare.
Enabling UCSF researchers with algorithms, digital tools, and/or clinical interventions with strong evidence of feasibility and acceptability.
Develop technical approaches and budgets in order to implement these tools within the electronic medical record.
Supporting the development of scalable, low-cost infrastructure to enable ongoing research.

Specifically, the software and data engineer will develop, implement, and maintain infrastructure and applications that support research informatics priorities and research projects priorities The largest piece of infrastructure for which they will be responsible will be the Health Informatics Platform for Advanced Computing (HIPAC), cloud infrastructure which supports deployment and maintenance of artificial intelligence in the health system. The engineer will work closely with data scientists and health informatics experts in the ongoing design and optimization of this infrastructure, in addition to other complex software applications.

Competitive applicants for this position are software engineers who have experience writing and maintaining production-ready and scalable applications. Candidates are ideally proficient in Python and SQL, have working experience with common DevOps and CI/CD tools such as docker, and have experience developing cloud-based applications.
To see the salary range for this position (we recommend that you make a note of the job code and use that to look up): TCS Non-Academic Titles Search (ucop.edu)
Please note: The compensation ranges listed online for roles not covered by a bargaining unit agreement are very wide, however a job offer will typically fall in the range of 80% - 120% of the established mid-point. An offer will take into consideration the experience of the final candidate AND the current salary level of individuals working at UCSF in a similar role.
For roles covered by a bargaining unit agreement, there will be specific rules about where a new hire would be placed on the range.
To learn more about the benefits of working at UCSF, including total compensation, please visit: https://ucnet.universityofcalifornia.edu/compensation-and-benefits/index.html
Department Description
The University of California, San Francisco (UCSF) Department of Information Technology Academic Research Systems (ARS) group is chartered to provide data services and infrastructure that support the UCSF Research Community’s computing and analytic requirements through centralized informatics services in the areas of Data, Tools, Secure Compute Environments, and Consulting Services.
Required Qualifications
Bachelor's degree in Computer Science, Computer Engineering, or related area and/or equivalent experience/training.
Demonstrated advanced knowledge of full software development lifecycle
Advanced experience with Python; ability to write clean, efficient, and production-level Python code
Advanced experience with SQL (e.g., SQLServer, PostgreSQL)
Experience working with DevOps and CI/CD pipeline toolsets such as Docker, Jenkins, GitHub, etc.
Demonstrated experience in developing complex, automated testing
Advanced experience with cloud-based architecture in platforms such as AWS, GCP, Azure, etc.
Demonstrated effective communication and interpersonal skills
Demonstrated ability to communicate technical information to technical and non-technical personnel at various levels in the organization
Self-motivated and works independently and as part of a team. Able to learn effectively and meet deadlines
Demonstrated broad problem-solving skills
Demonstrated ability to interface with management on a regular basis
Strong interest in working with healthcare data and understanding the challenges that face complex healthcare delivery systems
Ability to work in a highly matrixed organization, reporting to multiple teams
Preferred Qualifications
Master’s degree or Ph.D. in Computer Science, Computer Engineering, or related area and/or equivalent experience/training.
Cloud development certifications such as AWS Developer – Associate
Epic Clarity or Clinical Data Model
Demonstrated experience with data modeling, data warehousing, and building ETL pipelines
Familiar with data analysis and machine learning tools such as Jupyter, Pandas, scikit-learn, Numpy/Scipy, TensorFlow, etc.
Familiar with data visualization tools (e.g., Tableau).
Experience with the Epic Clarity Data structures and data
About UCSF
The University of California, San Francisco (UCSF) is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It is the only campus in the 10-campus UC system dedicated exclusively to the health sciences. We bring together the world’s leading experts in nearly every area of health. We are home to five Nobel laureates who have advanced the understanding of cancer, neurodegenerative diseases, aging and stem cells.
Pride Values
UCSF is a diverse community made of people with many skills and talents. We seek candidates whose work experience or community service has prepared them to contribute to our commitment to professionalism, respect, integrity, diversity and excellence – also known as our PRIDE values.

In addition to our PRIDE values, UCSF is committed to equity – both in how we deliver care as well as our workforce. We are committed to building a broadly diverse community, nurturing a culture that is welcoming and supportive, and engaging diverse ideas for the provision of culturally competent education, discovery, and patient care. Additional information about UCSF is available at diversity.ucsf.edu

Join us to find a rewarding career contributing to improving healthcare worldwide.
Equal Employment Opportunity
The University of California San Francisco is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Organization
Campus
Job Code and Payroll Title
000652 APPLICATIONS PROGR 4
Job Category
Clinical Systems / IT Professionals
Bargaining Unit
99 - Policy-Covered (No Bargaining Unit)
Employee Class
Career
Percentage
100%
Location
Mission Center Building (SF)
Shift
Days
Shift Length
8 Hours
Additional Shift Details
Mon - Fri 8:00 to 5:00",#N/A,10000+ Employees,College / University,Education,Colleges & Universities,1864,$25 to $100 million (USD)
"ArchWell Health
4.0",4.0,Remote,IT - Data Engineer,"ArchWell Health is a new, innovative healthcare provider devoted to improving the lives of our senior members. We deliver best-in-class care at comfortable, accessible neighborhood clinics where seniors can feel at home and become part of a vibrant, wellness-focused community. Our members experience greater continuity of care, as well as the comfort of knowing they will be treated with respect by people who genuinely care about them, their families, and their communities.

ArchWell Health requires all new hires to provide proof that they are fully vaccinated from COVID-19, or represent that they will be fully vaccinated within 30 days of their start date.

Duties/Responsibilities:
Build data integrations from internal and external sources to centralize data into a Data Warehouse environment.
Monitor data integration operations, data quality, troubleshoot, and resolve problems.
Profile data sources and map to target table formats.
Develop and monitor data quality processes and address problems.
Develop, unit test and system test integration components.
Create support documentation describing the functionality of the integrations.
Participating in technical design & requirements gathering meetings.
Participate in planning and implementing data integration and data migration activities.
Perform QA tests to ensure data integrity and quality.
Research data issues between source systems and the data warehouse.
Required Skills/Experience:
Bachelor’s degree required; Master's degree (in data science, computer science or MIS, mathematics, engineering, or related field) preferred.
5+ years of prior experience in Data Management / ETL / ELT / Data Warehousing
Experience in writing Data Quality routines for cleansing of data and capturing confidence score
Experience with master data management
Strong knowledge of Structured Query Language (SQL) and Transact-SQL (T-SQL)
Experience using scripting languages such as JavaScript or Python
Experience Healthcare data models, datasets, and source systems (e.g. EHR, claims, labs, etc.)
Experience with healthcare reference data (ICD, CPT etc.)
Experience with agile delivery methodologies
Data Modeling experience preferred.
Strong organizational, administrative, and analytical skills required.
Experience managing and working in cloud environments such as Amazon Web Services or Azure
Knowledge of HIPAA; ability to implement systems and processes in accordance with regulations
Excellent interpersonal communication skills, both written and verbal
ArchWell Health is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to their race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other protected classification.",#N/A,201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"McDonald's Corporation
3.5",3.5,"Chicago, IL","Data Engineer, Global Data, CRM","Company Description

McDonald’s evolving Accelerating the Arches growth strategy puts our customers and people first, and leverages our competitive advantages to strengthen our brand. We are recognized on lists like Fortune’s Most Admired Companies and Fast Company’s Most Innovative Companies.
Doubling Down on the 4Ds (Delivery, Digital, Drive Thru, and Development)
Our growth pillars emphasize the important role technology plays as the leading, global omni-channel restaurant brand. Technology enables the organization through digital technology, and improving the customer, crew and employee experience each and every day.
Global Technology forging the way
Leading the digitization of our business is the Technology organization made up of intrapreneurs who build industry defining tech using the latest innovations and platforms, like AI and edge computing to deliver on the next set of cutting-edge opportunities for the business. At McDonald’s you get to solve technology innovation challenges at an incredible scale, and work across global teams who are always hungry for a challenge. This provides access to exciting career paths for technologists. It’s bonus points when you get to see your family and friends use the tech you build at their favorite McD restaurant.

Job Description

We are seeking a highly motivated individual for the Data Engineering Manager for the Marketer Customer Data Platform team on Global Data and Architecture. This role will manage, develop, and lead the execution and implementation cloud-based data delivery systems for the MCD Customer Data Platform. This role will specialize in leading and managing the engineering teams in both onshore and offshore locations, as well as working with the relevant product management teams on overall product direction and implementation of the product vision.
What you will do:
Lead and manage the day-to-day operations for the CRM Data Engineering team with both, onshore and offshore, resources to with a focus on CDP/ CRM customer data technologies
Independently drive efforts in marketing technology and automation, collaborating with key stakeholders and business units. Must be able to find marketing solutions alongside building a long-term strategic vision of a marketing ecosystem across a highly technical, complex data stack
Participate in activities of the data engineering team on the solution, design, architect, implementation, support, and monitoring of CRM & CDP related data products
Develop and implement monitoring and remediation structure and applications for critical data flows
Continuously develop and deploy solutions to streamline and enhance the processes, logic, and tactics around customer cohort identification, and targeting via corporate CRM, marketing, and advertising systems. Additionally, develop the overall best practice framework for use by
Gather business and functional requirements and translate into robust, scalable, and user-friendly solutions

Qualifications
Bachelor’s degree in Information Technology or related technical fields such as computer science, marketing tech, mathematics, engineering, etc.
Minimum 2-5 years of relevant work experience in data analytics, engineering, computer science, mathematics or similar logic based fields
Minimum 2 years hands on experience with complex marketing automation tech stacks, leveraging CDP tools such as mParticle, Segment, and Tealium
Strong analytics skills, root cause analysis experiences on internal and external data and processes to answer business questions and improve application performance
Comfortable partnering and working with a wide range of stakeholders at various levels of management and leadership
Ability to focus on details as well as big picture
Competence and comfort focusing on specific detail engineering questions, as well as “big picture” questions
Ability to leverage critical thinking, problem-solving, and analytical skills to improve user experience, adoption, and outcomes
Curious, task-oriented self-starter with a strong bias for action; Proven track record of “getting things done”
Self-motivated with ability to set priorities and mentor others in a performance driven environment.
Passion for technology and innovation & uncompromising attention to detail & passion for problem solving
Desired Skills

Extensive experience working with Agile methodologies and expertise in production deployment processes and continuous deployment best practices.
1-2 years of hands-on experience with Braze and/or similar market automation delivery platforms
1-2 years of hands on experience with marketing automation and technology experience, understanding key components of marketing technology stacks, including CDP tools such as mParticle, Tealium, or Segment
Experience with clickstream tracking architecture implementations, and knowledge of Adobe Analytics and/or Google Analytics
Demonstrated ability to problem solve in complex technology solutioning
Experience developing Confluence pages and working with Jira
Demonstrable knowledge of multiple programming languages leveraged in martech/adtech stacks – javascript, ruby (liquid logic), html/css, python etc.

Additional Information

McDonald’s is committed to providing qualified individuals with disabilities reasonable accommodations to perform the essential functions of their jobs. Additionally, if you (or another applicant of whom you are aware) require assistance accessing or reading this job posting or otherwise seek assistance in the application process, please contact recruiting.supportteam@us.mcd.com
McDonald’s provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to sex, sex stereotyping, pregnancy (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), race, color, religion, ancestry or national origin, age, disability status, medical condition, marital status, sexual orientation, gender, gender identity, gender expression, transgender status, protected military or veteran status, citizenship status, genetic information, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Nothing in this job posting or description should be construed as an offer or guarantee of employment.","$104,499 /yr (est.)",10000+ Employees,Company - Public,Restaurants & Food Service,Restaurants & Cafes,1955,$10+ billion (USD)
"aventiv
3.5",3.5,"Carrollton, TX",Data Center Implementation Engineer,"Welcome to Aventiv! Please watch this brief video to find out if this is the place you want to be!
https://vimeo.com/391578629/5ba31cc5e9

Job Purpose:
Responsible for Engineering and installation of all hardware systems into the enterprise Data Center environments. Spares management, logistics management, vendor management, configuration management and hardware asset tracking will also be key deliverables of this assignment.
THIS IS A HYBRID POSITION WITH APPROXIMATELY 10% TRAVEL REQUIRED.

Essential Duties
Prepare all change controls related to hardware Engineering and installations
Work with various departments to compile requirements and update all Data Center Engineering.
Perform all Installations for IT hardware
Provide Spares management tracking of the Spares hubs kept to maintain component failures
Provide details to create hardware project budgets.
Perform or manage vendors as required to perform periodic preventative maintenance.
Provide all logistics support for all material deliveries to and between IT Data Center environments.
Perform other duties as assigned

Knowledge, Skills, and Abilities
Highly motivated
Good organizational skills
Great interpersonal and communication skills
Excellent analytical and reporting skills
Solid IT Data Center background
Proficient at using MS Visio, Excel, and Word
Knowledgeable and trouble ticketing (Heat) and Environmental monitoring Systems (Orion)

Minimum Qualifications
HS Diploma or GED
Minimum 5 years’ experience in Data Center Engineering and implementation
Experience with AC and DC electrical systems, Data Center Rack, flooring and cable management systems, Camera, environmental and infrastructure monitoring systems used within Data Centers, and Safety practices for Data Center implementations
10% travel required

Preferred Qualifications
Bachelor’s degree in Electrical Engineering, Mechanical, or Business Administration or equivalent field of study
Network Engineering experience a plus
System administration (Microsoft or Linux) experience a plus
Project Management experience or certification is plus

Physical Demands
Standing, sitting, walking, speaking, listening, bending, reaching, pushing, pulling, lifting, grasping and manipulating tools, typing, using peripheral computer tools.
May be required to lift up to 50 pounds.

Salary and Benefits:
At Aventiv, our salary and benefits are designed to fit you as a whole person. We offer a salary range based on experience and qualifications to ensure your unique contributions are met with our most competitive offer.
$83,500-$95,200 /year
Health Insurance
401(k)
Disability
Life Insurance
Paid Time Off
Voluntary Benefits

Aventiv Privacy Policy:
www.aventiv.com/privacy

Equal Employment Policy:
Aventiv is proud to be an equal opportunity employer. All decisions regarding recruiting, hiring, promotion, assignment, training, termination and other terms and conditions of employment will be made without regard to race, color, national origin, biological sex, sexual orientation, gender identity, gender expression, gender presentation, religion, age, pregnancy, disability, work-related injury, veteran status, genetic information, marital status, or any other factor that the law protects from employment discrimination. We do not discriminate based on genetic information in accordance with the Genetic Information Nondiscrimination Act.","$89,350 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,$500 million to $1 billion (USD)
"Certec Consulting
5.0",5.0,"Durham, NC",Oracle PL/SQL Data Engineer 1189,"Title: Oracle PL/SQL Data Engineer 1189
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B, Hire Friendly ) ONLY NO C2C
50-55/Hour W2



Location: hybrid 5 days onsite/month. Locations are Boston MA, Durham NC, Merrimack NH or Westlake TX
Duration: 9+ months, open ended

What does your team do?
This position is for equity team and will soon need another req for fixed income. Could place this candidate on either team. They take care of all things data within Asset Management, data transformation, data quality, building API.
Primarily responsible for the development of large scale data efforts tied to the cloud such as moving data to new cloud based solutions and building data lakes etc. To accomplish this the resource uses AWS, Python, Snowflake, and other data driven technologies
What are the top must have skills?
6-10 yrs
Oracle pl/sql sql is primary skill, AWS,
Go to our Website Job listing here: Job Listing 1189
Please download and complete this Matrix prior to submission.
PM 1189
Then
PLEASE USE EASY ""APPLY BUTTON"" (not just apply button) TO SUBMIT RESUME AND SKILLS MATRIX
Job Listing 1189
The send an email to the listers email address with just candidate name and job number. NO need to attached resume or anything else.
Thanks,
Jay Kernes
Certec Consulting, inc
Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$85,804 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"BOEING
3.9",3.9,"Everett, WA",Product Data Mgmt Engineer,"At Boeing, we innovate and collaborate to make the world a better place. From the seabed to outer space, you can contribute to work that matters with a company where diversity, equity and inclusion are shared values. We’re committed to fostering an environment for every teammate that’s welcoming, respectful and inclusive, with great opportunity for professional growth. Find your future with us.
This is a position within the Airplane Level Engineering Integration (ALEI) function, specifically the Visualization Center of Excellence team (ViCE). This role is critical in assisting with design reviews for all BCA Programs to support first pass quality and verify the product is integrated, interference free, is producible, maintainable, serviceable and safe.
This position must meet Export Control compliance requirements, therefore a “US Person” as defined by 22 C.F.R. § 120.15 is required. “US Person” includes US Citizen, lawful permanent resident, refugee, or asylee.
This position is an onsite position. The selected candidate will be required to perform work onsite in Everett, WA.
Position Responsibilities:
Collaborates with teams in the development, analysis, management and compliance verification of process and product baselines of complex products. Defines, plans, coordinates and conducts product and subsystem level technical design reviews and audits for new and derivative products.
Analyzes complex product trades and/or changes and develops technically complete change proposals.
Contributes to the development and implementation of Configuration and Data Management standards, processes and tools.
Defines and allocates Configuration and Data Management requirements for product hardware, software and engineering design data systems throughout the product lifecycle.
Coordinates the integration of product elements and analyzes & resolves issues with engineering product structure.
Develops, integrates and implements engineering technical program plans including impacts, risks and incorporation of lessons learned spanning multiple engineering functions.
Works under general direction.
Additional job description:
Support Enterprise Level PI process and tools development to support the Enterprise Model Based Engineering (MBE) and Digital Transformation Plans
Develop and Maintain the PI closure criteria for the Enterprise Standard Gated Process (ESGP Gate 2 through Gate 8)
Develop and Maintain the PI closure criteria for the Non-Recurring Product Development (NRPD) Process
Support PI Project Management plans across multiple Product Development programs
Create & Coordinate Tier (1) and (2) PI Milestones with all affected program stakeholders for an integrated program plan and schedule – summary presentations and artifacts to support all PI milestones and for Gate 2, 3, 4, 7, 8 closure
Create Program Level PI Plans and Lead Physical Integration across functions to support design First Pass Quality and to verify the product is Integrated, Interference Free, Producible, Maintainable, Serviceable, and Safe
Product Design
Establish & Maintain Accurate Digital Integration Environments to support configuration management
Manage Geometry Completeness / Maturity State Progression and Closure
Verify Geometry Model attributes for accurate integration (PIN, Instance Type, other)
Verify physical interface development and closure
Manage Special Purpose Models (space reservations, moving parts, threat models, stayout zones, other)
Manage Interferences / System Separations
Visualize Engineering Bill of Material (EBOM)
Production System
Verify engineering design supports the production system objectives for parts, plans, tools
Visualize and simulate of the Condition of Assembly (COA) / Condition of Support (COS)
Visualize Production System Special Purpose Models (manufacturing stay-out zones, tooling paths, other)
Visualize Tooling, Equipment, and associated key interfaces to the product
Support PE with visualization and development of Installation Plans
Support PE with visualization of the Manufacturing Bill of Material (MBOM)
Support, Services, and Safety
Support Airplane / Product Safety Engineering with the System Separation Requirements Team (SSRT)
Detailed 3D Design Integration including Kinematic Simulations
Maintainability (accessible, removable, replaceable per service requirements)
Safety - Human Model check before layout closure for EHS, HF / ERGO (reach, access, lifting, hazards, confined space, fall hazards, trip hazards, other)
Support (ground ops, maintenance, simulations, and training)
Create and manage the Program Physical Interface / Bracket Management Plan
Create and manage the Program Interference Management Plan
Provide visualization and integration support for the Systems Separation Requirements Team (SSRT) and associated team level Systems Separation Analysis
Tools / Systems Used:
Integration Visualization Tool (IVT) for massive model visualization to support geometry integration and analysis
IC.IDO / Virtual Reality (3D Immersive) Tool for integration, simulation, mixed reality, and analysis (interference, separation, production/build, maintenance, human factors /ergonomics, kinematics / physics)
Virtual Reality (VR) to support 3D immersive integration and simulation
CATIA V5 / ENOVIA Tools for authoritative source geometry design and configuration management
(3DX) Tool for Collaborative integration and analysis to support Enterprise Digital Transformation
Basic Qualifications (Required Skills/Experience):
Systems thinking
Excellent oral and written skills
Strong background in model-based definition
Adaptive thinking, strong team player
Preferred Qualifications (Desired Skills/Experience)
Background in IVT and CATIA
Typical Education/Experience:
Education/experience typically acquired through advanced technical education from an accredited course of study in engineering, computer science, mathematics, physics or chemistry (e.g. Bachelor) and typically 5 or more years' related work experience or an equivalent combination of technical education and experience (e.g. PhD, Master+3 years' related work experience). In the USA, ABET accreditation is the preferred, although not required, accreditation standard.
Relocation: This position offers relocation based on candidate’s eligibility.
Shift:
This position is 1st shift.
Union:
This is a union represented position.
Drug Free Workplace:
Boeing is a Drug Free Workplace where post offer applicants and employees are subject to testing for marijuana, cocaine, opioids, amphetamines, PCP, and alcohol when criteria is met as outlined in our policies.
At Boeing, we strive to deliver a Total Rewards package that will attract, engage and retain the top talent. Elements of the Total Rewards package include competitive base pay and variable compensation opportunities.
The Boeing Company also provides eligible employees with an opportunity to enroll in a variety of benefit programs, generally including health insurance, flexible spending accounts, health savings accounts, retirement savings plans, life and disability insurance programs, and a number of programs that provide for both paid and unpaid time away from work.
The specific programs and options available to any given employee may vary depending on eligibility factors such as geographic location, date of hire, and the applicability of collective bargaining agreements.
Please note that the information shown below is for general guidance only. Pay is based upon candidate experience and qualifications, as well as market and business considerations.
Summary Pay Range Level 3: $96,050-$129,950

Export Control Requirements: U.S. Government Export Control Status: This position must meet export control compliance requirements. To meet export control compliance requirements, a “U.S. Person” as defined by 22 C.F.R. §120.15 is required. “U.S. Person” includes U.S. Citizen, lawful permanent resident, refugee, or asylee.

Export Control Details: US based job, US Person required

Equal Opportunity Employer:
Boeing is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, physical or mental disability, genetic factors, military/veteran status or other characteristics protected by law.","$113,000 /yr (est.)",10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1916,$10+ billion (USD)
"TransPecos Banks
4.2",4.2,Remote,Data Integrations Engineer-Mulesoft,"Job Description
Job Title: Mulesoft/API Engineer
Summary: We are seeking a talented API Engineer with deep Mulesoft experience to join our team. In this role, you will be responsible for designing, developing, testing, and deploying integration solutions that connect various systems and applications across the organization. You will work with cross-functional teams to identify integration requirements, design and implement solutions, and ensure that our systems are performing at their best. The ideal candidate will have experience in Mulesoft and a deep understanding of data integration concepts and technologies, as well as strong problem-solving skills and the ability to work collaboratively with cross-functional teams.
Wage Type: Salaried
Organizational Structure:
Reports to: Head of Engineering/Chief Architect
Provides guidance to other technical team members
Essential Duties & Responsibilities:
To perform this job successfully, an individual must be able to perform each of the essential duties satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Design and develop integrations between various systems and applications, using integration technologies such as REST APIs, SOAP, JSON, XML, and ETL tools
Collaborate with cross-functional teams to understand integration requirements, design and implement solutions that meet business needs.
Ensure high-quality, reliable, and scalable integrations by performing unit testing, integration testing, and troubleshooting issues.
Create and maintain technical documentation related to integrations, including specifications, design documents, and test plans.
Work with third-party vendors and partners to integrate their systems with our own
Stay current with industry trends and best practices in integration technologies and tools
Design, develop and implement Mulesoft-based integration solutions for our enterprise systems.
Qualifications:
Required Knowledge/Skills:
5+ years of experience in designing and developing integrations between various systems and applications
Strong experience with integration technologies such as REST APIs, SOAP, JSON, XML, and ETL tools
Strong programming skills in Java, Python, or other programming languages
Strong problem-solving skills and ability to troubleshoot issues quickly and efficiently
Excellent communication and collaboration skills with the ability to work effectively in a team environment
Experience in designing and developing integration solutions using Mulesoft.
Strong understanding of SOA architecture and principles.
Experience with cloud technologies, such as AWS or Azure (Azure preferred).
Strong analytical and problem-solving skills.
Competent in API and fundamental network security
Mulesoft Developer Certifications are a plus.
Talent:
Strong project management and organizational skills.
Must be a self-starter and self-motivated individual that is goal oriented, organized, and analytical and can bring a positive impact to our organization.
Energetic, resourceful, and appropriate work intensity to get the work done
Strong people acumen and relationship skills; Naturally pre-disposed to quickly establish positive personal and professional relationships.
Desired Experiences:
Familiarity with Microsoft tools like Power BI and Dataverse
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience working in a start-up or rapidly growing organization.
Education:
Degree in Information Technology, Computer Science or Management Information Systems
MuleSoft Developer Certifications are a plus.
Non-degree certifications also considered.
Other:
Ability to interpret a variety of instructions furnished in written, oral, diagram or schedule form.
Must be able to lift to 20 pounds.
Job Type: Full-time
Benefits:

401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance

Schedule:

Monday to Friday
Weekend availability
Application Question(s):

How many years as a developer have you worked in a Financial Institution?
Will you now or in the future require H1B sponsorship?

Experience:

MuleSoft: 5 years (Preferred)
Azure: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
APIs: 5 years (Preferred)
SOA architecture: 1 year (Preferred)

License/Certification:

MuleSoft Developer Certification (Preferred)
Work Location: Remote",#N/A,Unknown,Company - Private,Financial Services,Banking & Lending,#N/A,Unknown / Non-Applicable
"R1 RCM, Inc.
3.4",3.4,Remote,Data Engineer III,"The Software Engineer III serves as a Data Engineer focused on building next generation of healthcare products and services. As part of a collaborative, motivated Agile-based team, the Software Engineer III is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
Responsibilities
Create and maintain optimal data pipeline architecture to assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders to optimize the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies.
Work with stakeholders including the Product, Data and Engineering teams to assist with data-related technical issues and support their data infrastructure needs.
Adhere to and ensure engineering best practices using modern SDLC that enables CI/CD and favors automation, auditability, automated testing, infrastructure, and policy as code.
Ability to collaborate effectively with other architects and engineers delivering end to end, high-performing, highly reliable, scalable, and operable solutions.
Required Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical field; or equivalent experience.
3+ years of experience delivering data engineering applications and services.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets .
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience in any one of the big data technologies (Hadoop, MapReduce , Spark, Data Lake etc. )
Experience in any one of the stream-processing systems ( Kafka, Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages : Python, Java, Scala, etc.
Understanding of modern software development including version control, unit testing, and continuous integration and deployment.
Strong communication skills, with proven ability to present complex ideas and to document concisely.
Preferred Qualifications
Working knowledge of Cloud providers and services such as Amazon AWS or Microsoft Azure.
Experience with relational SQL and NoSQL databases, including MongoDB and ElasticSearch.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12.
Knowledge of the healthcare revenue cycle, EMRs, practice management systems.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",#N/A,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2003,Unknown / Non-Applicable
"Chewy
3.5",3.5,"Richardson, TX",Data Engineer II,"Our Opportunity:
Chewy’s Data Analytics team has an exciting opportunity for a Data Engineer III to join the pack. Leveraging your strong expertise and background in data engineering and data analysis, you will be a part of a team responsible for operational and tactical reporting generating insights to grow Customer Service operations and planning. This includes building high quality data pipelines that drives analytic solutions and creating data products for analytics and data scientist team members to improve their productivity. Our organization is a fast-paced environment with new challenges and new opportunities each day. You will be responsible for building and implementing data products and technologies which will handle the growing business needs and play a key role in redefining what it means to be a world-class customer service organization.
What You'll Do:
Design, develop, optimize, and maintain data architecture and pipelines using design and programming patterns that follow best-in-class practices and principles.
Manage, maintain, and improve our SSOT tables and data marts, which drive critical business decisions every day.
Work closely with analytics teams and business partners, serving as a trusted partner who can advise, consult, and communicate data solutions.
Mentor and coach other data practitioners on data standards and practices.
Lead the evaluation, implementation and deployment of emerging tools and process for data engineering to improve overall productivity for the organization.
Partner with leaders, vendors, and other data practitioners across Chewy to develop technical architectures for strategic enterprise projects and initiatives.
Document technical details of work and follow agile sprint methodology, using tools like Jira, Confluence etc.

What You'll Need:
Bachelor of Science or Master’s degree in Computer Science, Engineering, Information Systems, Mathematics or related field
3+ years of enterprise experience as a data engineer and/or software engineer
3+ years applying and implementing database and data modeling techniques
3+ years working with enterprise data warehouse (ex. Snowflake, Vertica) and cloud environments (ex. AWS)
3+ years of experience building data integrations and pipelines from data lake, APIs, relational databases, and third-party systems
Strong software development skills in SQL
Self-motivated with strong problem-solving and self-learning skills.
Bonus:
Strong working knowledge of Python programming
Excellent communication and collaboration skills with ability to influence and guide stakeholders
Experience building dimensional models in data warehouses
Experience with data streaming tools and technologies like Kafka, Kinesis, or similar technologies
AWS Developer Certifications
E-commerce, Retail or startup experience
Experience in BI tools such as Tableau, Plotly, Power BI, etc.
Chewy is committed to equal opportunity. We value and embrace diversity and inclusion of all Team Members. If you have a disability under the Americans with Disabilities Act or similar law, and you need an accommodation during the application process or to perform these job requirements, or if you need a religious accommodation, please contact CAAR@chewy.com.

If you have a question regarding your application, please contact HR@chewy.com.

To access Chewy's Customer Privacy Policy, please click here. To access Chewy's California CPRA Job Applicant Privacy Policy, please click here.",#N/A,10000+ Employees,Company - Public,Retail & Wholesale,Pet & Pet Supplies Stores,2011,$5 to $25 million (USD)
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Wilmington, DE","Software Engineer III, Cloud Data Engineer","As a Software Engineer III, Cloud Data Engineer, within Corporate Enterprise Technology, in Finance, Risk, Data, & Controls, you serve as a seasoned member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. You are responsible for carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Executes software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems
Creates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems
Produces architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets both on-prem and on AWS cloud in service of continuous improvement of software applications and systems
Proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture
Contributes to software engineering communities of practice and events that explore new and emerging technologies
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Formal training or certification on software engineering concepts and 3+ years applied experience
3+ years of working on Big Data Platforms and building frameworks for data pipelines
2+ years of building cloud solutions - AWS preferred.
1+ year of working on cloud data lake solution. Experience working with Terraform, Glue DB, Collibra, Athena, Snowflake, Redshift, EMR would be big plus.
Proficient in coding in one or more languages - Java, Scala and Python are mostly used.
Experience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages
Solid understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security

Preferred qualifications, capabilities, and skills
Advanced knowledge of Apache Spark, Scala, Java, Python and Spring
Understanding of integration technologies such as Apache Kafka
Working knowledge of API-Apigee Edge, Swagger
Containers-Docker advanced development, Kubernetes
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans","$104,623 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
Splunk OLD (Read Only),#N/A,"San Jose, CA",Software Engineer - Data Platform - Remote,"A little about us. Splunk is here to build a safer and more resilient digital world. We’re proud to say that we’re the key to enterprise resilience for more than 11,000 enterprise organizations that use our Unified Security and Observability Platform to keep their systems secure and reliable. We’re also especially proud of our award-winning culture and our regular appearance on those “Best Places to Work” lists.
If you end up joining us, we’ll only ask you one thing: bring your whole, authentic self, what we call your million data points. So bring your work experience, problem-solving skills and talent, of course, but also bring your joy, your passion and all the things that make you, you. Because when you feel free to be you, it makes it a lot easier for us to be us.
Role:
Are you interested in being part of building the next-generation and highly-scaled industry-leading data platform for machine data? The Splunk Data Platform Search Execution team is looking for a Software Engineer to join our backend distributed systems engineering team.
You will be working on the core of Splunk's Search technology and be part of the team to solve the most challenging and exciting problems in the Search backend area to delight our customers with high-performance, reliable, and efficient solutions.
We give our engineers an environment in which they can contribute from day one while also providing learning and growth opportunities. You'll learn how our entire stack works, from data ingestion and storage to searching, reporting, and building dashboards, all in distributed environments. The work you’ll do will directly impact the experience of our customers.
Responsibilities:
Design, develop, and maintain features for Splunk search infrastructure.
Build robust, fault-tolerant distributed systems in a multi-threaded/multi-process environment.
Analyze and improve the scalability of data collection, routing, storage, and retrieval.
Define and perform various search language layer optimizations/transformations.
Requirements:
5 years of related experience with a technical Bachelor’s degree; or equivalent practical experience; or 3 years and a technical Master’s degree; or equivalent practical experience.
Master knowledge of developing and debugging any object-oriented language like C++.
You have knowledge of backend systems, storage, filesystem, memory, and multithreading.
You have familiarity with any query language and processing like SQL, SPL, etc.
You have a proven foundation in operating systems, data structures, algorithms, and software design.
You have knowledge of modern distributed system design and implementation in the Unix/Linux environment.
Passion for solving hard problems and exploring new technologies.
You have knowledge and experience with AWS services, like EC2, S3, etc.
What We Offer You
A constant stream of new things for you to learn. We're always expanding into new areas, bringing in open source projects and contributing back, and exploring new technologies.
A set of exceptionally talented and dedicated peers, all the way from engineering and QA to product management and customer support.
A stable, collaborative and encouraging work environment.
We don't expect people to work 12-hour days. We want you to have a successful time outside of work too. Want to work from home sometimes? No problem. We trust our Colleagues to be responsible with their time and dedication, and believe that balance helps cultivate an outstanding environment.
We value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which the candidate is applying.
For job positions in San Francisco, CA, and other locations where required, we will consider employment qualified applicants with arrest and conviction records.

Note: Splunk provides flexibility and choice in the working arrangement for most roles, including remote and/or in-office roles. We have a market-based pay structure which varies by location. Please note that the base pay range is a guideline and for candidates who receive an offer, the base pay will vary based on factors such as work location as set out below, as well as the knowledge, skills and experience of the candidate. In addition to base pay, this role is eligible for incentive compensation and may be eligible for equity or long-term cash awards.
Benefits are an important part of Splunk's Total Rewards package. This role is eligible for a competitive benefits package which includes medical, dental, vision, a 401(k) plan and match, paid time off, an ESPP and much more! Learn more about our comprehensive benefits and wellbeing offering at https://splunkbenefits.com.
Base Pay Range
SF Bay Area, Seattle Metro, and New York City Metro Area
Base Pay Range: $136,000.00 - 187,000.00 per year
California (excludes SF Bay Area), Washington (excludes Seattle Metro), Washington DC Metro, and Massachusetts
Base Pay Range: $124,000.00 - 170,500.00 per year
All other cities and states excluding California, Washington, Massachusetts, New York City Metro Area and Washington DC Metro Area.
Base Pay Range: $116,000.00 - 159,500.00 per year","$137,750 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Delta
4.3",4.3,"Atlanta, GA",Senior Data Engineer Modeler,"United States, Georgia, Atlanta
Information Technology
11-May-2023
Ref #: 20782
LinkedIn Tag: #LI-JM2
How you'll help us Keep Climbing (overview & key responsibilities)
Location: Atlanta GA - NOT Remote

Delta IT is on a journey of transformation. We are changing the way we do business from top to bottom. As leaders with vision within Delta, we strive to build important and innovative solutions and are looking for team members to help us realize our vision.

Delta employees are problem solvers, doers, innovators.

We are proactive.

We are collaborative.

We deliver impact to our customers.

Join us on our transformation journey in becoming a best-in-class IT organization at the world's best airline!

The Senior Data Engineer, Modeler to join our Enterprise Data team. This position is responsible for building, modifying, modernizing the conceptual, logical, and physical data models used in Enterprise Data databases. The position also requires establishing and maintaining effective partnerships with the Business and IT stakeholders.

We are looking for someone with strong analytical and organizational skills to transform data into insights, distill requirements, and develop processes. The candidate should have excellent communication skills, business maturity, and feel comfortable in a fast-paced environment.

Responsibilities
Understand the data needs of the company for ingestion, migration, storage, and access
Work with business teams to gather requirements for the database design and model
Collaborate with the Enterprise Data team and business teams to define requirements, then design and build database models
Design and build conceptual, logical, and physical data models in accordance with companys data standards
Apply relational and dimensional models for raw ingestion and curated/semantic layers
Create Physical Data Structures (DDLs) and corresponding metadata
Creating and maintaining data reference architecture architectures and integration patterns
Updating knowledge by tracking and understanding emerging large data and modeling practices and standards
Benefits and Perks to Help You Keep Climbing
Our culture is rooted in a shared dedication to living our values Care, Integrity, Resilience, Servant Leadership, and Teamwork every day, in everything we do. At Delta, our people are our success. At the heart of what we offer is our focus on Sharing Success with Delta employees. Exploring a career at Delta gives you a chance to see the world while earning great compensation and benefits to help you keep climbing along the way:
Competitive salary, industry-leading prot sharing program, and performance incentives.
401(k) with generous company contributions up to 9%
Paid time off including vacation, holidays, paid personal time, maternity and parental leave.
Comprehensive health Benefits including medical, dental, vision, short/long term disability and life Benefits.
Family care assistance through fertility support, surrogacy and adoption assistance, lactation support, subsidized back-up care, and programs that help with loved ones in all stages.
Holistic Wellbeing programs to support physical, emotional, social, and financial health, including access to an employee assistance program offering support for you and anyone in your household, free financial coaching, and extensive resources supporting mental health.
Domestic and International space-available flight privileges for employees and eligible family members
Career development programs to achieve your long-term career goals.
World-wide partnerships to engage in community service and innovative goals created to focus on sustainability and reducing our carbon footprint
Business Resource Groups created to connect employees with common interests to promote inclusion, provide perspective and help implement strategies
Recognition rewards and awards through the platform Unstoppable Together
Access to over 500 discounts, specialty savings and voluntary benefits through Deltaperks such as car and hotel rentals and auto, home, and pet insurance, legal services, and childcare
What you need to succeed (minimum qualifications)
7 or more years of experience in Information Technology or related technical capacity
Expert in concepts and principles of data modeling
Knowledge of entity relationship, dimensional modeling, big data, enterprise data, and physical data models
Knowledge of relational databases and data architecture computer systems, including SQL Familiarity
Ability to design, build, and develop a new product, technology, or service from feasibility through to production
Familiarity with data modeling software such as SAP PowerDesigner, Microsoft Visio, E/R Studio or Erwin Data Modeler
Must have hands-on experience with cloud platforms; AWS preferred
Knowledge of big data platforms such as Teradata, Oracle DB, DB2, AWS Aurora, AWS Athena, etc.
Experience using Python and/or PowerShell scripting for data processing
Strong attention to detail
Excellent communicationwith both technical and business stakeholders
Ability to work in a fast-paced environment
Ability to work both independently and as part of a team
Understanding of the business and the ability to assess and address risk without negatively impacting the business
Consistently prioritizes safety and security of self, others, and personal data.
Embraces diverse people, thinking, and styles.
Possesses a high school diploma, GED, or high school equivalency.
Is at least 18 years of age and has authorization to work in the United States.
What will give you a competitive edge (preferred qualifications)
Bachelors or masters degree in Information Technology, Computer Science, Mathematics, Engineering, Information Systems, or equivalent
In-depth understanding of ETL and data ingestion processes used in large scale data warehouses
Knowledge of data architecture principles for on-prem and cloud solutions
Flexibility to adapt and plan for changing business objectives
Solid Understanding of Agile/Scrum development methodologies
Experience translating business outcome requirements into data model requirements
Ability to adapt communication style for technical and business audiences
< Go back","$124,024 /yr (est.)",10000+ Employees,Company - Public,Transportation & Logistics,"Airlines, Airports & Air Transportation",1928,$10+ billion (USD)
"University of California San Francisco
4.0",4.0,California,Software and Data Engineer,"The software and data engineer role includes the design, build, configuration, and support of research projects within UCSF’s APeX Enabled Research (AER) team. Most projects will be in partnership with other UCSF technical teams and involve highly customized research solutions. Communication skills and inventive technical solutioning are crucial.

The AER team provides a large array of services to the UCSF Research community, including project consultation, grant support, budget estimations, and project implementation and support. Project examples include:
Development of EHR-based interventions via clinical trials embedded within healthcare delivery systems to generate scientific evidence while delivering healthcare.
Enabling UCSF researchers with algorithms, digital tools, and/or clinical interventions with strong evidence of feasibility and acceptability.
Develop technical approaches and budgets in order to implement these tools within the electronic medical record.
Supporting the development of scalable, low-cost infrastructure to enable ongoing research.

Specifically, the software and data engineer will develop, implement, and maintain infrastructure and applications that support research informatics priorities and research projects priorities The largest piece of infrastructure for which they will be responsible will be the Health Informatics Platform for Advanced Computing (HIPAC), cloud infrastructure which supports deployment and maintenance of artificial intelligence in the health system. The engineer will work closely with data scientists and health informatics experts in the ongoing design and optimization of this infrastructure, in addition to other complex software applications.

Competitive applicants for this position are software engineers who have experience writing and maintaining production-ready and scalable applications. Candidates are ideally proficient in Python and SQL, have working experience with common DevOps and CI/CD tools such as docker, and have experience developing cloud-based applications.
To see the salary range for this position (we recommend that you make a note of the job code and use that to look up): TCS Non-Academic Titles Search (ucop.edu)
Please note: The compensation ranges listed online for roles not covered by a bargaining unit agreement are very wide, however a job offer will typically fall in the range of 80% - 120% of the established mid-point. An offer will take into consideration the experience of the final candidate AND the current salary level of individuals working at UCSF in a similar role.
For roles covered by a bargaining unit agreement, there will be specific rules about where a new hire would be placed on the range.
To learn more about the benefits of working at UCSF, including total compensation, please visit: https://ucnet.universityofcalifornia.edu/compensation-and-benefits/index.html
Department Description
The University of California, San Francisco (UCSF) Department of Information Technology Academic Research Systems (ARS) group is chartered to provide data services and infrastructure that support the UCSF Research Community’s computing and analytic requirements through centralized informatics services in the areas of Data, Tools, Secure Compute Environments, and Consulting Services.
Required Qualifications
Bachelor's degree in Computer Science, Computer Engineering, or related area and/or equivalent experience/training.
Demonstrated advanced knowledge of full software development lifecycle
Advanced experience with Python; ability to write clean, efficient, and production-level Python code
Advanced experience with SQL (e.g., SQLServer, PostgreSQL)
Experience working with DevOps and CI/CD pipeline toolsets such as Docker, Jenkins, GitHub, etc.
Demonstrated experience in developing complex, automated testing
Advanced experience with cloud-based architecture in platforms such as AWS, GCP, Azure, etc.
Demonstrated effective communication and interpersonal skills
Demonstrated ability to communicate technical information to technical and non-technical personnel at various levels in the organization
Self-motivated and works independently and as part of a team. Able to learn effectively and meet deadlines
Demonstrated broad problem-solving skills
Demonstrated ability to interface with management on a regular basis
Strong interest in working with healthcare data and understanding the challenges that face complex healthcare delivery systems
Ability to work in a highly matrixed organization, reporting to multiple teams
Preferred Qualifications
Master’s degree or Ph.D. in Computer Science, Computer Engineering, or related area and/or equivalent experience/training.
Cloud development certifications such as AWS Developer – Associate
Epic Clarity or Clinical Data Model
Demonstrated experience with data modeling, data warehousing, and building ETL pipelines
Familiar with data analysis and machine learning tools such as Jupyter, Pandas, scikit-learn, Numpy/Scipy, TensorFlow, etc.
Familiar with data visualization tools (e.g., Tableau).
Experience with the Epic Clarity Data structures and data
About UCSF
The University of California, San Francisco (UCSF) is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It is the only campus in the 10-campus UC system dedicated exclusively to the health sciences. We bring together the world’s leading experts in nearly every area of health. We are home to five Nobel laureates who have advanced the understanding of cancer, neurodegenerative diseases, aging and stem cells.
Pride Values
UCSF is a diverse community made of people with many skills and talents. We seek candidates whose work experience or community service has prepared them to contribute to our commitment to professionalism, respect, integrity, diversity and excellence – also known as our PRIDE values.

In addition to our PRIDE values, UCSF is committed to equity – both in how we deliver care as well as our workforce. We are committed to building a broadly diverse community, nurturing a culture that is welcoming and supportive, and engaging diverse ideas for the provision of culturally competent education, discovery, and patient care. Additional information about UCSF is available at diversity.ucsf.edu

Join us to find a rewarding career contributing to improving healthcare worldwide.
Equal Employment Opportunity
The University of California San Francisco is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Organization
Campus
Job Code and Payroll Title
000652 APPLICATIONS PROGR 4
Job Category
Clinical Systems / IT Professionals
Bargaining Unit
99 - Policy-Covered (No Bargaining Unit)
Employee Class
Career
Percentage
100%
Location
Mission Center Building (SF)
Shift
Days
Shift Length
8 Hours
Additional Shift Details
Mon - Fri 8:00 to 5:00",#N/A,10000+ Employees,College / University,Education,Colleges & Universities,1864,$25 to $100 million (USD)
"Octo
4.2",4.2,"Chantilly, VA",Data Engineer,"You…
As a Data Engineer, you will be joining the team that is deploying and delivering a cloud-based, multi-domain Common Data Fabric (CDF), which provides data sharing services to the entire DoD Intelligence Community (IC). The CDF connects all IC data providers and consumers. It uses fully automated policy-based access controls to create a machine-to-machine data brokerage service, which is enabling the transition away from legacy point-to-point solutions across the IC enterprise.
Us…
We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client’s missions.
Program Mission…
The CDF program is an evolution for the way DoD programs, services, and combat support agencies access data by providing data consumers (e.g., systems, app developers, etc.) with a “one-stop shop” for obtaining ISR data. The CDF significantly increases the DI2E’s ability to meet the ISR needs of joint and combined task force commanders by providing enterprise data at scale. The CDF serves as the scalable, modular, open architecture that enables interoperability for the collection, processing, exploitation, dissemination, and archiving of all forms and formats of intelligence data. Through the CDF, programs can easily share data and access new sources using their existing architecture. The CDF is a network and end-user agnostic capability that enables enterprise intelligence data sharing from sensor tasking to product dissemination.
Responsibilities...
Primary responsibility is to work with data providers within the IC and DoD Enterprise to identify and ingest data sets into the CDF data broker. In this role you will:
Develop, optimize, and maintain data ingest flows using Apache Nifi and Python.
Develop within the components in the cloud platform, such as Apache Kafka, NiFi, and HBase.
Communicate with data owners to set up and ensure CDF streaming and batching components are working (including configuration parameters).
Document SOP related to streaming configuration, batch configuration or API management depending on role requirement.
Document details of each data ingest activity to ensure they can be understood by the rest of the team
What we’d like to see…
A minimum of 3 years of experience with programming and software development including analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems
DoD 8570 IAT Level II Certification (e.g. Security+) or the ability to obtain the certification within 90 days
Demonstrable CentOS command line knowledge
Working knowledge of web services environments, languages, and formats such as RESTful APIs, SOAP, FTP/SFTP, HTML, JavaScript, XML, and JSON
Understanding of foundational ETL concepts
Experience implementing data ignorations with in the IC DoD Enterprise.
Desired Skills:
Experience or expertise using, managing, and/or testing API Gateway tools and Rest APIs (desired)
2+ Experience in Python Development
Experience or expertise configuring an LDAP client to connect to IPA (desired)
Advanced organizational skills with the ability to handle multiple assignments
Strong written and oral communication skills
Years of Experience: Junior Level (0-4 years),Mid Level (5-8 years), Senior Level (9+)
Education: Bachelor's degree in systems engineering, computer engineering, or a related technical field (preferred)
Location: Chantilly, VA
Clearance: Active TS/SCI w/ ability to obtain CI Poly (preferred)","$99,041 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,$100 to $500 million (USD)
"BOEING
3.9",3.9,"Everett, WA",Product Data Mgmt Engineer,"At Boeing, we innovate and collaborate to make the world a better place. From the seabed to outer space, you can contribute to work that matters with a company where diversity, equity and inclusion are shared values. We’re committed to fostering an environment for every teammate that’s welcoming, respectful and inclusive, with great opportunity for professional growth. Find your future with us.
This is a position within the Airplane Level Engineering Integration (ALEI) function, specifically the Visualization Center of Excellence team (ViCE). This role is critical in assisting with design reviews for all BCA Programs to support first pass quality and verify the product is integrated, interference free, is producible, maintainable, serviceable and safe.
This position must meet Export Control compliance requirements, therefore a “US Person” as defined by 22 C.F.R. § 120.15 is required. “US Person” includes US Citizen, lawful permanent resident, refugee, or asylee.
This position is an onsite position. The selected candidate will be required to perform work onsite in Everett, WA.
Position Responsibilities:
Collaborates with teams in the development, analysis, management and compliance verification of process and product baselines of complex products. Defines, plans, coordinates and conducts product and subsystem level technical design reviews and audits for new and derivative products.
Analyzes complex product trades and/or changes and develops technically complete change proposals.
Contributes to the development and implementation of Configuration and Data Management standards, processes and tools.
Defines and allocates Configuration and Data Management requirements for product hardware, software and engineering design data systems throughout the product lifecycle.
Coordinates the integration of product elements and analyzes & resolves issues with engineering product structure.
Develops, integrates and implements engineering technical program plans including impacts, risks and incorporation of lessons learned spanning multiple engineering functions.
Works under general direction.
Additional job description:
Support Enterprise Level PI process and tools development to support the Enterprise Model Based Engineering (MBE) and Digital Transformation Plans
Develop and Maintain the PI closure criteria for the Enterprise Standard Gated Process (ESGP Gate 2 through Gate 8)
Develop and Maintain the PI closure criteria for the Non-Recurring Product Development (NRPD) Process
Support PI Project Management plans across multiple Product Development programs
Create & Coordinate Tier (1) and (2) PI Milestones with all affected program stakeholders for an integrated program plan and schedule – summary presentations and artifacts to support all PI milestones and for Gate 2, 3, 4, 7, 8 closure
Create Program Level PI Plans and Lead Physical Integration across functions to support design First Pass Quality and to verify the product is Integrated, Interference Free, Producible, Maintainable, Serviceable, and Safe
Product Design
Establish & Maintain Accurate Digital Integration Environments to support configuration management
Manage Geometry Completeness / Maturity State Progression and Closure
Verify Geometry Model attributes for accurate integration (PIN, Instance Type, other)
Verify physical interface development and closure
Manage Special Purpose Models (space reservations, moving parts, threat models, stayout zones, other)
Manage Interferences / System Separations
Visualize Engineering Bill of Material (EBOM)
Production System
Verify engineering design supports the production system objectives for parts, plans, tools
Visualize and simulate of the Condition of Assembly (COA) / Condition of Support (COS)
Visualize Production System Special Purpose Models (manufacturing stay-out zones, tooling paths, other)
Visualize Tooling, Equipment, and associated key interfaces to the product
Support PE with visualization and development of Installation Plans
Support PE with visualization of the Manufacturing Bill of Material (MBOM)
Support, Services, and Safety
Support Airplane / Product Safety Engineering with the System Separation Requirements Team (SSRT)
Detailed 3D Design Integration including Kinematic Simulations
Maintainability (accessible, removable, replaceable per service requirements)
Safety - Human Model check before layout closure for EHS, HF / ERGO (reach, access, lifting, hazards, confined space, fall hazards, trip hazards, other)
Support (ground ops, maintenance, simulations, and training)
Create and manage the Program Physical Interface / Bracket Management Plan
Create and manage the Program Interference Management Plan
Provide visualization and integration support for the Systems Separation Requirements Team (SSRT) and associated team level Systems Separation Analysis
Tools / Systems Used:
Integration Visualization Tool (IVT) for massive model visualization to support geometry integration and analysis
IC.IDO / Virtual Reality (3D Immersive) Tool for integration, simulation, mixed reality, and analysis (interference, separation, production/build, maintenance, human factors /ergonomics, kinematics / physics)
Virtual Reality (VR) to support 3D immersive integration and simulation
CATIA V5 / ENOVIA Tools for authoritative source geometry design and configuration management
(3DX) Tool for Collaborative integration and analysis to support Enterprise Digital Transformation
Basic Qualifications (Required Skills/Experience):
Systems thinking
Excellent oral and written skills
Strong background in model-based definition
Adaptive thinking, strong team player
Preferred Qualifications (Desired Skills/Experience)
Background in IVT and CATIA
Typical Education/Experience:
Education/experience typically acquired through advanced technical education from an accredited course of study in engineering, computer science, mathematics, physics or chemistry (e.g. Bachelor) and typically 5 or more years' related work experience or an equivalent combination of technical education and experience (e.g. PhD, Master+3 years' related work experience). In the USA, ABET accreditation is the preferred, although not required, accreditation standard.
Relocation: This position offers relocation based on candidate’s eligibility.
Shift:
This position is 1st shift.
Union:
This is a union represented position.
Drug Free Workplace:
Boeing is a Drug Free Workplace where post offer applicants and employees are subject to testing for marijuana, cocaine, opioids, amphetamines, PCP, and alcohol when criteria is met as outlined in our policies.
At Boeing, we strive to deliver a Total Rewards package that will attract, engage and retain the top talent. Elements of the Total Rewards package include competitive base pay and variable compensation opportunities.
The Boeing Company also provides eligible employees with an opportunity to enroll in a variety of benefit programs, generally including health insurance, flexible spending accounts, health savings accounts, retirement savings plans, life and disability insurance programs, and a number of programs that provide for both paid and unpaid time away from work.
The specific programs and options available to any given employee may vary depending on eligibility factors such as geographic location, date of hire, and the applicability of collective bargaining agreements.
Please note that the information shown below is for general guidance only. Pay is based upon candidate experience and qualifications, as well as market and business considerations.
Summary Pay Range Level 3: $96,050-$129,950

Export Control Requirements: U.S. Government Export Control Status: This position must meet export control compliance requirements. To meet export control compliance requirements, a “U.S. Person” as defined by 22 C.F.R. §120.15 is required. “U.S. Person” includes U.S. Citizen, lawful permanent resident, refugee, or asylee.

Export Control Details: US based job, US Person required

Equal Opportunity Employer:
Boeing is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, physical or mental disability, genetic factors, military/veteran status or other characteristics protected by law.","$113,000 /yr (est.)",10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1916,$10+ billion (USD)
"Republic National Distributing Company
3.8",3.8,"Atlanta, GA",Data Engineer - Senior,"Overview: The Senior Data Engineer is responsible for managing and organizing RNDC's enterprise data. They will translate requirements and designs into functional data pipelines while ensuring the continued quality and completeness of information. Senior Data Engineers will combine raw information from different sources to create consistent and machine-readable datasets that are easy to analyze and support company initiatives. They will support other Data Engineers and Data Analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They will also implement methods to improve data reliability and quality, improve data visibility and reduce effort through automation. Responsibilities:
Contribute on a team of data engineers through design, demand delivery, code reviews, release management, implementation, presentations, and meetings.
Mentor fellow data engineers and contribute to ongoing process improvements for the team
Evaluate business needs and objectives and align architecture/designs with business requirements
Build the data pipelines required for the optimal extraction, transformation, integration and loading of raw data from a wide variety of data sources
Assemble large, complex data sets and model our data in a way that meets functional / non-functional business requirements
Create data tools for analytics team members that assist them in generating innovative industry insights that provide our business a competitive advantage
Implement data tagging mechanisms and metadata management so data is accurately classified and visible to the organization
Build processes to help identify and improve data quality, consistency and effectiveness
Ensure our data is managed in a way that it conforms to all information privacy and protection policies
Use agile software development processes to iteratively make improvements to our data management systems
Identify opportunities for automation
Be an advocate for best practices and continued learning Qualifications: Bachelor's/Tech School degree in Computer Science, Information Systems, Engineering or equivalent and/or commensurate years of real-world experience in software engineering. 4+ years of relevant experience in data management3+ years in data engineering with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELTExperience with performance analysis and optimizationExperience in data acquisition, transformation and storage design using design principles, patterns and best practicesData engineering certification is a plus Informatica, Kafka, CDC, SQL, Irwin, Python, AWS (S3, Athena, Glue, Kinesis, Redshift), Spark, Scala, AI/MLWe are an Equal Opportunity employer.","$97,233 /yr (est.)",5001 to 10000 Employees,Company - Private,Retail & Wholesale,Wholesale,1898,$5 to $10 billion (USD)
"Veolia
3.8",3.8,"Paramus, NJ",Senior Data Engineer,"Company Description Veolia Group aims to be the benchmark company for ecological transformation. With nearly 220,000 employees worldwide, the Group designs and provides game-changing solutions that are both useful and practical for water, waste and energy management. Through its three complementary business activities, Veolia helps to develop access to resources, preserve available resources and replenish them. In 2021, the Veolia group provided 79 million inhabitants with drinking water and 61 million with sanitation, produced nearly 48 million megawatt hours and recovered 48 million tonnes of waste. Veolia Environnement (Paris Euronext: VIE) achieved consolidated revenue of 28,508 billion euros in 2021. www.veolia.com
Job Description
Develop and operate data management tools, monitoring data flows, data quality, data cleansing and data processing
Create and document logical data integration strategies for data flows between disparate systems and the enterprise data warehouse/data lakes
Collaborate with different stakeholders (engineers, data stewards) to collect required data from internal and external systems
Work in an Agile environment that focuses on collaboration and teamwor
Improve and extend existing data infrastructure services
Monitor production job schedule and correct job failures in a timely manner

Qualifications
MS degree in Computer Science or computer related field from an accredited institution.
5+ years hands proven experience as a Data Engineer or similar role.
5+ years of strong experience building, running and maintaining datalake(s) and warehouse(s) in a cloud environment.
More than 4 years of experience developing with Python.
4+ years performing with production environments in a DevOps culture managing code composed of multi-developer teams, following industry best practices.
4+ years SQL development experience.
Experience with data modeling
4+ years bash scripting experience.
Strong experience with Git, CI/CD (preferably GitLab) and Docker.
Experience deploying and running services in Cloud Big Data platforms such as BigQuery and Snowflake.
Strong experience with GCP services.
Experience designing and building data pipelines using tools like Apache Beam, CDAP (Data Fusion) or other ETLs.
Knowledge with CDC design patterns and their challenges.
Experience with DAG workflows orchestration such as Apache Airflow.
Experience with NoSQL databases is a plus (i.e Firestore, MongoDB).
Experience designing and developing APIs is a plus (i.e using FastAPI, Flask).

(Nice to have) Google Cloud Data Engineer
Abilities:
Being able to work in a large company with different stakeholders.
Embrace mentorship through design sessions, code reviews, and community building.
Take ownership and support solutions you develop.
Value collaboration with other members of the team.
Have a product mindset.
Good communication.

Additional Information

A subsidiary of Veolia group, Veolia North America (VNA) offers a full spectrum of water, waste and energy management services, including water and wastewater treatment, commercial and hazardous waste collection and disposal, energy consulting and resource recovery. VNA helps commercial, industrial, healthcare, higher education and municipality customers throughout North America. Headquartered in Boston, Mass., Veolia North America has more than 10,000 employees working at more than 350 locations across the continent. www.veolianorthamerica.com As an inclusive company, Veolia is committed to diversity and gives equal consideration to all applications, without discrimination. We are an Equal Opportunity Employer! All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. As an inclusive company, Veolia is committed to diversity and gives equal consideration to all applications, without discrimination. We are an Equal Opportunity Employer! All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.
Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. The Company reserves the right to modify this information at any time, subject to applicable law.","$119,169 /yr (est.)",10000+ Employees,Company - Public,"Energy, Mining & Utilities",Energy & Utilities,1853,$1 to $5 billion (USD)
"Equifax
3.6",3.6,"Alpharetta, GA",Data Reliability Engineer - P3,"Equifax is where you can power your possible. If you want to achieve your true potential, chart new paths, develop new skills, collaborate with bright minds, and make a meaningful impact, we want to hear from you.
The Data Reliability Engineer position is a high-performing role in support of data reliability and quality functions for various domains within USIS’s broader data library.
This individual will work within a team of data reliability engineers and data stewards, who serve as experts for the data sources, to ensure the appropriate quality and internal and external support are in accordance with our service level commitments. Candidates should be courageous, tenacious, creative, optimistic and curious, with an appetite for growth in business-minded leadership.
This role is located in our Alpharetta, Georgia office. We have a hybrid work model which requires being in the office 3 days/week.
What you’ll do
Domain Expertise: Understand the lifecycle of the Equifax data sources across all stages from ingestion to production
Drive Data Reliability and Quality across our Data Pipeline
Quality Monitoring and Incident Management: Own threshold-based reports that efficiently and reliably identify areas for further investigation and improvement. Act on the findings of these reports in a timely manner, prioritizing effort based on business impact.
Continuous Improvement: Work with an evergreen roadmap of initiatives, powered by work in Quality Monitoring and Customer Engagement, helping to drive these monitors and incident management process to an automated approach for each data domain/index.
Be an ambassador for our data assets and encourage data awareness which can assist with new opportunities and points of synergy.
What experience you need
5+ years of relevant experience with industry-leading lenders and/or data companies, with a working knowledge of data structure, data handling, the credit lifecycle and data management
5+ years of analysis experience in credit risk, telecommunications, financial services, marketing, fraud, or insurance
3+ years of experience with data handling/quality tools and Big Data ecosystems i.e. SQL, Google Cloud Platform, BigQuery
3+ years of experience with data, business insights and data & data processing audit analysis which identify and drive ongoing excellence in data quality practices
What could set you apart
A strong sense of governance protocols, the need to respect highly sensitive data and proactive attention to regulatory, security and compliance driven policies
Strong aptitude and proven ability to develop data-driven solutions to meet business objectives
Demonstrates leadership with cross-functional efforts; develops effective working relationships with peers, managers, and senior management within and across organizational lines
The ability to manage multiple high-impact projects at the same time
Creativity and drive, coupled with a desire to grow in thought leadership, initiative and strategic thinking
Experience with Collibra, or similar metadata management tool
Experience with Spotfire, or similar data visualization tool
Experience with SQL, Google BigQuery, or equivalent database queries and views
We offer comprehensive compensation and healthcare packages, 401k matching, paid time off, and organizational growth potential through our online learning platform with guided career tracks.
If this sounds like somewhere you want to work, don’t delay, apply today - we’re looking for you!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.","$85,081 /yr (est.)",10000+ Employees,Company - Public,Management & Consulting,Research & Development,1899,$1 to $5 billion (USD)
"LTIMindtree
3.8",3.8,"Ontario, CA",Senior Data Engineer,"SQL DB /Technical Skills
o Experience with SQL Server or an equivalent database product – Must have.
o Experience in troubleshooting and problem solving role – Must have.
o Experience with Relational Database management systems and concepts
o High Availability: Experience/Knowledge of Windows Server Cluster Services in On-Premise environments
o Experience in SQL Server Performance
o Experience in Database and Server Administration
o Should able to deploy and Troubleshoot SQL Server Failover Cluster Instances etc.
o Experience/Knowledge of Windows Server Cluster Services in Azure cloud
o Having Azure domain experiences is a big asset.
o Experience in a customer facing or customer support role
Time of operation: This track will be in 24X7 shift model. Therefore, candidates should be flexible to cover during holidays and weekends to accommodate MS roster

Disclaimer :L&T Infotech has an accommodation process in place and provides accommodations for applicants with disabilities. If you require a specific accommodation because of a disability or a medical need during our recruitment processes, please let us know so that arrangements can be made for the appropriate accommodations to be in place.

Job Segment: Database, System Administrator, SQL, Engineer, Technology, Customer Service, Engineering","$108,343 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Information Technology Support Services,1997,Unknown / Non-Applicable
"OpenSea
4.6",4.6,Remote,"Senior Data Engineer, Infrastructure","OpenSea is the first and largest marketplace for non-fungible tokens, or NFTs. Applications for NFTs include collectibles, gaming items, domain names, digital art, and many other items backed by a blockchain. OpenSea is an open, inclusive web3 platform, where individuals can come to explore NFTs and connect with each other to purchase and sell NFTs. At OpenSea, we're excited about building a platform that supports a brand new economy based on true digital ownership and are proud to be recognized as Y Combinator's #3 ranked top private company.

When hiring candidates, we look for signals that a candidate will thrive in our culture, where we default to trust, embrace feedback, grow rapidly, and love our work. We also know how critical it is to celebrate and support our differences. Employing a team rich in diverse thoughts, experiences and opinions enables our employees, our product and our community to flourish. We are dedicated to equal employment opportunities regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. To help facilitate this, we support remote, hybrid or onsite work at either New York City, San Francisco or the Silicon Valley for the majority of our opportunities.

Our engineering team at OpenSea is in search of a strong and curious Data Engineer to take charge of our analytics and machine learning pipelines. As a member of our data engineering team, you will collaborate with other engineers, data analysts, data scientists, and product managers, contributing significantly to the growth of one of the most rapidly expanding NFT marketplaces in the Web3 ecosystem.
Responsibilities
Design, build, and maintain data pipelines from end-to-end, ensuring data accuracy, availability, and quality for the Analytics and Data Science teams
Collaborate closely with Data Scientists to understand data requirements, develop data models, and optimize data pipelines for advanced analytics and machine learning use cases
Develop and maintain scalable, efficient, and reliable ETL processes, using best practices for data ingestion, storage, and processing
Work with stakeholders to identify and prioritize analytics requirements, and build out necessary analytics tools and dashboards
Proactively monitor data pipelines, troubleshoot, and resolve data-related issues
Contribute to the continuous improvement of data engineering practices, including documentation, code reviews, and knowledge sharing
Desired Experience
5+ years of experience in data engineeringExperience with big data technologies such as Snowflake, Hadoop, Spark, Airflow, or Flink
Strong knowledge of AWS services, particularly those related to data storage, processing, and analytics (e.g., S3, Redshift, Glue, EMR, Kinesis, Lambda, and Athena)
Expert in SQL and proficiency in at least one programming language (Python, Go, Java)
Familiarity with data warehousing concepts and schema design principles (e.g., Star Schema, Snowflake Schema)
Strong problem-solving skills, a data-driven mindset, and a passion for working with large, complex datasets
Excellent communication and collaboration skills, with the ability to work effectively across teams and stakeholders

If you don't think you meet all of the criteria below but still are interested in the job, please apply. Nobody checks every box, and we're looking for someone excited to join the team.

The base salary for this full-time position, which spans across multiple internal levels depending on qualifications, ranges between $160,000 to $305,000 plus benefits & equity.

#LI-Remote",#N/A,201 to 500 Employees,Company - Private,Information Technology,Internet & Web Services,2017,Unknown / Non-Applicable
"Farm Credit Financial Partners
3.4",3.4,"Springfield, MA",Data Engineer III,"For over 25 years, Farm Credit Financial Partners, Inc. (FPI) has provided technology products and services to the Farm Credit System. We care deeply about the agricultural credit associations (ACAs) we serve through our mission of delivering trusted technology solutions to help American agriculture thrive. As a customer-owned service organization, we support six ACAs from Maine to California with over 62,000 customer-members and over $40 billion in loan volume . Everyone here contributes to the success of our customers, and to the vibrant culture that makes FPI a great place to work. Throughout the year, you will find us having fun and jamming out to FPI’s band, coming together to support local charities, and celebrating our wins together.
We offer a robust benefits package that includes competitive earnings, hybrid and remote work options, tuition reimbursement, generous 401(k) matching, and development opportunities through company-sponsored trainings and certifications.
Come grow with us: financialpartners.com .
Farm Credit Financial Partners, Inc. is an Equal Opportunity Employer, and all qualified applicants will receive consideration for employment without regard to age, race, color, national origin, sex or gender, religion, pregnancy, marital status, status as a veteran, sexual orientation, gender identity, disability, or any other characteristic protected by law. EEO / AA / Minorities / Female / Disabilities / Veterans

JOB SUMMARY: The Data Engineer III is responsible for transforming data that can be easily analyzed. The position will be responsible for expanding and optimizing our data and data pipeline for our Association partners. The Data Engineer III primarily works with project teams on developing new data platforms to support strategic initiatives in alignment with business and/or enterprise strategies.
ESSENTIAL FUNCTIONS:
Work with architects, modelers and other data engineers to implement design and assemble large, complex data sets that meet end user business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Participate in technical design and code reviews to ensure quality and best practices are maintained.
Perform as technical lead on smaller projects and be a collaborative member on larger projects.
Contribute to the effective data governance of the organization’s business data. This includes data quality, data management, data policies, business process management, and risk management surrounding the handling of organizational data.
ADDITIONAL FUNCTIONS:
Coach and mentor junior engineers
Communicate effectively with stakeholders regarding project status and delivery timeframe
Foster innovative team culture and process improvement during development phase
OTHER DUTIES: This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that
are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without
notice.

QUALIFICATIONS:
Bachelor’s degree in Computer Science or MIS, and 5+ years’ experience in data modeling and cloud technologies. Strong working experience with the Azure technology stack including:
Databricks
Data Factory
SQL
Python
Power BI
Experience with Agile and Waterfall methodologies
Strong organization, analytical, and communication skills
Proficiency in the following technologies: R, Microsoft Office Suite
Strong customer service focus (data consumers as customers)
Familiar with software development best practices
WORK ENVIRONMENT: Typical noise levels for an open, cubicle-styled environment.
PHYSICAL DEMANDS: This position requires periods of standing, walking, and the use of computer equipment. Additional physical demands include, but may not be limited to, talking or hearing, push/pull, stooping, kneeling, reaching w/hands and arms, and lifting at least 10 pounds.
Working off-hours nights and weekends may be required on occasion for mission-critical needs.
WORK AUTHORIZATION: Authorization to work in the United States is required.
REASONABLE ACCOMMODATION : Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.","$119,377 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1995,$25 to $100 million (USD)
"TransPecos Banks
4.2",4.2,Remote,Data Integrations Engineer-Mulesoft,"Job Description
Job Title: Mulesoft/API Engineer
Summary: We are seeking a talented API Engineer with deep Mulesoft experience to join our team. In this role, you will be responsible for designing, developing, testing, and deploying integration solutions that connect various systems and applications across the organization. You will work with cross-functional teams to identify integration requirements, design and implement solutions, and ensure that our systems are performing at their best. The ideal candidate will have experience in Mulesoft and a deep understanding of data integration concepts and technologies, as well as strong problem-solving skills and the ability to work collaboratively with cross-functional teams.
Wage Type: Salaried
Organizational Structure:
Reports to: Head of Engineering/Chief Architect
Provides guidance to other technical team members
Essential Duties & Responsibilities:
To perform this job successfully, an individual must be able to perform each of the essential duties satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Design and develop integrations between various systems and applications, using integration technologies such as REST APIs, SOAP, JSON, XML, and ETL tools
Collaborate with cross-functional teams to understand integration requirements, design and implement solutions that meet business needs.
Ensure high-quality, reliable, and scalable integrations by performing unit testing, integration testing, and troubleshooting issues.
Create and maintain technical documentation related to integrations, including specifications, design documents, and test plans.
Work with third-party vendors and partners to integrate their systems with our own
Stay current with industry trends and best practices in integration technologies and tools
Design, develop and implement Mulesoft-based integration solutions for our enterprise systems.
Qualifications:
Required Knowledge/Skills:
5+ years of experience in designing and developing integrations between various systems and applications
Strong experience with integration technologies such as REST APIs, SOAP, JSON, XML, and ETL tools
Strong programming skills in Java, Python, or other programming languages
Strong problem-solving skills and ability to troubleshoot issues quickly and efficiently
Excellent communication and collaboration skills with the ability to work effectively in a team environment
Experience in designing and developing integration solutions using Mulesoft.
Strong understanding of SOA architecture and principles.
Experience with cloud technologies, such as AWS or Azure (Azure preferred).
Strong analytical and problem-solving skills.
Competent in API and fundamental network security
Mulesoft Developer Certifications are a plus.
Talent:
Strong project management and organizational skills.
Must be a self-starter and self-motivated individual that is goal oriented, organized, and analytical and can bring a positive impact to our organization.
Energetic, resourceful, and appropriate work intensity to get the work done
Strong people acumen and relationship skills; Naturally pre-disposed to quickly establish positive personal and professional relationships.
Desired Experiences:
Familiarity with Microsoft tools like Power BI and Dataverse
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience working in a start-up or rapidly growing organization.
Education:
Degree in Information Technology, Computer Science or Management Information Systems
MuleSoft Developer Certifications are a plus.
Non-degree certifications also considered.
Other:
Ability to interpret a variety of instructions furnished in written, oral, diagram or schedule form.
Must be able to lift to 20 pounds.
Job Type: Full-time
Benefits:

401(k)
401(k) matching
Dental insurance
Health insurance
Health savings account
Life insurance
Paid time off
Parental leave
Professional development assistance
Referral program
Tuition reimbursement
Vision insurance

Schedule:

Monday to Friday
Weekend availability
Application Question(s):

How many years as a developer have you worked in a Financial Institution?
Will you now or in the future require H1B sponsorship?

Experience:

MuleSoft: 5 years (Preferred)
Azure: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
APIs: 5 years (Preferred)
SOA architecture: 1 year (Preferred)

License/Certification:

MuleSoft Developer Certification (Preferred)
Work Location: Remote",#N/A,Unknown,Company - Private,Financial Services,Banking & Lending,#N/A,Unknown / Non-Applicable
"Banner Health
3.3",3.3,"Dallas, TX",Cybersecurity Data Protection Engineer II,"Primary City/State:
Phoenix, Arizona
Department Name:
IT Data Protection-Corp
Work Shift:
Day
Job Category:
Information Technology
Primary Location Salary Range:
$40.11 - $66.85 / hour, based on education & experience
In accordance with State Pay Transparency Rules.
Innovation and highly trained staff. The Information Technology professionals at Banner Health are utilizing cutting edge technology to change health care for the better. If you’re ready to change lives, we want to hear from you.
In this role you will get to work with industry-leading tools and technologies to protect Banner’s sensitive data. You will be responsible for engineering CASB, DLP (and possibly other solutions), training operational staff on response procedures, and working with business partners such as HR, Legal, and Privacy to deliver solutions that secure and enable Banner business. We are looking for a cybersecurity engineer that will proactively implement solutions to complex technical problems, develop and maintain meaningful metrics, collaborate with other technical and business partners, and drive efficiency. The typical schedule for this role will be determined upon hire. Successful candidate can work remote with occasional on site visits OR can work from Banner Corporate (Phoenix Plaza - off Thomas and Central).
Banner Health IT was awarded Inside Pro and Computerworld's 100 Best Places to work in IT for 2020, 2021, 2022, and 2023!
Within Banner Health Corporate, you will have the opportunity to apply your unique experience and expertise in support of a nationally-recognized healthcare leader. We offer stimulating and rewarding careers in a wide array of disciplines. Whether your background is in Human Resources, Finance, Information Technology, Legal, Managed Care Programs or Public Relations, you'll find many options for contributing to our award-winning patient care.
POSITION SUMMARY
This position designs, develops, configures, implements, tunes, maintains solutions, resolve technical and business issues related to cybersecurity threat & vulnerability management, identity management, security operations center, forensics, and data protection. Cybersecurity Engineers work with Cybersecurity Architects to execute strategic cyber initiatives, evaluate security components of the network, applications and end-user devices, and provides guidance to ensure new systems meet regulatory and technical standards. Cybersecurity Engineers participate in root-cause analysis efforts to determine improvement opportunities when failures occur. Manage Cyber systems, ensures they are tuned, on the current release and manages appropriate change management across the IT organization and the business.

CORE FUNCTIONS
1. Leads in the design and implementation of cybersecurity solutions.

2. Leads in providing technical expertise and support for cybersecurity solutions, including operational aspects of the software, hardware, network/firewall.

3. Leads in the design, implementation, and compliance of secure configurations for applications and infrastructure components.

4. Leads in technical assessments of systems and applications to ensure compliance with policy, standards and regulations.

5. Leads in the ongoing evaluation and development of security policies and procedures. Leads the revision of policies and procedures, as needed.

6. Serves as technical lead of cybersecurity projects, including the development of project scope requirements, cybersecurity product implementation, tuning, operational support model creation.
7. Under general direction, this position is responsible for cybersecurity across multiple departments system-wide and requires interaction at all levels of staff and management. Work closely on cross functional IT Teams.

Performs all functions according to established policies, procedures, regulatory and accreditation requirements, as well as applicable professional standards. Provides all customers of Banner Health with an excellent service experience by consistently demonstrating our core and leader behaviors each and every day.

MINIMUM QUALIFICATIONS

Must possess strong knowledge of business, information security and/or computer science as normally obtained through the completion of a bachelor's degree in Computer Science, Information Security, Information Systems, or related field.

Four to six years of experience of enterprise-scale information security engineering, preferably in healthcare. Must also possess one to three years’ experience in a healthcare environment or an equivalent combination of relevant education, technical, business and healthcare experience. Experience, IT operations, automation of cybersecurity processes, coding and scripting languages, ability to document cybersecurity processes as well as use case development. Experience with the assessing cyber products, including vendor selection, define requirements, contractual documentation development. Experienced in planning, designing and implementing cybersecurity solutions. Experienced in operating, maintaining and implementing, upgrading and lifecycle of cybersecurity solutions. Proficient understanding of regulatory and compliance mandates, including but not limited to HIPAA, HITECH, PCI, Sarbanes-Oxley. Advanced knowledge of Security Engineering Principles, including risk management, resilience, vulnerability management, Information Security, NIST, MITRE ATT@CK, etc. Expertise in Cyber products supporting Data Loss Prevention, EDR, AntiVirus, Perimeter services, Threat systems, cyber platform analytics, SIEM, CASB, CLOUD Security, ETC. Requires independent judgment, critical decision making, excellent analytical skills, with excellent verbal and written communications. Ability to think quickly under difficult or complex conditions and clearly communicate to appropriate staff; ability to balance project workloads with customer support and on-call demands. Must demonstrate knowledge of information technology and information security principles and practices. Requires communication and presentation skills to engage technical and non-technical audiences. Requires ability to communicate and interact across facilities and at various levels. Incumbent will have skills to mentor less experienced team members. As is typical in this industry, variable shifts and hours and responding to after-hours notifications may be required.

PREFERRED QUALIFICATIONS

Certification in two or more of the following areas: Systems Security Certified Practitioner (SSCP), HealthCare Information Security & Privacy Practitioner, (HCISPP), CompTIA Security+, Certified Information Systems Security Professional (CISSP) – Engineering (ISSEP), Certified Ethical Hacker (CEH), SANS GIAC, or Certified Information Systems Auditor (CISA). Three plus years as a System Administrator, Security Operations or in IT Operations. Or three plus years in risk management or GRC experience in the healthcare/medical environment. Must also possess three plus years’ experience in a healthcare environment or an equivalent combination of relevant education, technical, business and healthcare experience.

Additional related education and/or experience preferred.
EOE/Female/Minority/Disability/Veterans
Our organization supports a drug-free work environment.
Privacy Policy",#N/A,10000+ Employees,Nonprofit Organization,Healthcare,Health Care Services & Hospitals,1999,$5 to $10 billion (USD)
"aventiv
3.5",3.5,"Carrollton, TX",Data Center Implementation Engineer,"Welcome to Aventiv! Please watch this brief video to find out if this is the place you want to be!
https://vimeo.com/391578629/5ba31cc5e9

Job Purpose:
Responsible for Engineering and installation of all hardware systems into the enterprise Data Center environments. Spares management, logistics management, vendor management, configuration management and hardware asset tracking will also be key deliverables of this assignment.
THIS IS A HYBRID POSITION WITH APPROXIMATELY 10% TRAVEL REQUIRED.

Essential Duties
Prepare all change controls related to hardware Engineering and installations
Work with various departments to compile requirements and update all Data Center Engineering.
Perform all Installations for IT hardware
Provide Spares management tracking of the Spares hubs kept to maintain component failures
Provide details to create hardware project budgets.
Perform or manage vendors as required to perform periodic preventative maintenance.
Provide all logistics support for all material deliveries to and between IT Data Center environments.
Perform other duties as assigned

Knowledge, Skills, and Abilities
Highly motivated
Good organizational skills
Great interpersonal and communication skills
Excellent analytical and reporting skills
Solid IT Data Center background
Proficient at using MS Visio, Excel, and Word
Knowledgeable and trouble ticketing (Heat) and Environmental monitoring Systems (Orion)

Minimum Qualifications
HS Diploma or GED
Minimum 5 years’ experience in Data Center Engineering and implementation
Experience with AC and DC electrical systems, Data Center Rack, flooring and cable management systems, Camera, environmental and infrastructure monitoring systems used within Data Centers, and Safety practices for Data Center implementations
10% travel required

Preferred Qualifications
Bachelor’s degree in Electrical Engineering, Mechanical, or Business Administration or equivalent field of study
Network Engineering experience a plus
System administration (Microsoft or Linux) experience a plus
Project Management experience or certification is plus

Physical Demands
Standing, sitting, walking, speaking, listening, bending, reaching, pushing, pulling, lifting, grasping and manipulating tools, typing, using peripheral computer tools.
May be required to lift up to 50 pounds.

Salary and Benefits:
At Aventiv, our salary and benefits are designed to fit you as a whole person. We offer a salary range based on experience and qualifications to ensure your unique contributions are met with our most competitive offer.
$83,500-$95,200 /year
Health Insurance
401(k)
Disability
Life Insurance
Paid Time Off
Voluntary Benefits

Aventiv Privacy Policy:
www.aventiv.com/privacy

Equal Employment Policy:
Aventiv is proud to be an equal opportunity employer. All decisions regarding recruiting, hiring, promotion, assignment, training, termination and other terms and conditions of employment will be made without regard to race, color, national origin, biological sex, sexual orientation, gender identity, gender expression, gender presentation, religion, age, pregnancy, disability, work-related injury, veteran status, genetic information, marital status, or any other factor that the law protects from employment discrimination. We do not discriminate based on genetic information in accordance with the Genetic Information Nondiscrimination Act.","$89,350 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2004,$500 million to $1 billion (USD)
"(ISC)2
3.3",3.3,Remote,Data Engineer,"Overview:
(ISC)² is an international nonprofit membership association focused on inspiring a safe and secure cyber world. (ISC)² offers a portfolio of credentials that are part of a holistic, pragmatic approach to security. Our association of candidates, associates, and members, nearly 330,000 strong, is made up of certified cyber, information, software, and infrastructure security professionals who are making a difference and helping to advance the industry. Our vision is supported by our commitment to educate and reach the general public through our charitable foundation – The Center for Cyber Safety and Education™. For more information on (ISC)², visit www.isc2.org, follow us on Twitter, or connect with us on Facebook and LinkedIn.

We are committed to an inclusive and equitable environment that values the unique perspectives and experiences of our entire workforce. We strive for a true sense of belonging for all our employees and to foster authenticity, trust, empowerment and connectedness that leads to everyone’s success. For more information, visit www.isc2.org/dei.
Position Summary:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Responsibilities:
Implement Azure Data Services and tools to ingest , egress and transform data from multiple sources and create and maintain optimal data pipeline architecture.
Responsible for creating ETL pipeline with Azure Ecosysem like Azure Synapse ,Azure Data Factory.
Implement and support ETL related jobs to curate , transform and aggregate data to create models for end user consumption.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Azure from Sales Force ,Pearson Vue etc.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Maintain ownership of a given pipeline or domain and raise flags to leadership when appropriate regarding architectural concerns.
Demonstrates commitment to valuing diversity and contributing to an inclusive working and learning environment.
Miscellaneous duties as assigned.
Qualifications:
Bachelor’s degree in computer science or other equivalent degree
7+ years of experience in a Data Engineer role.
Experience with Cloud Data warehouses such as Azure , AWS and Google BigQuery
Experience with big data tools: Hadoop, Spark and Kafka.
Experience with Azure Dedicated and Azure Synapse
Experience with Analytics one or more of the following reporting tools; Tableau, PowerBI, Looker, Domo and Microstrategy
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++,etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Skills/Competencies:
Ability to demonstrate and support the 5 Company Core Values: Integrity, Excellence, Unity, Accountability, Agility
Ability to build an inclusive culture that encourages, supports and celebrates diversity; serve as a role model to promote DEI best practices.
Strong analytic skills related to working with unstructured datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills.
Physical & Mental:
Up to 5% travel may be required.
Work normal business hours and extended hours when necessary.
Remain in a stationary position, often standing or sitting, for prolonged periods
Regular use of office equipment in a remote environment such as a computer/laptop and monitor computer screens
Dexterity of hands and fingers to operate a computer keyboard, mouse, and other computer components
#LI-remote
Equal Employment Opportunity Statement:
All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic as protected by applicable law.
Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.",#N/A,201 to 500 Employees,Nonprofit Organization,Management & Consulting,Membership Organizations,1989,$25 to $100 million (USD)
Luttechub,#N/A,"Columbus, OH",Lead Data Engineer / Data Architect,"Lead Data Engineer / Data Architect
Only candidate with years of experience: +7
Those authorized to work in the U.S. are encouraged to apply.
Remote
Job description
Responsibilities
Stitch and normalize sparse and noisy data across various data sources.
Undertake the preprocessing of structured and unstructured data
Design, develop, test, implement, and support technical solutions in full-stack development tools and technologies.
Work with a team of developers with deep experience in machine learning, distributed microservices, and full-stack systems
Utilize programming languages like Python, Java, and Open Source RDBMS and cloud-based data warehousing services such as Aurora or Big Query
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with engineering and product development teams
Deliver on timeline commitments where necessary
Build trust and strong relationships at all levels
Desirable Qualities:
Strike a balance between critical thinking and actual hands-on analyses.
Have a keen interest in learning data science while bringing engineering rigor to the team.
Research mindset- the ability to structure a project from idea to experimentation to prototype to implementation.
Be a driven and focused self-starter, great communicator, with exceptional follow-through. You aggressively tackle work and love the responsibility of being individually empowered.
Be resolute to overcome challenges that will inevitably arise.
Qualifications & Experience
Required
Must have designed, developed, and supported a complex software solution.
Proficient in SQL.
Proficient in dimensional/multidimensional data modeling.
Experience with graph, transactional, and operational data modeling is a plus.
Familiarity with all stages of the product development cycle. Experience maintaining engineering best practices, including defect tracking, design reviews, and appropriate testing.
Hold strong organizational and problem-solving skills.
Take a pragmatic, product-oriented approach.
Possess the ability to work cross-functionally with minimal supervision.
Preferred
B.S. in Computer Science, Electrical Engineering, Mathematics, Statistics, Physics, or similar quantitative fields/work experience.
Experience with cloud environments
7+ years of experience in application or data warehousing development
7+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)
4+ years of experience working on streaming data applications
5+ years of experience with Agile engineering practices
5+ years of experience developing Java-based software solutions, scripting and OOP languages
5+ years of experience with UNIX/Linux, including basic commands and shell scripting
4+ years of experience with GCP
4+ years of experience with Ansible / Terraform
3+ years in the healthcare industry and knowledge of their business practices.
Job Type: Contract
Pay: $65.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Columbus, OH 43081: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 7 years (Required)
data warehousing development: 7 years (Required)
scripting language (Python, Perl, JavaScript, Shell): 7 years (Required)
Work Location: Hybrid remote in Columbus, OH 43081",$65.00 /hr (est.),201 to 500 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Palo Alto Networks
4.2",4.2,"Santa Clara, CA",Principal Engineer Software (Cloud / Data Lake),"Company Description

Our Mission
At Palo Alto Networks® everything starts and ends with our mission:
Being the cybersecurity partner of choice, protecting our digital way of life.
We have the vision of a world where each day is safer and more secure than the one before. These aren’t easy goals to accomplish – but we’re not here for easy. We’re here for better. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.
We’re changing the nature of work. Palo Alto Networks is evolving to meet the needs of our employees now and in the future through FLEXWORK, our approach to how we work. From benefits to learning, location to leadership, we’ve rethought and recreated every aspect of the employee experience at Palo Alto Networks. And because it FLEXes around each individual employee based on their individual choices, employees are empowered to push boundaries and help us all evolve, together.

Job Description

Your Career
Cortex Data Lake (CDL) enables AI-based innovations for cybersecurity with the industry’s only approach to normalizing and stitching together an enterprise’s data. Cortex Data Lake can:
Radically simplify customer security operations by collecting, integrating, and normalizing an enterprise’s security data.
Effortlessly run advanced AI and machine learning with cloud-scale data and compute.
Constantly learns from new data sources to evolve customer defenses.
CDL is built to benefit from public cloud scale and locations ready for elastic scale, eliminating the need for local compute and storage. CDL has strict privacy and security controls in place to prevent unauthorized access to sensitive or identifiable information. Its infrastructure is secured with industry-standard best practices for security and confidentiality, including rigorous technical and organizational security controls.

Principal Engineers are
Technologists who can architect solutions to complex problems
Experts who deliver on critical business needs in their domains
Role models and mentors who exemplify the best of Palo Alto Networks culture
Leaders who can communicate cogently with hands-on engineers as well as executives
Hands-on engineers that can code and build great products
Your Impact
Driving the technical roadmap with hands-on participation in developing next generation data processing systems optimized for AI-powered use cases
Conceptualize, Architect and develop highly efficient multi-cloud data pipelines to process & store multi-tenant data at Petabyte scale to enable AI/ML Applications
Keep the product updated with best-of-the-times technology in a fast-evolving environment
Work with SRE and Technical Support teams to investigate and resolve critical customer defects
Recruit and Mentor new team members

Qualifications

Your Experience
8+ years of hands-on experience in building large enterprise applications with 3+ years experience in technical leadership roles building large scale data pipelines.
Must have extensive hands-on programming skills in Java
Experience in Python programming, expertise in Linux and shell scripting skills are a significant plus
Experience with one or more cloud platforms such as AWS, GCP , their services (DataFlow, BQ, Athena etc) and Kubernetes
Strong knowledge of databases SQL, NoSQL, Time series, GraphDB etc
Good leadership and communication skills
High energy with the ability to work in a fast-paced environment

Additional Information

The Team
Our engineering team is at the core of our products and connected directly to the mission of preventing cyberattacks. We are constantly innovating — challenging the way we, and the industry, think about cybersecurity. Our engineers don’t shy away from building products to solve problems no one has pursued before.

We define the industry instead of waiting for directions. We need individuals who feel comfortable in ambiguity, excited by the prospect of a challenge, and empowered by the unknown risks facing our everyday lives that are only enabled by a secure digital environment.
Our Commitment
We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.
We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.
Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.
All your information will be kept confidential according to EEO guidelines.
The compensation offered for this position will depend on qualifications, experience, and work location. For candidates who receive an offer at the posted level, the starting base salary (for non-sales roles) or base salary + commission target (for sales/commissioned roles) is expected to be between $140,100yr to $226,600/yr. The offered compensation may also include restricted stock units and a bonus. A description of our employee benefits may be found here .
#LI-TD1","$183,350 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Enterprise Software & Network Solutions,2005,$1 to $5 billion (USD)
"Hexagon Asset Lifecycle Intelligence
3.9",3.9,"Houston, TX","Software Support Engineer, Reference Data (Remote - USA)","Responsibilities:
Are you a problem solver, someone who enjoys troubleshooting and resolving complex issues? Hexagon’s Asset Lifecycle Intelligence division (Hexagon) is seeking a full-time Software Support Engineer, Reference Data to join our global Reference Data support team. The role offers flexibility, with limited travel required, and the option to work from either home or any one of Hexagon’s US office locations.
Works as part of a global team to support customers by troubleshooting and resolving technically complex support issues related to Hexagon’s Asset Lifecycle Intelligence division’s Solutions and interfaces.
Regularly interacts with solution development and product ownership teams to investigate problems, discuss, and document design issues, and provide customer feedback.
Possesses strong interpersonal skills and effectively communicates with internal and external stakeholders.
Exercise a sizeable degree of self-direction and decision making focused on delivering a solution to customer’s workflow implementation issues.
Accurately documents support activities and outcomes; Maintains knowledge base.
Develops strong proficiencies in Customer Relationship Management systems.
Maintains adherence to customer service level agreements.
Develops technical documents, instructions, and training for internal and external purposes.
Keeps updated on current Hexagon’s Asset Lifecycle Intelligence division’s solution portfolio through training and sprint reviews.
May conduct customer training or work on service activities when required.
#LI-REMOTE #LI-RM
Qualifications:
As a minimum possess strong competency of Intergraph Smart Reference Data with knowledge of additional Hexagon Solutions an advantage.
Communicate via the telephone, email, online video, and in person, employing excellent oral and written communication skills.
Experience participating in consulting functions and services.
Customer Service experience and ability.
Degree qualification in an Engineering or related discipline or equivalent industry experience.
Fluency in the basic technologies underlying the IT industry.
US Citizenship is required","$61,366 /yr (est.)",1001 to 5000 Employees,Company - Public,Information Technology,Computer Hardware Development,1992,$500 million to $1 billion (USD)
"Intone Networks
4.5",4.5,Remote,Lead Data engineer,"MUST HAVE: - 10+ years of experience in: Technical solutioning and system architecture design Evaluation and Recommend of ETL tools Implementation of recommended ETL tools Strong in SQL and Python, with 3+ years hands-on coding experience with both Experience building automated big data pipelines Experience performing data analysis and data exploration Experience working in an agile delivery environment Strong critical thinking, communication, and problem solving skills Experience with big data frameworks (i.e. Hadoop and Spark) Experience with cloud-based platforms (i.e. Azure, GPC, AWS) Experience working in multi-developer environment, using version control (i.e. Git) Experience with real-time and streaming technology (i.e. Azure Event Hubs, Azure Functions Kafka, Spark Streaming) Experience with deployment/scaling of apps on containerized environment (i.e. Kubernetes, AKS) Experience partnering cross-functionally with other technical teams (i.e. data ingestion, data science, operational systems) to align priorities and achieve deliverable outcomes Experience with setting coding standards, performing code reviews, and mentoring junior developers Experience overseeing project delivery by mentoring junior technical developers PREFERRED TO HAVE: Previous healthcare experience and domain knowledge Exposure/understanding DevOps best practice and CICD (i.e. Jenkins) Exposure/understanding of containerization (i.e. Kubernetes, Docker) Experience with Snowflake and hands-on query tuning/optimization. Experience with orchestrating pipelines using tools (i.e. Airflow, Azure Data Factory) Experience with REST API/Microservice development using Python Full legal Name: Contact Number: Email: Current Location: Work Authorization: Years of Total Work Experience: DOB Linkedin: Pay rate Education Details and Years of Graduation:",#N/A,201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$5 to $25 million (USD)
"JM Family Enterprises
3.4",3.4,"Deerfield Beach, FL",Senior Data Engineer,"Sr. Data Engineer will work in the Actuary, Data & Analytics Department of Jim Moran and Associates Inc., a division of JM Family Enterprises, and will play a critical role in development and management of organizational data assets for data science, advanced analytics and self-service reporting to deliver business insights and drive actionable results.
You will interface with business stewards, data end-users and the data storage/architecture team, leveraging a common data platform, to fulfill data-related requests that meet business needs.
Key Responsibilities:
Acquire data from primary or secondary data sources, prepare, integrate, and maintain data structures in support of advanced analytics, self-service reporting, and strategic data initiatives
Work closely with management, data product team, and data end-users to prioritize business and information needs
Validate data for completeness and accuracy based on guidelines set by project stakeholders or downstream data and analytics systems
Perform necessary data profiling, data cleansing, aggregations, and normalization to deliver data products that are available for end-user consumption
Work with data providers to improve data management workflow
Develop and execute standard or custom queries and/or reports to retrieve data
Conduct root cause study of business problems; suggest areas for process improvement
Execute and promote data management and data governance strategies and standards
Qualifications:
Bachelor’s degree in computer science, Management Information Systems, or related field
5+ years of experience in data warehousing, data lifecycle management, database designing, data modelling and computer programming
Excellent understanding of Structured query language (SQL), Coding experience with a general-purpose, dynamic programming language (e.g., Python, PERL)
Strong learning orientation and curiosity
Comfortable learning new systems/software applications
Strong analytical thinking skills and problem-solving skills
Solid written and oral communication skills
Detail oriented
Good time management skills and multitasking ability
Knowledge in the following is preferred:
MS Azure: Azure Data Lake Store, Synapse
Data Factory/SSIS or related tool
Python or similar programming languages
Business visualization tools (e.g., Tableau, PowerBI)
Statistical software (e.g., SAS, SPSS)
#LI-AM1
#LI-Onsite
JM FAMILY IS PROUD TO BE AN EQUAL OPPORTUNITY EMPLOYER
JM Family Enterprises, Inc. is an Equal Employment Opportunity employer. We are committed to recruiting, hiring, retaining, and promoting qualified associates without regard to age, race, religion, color, gender, sex (including pregnancy, childbirth and related medical conditions), sexual orientation, gender identity, gender expression, mental or physical disability, national origin, marital status, citizenship, military status, genetic information, veteran status, or any other characteristic protected by federal, state, provincial, or local law.

DISABILITY ACCOMMODATIONS
If you have a disability and require a reasonable accommodation to complete the job application process, please contact JM Family’s Talent Acquisition department at
talentacquisition@jmfamily.com
for assistance. If you have an accommodation request for one of our recruiting events, please notify us at least 72 hours prior so that we may provide assistance.","$116,093 /yr (est.)",1001 to 5000 Employees,Company - Private,Retail & Wholesale,Vehicle Dealers,1968,$10+ billion (USD)
"Akamai
4.5",4.5,United States,Senior Software Engineer - Data Platform - Remote,"Do you love Big Data Development and are a team player?
Would you like to work with cutting-edge technology?
Join our world-class Platform team
Our CTG (Cloud Technology group) develops and operates multiple services deployed on the cloud. These services collect, process, and store data about every transaction that is executed on Akamai edge. The Data is being consumed for the purposes of business analytics and decisions support, customer analytics, Akamai's cost structure management & Akamai's network management.
Make a global impact
You will work with talented team members that develop a Data Warehouse, which provides support for ingesting massive data volumes and executing Big Data analytical queries on that data. Technology stack that we use include Spark and Spark Structured Streaming, Kubernetes on Cloud (Azure - AKS), Spring, Prometheus, Grafana, In-house implementations and more.
As a Senior Software Engineer, you will be responsible for:
Designing, developing and rolling out Big Data systems and services
Collaborating with other teams to deliver to market mission critical data systems
Developing new and enhancing existing capabilities within data warehousing services
Following SW development methodology best practices
Handling feature development from idea inception through design and testing to operational deployment
Working on projects that focus on system scalability, performance and security
Do what you love
To be successful in this role you will:
Have 5 years of relevant experience and a Bachelor's Degree in Computer Science or related field, or equivalent experience as a Senior Backend Engineer
Have experience developing Java or Scala software
Demonstrate solid communication skills and be able to present to various stakeholders
Be a self-starter, proactively taking the initiative to solving problems and motivation to improve results
Work in a way that works for you
FlexBase, Akamai's Global Flexible Working Program, is based on the principles that are helping us create the best workplace in the world. When our colleagues said that flexible working was important to them, we listened. We also know flexible working is important to many of the incredible people considering joining Akamai. FlexBase, gives 95% of employees the choice to work from their home, their office, or both (in the country advertised). This permanent workplace flexibility program is consistent and fair globally, to help us find incredible talent, virtually anywhere. We are happy to discuss working options for this role and encourage you to speak with your recruiter in more detail when you apply.

Learn what makes Akamai a great place to work
Connect with us on social and see what life at Akamai is like!

We power and protect life online, by solving the toughest challenges, together.
At Akamai, we're curious, innovative, collaborative and tenacious. We celebrate diversity of thought and we hold an unwavering belief that we can make a meaningful difference. Our teams use their global perspectives to put customers at the forefront of everything they do, so if you are people-centric, you'll thrive here.
Working for you
At Akamai, we will provide you with opportunities to grow, flourish, and achieve great things. Our benefit options are designed to meet your individual needs for today and in the future. We provide benefits surrounding all aspects of your life:
Your health
Your finances
Your family
Your time at work
Your time pursuing other endeavors
Our benefit plan options are designed to meet your individual needs and budget, both today and in the future.
About us
Akamai powers and protects life online. Leading companies worldwide choose Akamai to build, deliver, and secure their digital experiences helping billions of people live, work, and play every day. With the world's most distributed compute platform from cloud to edge we make it easy for customers to develop and run applications, while we keep experiences closer to users and threats farther away.
Join us
Are you seeking an opportunity to make a real difference in a company with a global reach and exciting services and clients? Come join us and grow with a team of people who will energize and inspire you!

Akamai Technologies is an Affirmative Action, Equal Opportunity Employer that values the strength that diversity brings to the workplace. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of gender, gender identity, sexual orientation, race/ethnicity, protected veteran status, disability, or other protected group status.

#LI-Remote
Compensation
Akamai is committed to fair and equitable compensation practices. The base salary for this position ranges from $113,430 - $170,043/year; a candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location. The compensation package may also include incentive compensation opportunities in the form of annual bonus or incentives, equity awards and an Employee Stock Purchase Plan (ESPP). Akamai provides industry-leading benefits including healthcare, 401K savings plan, company holidays, vacation (in the form of PTO), sick time, family friendly benefits including parental leave and an employee assistance program including a focus on mental and financial wellness; Eligibility requirements apply.","$141,737 /yr (est.)",5001 to 10000 Employees,Company - Public,Information Technology,Information Technology Support Services,1998,$1 to $5 billion (USD)
"Phasorsoft LLC
4.0",4.0,"Seattle, WA",1348 - Cloud Data Engineer - Onsite - W2,"NOTE
NOT FOR C2C
We are looking for the candidates who can work on w2 ( Pay Roll )
Job description
Responsibilities:
Design and implement ETL data pipelines using Azure pipelines or other cloud technologies.
Extract data from diverse sources and transform it using Pyspark and Python languages within Azure Data Bricks.
Develop and optimize advanced SQL queries for data retrieval and manipulation.
Collaborate with cross-functional teams to understand data requirements and perform data mapping.
Utilize Snowflake for data storage and retrieval.
Follow Scrum/Agile methodologies to ensure timely delivery of projects.
Apply Kafka experience to enable real-time data streaming and processing.
Write Pyspark code to enhance data processing capabilities.
Use Powershell for automation and orchestration tasks.
Requirements:
Strong exposure to cloud platforms such as GCP, AWS, or Azure.
Proven experience in ETL development and cloud-based data pipelines.
Proficiency in using Azure Data Bricks and Pyspark for data transformations.
Advanced SQL skills for complex data querying and manipulation.
Familiarity with Snowflake for data warehousing.
Knowledge of data mapping and data modeling concepts.
Experience working in Agile/Scrum development environments.
Previous exposure to Kafka for real-time data streaming is preferred.
Proficiency in Pyspark coding.
Familiarity with Powershell scripting for automation.
Job Type: Contract
Pay: $76,479.45 - $158,309.78 per year
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Seattle, WA 98101: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Required)
SQL: 1 year (Required)
Data warehouse: 1 year (Required)
Work Location: In person","$117,395 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"SpartanNash
4.0",4.0,"Avondale Estates, GA",Senior Data Engineer,"At SpartanNash, we deliver the ingredients for a better life through customer-focused innovation. We do this for our supply chain customers and U.S. military commissaries, retail store guests and, most importantly, our Associates. In fact, we see a day when each will say, “I can’t live without them.”

Our SpartanNash family of Associates is 17,500 strong, ranging from bakery managers to order selectors; from IT developers to vice presidents of finance; from HR Business Partners to export specialists. Each of them plays an integral role in SpartanNash’s People First culture, Operational Excellence and Insights that Drive Solutions. Ready to contribute to the success of our food solutions company? Apply now!

Location: 850 76th Street S.W. - Byron Center, Michigan 49315

Job Description:
Position Summary:
This role is responsible to create, integrate, implement, and support large, highly complex systems and/or suites of applications. Provide expert technical leadership and guidance in projects. Oversee and review the programming/development work of others.

Here’s what you’ll do:
Technical Expert / Reference Resource acting independently under limited direction provides expert level technical design and development services and guidance for one or more systems or specialized applications
Provides technical leadership on complex solutions Scope and Function Definition develops, defines and documents project/system/function scope and objectives
Designs or enhances systems, applications, functions, and procedures to solve complex problems and meet business objectives
Prepares detailed functional specification Solution Development has full technical knowledge of all phases of analysis, integration, development, quality assurance, and implementation procedures
May be responsible for multiple phases of a project
May be responsible for the instruction, oversight, and review work of other associates
Application/System Troubleshooting interfaces with appropriate corporate area to provide detailed application/system information to assist with problem determination and resolution
Business Liaison/Mentor Acts as the primary reference resource interacting with business users to provide detailed application/system information
Mentors and provides guidance and training to other development associates
Maintain an efficient infrastructure by recognizing and reducing latencies and gaining efficiencies where recognized or needed
Participate in the on-call support rotation

Here’s what you’ll need:
Bachelor's Degree (Required) Computer Science, Computer Science Information Technology, or related field or equivalent combination of education and/or experience
Minimum 5+ years of work experience in Business Intelligence, Relational Databases, and ETL processes with modern ETL/ELT tools (Talend/Matillion/DataStage/Informatica) preferably in a cloud environment
Experience with Cloud Databases (Snowflake preferred) and Big data technologies
Qlik Replicate/Qlik Compose experience preferred
Advanced SQL skills, able to write and optimize complex queries in multiple database platforms (DB2, Oracle, Informix, SQL Server, MySQL, Netezza, Snowflake)
Experience with Unix scripting
Strong written and verbal communication skills, and effective interpersonal skills to work with internal and external customers and vendors
Good analytical/research and problem-solving skills
As part of our People First culture, SpartanNash is proud to offer a robust and competitive Total Rewards benefits package .

SpartanNash is an Equal Opportunity Employer, including disability and veteran, that celebrates diversity and believes employing a diverse workforce is key to our success. We are committed to providing equal employment opportunities to all individuals.

We are not able to sponsor work visas for this position.","$114,024 /yr (est.)",10000+ Employees,Company - Public,Retail & Wholesale,Grocery Stores,#N/A,$5 to $10 billion (USD)
"Healogics, Inc
3.0",3.0,"Jacksonville, FL",Data Solutions Engineer - Visualization,"Job Summary:

Healogics is the largest provider of advanced wound care services in the United States, treating more than 300,000 chronic wound patients annually across over 600 sites. With an aging society, obesity and diabetes on the rise, and an uptick in surgical procedures, the number of patients with non-healing wounds that would benefit from expert care is dramatically increasing. As a result, the company is working to provide our differentiated, quality outcomes to as many patients that would benefit through our out-patient clinic partnerships.

At a Data Solutions Visualization Engineer, you will provide technical leadership in the area of big data systems development including data ingestion, data curation, data storage, high-throughput data processing, analytics, user access, and security. You will partner with the Data Solutions team and business stakeholders to understand their goals, how they think about their area of the business, and how we can support them through analytics to Find, Treat and Heal patients in the community. You’re an active listener who asks good questions and puts the company first, you move seamlessly between code and analytical concepts, and you appreciate beautiful & simple technical solutions.

All Healogics employees must perform their job responsibilities according to all Healogics policies, Hospital policies, as well as to accrediting organizations, federal and state regulation, and to the Centers for Medicare and Medicaid Services (CMS) guidelines, as applicable.
Essential Functions/Responsibilities:

Architect and engineer tools, visualizations, and dashboards for the business to have everything they need at their fingertips to answer their difficult questions.
Be a bridge between the business and technical teams, enabling insight that can empower better decision-making.
Produce dashboards / visualizations for the business to explore.
Deliver ad hoc analysis using Data Solutions tools (ETL and Reporting Services, SQL) to answer timely questions.
Work with non-technical people to analyze their information needs and create effective solutions for them.
Acquire data from source systems and integrate them with the data warehouse to support any of the above.
Execute QA testing and resolve any defects.
Can speak knowledgably about the business of Healogics and offer solutions to complex problems.
Evaluate new visualization technologies and methods to constantly improve the offerings of Data Solutions.
Work interactively with all levels of company employees, including executives, and guide and advise them towards the answers for business questions.
Performs other duties as required.

Required Education, Experience and Credentials:

Bachelor of Science or Bachelor of Arts degree in Information Technology (or a related field) required; or equivalent combination of education and experience.
Minimum of three (3) years of SQL Server experience, real world experience in dimensional data warehouse development and design.
Experience building and maintaining reporting systems using tools such as Microsoft SSRS and Tibco Spotfire, required.
Experience with cloud services such as AWS and Azure, desired

Required Knowledge, Skills, and Abilities:

Excellent oral and written communication skills.
Strong knowledge of Windows operating system, virtual systems, and cloud systems.
Strong knowledge of Microsoft SQL Server.
Implementation experience with industry standard Business Intelligence/Data Visualization tools
Excellent problem-solving skills.
Interpersonal skills.
Analytical thinking skills.
Conceptual thinking skills.
Project scheduling skills.

Please contact Ashley at Ashley.Helfrich@healogics.com for more information!

Salary for this position generally ranges between $71,800-$101,400. This range is an estimate, based on potential employee qualifications: education, experience, geography as well as operational needs and other considerations permitted by law.","$92,219 /yr (est.)",1001 to 5000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,1995,Unknown / Non-Applicable
"hims & hers
4.2",4.2,United States,Senior Data Engineer,"Hims & Hers Health, Inc. (better known as Hims & Hers) is a multi-specialty telehealth platform building a virtual front door to the healthcare system. We connect consumers to licensed healthcare professionals, enabling people to access high-quality medical care—from wherever is most convenient—for numerous conditions related to sexual health, hair care, mental health, skincare, primary care, and more.
With products and services available across all 50 states and Washington, D.C., Hims & Hers is on a mission to help the world feel great through the power of better health. We believe how you feel in your body and mind transforms how you show up in life. That's why we're building a future where nothing stands in the way of harnessing this power. We normalize health & wellness challenges—and innovate on their solutions—to make feeling happy and healthy easy to achieve. No two people are the same, so we provide access to personalized care designed for results. At our core, our mission is deeply personal—because we too are customers.
In January 2021, the company was listed on the NYSE and is traded under the ticker symbol ""HIMS"". To learn more about our brand and offerings, you can visit forhims.com and forhers.com.
About the Role:
As a Senior Data Engineer, you will work with analytics engineers, product managers, engineers, security, DevOps, designers and others to build a data platform that backs the self-service analytics, machine learning models, and products serving 900,000+ Hims & Hers users.
You Will:
Architect and develop data pipelines to optimize for performance, quality, and scalability
Collaborate with analytics engineering data analysts, and business partners to build tools and data marts that enable self-service analytics
Build, maintain & operate scalable, performant, and containerized infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Design, develop and own robust, scalable data processing and data integration pipelines using Python, dbt, Kafka, Airflow, Spark, and REST API endpoints to ingest data from a variety of external data sources to data lake
Develop testing frameworks and monitoring to improve data quality, observability, and data quality
Partner with the analytics engineers to ensure the performance and reliability of our data sources
Orchestrate sophisticated data flow patterns across a variety of disparate tooling
Partner with machine learning engineers to deploy predictive models
Partner with the security team to build frameworks and implement data compliance and security policies
Partner with DevOps to build IaC and CI/CD pipelines
Support code versioning and code deployments for data pipelines
You Have:
5+ years of professional experience designing, creating and maintaining scalable data pipelines using Python, API calls, and scripting languages
Demonstrated experience writing clean, efficient & well-documented Python code and are willing to become effective in other languages as needed
Demonstrated experience writing complex, highly-optimized SQL queries across large data sets
Experience with cloud technologies such as AWS or Google Cloud Platform
Experience with IaC technologies like Terraform
Experience with data warehouses like BigQuery, Databricks, Snowflake, and Postgres
Experience with event streaming technologies like Kafka / Confluent
Experience with modern data stack (Airflow, Databricks, dbt, Fivetran, Tableau / Looker)
Experience with containers and container orchestration tools such as Docker or Kubernetes
Project management skills and a demonstrated ability to work autonomously
Understanding of SDLC and Agile frameworks
Nice to Have:
Experience with Machine Learning & MLOps
Experience building data models using dbt
Experience with Javascript
Experience with CI/CD (Jenkins, GitHub Actions, Circle CI)
Experience designing and developing systems with desired SLAs and data quality metrics
Experience with microservice architecture
Our Benefits (there are more but here are some highlights):
Employee Stock Purchase Program
An inclusive culture where we are always seeking improvement and cherish your input
Great compensation package with equity compensation
Unlimited PTO (10 holidays off), Mental Health days (1 day off per quarter)
Generous Parental Leave
High-coverage medical, dental & vision
Mental health & wellness benefits
Offsite team retreats
Access to Amazon HIMS Store to order any additional equipment to ensure you have the gear you need
Employee discounts on hims & hers & Apostrophe online products, and Apple Store
$75 monthly connectivity stipend (phone/internet)
401k Match
We are focused on building a diverse and inclusive workforce. If you're excited about this role, but do not meet 100% of the qualifications listed above, we encourage you to apply.
Hims is an Equal Opportunity Employer and considers applicants for employment without regard to race, color, religion, sex, orientation, national origin, age, disability, genetics or any other basis forbidden under federal, state, or local law. Hims considers all qualified applicants in accordance with the San Francisco Fair Chance Ordinance.

#LI-Remote
Outlined below is a reasonable estimate of H&H’s compensation range for this role.

H&H also offers a comprehensive Total Rewards package that includes equity grants of restricted stock (RSU’s) so that H&H employees own a piece of our company.

The actual amount will take into account a range of factors that are considered in making compensation decisions including but not limited to, skill sets, experience and training, licensure and certifications, and location.

Consult with your Recruiter during any potential screening to determine a more targeted range based on the job-related factors. We don’t ever want the pay range to act as a deterrent from you applying!
An estimate of the current salary range is
$125,000—$175,000 USD",#N/A,201 to 500 Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2017,Unknown / Non-Applicable
"Commonwealth Care Alliance
3.5",3.5,"Boston, MA",Lead Data Engineer,"Why This Role is Important to Us:


The Data Engineer role at Commonwealth Care Alliance (CCA) is a heavy technical and architect role that designs, contributes to, and maintains data pipelines and systems that support clinical point of care and business decisions. This role is highly collaborative with other data-focused departments at CCA including Analytics, Actuarial, and Finance. This role is a “forward-deployed” engineer that is expected to engage with and continually build a greater understanding of the clinical and operational context.
The Data Engineer must understand existing data, infrastructure, and transactional systems to design the appropriate usage of existing data assets or the development of new data assets, including large structured and unstructured data. The Data Engineer must automate robust workflows to efficiently perform extract, load, transform, modeling, and computational tasks in continuous integration and continuous delivery pattern. The Data Engineer will also support and contribute to rapid prototyping and deployment of analytics, model training, and model deployment. This role will report to the Director of Data Strategy, Mgmt. and Sol. at CCA.



What You'll Be Doing:


Engage with stakeholders on pre-defined projects
You will be responsible to understand the business requirements and architect robust data platform on cloud technologies
Build scalable and robust data model/infrastructure
Help managing enterprise data platform
Dealing with data governance, such as documentation, data integrity/quality and data security
Build data pipelines (ETL, validation, automation, monitoring and logging...) that enable analysts and other stakeholders across the organization for data-focused product and data-driven business decisions
Work closely with product managers and engineers to design, implement, test and continually improve scalable applications and services running on Azure
Make changes on the existing data system to optimize and improve accuracy of the data process
Give instruction or/and help stakeholder on the best practice to pull and use data
Create scalable and actionable solutions, in the form of analytics, reports and dashboards for stakeholders to solve business and technical problems through ad-hoc requests
Write andrevise technical documents and blogs, including design, development and application
Complete data engineering projects under supervision and guidance
You will be responsible to understand the business requirements and architect robust data platform on cloud technologies
You will be responsible for creating reusable and scalable data pipelines
You will be responsible for development and deployment of new data platforms
You will be responsible for deploying AI algorithms into the data platform to run predictive analytics at scale
Help the Data Engineering team produce high-quality code that allows us to put solutions into production
Collaborate with IT Architect and IT Security and Privacy teams to architect and deploy data pipeline solutions that are secure and performant
Day to day management and reliability of data pipelines deployed securely on the public cloud
Executes quality excellence through standards, best practices, and continuous improvements



What We're Looking For:


Education Required:
Master's degree + 5 year of equivalent work experience (STEM major or related field preferred)
OR bachelor's degree + 8 years of equivalent work experience (STEM major or related field preferred)
Experience Required:
Experience in design and develop code, scripts, and data pipelines that leverage structured and unstructured data
Strong foundation in data engineering principles and an architectural best practice
Experience in architecting solution on public cloud (preferably Azure)
Experience in instituting data architecture best practices (i.e. dimensional modeling, ETL pipeline, large scale distributed ETL pipelines)
Extensive experience of database query languages (i.e. SQL or equivalent), database design, optimizing queries, internals knowledge of query planning
Experience in authoring or reviewing system design documents for enterprise solutions
Knowledge, Skills & Abilities Required:
Technical Skills:
SQL, Oracle and Python
ETL tool (Informatica, Talend)
BI tool (Looker, Tableau, PowerBI ...)
Git-based version control systems
Strong knowledge of data modeling and mapping
Good understanding of different dimensional modeling techniques such as Star vs SnowFlake Schemas
Soft skills:
Be organized and flexible
Take initiative to own projects
Strong analytical skills
Strong verbal and written communicational skills
Be able to work with interdisciplinary teams
Be able to explain technical concepts/results to non-technical audiences
Passion for creating work that is well-documented and reproducible
Ability to work with short iteration times in agile mode as well as the ability to carry out projects in a self-directed manner
Healthcare: passion for working at a healthcare organization
Preferred Skills and Experience:
Experience with R
Experience in healthcare or health insurance organization
Experience with clinical claims and health record data
Experience building data pipelines in the cloud under the constraints of HIPAA
Publication or presentation of innovation in data science, machine learning or related area
Experience with Azure cloud and data services (cosmos, synapse, datafactory, databricks etc.)
Understanding of infrastructure (including hosting, container based deployments and storage architectures) would be advantageous","$133,540 /yr (est.)",1001 to 5000 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2003,$1 to $5 billion (USD)
"Nova Ltd.
4.5",4.5,"Fremont, CA",Big Data/Machine Learning Software Engineer,"The Data Analytics team at Nova is looking for a Big Data/Machine Learning Software Engineer to work with Data Scientists and other Software Engineers to gather requirements and implement solutions. For you, it is exciting to see your code interacting with algorithms and data to produce results via complex interactions in Big Data systems. In this role, you will work with Data Scientists to adapt new and existing Machine Learning algorithms to Big Data microservices and build functionality to access those via web-based user interfaces.
Nova Ltd. is a leading innovator and key provider of metrology solutions for advanced process control used in semiconductor manufacturing. Our products are used in-line by leading chip foundries as well as original equipment manufacturers. Nova’s technology serves critical sectors of patterning, thin film deposition, CMP and diffusion in leading logic and memory fabs worldwide. market.

About us:
Nova provides insights into process control in the world’s most technologically advanced industry. We employ physics, mathematics, algorithms, software and hardware expertise to redefine the limits of what is possible in semiconductor manufacturing.
We invite you to join our dreamers and winners and brilliant high- aimers who see impossible as the starting point to exciting challenges, and work together in multidisciplinary global teams to find answers.
We dive deep to extract unique insights and provide our customers and partners with crucial decision-making data. Each and every one of us helps redefine what people can achieve through technology.
Why Nova:
Fortune magazine chose Nova as one of the fastest growing companies in the world in 2019 and 2020
Great Place to Work-Certified™ 2022 & 2023
Opportunity to collaborate with the best in this field, our 1000+ employees love coming to work every day in our 12 offices across the globe and share their passion for technology and innovation

Requirements:
Self-starter and quick learner of new technologies and processes – we are using some technologies not easily found in current skill portfolios, so this is the most important requirement! If you are under-skilled in some of the other requirements below, evidence of your ability to learn quickly will be seriously considered. Example of technology needed to learn quickly is customization/extension of ElasticSearch-Logstash-Kibana (ELK) instances
5+ years of software development experience in Agile environment
5+ years of coding and development experience using Python 2/3, preferably using OO approach
1-2 years of Big Data deployment, monitoring and troubleshooting of microservices, and interactions with the following Big Data concepts:
Extract/Transform/Load workflows
Kafka or other message queues
Hive/Hadoop/Impala and Big Data data storage including NOSQL solutions
Big Data configuration and monitoring using a number of technologies
Extensive experience working in a multi-threaded environment
Experience working with SQL databases and/or Big Data datastores (HDFS, etc)
Exposure to web-based application development – optimally Dash (with Python), React or Angular to facilitate workflows and graphic visualization
Extra Spice:
Strong communication and problem-solving skills – possess the ability to translate business requirements into application code
Ability to take ownership of the complete software development cycle from requirements gathering to design to implementation
Team player who will work in a collaborative environment with users and the engineering team
Passionate about well-designed software that is modifiable, efficient, reliable and meets coding standards
Experience customizing/extending ElasticSearch-Logstash-Kibana (ELK) instances

Role Responsibilities:

What will you do as a Big Data Machine Learning Software Engineer?
Implement Machine Learning solutions that live on Big Data systems, with input and guidance from Data Scientists and the algorithms they develop to effect Machine Learning
Work as part of a Scrum team to analyze requirements, scope, estimate, implement, and test changes to meet these requirements in a Big Data system
Debug existing source code, analyze logs and fix bugs as needed
Work independently and collaboratively as needed
Take ownership of assigned tasks and finish in a timely manner
Continuously learn and improve skills
Apply significant attention to detail to ensure all tasks are carried out to the highest standard
What will make you succeed in the role?
Extensive experience in Python software development
Experience configuring, deploying, and monitoring micro services in a Big Data system
Experience with User Experience (UX) and implementation of python approaches to meet UX needs
Working knowledge of GIT
Working knowledge of JIRA
Test driven development
Database application development and data modeling techniques
Experience with Scrum/Agile","$138,647 /yr (est.)",501 to 1000 Employees,Company - Public,Manufacturing,Machinery Manufacturing,1993,$100 to $500 million (USD)
"Quirch Foods
3.0",3.0,"Coral Gables, FL",Data Engineer,"About Quirch Foods
In business for over 50 years, Quirch Foods is a global distributor and exporter of fresh and frozen food products with a unique focus on ethnic cultures. We service our customers from 5 distribution centers strategically located in the United States. We have consistently been ranked among the top 50 exporters in the US and are one of the largest importers of seafood. Our customers include independent grocers, chain supermarkets, foodservice distributors, cruise lines, restaurants and food processors/manufacturers. We have an extensive product list with over 8,000 SKU's and carry niche brands such as Panamei, Mambo and Chiquita along with national brands like Tyson, IBP, Excel, JBS, Iowa Premium, Smithfield, Dietz & Watson, along with many others. We are headquartered in Coral Gables, FL.
Quirch is a portfolio company of Palladium Equity Partners, a leading mid-market private equity fund based in New York City, which partnered with the Executive Team to capitalize on Quirch Foods’ record of excellence and to grow it decisively beyond its current business.
Position Summary
At Quirch Foods we believe in leveraging technology as a source of competitive advantage. This includes customized ERP Solutions, Business Intelligence, EDI, Web and Mobile Solutions. We are currently looking for a Data Engineer to join our development efforts.
Essential Duties and Responsibilities:
Develop, test and maintain custom business solutions by leveraging state of the art software development practices using an agile methodology
Evaluate business requirement and help define problems and develop solutions
Participate in design and architecture discussions with business leaders, end users and IT team members
Document business requirements and solution, document code and provide support for creation of end user documentation
Update business knowledge, technical skills and soft skills, leverage educational opportunities, participate in professional organizations
Experience:
MS SQL Programming
Visual Studio\C#\WPS Experience
Microsoft SSAS, SSIS, SSRS desired but not required
Biztalk experience desired but not required
Skills and Requirements:
Bachelor’s degree in Computer Science, Engineering, Math or equivalent and\or related experience and training preferred
Strong analytical and problem-solving skills
Strong collaborator and team player
Great organizational skills, attention to detail and follow thru
Effective oral and written communication skills
Extensive familiarity with data management principles
Benefits:
Professional growth and developmental opportunities to grow your skills using state of the art technology
Comprehensive benefits package that includes: Medical, Dental, Prescription Drug Plan, Disability Plan, Life insurance Plan
401K savings Plan
Paid Holidays
Personal Time off
Employee Discounts
Quirch Foods is an Equal Opportunity Employer (EOE). Qualified applicants are considered for employment without regard to age, race, color, religion, sex, national origin, sexual orientation, disability, or veteran status. All applicants must be eligible to work in the United States.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Vision insurance
Experience level:
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Coral Gables, FL 33134: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
What is your desired compensation?
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Data Engineering: 1 year (Preferred)
Work Location: In person","$89,340 /yr (est.)",201 to 500 Employees,Company - Private,Retail & Wholesale,Wholesale,1967,$500 million to $1 billion (USD)
"Iron Bow Technologies
3.5",3.5,"Washington, DC",Sr. Data Center Engineer,"Iron Bow Technologies is seeking a Sr. Data Center Engineer to support our Federal customer, The Department of Justice (DOJ), Criminal Division (CRM), in Washington, DC. The selected candidate will support our flexible and scalable solution with the DOJ CRM to extend cloud-native features to existing legacy and next-generation applications while also future proofing the infrastructure to easily accommodate future requirements, such as transitioning to a public cloud.
This position is hybrid and requires 2 days a week onsite at the customer location in NW Washington, DC and support at the customer data center as needed in Sterling, VA.
Essential Duties and Responsibilities:
Assess and provide VMware solutions to include vCenter, vSphere, SRM, ESXi
Primary storage solutions and data file storage management, NAS file storage solutions
Data Center blade servers and rack servers
Server virtualization
Experience:
10 years of related experience providing system administration support in a virtualized environment, including at least 6 years of VMWare solutioning, including vCenter, vSphere, SRM, and ESXi
Data Center operations experience, with SAN and NAS
Experience supporting primary storage solutions and data file storage management, NAS file storage solutions
Hands on experience with Data Center blade and rack servers
Ability to work staggered shift hours when needed to support critical coverage and/or time-sensitive needs
Be a member of our Team providing coordinated 24x7 IaaS on-call support
U.S. Citizen
Currently hold or obtain/maintain a Public Trust High clearance
Education:
BS in Computer Science, Information Science, or other relevant discipline is preferred – additional years of experience may be considered in lieu of the degree
Preferred Certifications:
VMware Certified Professional
Primary storage certifications such as HPE, Dell-EMC, NetApp, or Pure Storage
VCP-DCV
Travel Requirements:
No travel
PRINCIPALS ONLY. NO AGENCIES PLEASE. MUST BE A US CITIZEN ABLE TO OBTAIN A PUBLIC TRUST SECURITY CLEARANCE.
Iron Bow Technologies is a leading IT solution provider dedicated to successfully transforming technology investments into business capabilities for government, commercial and healthcare clients. Our depth of technical expertise, global reach and strategic partnerships with industry leaders as well as disruptive technology partners uniquely positions Iron Bow to target the right solution to ensure your successful business outcome. We partner with our clients from planning and implementation through ongoing maintenance and management to deliver solutions that are strong, flexible and on target .
Iron Bow Technologies is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#LI-TM1",#N/A,501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,1983,$500 million to $1 billion (USD)
TEKletics,#N/A,"Scottsdale, AZ",Azure Data Engineer,"Azure Data Engineer
Tekletics is an information technology company focused on providing quality resources to support our customers needs. Successful candidates will work with world class organizations to deliver projects. Each candidate is required to have a strong work ethic and the ability to handle high pressure situations.
A little about this role:
As the Azure Data Engineer, you are primarily responsible for the collection and transformation of data across a multitude of data sources. This individual is also responsible for the optimization of the environment, structure, and processes associated with said data.
A day in the life:
Data Warehouse - As the Azure Data Engineer, you are responsible for the data warehouse design, development, testing, support, and configuration. You will review business requests for data warehouse data and data warehouse usage. You will also research data sources for new and better data feeds ensuring consistency and integration with existing warehouse structure.
Data Collection – You will be responsible for developing automated data pipelines and/or data integrations within the Azure Synapse environment. You will use SQL, Python scripts and Azure Functions to automate data collection from a wide variety of sources.
o API utilization – ability to leverage REST APIs as needed.
Data Transformation – You will create BI (Business Intelligence) and Data Warehousing cube design. You will create and manage ETL/ELT processes to transform and load data into data warehouse for reporting and analytics.
Data Optimization – As the Azure Data Engineer, you will create and maintain standards and policies. You will identify, design, and implement internal process improvements, including automation of manual processes and optimization of data delivery. You will continuously improve data reliability, efficiency, and quality.
Training – Identify and demonstrate techniques to optimize reporting for Data Visualization Analyst, provide and participate in internal and external training sessions, and produce documentation to support understanding and learnings around the Presence data/reporting environment.
This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Projects and responsibilities may change at any time with or without notice due to our business, industry, and/or market changes.
What we are looking for:
Previous experience using SQL, Python scripts and Azure Functions
Experienced in Azure Synapse environment
Experience designing, building, and maintaining data processing system
Dependable, extroverted, diplomatic person, able to problem-solve successfully with a wide variety of people and issues
Attention to detail and strong organizational skills, self-motivated
Ability to work independently while being a strong team player
Ability to mentor junior level developers
Passion for innovation and “can do” attitude to thrive in a fast-paced environment
Proficient in time management and adhering to deadlines
Knowledge and interest of the natural products/brands and retail landscape is a plus
Proficient computer (MS Office applications) and data-mining skills
Flexibility to successfully multi-task in a fast-paced environment with a positive attitude
Regular and predictable attendance is required
Ability to manage time and deadlines
Job Type: Full-time
Pay: $79,947.00 - $142,509.31 per year
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Scottsdale, AZ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure: 3 years (Required)
SQL: 3 years (Required)
Data warehouse: 3 years (Required)
Work Location: In person","$111,228 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"R1 RCM, Inc.
3.4",3.4,Remote,Data Engineer III,"The Software Engineer III serves as a Data Engineer focused on building next generation of healthcare products and services. As part of a collaborative, motivated Agile-based team, the Software Engineer III is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
Responsibilities
Create and maintain optimal data pipeline architecture to assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders to optimize the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies.
Work with stakeholders including the Product, Data and Engineering teams to assist with data-related technical issues and support their data infrastructure needs.
Adhere to and ensure engineering best practices using modern SDLC that enables CI/CD and favors automation, auditability, automated testing, infrastructure, and policy as code.
Ability to collaborate effectively with other architects and engineers delivering end to end, high-performing, highly reliable, scalable, and operable solutions.
Required Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical field; or equivalent experience.
3+ years of experience delivering data engineering applications and services.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets .
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience in any one of the big data technologies (Hadoop, MapReduce , Spark, Data Lake etc. )
Experience in any one of the stream-processing systems ( Kafka, Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages : Python, Java, Scala, etc.
Understanding of modern software development including version control, unit testing, and continuous integration and deployment.
Strong communication skills, with proven ability to present complex ideas and to document concisely.
Preferred Qualifications
Working knowledge of Cloud providers and services such as Amazon AWS or Microsoft Azure.
Experience with relational SQL and NoSQL databases, including MongoDB and ElasticSearch.
Knowledge of healthcare data standards such as HL7, FHIR, EDI X12.
Knowledge of the healthcare revenue cycle, EMRs, practice management systems.
Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.

Our associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including:
Comprehensive Medical, Dental, Vision & RX Coverage
Paid Time Off, Volunteer Time & Holidays
401K with Company Match
Company-Paid Life Insurance, Short-Term Disability & Long-Term Disability
Tuition Reimbursement
Parental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.
If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.
CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California Consent",#N/A,10000+ Employees,Company - Public,Healthcare,Health Care Services & Hospitals,2003,Unknown / Non-Applicable
"ECI - Sacramento
4.4",4.4,"Sacramento, CA",Lead Data Quality Engineer (C),"Senior Lead Data Quality Engineer with Snowflake experience:
12-24 months.
Public sector experience is preferred.
Must have these specific experiences in data management, data integration, data qualify and Lead experience.
Must have Snowflake platform experience.
Project will start end of June.

Mandatory Qualifications:
Must have a bachelor's degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field. Please attach degree(s).
Must have 10 or more years of experience working in Information Technology.
Must have at least seven (7) years or more of work experience in data management disciplines including data integration, modeling, optimization, and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.
Must have at least five (5) years of experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative.
Must have at least three (3) years of experience in data integration, data warehouse, big-data-related initiatives, development, and implementation.
Must have at least three (3) years of experience in architecture patterns and data integration design principles.
Must have at least three (3) years of experience with database technologies (e.g., SQL, NoSQL, SQL Server).
Must have at least three (3) years of experience with the Snowflake Data Warehouse Platform.
Must have at least three (3) years of experience with extract, transform, and load (ETL) and other business intelligence tools (e.g., SSIS, Azure Data Factory, Power BI, Tableau).

Here are the desired which is best to get as many as possible:
Desired Qualifications:
Possess an advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (post-graduation diploma or related) or a related quantitative field.
Possess Azure Cloud and Snowflake certifications.
Three (3) years working experience in the Azure Cloud platform using Synapse, Azure BLOB Storage/ADLS, Azure Data Factory, and Purview.
One (1) year experience with Azure DevOps and CI/CD pipelines.



About ECI - Sacramento:

Estrada Consulting, Inc. (ECI) delivers technology-enabled services and solutions to clients all over the USA and British Columbia. We provide system integration, custom application development, data warehouse and business intelligence, project management, custom reporting solutions and consulting services to mid-size and large enterprises in all major industries. The Company headquarters is in Sacramento, California, and was established in year 2000. Visit http://www.estradaci.com/ to learn about our projects, managed services, awards and certifications delivering value for a range of businesses and government agencies.",$75.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,Unknown / Non-Applicable
"Solect
3.4",3.4,"San Francisco, CA",Senior Data Engineer,"Job Locations US-CA-San Francisco | US-TX-Houston
Posted Date 1 month ago(4/7/2023 5:15 PM)
# of Openings
1
Category
Enterprise Technology
Job ID
2023-2250
Overview
Company Overview
Pattern Energy is a leading renewable energy company that develops, constructs, owns, and operates high-quality wind and solar generation, transmission, and energy storage facilities. Our mission is to transition the world to renewable energy through the sustainable development and responsible operation of facilities with respect for the environment, communities, and cultures where we have a presence.

Our approach begins and ends with establishing trust, accountability, and transparency. Our company values of creative spirit, pride of ownership, follow-through, and a team-first attitude drive us to pursue our mission every day. Our culture supports our values by fostering innovative and critical thinking and a deep belief in living up to our promises.

Headquartered in the United States, Pattern has a global portfolio of more than 35 power facilities and transmission assets, serving various customers that provide low-cost clean energy to millions of consumers.
Responsibilities
Job Purpose
Pattern Energy is embarking on a major business transformation to scale processes and systems to enable the significant growth of our business. One critical aspect of this transformation is building an enterprise data architecture capable of supporting strong enterprise analytical capabilities and interconnected data across functions for improved and real-time decision making. This new role of Senior Data Engineer will be instrumental in building this future.

The Senior Data Engineer will help guide Pattern’s journey by building novel and modern tools, pipelines, and data systems. Through lived experience this role will act in a key strategic resource in our transition from siloed, disparate data assets to a grand, unified, data estate. The senior data engineer will lead development of data systems, namely data lakehouse pipeline, and analytics environments to support data lifecycles across the business. These tools will help drive our efforts toward democratization of data, reducing duplication of effort, and adding value to the data stream.

Key Accountabilities
Work with the Data team to understand and interpret use cases around the business and develop tools/systems to support data lifecycle from the point of production to the point of consumption.
Implement modern technology and concepts to enable Pattern teams’ design of enterprise-grade systems capable of achieving data goals.
Develop pipelines and analytics tools, including automations wherever possible
Templatize these solutions to help our adoption of infrastructure-as-code
Qualifications
Experience/Qualifications/Education Required
Previous experience in building enterprise data lifecycle and cloud solutions.
5-7+ years experience, specific to data management, analytics, or data reporting.
B.S. in a technical field with M.S. preferred.
Significant experience delivering Data solutions using the Azure cloud stack
Experience with database, data lake, lakehouse design concepts and the use of SQL.
Experience with transformations and reporting solutions, namely Power BI
Experience with environment management, release management, code versioning, deployment methodologies, and CI/CD tools
Additional Requirements

Demonstrated excellence and ability to learn new programming skills and languages.
Strong understanding of security and governance principles, including access policies
Proven record of excellent communication and cooperation with varied stakeholders.
Renewable energy industry experience is strongly preferred.
Demonstrated experience building scalable data models & ingestion pipelines from a variety of systems including Enterprise Applications (e.g. ERP, CRM), Operational Assets (e.g. SCADA), Big Data (e.g. Weather or product configuration simulators).
Technical Skills
Experience throughout the Microsoft Integrated Data Platform (IDP) and related stacks including, but not limited to:
Azure, AAD, Synapse, Purview, Data Lake, Delta Lake, and Lakehouse design
Azure Data Factory, Data Explorer, Logic Apps, Power Automate
Databricks; PySpark, and optimized resource consumption, therein
The expected starting pay range for this role is $95,000 - $129,000 USD. This range is an estimate and base pay may be above or below the ranges based on several factors including but not limited to location, work experience, certifications, and education. In addition to base pay, Pattern’s compensation program includes a bonus structure for full-time employees of all levels. We also provide a comprehensive benefits package which includes medical, dental, vision, short and long-term disability, life insurance, voluntary benefits, family care benefits, employee assistance program, paid time off and bonding leave, paid holidays, 401(k)/RRSP retirement savings plan with employer contribution, and employee referral bonuses.

Pattern Energy Group is an Equal Opportunity Employer.","$112,000 /yr (est.)",51 to 200 Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,2009,$25 to $100 million (USD)
"Pfizer
4.1",4.1,"Tampa, FL","Senior Associate, Regulatory Quality Assurance Data Engineer","Pfizer has established a chief digital office which will lead the transformation of Pfizer into a digital powerhouse that will generate patient superior patient experiences that will result in better health outcomes.
As a member of the Digital Health Medicines and AI, Corporate Functions Data Solutions & Engineering team, you will help to realize Pfizer Digital strategy on the cloud by designing and deploying analytic solutions, work with high performing teams, and building lasting relationships with our business colleagues. This digital program will provide you with solid foundational understanding of the data types that drives business insights through data science and enterprise BI reporting solutions.
This role requires you to provide solution development and operations support for our Regulatory Quality Assurance (RQA) Domains (Audit Planning, Audits Made Easy, RQA Metrics, etc.). It will require you to maintain, enhance and provide operations support of our RQA solutions while leveraging our Data Hub, Dataiku, Snowflake, and data visualization product (Spotfire).
As the Senior Associate RQA Data Engineer, you will help in the pursuit of the Pfizer Digital strategy on the cloud by designing data products that drive business insights. You will be hands on and have opportunities to lead a group of vendors resources. You will be expected to partner with Digital Colleagues, (Enterprise Architecture, Client Partners, and Data Scientists, etc.) and Business Unit stakeholders to develop and sustain the RQA analytics strategy and data architecture. Data engineers also partner with solution development teams to ensure use case delivery goals are met while adhering to data architecture principles, guidelines, and standards.
Role Responsibilities
Reporting to the Senior Manager, Finance Sales Procurement Solution Delivery & Engineering, and as the Senior Associate RQA Data Engineer you will be responsible for data modeling, building/enhancing data interfaces, web applications, and visualizations that deliver insights that drive impactful business outcomes.
High level responsibilities may include (but are not limited to):
Leading the gathering, analysis and documentation of business and technical requirements
Create test plans, test scripts, and perform data validation
Enhance the design of the existing RQA Cloud Data Lake, Dataiku workflows, interface APIs, Web Apps, and our Spotfire dashboards.
Design automated solutions for building, testing, monitoring, and deploying ETL tools.
Develop internal APIs and DSS data solutions for thousands of end-users to power Audit applications and promote connectivity.
Opportunities to lead development teams that are driven to build impactful data products and visualizations
Perform root cause analysis and resolve Level 3 production and data issues
Coordinate with backend engineering team to analyze data to improve the efficiency, and speed of the application as well maintain elevated levels of data quality and data consistency.
Tune SQL queries, reports and ETL pipelines
Build and maintain data dictionary and process documentation
Present solutions to leadership, management, architects, and developers.
Work in conjunction with our cloud engineering staff, and partner with project managers, and analysts to deliver insights to the business
Professional Experience and Educational Requirements
Required:
Applicant must have a Bachelor’s degree with three years of relevant experience; OR Master’s degree with one year of relevant experience; OR Associate's degree with six years of relevant experience; OR eight years of relevant experience with a high school diploma or equivalent
At least 2 years of experience in data engineering, and/or reporting & analytics
Fundamental understanding of Data warehousing, data modeling, and data transformation
Exposure to web application development platforms such as Angular/Spring Boot; this is a must have.
Fundamental understanding of Cloud data warehouse solutions (Snowflake, Redshift, Spark, etc).
Working experience with one or more general purpose programming languages, including but not limited to: SQL, Java, Scala, Python, or JavaScript
Nice to Have:
Prior experience with data preparation and ETL: Dataiku, Informatica, Talend, Alteryx, etc is a major plus
Understanding of compliance and audit processes for GxP, SOX etc. is a plus
Cloud computing, machine learning, text analysis, NLP & Web development experience is a plus
Knowledge of Graph Databases (Neo4j, Titan, etc.) and query syntax is a plus
Experience with Semantic technologies and approaches is a plus
Full stack development using infrastructure cloud services (AWS preferred) and cloud-native tools and design patterns (Containers, Serverless, Docker, etc) is a plus and Open-Source technologies is a plus
Experience with sourcing and modeling data from application APIs
Prior experience with AWS Cloud stack (ECC, S3, Redshift) or Google Cloud Platform
Experience designing complex and inter - dependent data models for analytic, Machine learning use cases
Experience with Software engineering best-practices, including but not limited to version control (Git, TFS, Subversion, etc.), CI/CD (Jenkins, Maven, Gradle, etc.), automated unit testing, Dev Ops
Professional and Leadership Characteristics
Creative: Able to bring forth innovative ideas to improve our existing practices and takes calculated risks to innovate new capabilities within Pfizer Digital Business Analytics, with a focus on data products and analytics solutions
Analytical Thinker: Understands how to synthesize facts and information from varied data sources, both new and pre-existing, into discernable insights and perspectives; takes a problem-solving approach by connecting analytical thinking with an understanding of business drivers and how BA can provide value to the organization
Adaptable: Demonstrates flexibility in the face of shifting targets, thrives in new situations
Pioneering: Pushes self and others to think about new innovation and digital frontiers and ways to conquer them
Ambiguity Tolerant: Successfully navigates ambiguity to keep the organization on target and deliver against established timelines
Strong Data and Information Manager: Understands and uses analytical skills/tools to produce data in a clean, organized way to drive objective insights
Exceptional Communicator: Can understand, translate, and distill the complex, technical findings of the team into commentary that facilitates effective decision making by senior leaders; can readily align interpersonal style with the individual needs of customers
Highly Collaborative: Manages projects with and through others; shares responsibility and credit; develops self and others through teamwork; comfortable providing guidance and sharing expertise with others to help them develop their skills and perform at their best; helps others take appropriate risks; communicates frequently with team members earning respect and trust of the team
Proactive Self-Starter: Takes an active role in one’s own professional development; stays abreast of analytical trends, and cutting-edge applications of data

Work Location Assignment: Flexible
The annual base salary for this position ranges from $72,700.00 to $121,200.00. In addition, this position offers an annual bonus with a target of 7.5% of the base salary. Benefits offered include a retirement savings plan, paid vacation, holiday and personal days, paid caregiver/parental and medical leave, and health benefits to include medical, prescription drug, dental and vision coverage in accordance with the terms and conditions of the applicable plans. Salary range does not apply to the Tampa, FL location.
Relocation assistance may be available based on business needs and/or eligibility.
Sunshine Act
Pfizer reports payments and other transfers of value to health care providers as required by federal and state transparency laws and implementing regulations. These laws and regulations require Pfizer to provide government agencies with information such as a health care provider’s name, address and the type of payments or other value received, generally for public disclosure. Subject to further legal review and statutory or regulatory clarification, which Pfizer intends to pursue, reimbursement of recruiting expenses for licensed physicians may constitute a reportable transfer of value under the federal transparency law commonly known as the Sunshine Act. Therefore, if you are a licensed physician who incurs recruiting expenses as a result of interviewing with Pfizer that we pay or reimburse, your name, address and the amount of payments made currently will be reported to the government. If you have questions regarding this matter, please do not hesitate to contact your Talent Acquisition representative.
EEO & Employment Eligibility
Pfizer is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Pfizer also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Pfizer is an E-Verify employer.
Information & Business Tech
#LI-PFE","$96,950 /yr (est.)",10000+ Employees,Company - Public,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,1849,$10+ billion (USD)
"Petrolink International
2.9",2.9,"Houston, TX",Data Analysis Engineer,"Location:
Houston, Texas, U.S.A.
Job Type:
Full-time
Job Description:
Petrolink is a global company that specializes in user-driven technology for the Energy Industry.
We are looking for a Data Analysis Engineer to assist with creating and delivering algorithms primarily used by our engineering solutions.
This position is based in our U.S. office in Houston, Texas.
Duties & Responsibilities:
Research and provide logic for engineering solutions algorithms.
Collaborate with Subject Matter Experts (Drilling Engineers, Mud loggers, Pore Pressure Analysts, and other specialists from the Drilling Industry) to understand different challenges in the Drilling Industry and the practices in place to address/solve these challenges.
Code the algorithms using C# to run on Petrolink’s proprietary analytics engine.
Develop, test, and maintain algorithms.
Support escalations related to issues in algorithms.
Learn existing algorithms to provide support when required.
Prepare training material and sessions for stakeholders as part of the production release.
Skills & Qualifications:
Proficient in C#.
Experience with Python is an advantage.
Knowledge in drilling practices, and ability to speak to other drilling experts and capture requirements and logic.
Knowledge in drilling performance KPIs is an advantage.
Prior experience with WITSML is an advantage.
Function and communicate well in a team having diverse expertise.
Bi-lingual (English and Spanish) preferred.
At least 2 years’ relevant experience.
Benefits:
Health insurance
Dental insurance
401(k) with matching
Health savings account
Life insurance
Long-term disability
This job description is intended to explain the general nature and level of work being performed. Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Applicants must be legally authorized to work in the United States for any employer.
To apply for this position, applicants must meet the above criteria.
Interested applicants fulfilling the above criteria, please send your resume to careers.usa@petrolink.com.","$81,133 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"CEDENT
5.0",5.0,"Phoenix, AZ",Big Data Support Engineer for cloud migration project-Remote,"Title: Big Data Support Engineer for cloud migration project @ Phoenix, AZ or Remote
Terms of Hire: 12 months.
Salary: $ Open K/ YR + Benefits.
Job Summary

Years of Experience: 5 – 7
Bachelor’s Degree: Computer Science or engineering
Start Date: ASAP
Interview Type: Video

Job duties:
Big Data Engineer position for cloud migration project
Writing pyspark & hive code to build use case.
Design solution
Troubleshoot production issue
Discussing business requirement with product team
Write and Execute test case.
Skill Requirements:
5 to 7 years of experience in big data space
Should be good in Hadoop concepts and technology
Hands on working experience on Pig, Hive, Sqoop, Spark, Python, Shell scripting.
Good understanding of Data processing using big data tools.
Experience in optimizing Hadoop jobs and spark jobs
Experience and knowledge of building REST web services.
AWS cloud experience is must
Programming experience in UNIX and Python
Good communication skill
SHOULD HAVE EXPERIENCE WITH OOZIE SCHEDULING
SHOULD BE FAMILIAR WITH AGILE METHODOLOGY
SHOULD HAVE STRONG HOLD ON SQL QUERIES
SHOULD BE AWARE OF USING GIT REPO AN D RESPECTIVE COMMANDS

You Will Enjoy:
An opportunity to be a part of a great culture, an awesome team, a challenging work environment, and some fun along the way!
Apply today to learn more and be part of our Growth story.
All applications will be kept strictly confidential and once shortlisted, our team will be in touch with you for further discussions.","$81,771 /yr (est.)",1 to 50 Employees,Contract,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
"Nisum
4.0",4.0,Remote,Data Engineer GB4790,"Location: Remote, USA
Team: Data Science & Analytics
Work Type: Full Time
Nisum is a leading global digital commerce firm headquartered in California, with services spanning digital strategy and transformation, insights and analytics, blockchain, business agility, and custom software development. Founded in 2000 with the customer-centric motto “Building Success Together®,” Nisum has grown to over 1,800 professionals across the United States, Chile,Colombia, India, Pakistan and Canada. A preferred advisor to leading Fortune 500 brands, Nisum enables clients to achieve direct business growth by building the advanced technology they need to reach end customers in today’s world, with immersive and seamless experiences across digital and physical channels.

What You'll Do
Defines, designs, develops, and test software components/applications using Spark, Sql, and PySpark.
Building solutions using a variety of open-source tools Microsoft Azure services and a proven track record in delivering high-quality work to tight deadlines.
Ability to work with the customer as part of the Agile model of delivery.
Design and Build Modern Data Pipelines and maintain data warehouse schematics, layouts, architectures, and relational/non-relational databases for data access and Advanced Analytics
What You Know
Hands-on distributed computing development experience using PySpark, Spark SQL, and Databricks delta tables.
Should have experience in structured streaming, stateless, and state full.
Familiarity with Azure resources (AKV, Managed identity, SPN, ADLS, etc..).
Must have good craftsmanship skills including unit testing, and code quality, and be a creative thinker/problem solver.
Experience in CICD and operation tooling integrations is a plus.
#Li-Remote
Education
Bachelor’s degree in Computer Science, Information Systems, Engineering, Computer Applications, or related field
Benefits
In addition to competitive salaries and benefits packages, Nisum US offers its employees some unique and fun extras:
Professional Development - We offer in-house technical training and professional learning programs aimed at developing skills across a broad spectrum of topics such as technology, leadership, role-based training, and process expertise. We also offer an annual stipend for employees to attend external courses in order to maintain professional certifications
Health & Wellness Benefits - We believe that your health and welfare are important, and we strive to ensure that you have affordable options available to you, including some plans that are subsidized for employees and their families by up to 90%. We also have dental and vision plans in the US where Nisum pays 100% of premiums for employees
Volunteerism Pay - We believe in giving back and in the US, our employees are eligible for up to 40 hours of paid time off each year to volunteer towards the causes that they are most passionate about. This is in addition to personal PTO and paid holidays
Additional Benefits - We offer all the other important benefits to keep employees and their families healthy and financially secure, such as 401(k) retirement savings with a company match, pre-tax parking and transit programs, disability insurance, and Basic Life/AD&D, alongside exclusive employee discounts on a wide variety of products and services
Compensation Band
$120-125k per year
Nisum is an Equal Opportunity Employer and we are proud of our ongoing efforts to foster diversity and inclusion in the workplace.","$122,500 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,Unknown / Non-Applicable
WellTrust Medical Group,#N/A,"Charlotte, NC",Data Engineer,"The Data Engineer will assist the senior analyst in complex modeling and projects to ensure their completion within the allotted timeline. Helps track measures of success and develop reports that measure operational and customer metrics. Demonstrates ability to troubleshoot data consumption and data quality problems.

Responsibilities:
Build and maintain reports using My SQL Server
Identify optimal data sources, structure data validation, and cache optimization.
Performs business problem modeling and quantitative analytics including regression analysis, time-series, hierarchical cluster analysis, and data mining.
Helps track measures of success and develop reports that measure operational and customer metrics.
Demonstrates ability to troubleshoot data consumption and data quality problems.
Evaluates and recommend improvements to current models, metrics, and analytic processes to improve operational efficiency and insight.
Design, document, and communicate effective technical solutions meeting end-user requirements
Perform analyses with large amounts of complex data, and uses data visualization tools to communicate results.
Requirements:

Bachelor's degree or equivalent experience in a quantitative field (Mathematics, Statistics, Computer Science, Economics, Finance, Physics, Engineering)
3+ years of professional working experience as a Data Engineer or Analyst
1-3 years of data warehouse experience
Build and maintain reports using My SQL Server
Strong analytical and creative problem-solving skills
Design, document, and communicate effective technical solutions meeting end-user
SQL and Excel Macro development
Strong understanding of various data environments such as SQL, Oracle, Big Query, etc.
Experience with data visualization tools such as Looker Studio, Tableau, D3.js, etc.
Proficient in R, Python, MATLAB, Java, or another programming language.",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"shaped.ai Inc.
4.0",4.0,"New York, NY",Data Engineer,"Hello! We’re building the future of content discovery infrastructure with AI — come join us!

Shaped (YC W22) is an API for developers to seamlessly add personalized ranking and recommendation to their products. These frictionless discovery experiences help end-users find what they want faster, and, consequently, grow conversion and engagement business metrics. Behind the scenes, we’re building state-of-the-art ML infrastructure for training and deploying large AI models to power our personalization engine.
We’re a seed stage start-up backed by top-tier investors (e.g. Y-Combinator, Susa Ventures, Tribe Capital etc…) and executives (e.g. Google, Amazon, Uber, Dropbox etc…). Our team comes from Meta, Google, Apple and Uber. We’re a remote team but have a small office in Brooklyn, New York.
We’ve been lucky to have a huge amount of success since our initial launch earlier this year and now looking to expand our real-time data engineering capabilities. We are looking for a data engineer to design, build and optimize Shaped's real-time streaming data ingestion endpoints. You will be a founding engineer working on state-of-the-art infrastructure. As one of Shaped’s early employees you will help shape our product, culture and vision.
This role is for a full-time data engineer at $120k - $200k a year and 0.1 - 0.5% equity.

Responsibilities:
Manage the unification of our batch and real-time data processing pipelines.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Work on our real-time and batch data ingestion pipelines using modern data engineering pipelines/streams.
Work closely with a small team to inform the roadmap and co-develop a strong engineering culture with best practices.
Requirements:
Bachelor's in computer science, data science or mathematics related field. Master's degree or PhD will be advantageous.
5+ years of data engineering experience, ideally applied to machine learning use-cases.
Experience with AWS or common cloud technologies
Proficient with Python, SQL.
Extensive knowledge of both offline and online large scale data processing frameworks: Spark, Beam, Flink.
Extensive knowledge with stream technologies like Kakfa or Kinesis.
Excellent written and verbal communication skills.
We’re excited to work with you. Come build the future of AI and discovery with us!","$160,000 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Oak Ridge National Laboratory
3.6",3.6,"Oak Ridge, TN",Geospatial Data Engineer,"Requisition Id 7635
Overview:
The Geospatial Science and Human Security Division (GSHSD) is seeking a geospatial data engineer within the Location Intelligence group. This position is within the Division’s Human Dynamics section and part of the National Security Science Directorate. The Location Intelligence Group performs ground-breaking research into place-based knowledge extraction, narrative analysis, and geosocial data discovery. Leveraging primarily “non-traditional” geospatial data sources such as volunteered geographic information (VGI), internet of things (IoT) data, and telemetered sensor sources, and harmonizing with commercial and open data sources, the Location Intelligence group delivers novel approaches and technologies for describing places, points of interest, events, activities, and populations, as well as the patterns of interaction among the three. The group’s dynamic content portfolio addresses the timely need to broaden research in spatial data curation, multi-scale land-use modeling, generating higher-level features from ground photo imagery, disinformation detection, network spatialization, narrative analysis, and transformation to describe the landscape and patterns of activity within it, using novel and non-traditional data and theoretical techniques.

This position involves developing and deploying scalable geospatial applications supporting the group’s R&D portfolio. The position’s focus includes architecting geospatial services, working with the CI/CD deployment teams, performing stress testing of deployed services and applications, cloud-to-premise interfacing, and distributed computing to deploy real-time geospatial intelligence applications. The individual will collaborate with other scientists in developing transformative technologies that address these challenges through teaming and collaborative efforts. The work will involve harvesting and processing several hundred disparate data sources from all around the world. These sources contain text, sensors, ground-level imagery, social media, trajectory, and travel data, among others. Enabling this data for purposeful use requires designing and architecting RESTful services and distributed failover backend services. To confidently run them, individuals are required to have a good background in the following initiatives: Architect and design Spring boot and Java Application, open-source exploitation; geospatial OGC services architecture and development, Java and object-oriented concepts, application server failover and load balancing, and geospatial formats such as GeoJson and GeoPackage; and a working knowledge of GIS methodologies.

Our commitment to diversity:
As we strive to become the world’s premier research institution in the sciences and technologies that underpin critical national security missions, we are committed to creating an inclusive environment that highly values a diverse workforce. We recognize that breadth of perspectives, insights, and experiences is necessary to drive innovation and discovery mission-critical to national security sciences. Our commitment extends beyond our workforce to the next generation of researchers with STEM education outreach that seeks to engage a diverse range of students.

Major Duties and Responsibilities:
Architect services on NoSQL data (ElasticSearch), OGC GeoServer configuration, Java and Object-oriented application architecture fluency, Maven built Spring Boot, Docker, Kubernetes, CI/CD pipeline design and deployment.
Design and develop scalable and distributed data and backend services for GEOINT purposes.
Deployment and CI/CD and perform benchmark analysis and examination of results and logs from failover and robust service deployment.
Performing post-deployment logistics.
Examine new and developing technologies and tools to see if they can be used on the job.
Manage and update existing data format and perform regular improvements.
Augment existing research and development activities in location intelligence and association analysis.
Develop and support the plan for project data analysis and management.
As needed, provide technical assistance to other employees and customers.

Basic Requirements
Bachelor’s degree or higher in computer science, mathematics, or a related field and 2+ years of related experience.
Prior experience with J2EE application development, microservices, distributed Java Caching, regular expressions, web-based technologies.
Prior expertise with REST data services, APIs, and microservices as part of a service-oriented architecture (SOA).
Proficiency in open-source or commercial geospatial tools such as ArcGIS, QGIS, and PostGIS.
Prior expertise with relational (MS SQL Server, MySQL, PostgreSQL, and so on), document (JSON, ElasticSearch, and so on), spatial, graph, and other unstructured databases is required.
Experience with Python, R, and other similar languages for data manipulation, exploration, graph analysis, and statistical analysis.
Hands-on experience using cloud-enabled technologies and platforms that includes Google Cloud and AWS.
Excellent written and verbal communication and demonstrated ability to work in interdisciplinary teams.

Preferred Qualifications:
Agile software development approaches are a plus.
Working knowledge of both structured and unstructured data.
Working knowledge of version control systems such as Git.
Working knowledge of automated deployment and testing environments is a plus.
UML, Object-oriented programming, Linux, shell scripting, and other related skills are required.
Containerization technologies such as Docker and Conda have been used to design and manage computing environments.
Experience with geospatial ETL process design and implementation
Experience with database technologies such as MySQL and ElasticSearch to store, analyze, and manipulate data.
Experience in statistics, computational sciences, activity and event data, and working with social media data.

This position will remain open for a minimum of 5 days after which it will close when a qualified candidate is identified and/or hired.
We accept Word (.doc, .docx), Adobe (unsecured .pdf), Rich Text Format (.rtf), and HTML (.htm, .html) up to 5MB in size. Resumes from third party vendors will not be accepted; these resumes will be deleted and the candidates submitted will not be considered for employment.

If you have trouble applying for a position, please email ORNLRecruiting@ornl.gov.

ORNL is an equal opportunity employer. All qualified applicants, including individuals with disabilities and protected veterans, are encouraged to apply. UT-Battelle is an E-Verify employer.","$77,071 /yr (est.)",5001 to 10000 Employees,Government,Government & Public Administration,National Agencies,1943,Unknown / Non-Applicable
United Digestive,#N/A,"Atlanta, GA",Data Engineer,"GENERAL SUMMARY OF DUTIES: United Digestive’s Data Engineer will be responsible for developing and maintaining data pipelines, data warehouses, and data models to support the company's Business Intelligence reporting initiatives.
REPORTS TO: Director of Business Intelligence & Analytics
RESPONSIBILITIES:
Duties may include but are not limited to:
Implement, maintain, and improve data integration processes to collect, process, and manage large datasets
Collaborate with stakeholders to understand business requirements and design data solutions that meet their needs
Implement and maintain data quality checks and validations to ensure data accuracy and completeness
Optimize data processing and storage to ensure high availability, scalability, and performance
Develop and maintain automated data workflows and scheduling tools
Code ETL process using Azure Data Factory workflows and other technologies as appropriate
Troubleshoot and resolve data-related issues and incidents
Stay up to date with the latest technologies and trends in data engineering, specifically the Azure suite, and recommend improvements and optimizations to the data architecture and infrastructure
Ability to develop solutions and manage existing Azure DevOps pipelines
Maintains a high level of confidentiality with sensitive information
Performs any other duties and/or special tasks as assigned by United Digestive leadership
REQUIRED EDUCATION, SKILLS & EXPERIENCE:
Bachelor’s degree in Computer Science, Software Engineering, Information Technology, or in a related field required; healthcare-related Data Analytics experience preferred; 2+ years of data pipeline management required. Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL); experience with data modeling, data warehousing, and ETL processes, proficiency in various programming language (e.g., Python, SQL, Java), experience with cloud platforms (e.g., AWS, Azure, GCP) and big data technologies (e.g., Databricks, Hadoop, Spark, Kafka, Cassandra). Must have knowledge of data visualization and BI tools (e.g., Tableau, Power BI) and how to support these tools in an efficient manner.
ADDITIONAL SKILLS AND EXPERIENCE:
Data Engineer must have:
Excellent verbal and written communication skills.
Excellent organizational skills and attention to detail.
Ability to multi-task and prioritize job needs
Ability to deal with patients, visitors, co-workers, and physicians with courtesy and respect
Work under pressure; assess, respond to, and communicate issues in a timely manner.
Self-motivation and be a proactive individual with a professional can-do attitude
PHYSICAL/MENTAL/ENVIRONMENTAL DEMANDS
Requires sitting and standing associated with a normal office environment. Travel as business needs dictate.
DRUG FREE WORKPLACE
United Digestive is a drug free workplace. All offers of employment are contingent upon passing a pre-employment drug screening.
EQUAL OPPORTUNITY EMPLOYER
United Digestive is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, gender, color, or national origin in its employment practices.
Location: 1355 Peachtree Street NE STE 1600, Atlanta, GA 30309
Job Type: Full-time",#N/A,Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Farm Credit East
3.9",3.9,"Enfield, CT",Data Engineer,"Be part of a team focused on the success of our customers, the success of our communities, and the success of each other. Farm Credit East is the leading provider of loans and farm advisory services to farm, forest product, fishing, and other agricultural business owners across the northeast. We are One Team Working Together with a focus on our five pillars: Outstanding Customer and Employee Experience, Quality Growth, Operational Excellence, Commitment to our Communities, and Protecting Customer Information.

Position Summary
The Data Engineer is responsible for cleaning, managing, and sharing data that guides business decisions. Using ETL tools you will gather data from a variety of sources, checking for anomalies, automating processes, and generally making it easier for business stakeholders to generate valuable insights. This position will collaborate with internal and external organization to capture requirements, design, create, document, manage, and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.

Duties and Responsibilities
Work with product stakeholders to implement, maintain, and enhance data models and solutions used to define and measure quality of data domains.
Design data models to meet requirements
Perform ETL (Extract, Transform, and Load) on data to meet stakeholder specifications.
Design and develop data access methods, datasets, views etc.
Develops data modeling and is responsible for data acquisition, access analysis, archive, recovery, load design and implementation.
Coordinates new data developments to ensure consistency with existing warehouse structure.
Collaborates with internal customers to capture requirements, design, create, document, manage and fulfill requests for on-going and/or ad-hoc reports, dashboards, and scorecards.
Assists with the development, implementation, and maintenance of front-end presentation (dashboards), automated report solutions and other BI solutions to support tactical and strategic reporting needs of the organization.
Assists in identification of data integrity problems and recommends solutions.
Work collaboratively with key stakeholders both internally and externally, including but not limited to Senior Management, Business Unit Leaders, Knowledge Exchange, and Farm Credit Financial Partners (FPI).

Job Qualifications/Requirements
Bachelor’s Degree in Computer Science, Business, Finance, or other related field from an accredited University.
Experience with MSFT SQL Server
Microsoft Azure (Data Bricks, Data Factory, Logic Apps, Functions, etc.)
2 plus years of experience in Finance related informatics, performance measurement, or analysis with strong relational database SQL skills.
1 + years of experience using Microsoft Azure product to perform ETL
Familiar with Databricks Unity catalog

Farm Credit East is an Equal Opportunity Employer. As an Equal Opportunity Employer, we do not discriminate on the basis of race, color, religion, national origin, sex, sexual orientation, gender identity or expression, age, marital status, parental status, political affiliation, disability status, protected veteran status, genetic information or any other status protected by federal, state or local law. It is our goal to make employment decisions that further the principle of equal employment opportunity by utilizing objective standards based upon an individual's qualifications for a specific job opening. In compliance with the Americans with Disabilities Act (“ADA”), if you have a disability and would like a reasonable accommodation in order to apply for a position with Farm Credit East, please call 1-800-562-2235 or e-mail FarmCreditCareers@farmcrediteast.com .","$86,685 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Banking & Lending,1916,$100 to $500 million (USD)
"Macy’s
3.4",3.4,"Johns Creek, GA",Lead Data Engineer,"Macy's, Inc is building an Enterprise Data & Analytics team to further grow our capabilities in support of our mission to be a data - led, customer centric company. This team will focus on accelerating impact from analytics, coordinating an enterprise-wide roadmap, and ensuring proper data governance and management. As a member of this team, the engineer will help lead the charge to execute on our vision to build profitable lifetime customer relationships by embedding data & analytics at the heart of everything we do.
Position Overview:
The Lead Data Engineer is responsible for development and support of data products on a modern cloud-based data lake, leveraging expertise and knowledge of multiple technologies & data domains to help build a robust, scalable, and reliable data engineering platform.
The Lead Data Engineer is responsible for providing data services for enterprise-grade analytical environments, utilizing automated data pipelines at scale, and streamlining efficient data transformations for priority use cases, be involved hands on in development of the codebase and partner closely with business units and peer technology groups to support analytics execution.
Essential Functions
Solution Design & Implementation:

Work closely with business stakeholders, implement scalable solutions to meet requirements
Follow and improve existing processes and procedures
Lead a pod of data engineers, providing both technical oversight and supporting their growth
Build, maintain and simplify enterprise data pipelines with emphasis on reusability & data quality
Work with Legal and Privacy teams to adhere to data privacy and security requirements

Culture:
Train and mentor fellow engineers on both technical stack and data domain specifics
Establish a pro-active approach to data management, ensuring business stakeholders & platforms can access required data within the SLA window
Drive change management to increase user adoption of enterprise data repositories and leverage standardized data pipelines across use cases
Increase agility in identifying data issues and taking action to remediate

Qualifications
Education/ Experience:

Data engineering experience with:
3+ years of experience in designing and implementing cloud-based data solutions
3+ years of experience integrating with analytics reporting solutions (e.g. Tablaeu, PowerBI)
3+ experience with big data processing technologies such as Hadoop, Spark, etc.
5+ years of experience building & automating ETL data pipelines using enterprise grade tools
5+ years of experience building enterprise-grade data warehouses (either on-prem or on cloud)
8+ years of overall programming experience, including recent experience with Python & SQL
Ability to effectively share technical information, communicate technical issues and solutions to all levels of business stakeholders
Customer-centric and experienced with cross-functional collaboration
Excellent written and verbal communication skills

What we can offer you:

Exciting, challenging problems to solve - you'll never have a boring day at work
A refreshingly fun work environment where you will collaborate with a smart and talented team
Unique freedom to build and lead a team in next gen thinking
A chance to learn and participate in the growth of NYC’s largest retailer

This job description is not all inclusive. Macy’s Inc. reserves the right to amend this job description at any time. Macy's Inc. is an Equal Opportunity Employer, committed to a diverse and inclusive work environment.","$161,600 /yr (est.)",10000+ Employees,Company - Public,Retail & Wholesale,"Department, Clothing & Shoe Stores",1858,$10+ billion (USD)
"Shutterfly
3.3",3.3,"Tempe, AZ",Senior Data Engineer,"Description
At Shutterfly, we make life’s experiences unforgettable. We believe there is extraordinary power in the self-expression. That’s why our family of brands helps customers create products and capture moments that reflect who they uniquely are.

As part of the Data Engineering team you will tackle the scalability, performance and distributed computing challenges needed to collect, process and store data for Shutterfly. You will enhance the Data Warehouse at Shutterfly on AWS, supporting the breadth and depth of the company’s analytic needs.
What You'll Do as a Sr. Data Engineer II
Build design, develop, test, deploy, maintain and enhance full-stack data engineering solutions
Provide technical leadership to the Data Warehouse and Shutterfly teams who use Shutterfly’s Enterprise Data Warehouse
With your technical expertise own and manage project priorities, deadlines and deliverables
Evangelize the benefits of existing solutions and new technologies to drive the use and push the technology of the Data Warehouse forward
Work closely with Data Operations to improve CI/CD pipelines am improve the operation and performance of the Data Warehouse
The Skills You'll Bring
Expert knowledge Python, Spark, SQL scripting and Java
7+ years of hands-on experience with building data ingress frameworks and experience with dealing with a wide variety of source data formats
Hands-on experience in building data applications, including design, implementation, debugging, and support.
Supporting a diverse and inclusive workforce is important to Shutterfly not only because it directly reflects our value of Embracing our Differences, but also because it’s the right thing to do for our business and for our people. Learn more about our commitment to Diversity, Equity and Inclusion at Shutterfly DE&I.","$135,965 /yr (est.)",10000+ Employees,Company - Private,Information Technology,Internet & Web Services,1999,$1 to $5 billion (USD)
"Fathom Management LLC
2.0",2.0,Remote,Sr. Data Engineer Remote Opportunity,"Sr. Data Engineer

seeking a Senior Data Engineer who possesses expert level knowledge of appropriate data sources to address the specific requirements of projects for data modeling. Understand business requirements and translating into technical work. Design and implement features in collaboration with team engineers, product owners, data analysts, business partners using Agile/SCRUM Methodology.

This is a full- time position / 100% Remote.
The salary range of $140,000 - $160,000 will be based on technical experience and technical interview.

Responsibilities:

Ability to build programs or systems that can take data and turn it into meaningful information that can be studied.
Build ETL/ELT jobs and workflows to combine data from disparate sources.
Install continuous pipelines of huge pools of filtered information so that data analyst/scientists can pull relevant data sets for their analyses.
Build data workflows using SQL Server Integration Services (SSIS)
Build data workflows using Microsoft Azure (Azure Data Factory, Storage Accounts, Synapse)
Build data workflows using Databricks
Collaborate with business stakeholders, business operations, and product engineering teams in analyzing business problems, in building and testing solutions and data models.
Experience implementing and operating analytic models and services.
Document the current-state and target-state software architecture and create roadmap plans for success on various software components.
Assist in the design, implementation, and maintenance of complex solutions.
Build systems that collect, manage, and convert raw data into usable information for business analysts to interpret.
Make data accessible for evaluation and optimization
Collaborate with business stakeholders, business operations, and product engineering teams.
Coordinate activities with other technical personnel as appropriate.
Works with back-end data and develops tables using SQL scripts, SSIS, and SSMS.
Experience with Azure cloud platforms and Data bricks

Required Experience and Education:
Master's degree in computer science, systems engineering, or related technical discipline is preferred with 7-10 years of experience as a Data Engineer/Administrator or similar role. OR , B.S. in Computer Science with 15 years of relevant experience.

Benefits Overview: Full-time employees are offered comprehensive and competitive benefits package including paid vacation, sick leave, holidays, health insurance, life insurance, military leave, training, tuition reimbursement, a wellness program, short- and long-term disability, 401(k) retirement plan with company matches/immediate vesting, commuter benefits, and more.
COVID Policy: In accordance with the Federal Executive Order on Ensuring Adequate COVID Safety Protocols for Federal Contractors, this position requires that you are fully vaccinated at least 2 weeks before your start date. You will be required to provide proof of vaccination before you begin employment.
EEO Policy: It is our policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits, and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability, or any other characteristic protected by applicable federal, state or local law.","$150,000 /yr (est.)",1 to 50 Employees,Self-employed,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"YSI
3.7",3.7,Remote,Senior Data Engineer,"Position Title: Senior Data Engineer/ Oracle Apex Developer
Job Id: 202301002
Location: Herndon, VA (Remote)
Yakshna Solutions, Inc., (YSI) is a CMMI Level 3 assessed, ISO 9001, 20000:1, 27001 certified, woman-owned small business enterprises, headquartered in Herndon, Virginia, USA. YSI provides professional IT solutions and services to business corporations and government organizations. YSI is committed to serve its business communities as a leading IT vendor providing innovative, quality, and cost-effective IT business solutions and services.
Our benefits are very competitive that include 401(k), health, dental, and vision insurance, Life insurance, short-term and long-term disability insurance, paid time off, training, and professional development assistance.
YSI is seeking a highly qualified Senior Data Engineer. The selected candidate will be able to communicate effectively (written/verbal), possess strong interpersonal skills, be self-motivated, and be innovative in a fast-paced environment.
Responsibilities:
The Data Engineer will be the senior technical expert on work associated with data management, data quality and data structures, coordinating with the ADA as needed to ensure development and data structures are synchronized.
The Data Engineer will design the approach for data tasks and will contribute to completion of data tasks and oversee execution, providing advice and guidance as necessary to junior staff.
Required Qualifications and Skills:
Bachelors or master’s in relevant filed.
Good Data Engineering/Management experience
· Should be familiar with the the Civil Works missions for Hydropower, Recreation, Environmental Stewardship, and Water Supply.
· Extensive technical knowledge of programming in a software stack that includes an Oracle Relational database, SQL, PL/SQL, Oracle Spatial, JavaScript, Oracle REST Data Services (ORDS), and Oracle APEX to make the necessary revisions to the relevant systems. In addition to these general skills and experience, also possess the following:
· Knowledge and experience in developing and maintaining a relational database operated in Amazon Web Services (AWS cloud).
· Knowledge and experience of data entry and options to increase automation and efficiency.
· Knowledge and experience with documenting data systems and processes and creating reports for increased efficiencies.
· Knowledge and experience in optimizing the performance of existing Oracle based applications, including procedures, functions, etc.
· Knowledge and experience with creating Representational State Transfer (RESTful) services utilizing common data dissemination formats.
· Knowledge and experience with making websites 508 compliant.
· Knowledge and experience with authentication and authorization procedures
· Knowledge and experience with maintaining geospatial data in oracle relational database.
· Knowledge and experience with geospatial web services (OGC & ESRI REST)
· Knowledge and experience with cloud native development methodologies
Job Types: Full-time, Contract
Pay: $110,000.00 - $120,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Professional development assistance
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Herndon, VA 20170: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Preferred)
Experience:
Oracle Apex: 5 years (Preferred)
AWS CLOUD: 5 years (Preferred)
Erwin: 8 years (Preferred)
Data modeling: 8 years (Preferred)
Metadata: 5 years (Preferred)
Work Location: Remote","$115,000 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Machinery Manufacturing,2011,$1 to $5 billion (USD)
Seamless Migration LLC,#N/A,Virginia,Data Engineer (Full Scope Poly),"ABOUT US
Seamless Migration is a Service-Disabled Veteran-Owned Small Business (SDVOSB) started in 2021 with the purpose of enabling businesses and organizations through automation. Our goal is to help organizations discover, implement, and maintain solutions which evolve and mature with their ever-changing business needs. We believe in applying agile methodologies in all aspects of our business practices and use these methods to ensure effective results for our clients.
US Citizenship (Required)
CLEARANCE
TS/SCI W/ Full Scope Poly
LOCATION
Northern, VA
OVERVIEW
The Sponsor seeks a Data Engineer who will be responsible for designing, developing, testing, and maintaining data infrastructure projects such as data warehouses, data lakes, ETL pipelines, and data governance frameworks.
The goal of this position is to ensure that our organization processes and manages data efficiently and accurately to support data-driven decision-making.
Key Responsibilities include but are not limited to:
Design, develop and maintain data architectures, pipelines, and data integration solutions.
Collaborate with data analysts, data scientists, and other stakeholders to understand their data requirements.
Implement and manage data quality and data governance initiatives to improve data accuracy, completeness and consistency.
Work with our DevOps and IT teams to ensure sufficient infrastructure and access to required data sources.
Monitor data infrastructure and perform necessary fixes or performance tuning.
Ensure data security and privacy are maintained.
Stay up to date with industry trends and technologies related to data engineering and recommend new approaches to improve data processing capabilities.
REQUIRED SKILLS
Proven experience as a Data Engineer or similar role.
Deep knowledge of data modeling, ETL pipelines, and data warehousing concepts.
Proficiency in big data technologies such as Hadoop or Spark.
Proficiency in programming languages such as Python or Java.
Familiarity with data visualization tools such as Tableau or Power BI.
Experience with building and managing data pipelines using cloud services such as AWS or Azure.
Knowledge of relational database systems and SQL.
DESIRED SKILLS
Bachelor's degree in computer science, computer engineering, or related field.
2+ years of experience in a data engineering or related role.
Experience with data modeling, ETL, and data warehousing.
Excellent analytical and problem-solving skills.
Strong communication and collaboration skills.
BENEFITS
6% 401k match (100% vested)
100% paid Medical, Dental, Vision
$3,000 HSA contribution
28 days' vacation (PTO, Federal Holidays, and Sick Leave)
Flexible Hours
Tuition and certification reimbursement
Growth opportunities within an emerging defense company
All your information will be kept confidential according to EEO guidelines","$187,500 /yr (est.)",Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"JPMorgan Chase Bank, N.A.
3.8",3.8,"Atlanta, GA",Lead Software Engineer (Data),"We have an opportunity to impact your career and provide an adventure where you can push the limits of what's possible.

Job summary

As a Lead Software Engineer at JPMorgan Chase, you are an integral part of an agile team that works to enhance, build, and deliver trusted market-leading technology products in a secure, stable, and scalable way. As a core technical contributor, you are responsible for conducting critical technology solutions across multiple technical areas within various business functions in support of the firm's business objectives.

Job responsibilities
Executes creative data solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems
Develops secure and high-quality production code, and reviews and debugs code written by others
Identifies opportunities to eliminate or automate remediation of recurring issues to improve overall operational stability of software applications and systems
Leads evaluation sessions with external vendors, startups, and internal teams to drive outcomes-oriented probing of architectural designs, technical credentials, and applicability for use within existing systems and information architecture
Leads communities of practice across Software Engineering to drive awareness and use of new and leading-edge technologies
Adds to team culture of diversity, equity, inclusion, and respect

Required qualifications, capabilities, and skills
Formal training or certification on software engineering concepts and 5+ years applied experience
Hands-on practical experience delivering system design, application development, testing, and operational stability
Advanced in both hands on development and solutioning
Proficiency in AWS, Hadoop, Snowflake, Glue, Lake formation, etc.
Proficient in all aspects of data (migration, modernization, building platforms, etc.)
Advanced understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security
In-depth knowledge of the financial services industry and their IT systems
Practical cloud native experience

Preferred qualifications, capabilities, and skills
Experience in financial services industry, specifically Wealth Management
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.
Equal Opportunity Employer/Disability/Veterans","$117,975 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Banking & Lending,1799,$10+ billion (USD)
"Apple
4.2",4.2,"Cupertino, CA",Biomedical Data Engineer - Health Technologies,"Summary
Posted: Aug 8, 2022
Weekly Hours: 40
Role Number:200402289
The Health Technologies Team conceives and proves out innovative technology for Apple’s future products and features in health. We are seeking a highly capable Biomedical Data Engineer to join a multi-disciplinary team. Successful candidates will be able to integrate with our research study leads, data scientists and engineers to develop and support effective data analysis and machine learning workflows.
Key Qualifications
Experience with software engineering frameworks
Excellent coding skills in Python (e.g.,Pandas, Spark, Jupyter)
Workflow orchestrations (e.g., Airflow, Luigi)
Designing and maintaining (non-)relational databases (e.g. Postgres, Cassandra, MongoDB) and file systems (e.g. Parquet, CSV, JSON)
Great understanding of infrastructure designs
Linux, MacOS based development frameworks
iOS/ watchOS development (e.g., Swift, Objective-C)
Web Service APIs (e.g., AWS, REDCap, XNAT)
Version control frameworks (Git, virtualenv)
Familiarity with best practices for information security, including safe harbor privacy principles for sensitive data
Experience with biomedical sensors/platforms for measuring physiological signals in the health, wellness and/or fitness realms
Description
- Work closely with team members and study staff to design, build, launch and maintain systems for storing, aggregating and analyzing large amounts of data - Process, troubleshoot, and clean incoming data from human studies - Automate and monitor data ingestion and transformation pipelines, with hooks for QA, auditing, redaction and compliance checks per data management specifications - Create and maintain databases with existing and incoming clinical data - Architect data models and create tools to harmonize disparate data sources - Incorporate and comply with regulations as they pertain to electronic and clinical data and databases
Education & Experience
BS/MS in Computer Science, Engineering, Informatics, or equivalent with relevant 4+ years industry experience with biomedical, health or sensitive data.
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $104,000 and $190,000 annualized, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",#N/A,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1976,$10+ billion (USD)
"Introba Inc.
3.0",3.0,"New Haven, CT","Physical Security Engineer, Data Centers (Embedded)","WHERE PASSION + PURPOSE ALIGN
Introba is the union of Integral Group, a global network of deep green engineers and consultants and Ross & Baruzzini, a technology, consulting, and engineering firm.

Together as Introba, the organization provides world-class building engineering design, analytic, and consulting services at all scales, specializing in net-zero first thinking. Through the cultivation of thought leadership, we deliver sustainable and forward-thinking solutions to the most complex design challenges facing the world’s leading clients and partners.
Job Summary
Introba is the union of Integral Group, a global network of deep green engineers and consultants and Ross & Baruzzini, a technology, consulting, and engineering firm.

Together as Introba, the organization provides world-class building engineering design, analytic, and consulting services at all scales, specializing in net-zero first thinking. Through the cultivation of thought leadership, we deliver sustainable and forward-thinking solutions to the most complex design challenges facing the world’s leading clients and partners.

We are currently seeking an embedded Physical Security Engineer (remote position) to join our team.

Our security engineers are responsible for the design and engineering of electronic security systems, product research, development of design guidelines, and creation of design documents. The embedded security engineer role is a full-time position dedicated to one of our most prestigious clients.

While the successful candidate will be an Introba employee, the entirety of their daily responsibilities will be to provide engineering services for data center related security tasks for our client. These data centers help support the way we work, live, and communicate, and are located all over the world.
Responsibilities & Qualifications
The embedded security engineer will work with our client security team to perform the following:
Review existing security system design standards and develop recommendations for improvements to the current security program
Evaluate new equipment and technologies related to security.
Develop proof-of-concept documents for new equipment and technologies.
Manage master security detail libraries in partnership with outside security consulting firms
Manage system wiring diagrams and develop new standards for security design packages
Coordinate new technology design requirements with existing design document standards
Provide routine updates and improve upon master security specifications (CSI formatted Division 28 specifications)
Minimum of 10 years of experience in the design or installation of electronic security systems
Understanding of construction documentation
Understanding of computer networks and telecommunications systems
Good documentation and technical writing skills
Familiarity with Microsoft Office products, including Microsoft Teams
Proficiency in the use of computer-aided design software platforms (Revit and AutoCAD) considered strongly beneficial
Experience designing, installing, or coordinating security systems specifically for data centers considered strongly beneficial
Education Requirements

BS in electrical engineering, computer engineering, or computer science extremely beneficial, though strong technical experience in the security industry may be considered to meet this requirement

Additional Information
Equal Opportunity Employer

Introba is an equal opportunity employer, and we prohibit discrimination and harassment of any type as protected by federal, state, or local laws. We celebrate diversity and are committed to creating an inclusive environment for all employees.

Introba is an equal opportunity employer, and we prohibit discrimination and harassment of any type as protected by federal, state, or local laws. We celebrate diversity and are committed to creating an inclusive environment for all employees. The Company and its employees are required to comply with all local health authority, legal or lawful client requirements. You should ask your manager about this prior to starting, should you be subject to the current or potential future requirements.
We offer a competitive remuneration package, wellbeing benefits and an Employee Assistance Program to provide mental wellbeing support, guidance and advice. Our hybrid working patterns and flexible hours help our people to achieve a healthy work-life balance.","$111,898 /yr (est.)",1001 to 5000 Employees,Company - Private,"Construction, Repair & Maintenance Services",Architectural & Engineering Services,#N/A,Unknown / Non-Applicable
"Koch Ag & Energy Solutions
3.8",3.8,"Wichita, KS",Project Data Engineer / Analyst,"Description
Koch Ag & Energy Solutions (KAES) is looking for a Project Data Engineer/Analyst who will be a primary point of contact for project cost forecasting, change management, and risk management on small to large capital construction projects and turnarounds. This opportunity utilizes data from multiple sources to extract insights and perform analysis.
Our Team

We support cross site, cross functional teams throughout the project lifecycle on projects and turnarounds across (up to) 6 plants throughout the US and a site in Canada.
This position works a Monday – Friday, 8-hour day, and is based out of Wichita, KS with expected travel about 25% of the time.
What You Will Do
Use critical thinking, analysis, curiosity, and collaboration throughout the following to enable cost competitive project delivery (but not limited to) functions:
Analyze per project actual cost, schedule performance, and estimate details to forecast final cost per project at the cost breakdown structure level.
Track project progress against the baselines (scope, cost, and schedule).
Partner with project teams to identify issues and risks early and develop proactive resolutions and mitigations.
Monitor purchase order commitments, manage change orders, and adjust forecasts to align with reality.
Prepare various charts, tables and reports with insights to understand and communicate the forecast including what has changed and why.
Works with team members in assessing data store solutions that supports dashboards, insights, and other analytical solutions.
Demonstrate initiative to improve project outcomes, data integrations, processes, and forecasting quality.
Share knowledge and solve problems across teams to improve project results across KAES.
Work around the field construction sites without assistance.


Who You Are (Basic Qualifications)
Minimum of 2 years of experience in Projects within construction/oil & gas/manufacturing industry OR Bachelor’s Degree in a Business Administration or Engineering field
Demonstrated ability to analyze trends/variances and determine root causes
Experience creating and maintaining automated reporting processes
What Will Put You Ahead
Experience on projects and/or turnarounds in an industrial or construction environment.
Experience with reporting and analytics software such as Alteryx, Tableau, PowerBI, etc.
Experience with project cost management (including forecasting, change management, WBS/CBS buildup, risk management, reporting, etc.)
Familiarity with Project Management and Project Controls tools (e.g. EcoSys, Maximo, P6 or similar project portfolio or scheduling tools).
Experience performing risk based contingency assessments and identifying key drivers of risks.
Experience influencing change across an organization.


Relocation may apply based on candidate.

At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate’s knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.
Hiring Philosophy
All Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.
Who We Are

As a Koch company, Koch Ag & Energy Solutions (KAES) is a global provider of value-added solutions for the agriculture, turf and ornamental, energy and chemical markets. From agriculture to energy, KAES makes things grow better with plant nutrient and biological technologies. Our team of innovators unleash their potential while developing the technologies that feed and power the world.
At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.
Our Benefits
Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.
Equal Opportunities
Equal Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please visit the following website for additional information: http://www.kochcareers.com/doc/Everify.pdf","$81,180 /yr (est.)",10000+ Employees,Company - Private,"Energy, Mining & Utilities",Energy & Utilities,1940,$10+ billion (USD)
"Capital One
4.2",4.2,"Richmond, VA",Senior Data Engineer - Principal Associate,"West Creek 2 (12072), United States of America, Richmond, Virginia
Senior Data Engineer - Principal Associate
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance

Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Spark, SQL, Python, Scala or Java
2+ years of experience with a public cloud (AWS)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
No agencies please. Capital One is an Equal Opportunity Employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, physical and mental disability, genetic information, marital status, sexual orientation, gender identity/assignment, citizenship, pregnancy or maternity, protected veteran status, or any other status prohibited by applicable national, federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",#N/A,10000+ Employees,Company - Public,Financial Services,Banking & Lending,1994,$10+ billion (USD)
Redzara.us,#N/A,"Atlanta, GA",Azure Data Engineer,"Responsible for developing new and modernizing existing applications to Azure Cloud Platform.
The ideal candidate must demonstrate in-depth knowledge and understanding of Database concepts.
Requirements:
7+ years of experience in Data Warehousing and Business Intelligence
5+ Years in BI development leveraging the Microsoft SQL Server. Needs to be an expert in T-SQL
5+ Years in SSIS and use of Stored Procedures.
1-2 Years in Azure Data Factory, SQL VMs, build and maintain ETL
Ingest and aggregate data from both internal and external data sources to build our data lake
Strong understanding data modeling (i.e., conceptual, logical, and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Job Type: Contract
Pay: $43.49 - $57.00 per hour
Compensation package:
1099 contract
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Atlanta, GA 30334: Reliably commute or planning to relocate before starting work (Required)
Experience:
Azure Data Factory: 3 years (Required)
Azure: 3 years (Required)
Data warehouse: 1 year (Preferred)
Work Location: Hybrid remote in Atlanta, GA 30334",$50.25 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Contact Government Services, LLC
4.7",4.7,"Washington, DC",Data Engineer,"CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our Federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges. As a Data Engineer, you will be integral to data operations for the development and integration of multiple data types across a range of data sets and sources.

You will be responsible for the day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner. Processing data will include managing, manipulating, storing and parsing data in a data pipeline for variety of target sources. You will also support maintenance of applications and tools that reside on these systems such as upgrades, patches, configuration changes, etc. The work is performed in a multidisciplinary team environment using agile methodologies. The candidate we seek must be highly motivated and enthusiastic about implementing new technologies and learning about new data in a small team environment where deadlines are important.
Responsibilities:
Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists
Required Skills:
Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models.
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories
Contact Government Services strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting edge technology with world class personnel to deliver customized solutions that fit specific needs and solve the most challenging, dynamic problems. Our team emphasizes technical expertise and an empathy to public service to provide unique top tier support for our government engagements.
SOME BENEFITS OF THIS POSITION INCLUDE:
Health, Dental, & Vision insurance
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off & Observance of State/Federal Holidays
For more information about CGS please visit: https://www.cgsfederal.com
39U0b7kJu8","$103,213 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Loyal
3.9",3.9,"Atlanta, GA",Senior Cloud Data Operations Engineer,"Loyal is an organization centered on experience and building a platform that allows consumers to make meaningful decisions when it comes to healthcare. We deeply understand providers, locations, services, appointments, business rules, and moreover, we understand patients - who they are, the preferred method of communication, upcoming appointments, lapsed appointments, outstanding bills, health risks, and more. With this intelligence, our platform fuels highly relevant and personalized experiences across all mediums (website, email, voice…) allowing patients to get healthy, stay healthy, and have a better relationship with the health care provider.
**This is a remote role**
Summary
The Senior Cloud Data Operations Engineer is responsible for delivering Loyal's cloud-based data warehouse. They interface with groups across the organization to ensure we are collecting and storing data in a way that makes it easy to create and consume. They identify best practices for big data management, select the right tooling, create a workable data architecture, and drive an implementation roadmap others can depend on. The Senior Cloud Data Operations Engineer also works closely with our security group to ensure data security and privacy. They also mentor others on the team, providing technical leadership to those they work with.
You will
Create data streams and consolidate data sources into a single cloud-based data warehouse
Architect and catalog data warehouse storage schema
Work with product teams to enable easy data aggregation
Work with data consumers (analysts, data scientists, and AI/ML engineers) to ensure data is easily accessible and consumable
Create and execute a roadmap that meets data storage and availability goals
Ensure data is secure through data analysis, access control, and encryption
You have
Bachelor's degree in computer science, information systems, applied math or related field, or equivalent work experience
Minimum 5 years of experience working with cloud data processing services (AWS, Azure, or GCP).
Minimum 3 year experience with Big Data technologies, such as Databricks, Snowflake, Spark, Hadoop, etc.
Strong experience in creating data architecture and storage schemas required
Significant working knowledge of SQL and other query languages and tools
Significant experience with building relational database systems strongly preferred
Experience with ETL tools a plus
Knowledge of Python preferred
Software development, data analytics, or data science experience is a plus.
Experience working within a software as a service (SaaS) company preferred.
Experience working within a high growth and/or ambiguous environment, with proven experience to be dynamic preferred.
#LI-REMOTE
We know that potential candidates are often less likely to apply to a position if they don't match 100% of the job qualifications. Don't let that be why you miss out on this opportunity! We encourage you to apply if you can demonstrate many of these skills and competencies.
Loyal to our employees
We are a remote-friendly company! We encourage you to apply from anywhere in the United States. We also believe in a work/life balance that fulfills you while you're here and supports you when you're not. We built our benefits package to prove that we're committed to you having everything you need (including a little fun). Here is what we offer full-time employees:
Flexible paid time off, sick and personal days
At least one holiday per month (sometimes, more!)
Full health, dental, and vision insurance - Loyal pays the premium for all employees!
One Time Home Office Setup Stipend For Remote & Hybrid Roles
Monthly Internet Stipend for Remote & Hybrid Roles
Long term & short term disability
401[k] plan
16 Weeks Paid Parental Leave
2 Volunteer days per year
Matching Gift Program
Participation Grant Program
Annual Travel/Team Events up to twice per year (post-COVID)
Our Commitment
We believe that the key to Loyal's success is you. Your unique background, life experience, knowledge, self-expression, and talent make you uniquely you. Who you are, what you have experienced, and how you think inspires us to be innovative and bold.
Loyal is an equal opportunity employer. We hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. We welcome the unique contributions that you can bring in terms of your education, opinions, culture, ethnicity, race, ancestry, sex, gender identity and expression, national origin, citizenship, marital status, age, languages spoken, veteran status, color, religion, disability, sexual orientation, and beliefs.
We consider qualified applicants regardless of criminal histories, consistent with legal requirements.
Further, consistent with applicable federal and state law, Loyal provides reasonable accommodations when requested by qualified applicants or employees with disabilities, unless doing so would cause an undue hardship. Loyal's policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If you require a reasonable accommodation in connection with the application process, please contact the Talent Acquisition Department at talentacquisition@loyalhealth.com.
E-Verify
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security Administration (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment. Employers can only use E-Verify once you have accepted a job offer and completed the I-9 Form.
COVID-19 Vaccinations
Consistent with Loyal policy, candidates performing in-person work will be required to show proof of being fully vaccinated against COVID-19 upon commencing employment. Reasonable accommodations will be considered on a case-by-case basis for exemptions to this requirement in accordance with applicable law. If you require a reasonable accommodation to Loyal's vaccination requirement, please contact the People Department at people@loyalhealth.com.",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2015,$5 to $25 million (USD)
Infosoft,#N/A,"Peachtree Corners, GA",Data Pipeline Engineer - 2173,"Here are the job position details for your review:
Job Title: Data Pipeline Engineer (GStreamer/ DeepStream)
Pay Rate: $90 to 120/Hr.
Duration: 3 Months
Location: Peachtree Corners, GA
This position is for the Building Automation division.
We are looking for Data Pipeline Engineer (GStreamer) to participate in the development of Streaming data pipelines using NVIDIA Deepstream, GStreamer, Python programming, messaging queues, and related tools.
Preference is for candidates located in Atlanta, GA metro area. However, ok with non-local candidates who are open to 75% travel to Atlanta, GA.
Understanding the basics of our architecture/setup to be able to do setup/support for demos, testing, etc.
Basic computer hardware assembly and set-up
Possible set-up/installation of the application(s)
Possible development tasks depending on what the rest of the development team has for them
Develops business software applications, system software, and/or IT solutions on the basis of functional specifications.
Scope of position embraces both the customization or reuse/modification of system software and/or solutions as well as their development from scratch.
Consults with customers in creating functional specifications and work estimates, and supports Make-or-Buy decisions.
Analyses performance gaps of existing software environments and proposes measures to address efficiency problems.
Is also responsible for the implementation of data-driven components supporting data modeling and engineering in the context of data analytics (Data Stewardship).
Skills:
Python
Linux
Docker and Docker Swarm
Zero MQ / a good understanding of messaging
Desired Skills:
NVIDIA Deepstream SDK
Web Sockets
Flask
Protobuf
Possible:
GStreamer (if different than RTSP / H264 / MJPEG)
Job Type: Contract
Salary: $90.00 - $120.00 per hour
Ability to commute/relocate:
Peachtree Corners, GA: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
""Please share your Email ID""
Work Location: One location",$105.00 /hr (est.),Unknown,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Argo Data
2.7",2.7,"Richardson, TX",Senior Data Engineer,"ARGO is a leading provider of software and analytics solutions for both the financial services and healthcare industries. ARGO transforms business processes for financial service providers and healthcare organizations using proven business models and software innovation informed by real customer challenges, breakthrough technology, and rich analytics.
ARGO- Data Engineer II (Markham, ON; Memphis, TN; ...) (theapplicantmanager.com)
JOB SUMMARY
The Data Engineer II is responsible for designing and implementing data solutions to support business needs. The successful candidate will have a strong background in software engineering and database design, as well as experience with working with cloud-based tools and technologies.
EXPECTED WORK AND PERFORMANCE
This section is intended to provide a more detailed overview of what the expectations of the job are to include: work to be performed, expected performance results and how they will be measured, as well as connection to corporate strategy, values, and mission. Job duties and responsibilities include the following. Other duties may be assigned as needed.
Design, build, and maintain data pipelines for data extraction, transformation, and loading (ETL) from various sources.
Ensure data security and privacy protocols are implemented and enforced.
Monitor and optimize data storage and processing performance to ensure high performance and cost-effectiveness.
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and translate them into technical solutions.
Maintain documentation of data systems, processes, and policies.
Stay current with the latest data tools, technologies, and trends and recommend improvements.
Mentor and train junior data engineers as needed.
QUALIFICATIONS
Bachelor's degree in Computer Science, Software Engineering, or related field.
3-5 years of experience in software engineering and SQL database design.
Proficiency in programming languages such as Python.
Strong understanding of data warehousing, ETL processes, and data modeling.
Experience with cloud infrastructure technologies such as Microsoft Azure.
Excellent problem-solving and analytical skills.
Ability to work well in a team-oriented environment and communicate effectively with both technical and non-technical stakeholders.
PHYSICAL DEMANDS
The candidate will work at our corporate headquarters in Richardson, TX. While performing the duties of this job, the employee is regularly required to stand; walk; sit; use hands to finger, handle, or feel; reach with hands and arms; talk or hear. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.
ADDITIONAL REQUIREMENTS:
Applicants for U.S. or Canadian based positions with ARGO must be legally authorized to work in the United States or Canada. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available at this time for ARGO positions.
Pre-employment background screening will be conducted.
Job Type: Full-time
Pay: $70,745.42 - $141,624.04 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Health insurance
Life insurance
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Tuition reimbursement
Vision insurance
Compensation package:
Bonus pay
Performance bonus
Yearly bonus
Experience level:
5 years
Schedule:
8 hour shift
Ability to commute/relocate:
Richardson, TX 75081: Reliably commute or willing to relocate with an employer-provided relocation package (Required)
Experience:
Informatica: 3 years (Required)
SQL: 5 years (Required)
Data warehouse: 3 years (Required)
Python: 3 years (Required)
Azure: 3 years (Required)
Work Location: In person","$106,185 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Computer Hardware Development,1980,$25 to $100 million (USD)
