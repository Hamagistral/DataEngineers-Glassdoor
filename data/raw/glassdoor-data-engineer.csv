company,company_rating,location,job_title,job_description,salary_estimate,company_size,company_type,company_sector,company_industry,company_founded,company_revenue
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
"Apple
4.2",4.2,"Cupertino, CA",Data Engineer,"Summary
Posted: Dec 22, 2021
Weekly Hours: 40
Role Number:200327520
As part of our Video Engineering group, you’ll help deliver creative solutions to various problems that could impact the people all over the world. This Data Engineer will work closely with other members of the Video Engineering group to mine data, implement model evaluation pipeline, analyze large scale data, visualize data, and ensure the delivery is of the highest quality. This position will also require strong coding skills, presentation skills, and collaborating with multiple teams (ex: machine learning, cloud infrastructure support).
Key Qualifications
A curious mind
An obsession for quality
Background in Data science, Data mining, Multivariate statistics, Computer vision, Machine learning
Experience working with large scale data sets
Solid programming skills including:
Python
C/C++
Experience with data visualization and presentation, familiar with data analysis tools such as Tableau
Excellent problem solving and communication skills
Description
The responsibilities of this position includes the following for current and future products: - Implement algorithm evaluation methods - Analyze data and build data analysis tools - Deep-dive failure analysis - Discover new perspectives for old data - Produce / Present meaningful data visualization to higher-ups and across various involved teams
Education & Experience
Masters in Computer Science or relevant experience
Additional Requirements
Pay & Benefits
At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $50.72 and $76.44/hr, and your base pay will depend on your skills, qualifications, experience, and location.

Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits.

Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.",#N/A,10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1976,$10+ billion (USD)
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Jane Street
4.4",4.4,"New York, NY",Data Engineer,"About the Position
We are looking for a Data Engineer who can help us understand, clean, manage, and share the data that guides our trading. At Jane Street, having a thorough and accurate understanding of data is at the core of the work we do.
Using our mix of in-house and open-source software, you will analyze datasets gathered from a variety of sources, checking for anomalies, matching formats and symbologies, automating ETL processes, and generally making it easier for our traders to generate valuable insights.
You should be excited about digging deep into datasets and explaining your findings to different types of colleagues, working collaboratively with traders and software engineers.
While prior experience with financial data would be nice, we don’t expect you to have a finance background. We’re happy to hire talented engineers and teach them what they need to know.
About You
Top-notch programming skills in any language (Python a plus)
Experience with using SQL and relational databases
Experience with generating data visualizations
Knowledge of statistical techniques, including multivariate regression and time series analysis
Clear and concise communication skills; able to efficiently analyze and deconstruct technical problems
Fluency in English required
Base salary is $175,000 - $300,000. Base salary is only one part of Jane Street total compensation, which includes an annual discretionary bonus.
Jane Street is an Equal Opportunity Employer","$237,500 /yr (est.)",1001 to 5000 Employees,Company - Private,Management & Consulting,Research & Development,2000,Unknown / Non-Applicable
"Steward Health Care
2.7",2.7,"Westwood, MA",Data Engineer,"Position Purpose:
Reporting to the Manager of the Data Warehouse team, part of the larger Health Informatics group, the data engineer applies their technical expertise to meet the needs of the department and Steward Health Care Network (SHCN).

Key Responsibilities:
ETL/Automation
Design configurable data process flows with full automation
Develop ETL processes for data loading and data extraction
Schedule ETL processes for full process automation
Data Engineering
Responsible for data analysis to support building data processes and reporting
Design useful and accurate data marts that meet requirements
Apply SQL skills when designing and building data marts and data flows
Quality
Establish and utilize QC processes to ensure data integrity
Incorporate standard error logging and alerts to ensure data is loaded as expected
Documentation
Create and maintain clear documentation

Education / Experience / Other Requirements


Education:
Bachelor's degree in Computer Science, Mathematics, Statistics or related experience


Years of Experience:
5+ years of database related work
2+ years of focus on healthcare data

Specialized Knowledge:
Knowledge of healthcare data
Experience using relational databases, SQL Server experience preferred
Experience using ETL tools (SSIS, Informatica, etc.)
Strong SQL programming skills
Experience with scripting languages (PowerShell, R, Python, etc.)
Experience automating data flows
Experience with Health Catalyst tools preferred, but not required
Deep understanding of database structures and data design.
Creative, flexible, and self-motivated with sound judgment
Strong communication skills




Location: Steward Health Care Network · 1301.72330 Steward Health Care Network
Schedule: Full Time, Day Shift, 40 hours","$94,536 /yr (est.)",10000+ Employees,Hospital,Healthcare,Health Care Services & Hospitals,1998,Unknown / Non-Applicable
"Twitch Interactive, Inc.
3.8",3.8,"San Francisco, CA",Data Engineer,"3+ years of experience in data engineering, software engineering, or other related roles. 3+ years in relational database concepts with a solid knowledge of star schema, SQL, SQL Tuning, OLAP, Big Data technologies 3+ years of experience in generating and maintaining data pipelines from various data sources, in collaboration with diverse stakeholders. 3+ years of experience working with Amazon Webservices, S3, EMR, Redshift etc. Experience with best practices for development including query optimization, version control, code reviews, and documentation. Experience with coding languages like Python/Java/Scala
About Us: Twitch is the world's biggest live streaming service, with global communities built around gaming, entertainment, music, sports, cooking, and more. It's where millions of people come together to chat, interact, and make their own entertainment. We're about community, inside and out. You'll find coworkers who are eager to team up, collaborate, and smash (or elegantly solve) problems together. We're on a quest to empower live communities, so if this sounds good to you, see what we're up to on LinkedIn and Twitter, get interviewing tips on Instagram, and discover projects we're solving on our Blog. About the Role: Data is central to Twitch's decision-making process, and data engineers operate at the forefront of this by creating authoritative datasets that drives analysis and decision-making across all of Twitch. In this role you will be shaping the way that business performance is measured, defining how we transform our data, and scaling analytics methods and tools to support our growing business, leading the way for high quality, high velocity decisions. For this role, we're looking for an experienced data engineer to join our Content Data Science team, which is focused on empowering staff throughout Twitch to use and trust our business data. Your responsibilities may range from developing and enhancing our data warehouse which act as authoritative sources of truth across the company, driving data quality and trustworthiness across product verticals and business areas, building self-service business intelligence infrastructure for analysts, as well as connecting into data interfaces that enable everyone in Twitch to discover and analyze the data. In the process, you will have the opportunity to interact with technical and non-technical staff members throughout the company, and will report to the Director of Content Data Science. This position can be located in San Francisco, CA; Irvine, CA; Seattle, WA; New York, NY; and Salt Lake City, UT. You Will: Define and own team level data architecture for trusted, governed, dimensionally-modeled repository of data that enables Twitch staff to quickly and reliably answer their business questions. Keep existing data sources fresh against data quality issues, design, develop and maintain data quality assurance framework and continuously improve the processes for developing new ones raising the level of quality expected from our work. Conduct unit, integration, and system tests on our data sources in order to validate data against source systems, and continuously optimize performance in order to improve query speed and reduce cost. Improve search, discovery and literacy: Create exploration and visualization interfaces in our BI tools and evangelize the adoption of these sources across the company through education and training programs. Improve business and engineering team processes via data architecture, engineering, test, and operational excellence best practices. Make enhancements that improve data processes.
Bonus Points
A passion for data science and interest in growing / learning data science, machine learning at scale.
A passion for games and the gaming industry
Perks
Medical, Dental, Vision & Disability Insurance
401(k)
Maternity & Parental Leave
Flexible PTO
Amazon Employee Discount
Monthly Contribution & Discounts for Wellness Related Activities & Programs (e.g., gym memberships, off-site massages, etc.)
We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
Pursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

We are an equal opportunity employer and value diversity at Twitch. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status, or other legally protected status.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $105,700/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. Applicants should apply via our internal or external career site.","$105,700 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Internet & Web Services,1994,$10+ billion (USD)
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"Nike
4.1",4.1,"Boston, MA",Data Engineer,"Become part of the Converse Team

Converse is a place to explore potential, break barriers and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Converse, it’s about each person bringing skills and passion to a challenging and constantly evolving world to make things better as a team.
Converse, Inc. Boston, MA. Work closely with Project Management and Business teams to completely define specifications to ensure the project acceptance. Involved in preparation of functional and technical specifications with different cross teams. Lead team, defining solution options, providing estimates on effort and risk, and evaluating technical feasibility in Agile development process, including Scrum and Kanban. Work on troubleshooting data and analytics issues and perform root cause analysis to proactively resolve issues. Develop data extracts and feeds from the full spectrum of systems in the Converse ecosystem, including transactional ERP systems, POS data, product and merchandising systems. Engineer data products for a variety of Operations analytics use cases, ranging from reporting and data visualization to advanced analytics/machine learning use cases. Support designing technical specifications and data transformation models for junior developers. Ensure development is on track and meets specifications as defined by product management and the business. Responsible for data integrity of current platform and QA of new releases. Support the development and maintenance of backlog items and solution feature. Participate in sprint planning activities from a development perspective. Responsible for designing cloud-based data architecture using AWS stacks. Design and develop Python data science and data engineering libraries dealing with structured and unstructured data. Work with a variety of database types (SQL/NoSQL, columnar, object-oriented) and diverse data formats. Responsible for ETL with Spark and building data pipelines/orchestrations in Airflow and working on ETL tools like Matillion. Responsible for DevOps toolchain and Continuous Development, Continuous Integration and Automated Testing using Jenkins. Ensure and use data engineering for advanced analytics/data science and Software development skills.
Applicant must have a Bachelor’s degree in Computer Science, Information Systems, or Information Technology and 5 years of progressive post-baccalaureate experience in the job offered or a related occupation. Experience must include:
Data warehousing;
ETL or ELT;
Amazon Web Service (AWS) Cloud Services, including AWS S3, AWS Lambda, AWS EC2, AWS EMR or AWS DynamoDB;
Relational Database Management Systems (RDBMS), such as Oracle, Teradata, SQL Server or Snowflake;
Database Development with writing stored procedures, functions, triggers, cursors or SQL queries;
Hadoop, HDFS, Hive or Spark;
Programming languages, including Java or Python;
Business Intelligence Tools, such as Tableau;
Unix Shell scripting; and
Version control systems, such as Git, Bitbucket or Github
#LI-DNP
Converse is more than a company; it’s a worldwide advocate for self-expression. This belief motivates our employees, permeates our working environment and inspires our products. No two of us look or think exactly alike. We are each one-of-a-kind. Individually and as a culture, we have the freedom to create and grow professionally. Generous benefits packages only sweeten the experience. From Boston to Shanghai, from Brand Design to Finance, Converse is a brand that celebrates the unique and creative people of the world. Together, we’re different.","$115,797 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Consumer Product Manufacturing,1972,$10+ billion (USD)
"AgileEngine
5.0",5.0,Remote,Big Data SDET Engineer,"Test with the best! At AgileEngine, you're free to build your own team of A-players and choose out of 70+ available projects the one you'd like to work on. Join the squad of top-3% software experts who make a difference through off-the-wall solutions.
Must haves
Strong Scala and Spark (hands on). If not Scala, strong experience in either Java or Python
Big Data concepts
API testing experience
Understanding qa process to create test cases and test strategies
Independently able to contribute to automation framework as well interact with the stake holders as needed
SQL experience
AWS experience
Strong test mindset
Experience of deploying or using Terraform and Gitlabs would be a bonus
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
The company behind the world’s leading OS virtualization software leverages AgileEngine as a provider of external QA and testing automation expertise. We’ve developed our client’s test strategy complete with test plans, ETAs, analysis, and risk assessment. Our teams have also optimized the company’s test suites, halving the time it takes for the tests to run. Our other deliverables include the implementation of new tests and the development of documentation, how-to guides, and test instructions on an internal Wiki.
Our team is also working to establish a strong QA/AQA process for an industry-leading cybersecurity platform. We’ve developed our client’s test strategy, manual testing cases, and automated tests from scratch. Our deliverables include backend, API, and UI testing, as well as BDD integrated with a streamlined CI process.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"Zartico
4.5",4.5,"Salt Lake City, UT",Jr. Data Engineer,"Meet Zartico, the world’s first Destination Operating System.
Zartico’s mission is to empower DMOs to be better stewards of the world’s tourist destinations through improved data intelligence and decision-making. Makers of the first Destination Operating System, Zartico harnesses and streamlines complex data to provide a full spectrum of data science, benchmarking, and analytical services for use in marketing, community development, and sustainability efforts. Based in Salt Lake City, Utah, Zartico has over thirty years of experience in technology, tourism, and destination, travel and tourism marketing.
Zartico is looking to add a Junior Data Engineer to the team. In this role, Primary Responsibilities include but are not limited to the following:
Be part of Data Engineering Team to develop data infrastructure that is able to ingest and transform data at scale coming from many different sources, different customers, and in many different varieties. You will build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources and build robust data pipelines that collect, process and compute business metrics from activity data. You will be responsible for creating critical datasets for machine learning, growth funnels, business forecasting, and many other strategic initiatives.
To be successful in this role, you will need a strong background in Data Engineering, specifically, Python, SQL, Google Cloud Platform (BigQuery, GoogleCloud Storage, Airflow). In addition to needing an ability to problem-solve effectively, you will need to have strong written and oral communication skills.
What You’ll Do:
Effectively use, optimize, and automate processing pipelines, as well as explore new technologies to meet the expanding needs of our products.
Experience working as part of a Scrum team and familiarity with Scrum team ceremonies such as daily stand-up, backlog refinement, sprint retrospectives, sprint reviews, and sprint planning.
Collaborate with the product team to define high-level requirements for product development.
What you’ll bring :
You will need 1-2 years of experience as a Data Engineer, with fluency in SQL and programming languages, specifically Python. Experience with Ruby and scripting languages will also be important. You will also need a successful history of manipulating, processing, and extracting value from large, disconnected datasets. You will be expected to be an expert in coding, with an ability to promote solid design and coding standards.
Education and Certifications: BA/BS in a quantitative or computer science field.
Why Zartico?
We believe in a growth mindset. We are a learning organization.
We emphasize focus because we know that to achieve big dreams, you have to execute and get the small stuff right.
We lead with inclusion and value diversity. We believe in diversity of thought, perspective, and experience. Diversity of experience and perspectives creates a more robust product and a more beautiful world.
We dream in color and code.
We hustle.
We are humble and know that the sum of our parts is greater than any one of us as individuals.
Above all else, we do the right thing. We believe in transparency, honesty, and integrity.
We believe travel and tourism are a force for good because it builds connection, understanding, and appreciation of our world’s cultures, history, and natural resources. We believe data and the right metrics allow us to make better decisions because transparent data helps focus on the right issues, problems, and therefore, solutions, to be better stewards of our world's most precious destinations.
We’re building a global community—one that’s safe for people of all backgrounds. We are an equal-opportunity employer where our diversity and inclusion are central pillars of our company strategy. We look for applicants who understand, embrace and thrive in a multicultural and increasingly globalized world. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. When you join our team, you join the Zartico family.","$81,338 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2019,Unknown / Non-Applicable
"Aretec Inc
1.0",1.0,Remote,Junior Data Engineer,"POSITION TITLE: Junior Data Engineer YEARS OF EXPERIENCE: 1-3
LOCATON: 100% remote
*****Please Note: Aretec, Inc. does not offer Corp - 2 - Corp (C2C) employment. *****
Aretec is looking for a Junior Data Engineer. The Junior Data Engineer will be primarily responsible for design, development, support and enhancement of the data pipelines developed in AWS.

RESPONSIBILITIES:
You'll write clean and functional code on the front- and back-end
You'll write reusable and maintainable code
Coordinate with data migration plans
Ability to communication and collaborate with various teams and vendors.
Participates in functional and technical design.
Participation in Agile activities Scrum, Kanban.
Ensure coding, testing, debugging and implementation activities completed as required.
Flexible and adaptable with the ability to align to changing priorities
The developer should have great communication skills and be able to discuss and develop requirements with multiple levels of staff from corporate and field locations
An interest in and ability to understand financial reporting, accounting concepts and related accounting data
Participate in data flow diagramming and/or process modeling (code architecture)
Documents work and steps to completion as required
Follows AWS best practices to integrate with ecosystem and infrastructure
Ability to partner with domain architects to implement the defined solution architecture including application, infrastructure, data, integration, and security domains

REQUIRED SKILLS:
1-3 years of software engineering experience
1+ years of real industry experience
Experience with website development, web services and API development
Hands-on experience performing data engineering and transformation tasks using Python
Experience implementing backend in Python using frameworks such as Django or Flask
Knowledge of web technologies - both back and front-end development including, but not limited to JavaScript, React, CSS, HTML, T-SQL, and Python
Understand log monitoring and analytics
Experience Meeting both technical and consumer needs
Experience testing software to ensure responsiveness and efficiency
A general knowledge of index migrations, debugging and researching concepts are major pluses
Must be aware of CI/CD pipelines and well-versed in using GitLab for creating required pipelines for CI/CD

EDUCATION: Bachelors Degree in Mathematics/Statistics/Technology/Science/Engineering/Applied Mathematics or related field

CERTIFICATIONS: N/A","$102,500 /yr (est.)",51 to 200 Employees,Contract,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"Adobe
4.4",4.4,"New York, NY",Data Engineer,"Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

Job Description
Adobe Customer Solutions is looking for a full time Data Engineer with experience in building data integrations using AWS technology stack as part of the team's Data as a Service portfolio for Adobe’s Digital Experience enterprise customers.
Customer facing Engineers who enjoy tackling complex technical challenges, have a passion for delighting customers and who are self-motivated to push themselves in a team oriented culture will thrive in our environment
What you'll Do
Collaborate with Data architects, Enterprise architects, Solution consultants and Product engineering teams to gather customer data integration requirements, conceptualize solutions & build required technology stack
Collaborate with enterprise customer's engineering team to identify data sources, profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating customer data sources and third party data sources with Adobe solutions
Develop new features and improve existing data integrations with customer data ecosystem
Encourage team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Collaborate with a Project Manager to bill and forecast time for customer solutions
What you need to succeed

Proven experience in building/operating/maintaining fault tolerant and scalable data processing integrations using AWS
Proven track record in Python programming language
Software development experience working with Apache Airflow, Spark, MongoDB, MySQL
Experience using Docker or Kubernetes is a plus
BS/MS degree in Computer Science or equivalent proven experience
Ability to identify and resolve problems associated with production grade large scale data processing workflows
Excellent interpersonal skills
Experience crafting and maintaining unit tests and continuous integration.
Passion for crafting I ntelligent data pipelines that customers love to use
Strong capacity to handle numerous projects are a must
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists. You will also be surrounded by colleagues who are committed to helping each other grow through our outstanding Check-In approach where feedback flows freely.
If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the significant benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age, sexual orientation, gender identity, disability or veteran status.

Our compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $101,500 -- $194,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.","$147,900 /yr (est.)",10000+ Employees,Company - Public,Information Technology,Computer Hardware Development,1982,$5 to $10 billion (USD)
"Glow Networks
3.5",3.5,"Dallas, TX",Data Engineer,"Data Engineer
Pay Scale: $73 p/h, W2, no benefits
Duration: Full Time/Contract
Status: US Citizen or Green Card only
Reports To: Project Manager
Working Hours: Normal business hours
Work Location: Onsite, Customer Premises, Vancouver, WA 98683

Summary/Objective:
Glow Networks is a telecommunication staffing and consulting company based in Dallas, TX. We are seeking a Data Engineer, to work in Vancouver, WA 98683 location.

The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to implement data engineering pipelines to build solution to help provide trusted and reliable data to customers.

Responsibilities
Design and implement distributed data processing pipelines using Spark, Python, SQL and other tools and languages prevalent in the Big Data/Lakehouse ecosystem.
Analyzes design and determines coding, programming, and integration activities required based on general objectives.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
What you bring :
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
3+ years’ experience in Big Data Distributed ecosystems (Hadoop, SPARK, Hive & Delta Lake)
3+ years experience in Workflow orchestration tools such as Airflow etc.
3+ years’ experience in Big Data Distributed systems such as Databricks, AWS EMR, AWS Glue etc.
Leverage monitoring tools/frameworks, like Splunk, Grafana, CloudWatch etc.
Experience with container management frameworks such as Docker, Kubernetes, ECR etc.
3+ year’s working with multiple Big Data file formats (Parquet, Avro, Delta Lake)
Experience working on CI/CD processes such as Jenkins, Codeway etc. and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge and Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.
Have experience in building realtime streaming data pipelines
Experience in pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming etc

.EEO Statement: Glow Networks. provides equal opportunity in all of our employment practices to all qualified employees and applicants without regard race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability or genetic information and other characteristics that are protected by applicable law.

Other Duties: Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. This description reflects management’s assignment of essential functions, it does not proscribe or restrict the tasks that may be assigned. Duties, responsibilities, and activities may change at any time with or without notice.",$73.00 /hr (est.),51 to 200 Employees,Subsidiary or Business Segment,Information Technology,Information Technology Support Services,2003,$5 to $25 million (USD)
"Enterprise Knowledge LLC
4.0",4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”","$89,080 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD)
"Bose
3.8",3.8,"Framingham, MA",Data Analytics Engineer,"Job Description
Bose is about better sound, but better sound is just the beginning. We’re about inventing new technologies that truly benefit people and creating a culture where innovation and teamwork are highly valued. Working at Bose, you’re encouraged to question conventional thinking in the relentless quest to create products and experiences that change people's lives. Data analytics and data science are a crucial part of our mission, fueling the creation of new and innovative products, helping us to bring the right products to the right customers, and allowing us to astonish customers with carefully crafted and personalized experiences.

We are looking for a Data Analytics Engineer within our Data and Analytics Center of Excellence. The mission of this team is to develop world-class data science, machine learning, and statistical solutions to extract insights and enable data driven decisions that improve our business and enhance customer experiences.
This Data Analytics Engineer role will be critical to help drive analysis, data science, and reporting across many diverse data sets to make sure we deliver impact from our data products and services. This includes working with raw data sets that leads to development of logical and physical data models, data wrangling and design / building of semantic layer that helps to create dashboards and models that drive insights and actions. This work will directly inform and influence multiple division’s strategies. This is a unique opportunity to shape the experiences, technologies, and products that millions of people will use.
The ideal candidate has a deep understanding of data modeling, SQL, data warehouses / data lake and data engineering skills. You will often serve as an internal expert about the data, what it means and how it can be used to solve business problems. You can demonstrate that you have experience deeply understanding the data structure and transformations required to develop a semantic layer for data analysts and scientists. We are seeking a highly motivated, detail-oriented engineer who is passionate about data and enabling data-driven decisions.
Primary Responsibilities:
Leverage DBT to mine , analyze, and transform large structured and unstructured datasets, collaborating with analysts, data scientists, and business users to create actionable metrics and datasets that solve business questions.
Provide expertise in query building and data modeling to ensure data is made available in a timely, efficient, and comprehendible means.
Help define the strategy and standards for analytical engineering.
Help define testing and data quality programs leveraging dbt Cloud
Partner with our other teams to ensure documentation, privacy, and best in class coding practices are implemented within Analytical engineering.
Act as SME for specific data domains by enabling analytical data sets for ad-hoc analysis
Engage with key stakeholders in the business, product and software teams to clearly understand and scope analytical requests.
Qualifications:
Extensive background and demonstrated track record in building analytical models in SQL.
Strong technical skills, including experience with analytics, query and data visualization tools.
Experience with Snowflake, dbt, and Python is preferred.
Excellent communication skills, and the ability to explain deep technical results to diverse groups of stakeholders.
A life-long learner who is curious, has a passion for solving hard, ill-defined problems, has comfort taking initiative and who continuously seeks to improve their skills and understanding.
Enjoys working in a team environment as well as independently.
Education:
Bachelor’s or Master’s degree in math, physics, computer science, engineering, business, finance, marketing, economics or related quantitative or computational field
Work Experience:
3+ years of related data analytics experience in relevant consulting, finance, data science or market intelligence functions
Previous experience in a consumer electronics environment, particularly in a technical field, is a definite plus
Previous experience with mobile app data is a plus


What's in it for you:
Be a part of and work with a top notch, multidisciplinary, transparent, and agile team.
Collaborate with people like you who want to solve problems and have fun together.
Work on developing innovative Wellness products that improve people's lives.
Excellent work life balance and a continuous learning environment.
Highly competitive package as well as a comprehensive benefits program.
We strive to help our employees and customers reach their fullest potential.
Compensation for this role for a candidate based in Colorado is expected to be between $101,800 and $130,300 and for a candidate based in NYC to be between $101,800 and $130,300 . Actual pay may be higher or lower depending on geographic locations, skills, experience, and other factors permitted by law. This role is also eligible to receive a bonus, which is not guaranteed and may be based on individual and company performance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.","$98,163 /yr (est.)",5001 to 10000 Employees,Company - Private,Manufacturing,Consumer Product Manufacturing,1964,$1 to $5 billion (USD)
"Empower Federal Credit Union
4.0",4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000","$99,876 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,#N/A,$25 to $100 million (USD)
"APLOMB Technologies
4.4",4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$72,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"Small Batch Standard
4.1",4.1,Remote,Junior Data Engineer,"We're the premier, remote accounting, tax, and consulting firm built exclusively to serve the craft brewing industry.
Our mission is to help craft breweries grow profits and build deep successful relationships. And our team is filled with expert, autonomous, adaptable, technology-driven high performers.
Are you up for the challenge?
We're looking for a full-time, remote Junior Data Engineer to join our specialized team. The main objective of this role is to design, develop, implement, and improve both internal and external applications to support our brewery clients and team in accordance with the SBS Core Values.
About The Role
This role will report to our Technology/Product Manager and is accountable for fulfilling the following responsibilities:
Building our data pipeline and analysis applications. A key aspect of the consulting service we provide to clients involves the collection, aggregation, analysis, modeling, and usage of financial data and benchmarks. We use this data both internally to develop and inform strategy, as well as externally through our Benchmarks Assessment (https://sbstandard.com/assessment/) and Compass analysis product (https://sbstandard.com/levelup-compass/). You'll be responsible for working with our team to build out our data pipeline for these tools, and progressively increasing our ability to aggregate, analyze, query, and feed back this data into our reporting, analysis, and consulting work. Platforms we're building with include: SQL, Airflow, Excel Visual Basic for Applications (VBA), Google Apps Script, Intuit/QuickBooks Online.
New process and technology R&D. We're always looking for new opportunities to provide both our team and our clients access to additional tools that give them leverage, automate and streamline processes, and overall make work more efficient. Part of your time will be dedicated to researching, testing, and prototyping new tech and application options.
Participate and contribute to the overall success of our team. Each week the team meets to share wins, progress, and knowledge, as well as identify and solve issues at multiple levels (company, team, individual). Your full participation in this process is critical to ensure that we are operating as a cohesive, high-performance unit.
About You
We're looking for an individual who:
Is a problem solver through-and-through. Everywhere you look, you both (a) see problems to solve, and (b) see solutions and new ways of doing things that just haven't been done yet. You know how to think outside of the box, are willing to “go there” with new ideas and solutions that haven't been done before, and have the confidence to start building, testing, iterating, and making sh*t work.
Is a systems thinker. You understand both the big picture and how the functional components fit together, and have the ability to take a specific analysis outcome and generalize it to fit a wide range of scenarios through structure and sound system design.
Can fail fast, iterate, and learn. You're an independent, self-directed, learner who isn't afraid to “move fast and break stuff” knowing that failure is a prerequisite to success, ESPECIALLY in product development. You may not have traditional credentials, but what you do have is the ability to rapidly learn, adopt, test, and understand new languages, platforms, tools, and solutions.
Is a manager of one. Unlike working within a traditional firm, in this role you'll be in the driver's seat, managing your workflow and workload in order to meet the standard set of deliverables required for each client.
About Our Culture
We're fully remote, with team members and clients located all across the U.S. and have developed our own unique culture we call The SBS Way, within which we operate, evaluate performance, and make decisions using our core values as a guide:
Be Antifragile. Everything we do, good or bad, makes us better. And every experience is an opportunity for learning and continuous improvement.
Play The Long Game. We make decisions, to the best of our ability, in the long-term interest of our firm, our team, our clients, and our broader industry and community.
Embrace Technology. We welcome new technologies with open arms, and are always exploring, testing, and implementing them in the interest of enhancing both our internal capabilities and our client's outcomes.
Build and Trust The Process. Each member of the team is committed to building, following, and improving the processes we use to deliver exceptional results for our clients.
Act as A Team of Expert Knowledge Workers. We openly and willingly collaborate, communicate, and provide rapid, direct feedback in the interest of learning, improving and developing ourselves.
Working At SBS
What it's like working at our firm:
High flexibility. We believe in the ability of our team to determine the best way to complete their work. We measure outputs, not inputs. We don't have time sheets. We don't track hours. We don't pay attention to when and where our team works. Your schedule is yours to make.
High accountability. What we care about most is that we deliver on what we promise to our clients. In this respect, we measure and manage to our deliverable performance metrics and ensure each team member takes ownership over their accomplishment with a high level of quality that aligns with our core values
Great pay for great work. We pay based on the characteristics that matter: position (and its market value), level of mastery, and longevity with the firm. All of which aim to ensure each member of the team feels they are compensated well and can focus on great work.
Merit-based career progression. We have clearly established career tracks, performance benchmarks, and mastery levels set for all of our core positions. How quickly you progress is entirely under your control, with a quarterly review and bi-annual promotion consideration cycle in place to evaluate your progress.
Generous benefits. We offer a generous benefits package that includes medical, dental, and vision insurance enrollment; as well as an IRA match, tech stipend, 3 weeks of paid time off, and entry into our profit share bonus program after two years of service.
Personal and and team development. In addition to our overall continuous learning focus, we also provide support for personal development in the form of expense coverage for continuing education (books, courses, training, certifications, etc.) as well as experiential learning (brewery visits, industry events and conferences, etc.). Each year we also meet in person for an all-expenses-paid annual retreat as a team. No work. Lots of fun. Lots of client beer.
Job Requirements
The following basic requirements must be met:
Previous experience in SQL development and database management.
Previous experience building useful applications in scripting languages like VBA, Google Apps Script, Python, PHP, etc.
Can do effective cross-functional work in a remote environment.
Have crystal clear professional written and verbal communication skills.
Have exacting organizational standards and a calm and friendly attitude.
Available and responsive during normal business hours (9am-5pm Eastern Time, Monday-Friday).
Have a strong, consistent internet connection and a work environment conducive to video calls.
Preferred qualifications include:
Direct previous experience building data pipelines.
Direct previous experience building Airflow workflows and applications.
Experience building out and managing API connections.
Experience working with Quickbooks Online or similar accounting or finance platforms.
Experience using Podio or similar remote project management tools (e.g. Trello, Asana, etc.).
Next Steps
If the position, culture, values, and mission at Small Batch Standard sound like they're the right fit for you, please apply here.","$64,000 /yr (est.)",1 to 50 Employees,Company - Private,Financial Services,Accounting & Tax,2010,Unknown / Non-Applicable
"Buchanan & Edwards
3.8",3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location","$138,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD)
"Tripoint Solutions
4.5",4.5,Remote,Data Engineer (Remote),"Tripoint Solutions is seeking a Data Engineer to join our team.
The Data Engineer will be part of a team responsible for ensuring the success of a highly visible, results-driven federal client through the development of a cloud-based next generation system.
This position requires the applicant to parse disparate data sources, including structured and unstructured elements, to find the patterns and meaning in large quantities of data. The successful candidate will leverage machine learning as well as best of breed pipeline technology to process and store a variety of data elements.
Location: This position is eligible for fully remote work. Selected candidates living within a 25 miles radius of the NITAAC office in Rockville, MD will be required to come into the office once a week. The selected candidate must be currently located in, or willing to relocate to, a state supported by Tripoint Solutions corporate offices (AL, DC, FL, IL, LA, MD, MI, MN, MS, NJ, NC, PA, TN, TX, or VA).
The successful candidate will be accountable to:
Creating and maintaining optimal data pipeline architecture.
Assembling large, complex data sets that meet functional / non-functional business requirements.
Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Building the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Keeping data separated and secure across national boundaries through multiple data centers and AWS regions.
Strong interest to learn and stay up to date on relevant technologies, trends, industry standards and identify new ones to implement.
What you bring
Experience, Education & Training:
Bachelor's degree in computer science, Math, Analytics, Statistics, Informatics, Information Technology or equivalent quantitative field.
5 years of experience working in a Data Engineer or Data Scientist role.
Experience with cloud data services (AWS preferred).
Experience solutioning and applying Natural Language Processing (NLP) and or Machine Learning (ML) technologies
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience with Microsoft SQL, database development and design.
Experience building processes supporting data integration, transformation, data structures, metadata, dependency and workload management.
Demonstrated success in manipulating, processing and extracting value from large disconnected data sets.
Demonstrated accomplishments in designing, coding, testing and supporting data analytics and reporting solutions in a cloud environment.
Experience with object-oriented/object function scripting languages: Python, Java
Concept experience; information retrieval, search engine, document data extraction
Preferred experience with AWS cloud services: Textract, Comprehend, GlueMaker, Athena, Notebook
Working knowledge of message queueing, stream processing, and highly scalable ‘big data’ data stores.
Clearance Requirements:
Applicants selected may be subject to a government security investigation and must meet eligibility requirements for potential access to classified information. Accordingly, US Citizenship or Green Card is required.
What we offer
About Tripoint Solutions
We are technology innovators, partnered with state-of-the-art providers, such as AWS, ServiceNow, and UiPath, to drive digital transformation in the federal space. TPS teams are bringing automation and data science into areas of the government that are crying out for fresh tech—making positive impacts felt by tens of thousands of users, countless citizens, and all six branches of the military each day. Our Agile teams are responsible for envisioning, launching, and operating the massive data systems and analytics platforms used to manage $14.5B in government procurements and $200B in military real estate assets globally. At TPS, we apply the power of cloud technologies to help the government think smarter and function better—for everyone.
TPS Company Values
We value and respect each employee's dedicated work and unique contributions; as they directly impact who we are and what we do.
Your talent and innovative thinking bring leading-edge solutions to our customers.
Our success is driven by the dedication of our employees.
Employee-generated solutions have sustained our continued success and customer satisfaction
Benefit Offerings
Tripoint Solutions builds flexibility into health benefit plan choices, covers most of the monthly premiums, and helps employees build a career with impact through our generous professional development program.
We offer all full-time employees:
Medical, Dental, Vision benefits with a national provider network (company pays 100% of Vision and Dental premiums)
Flexible Spending and Health Savings Accounts (FSA & HSA)
Company-paid Life and Disability insurance including Short-Term, Long-Term, and Accidental
Paid-time off (PTO), accruing with each year of service, up to 20 days, plus 11 paid holidays
401(k) Retirement Plan - No waiting period to contribute and company makes 3% contribution of eligible pay in addition to annual profit-sharing contribution option
Eligibility to receive impact bonuses each quarter
Referral Program
Professional Development Reimbursement Program to pursue undergraduate, graduate, training, and certifications
Monthly transportation, parking, and cell phone service reimbursement
COVID-19 Related Information
Tripoint Solutions does not have a vaccination mandate applicable to all employees. However, to protect the health and safety of its employees and to comply with customer requirements, Tripoint Solutions may require employees in certain positions to be fully vaccinated against COVID-19. Vaccination requirements will depend on the status of the federal contractor mandate and customer site requirements. Furthermore, remote work arrangements are subject to change based on customer site requirements.
Tripoint Solutions is an Equal Opportunity Employer/Veterans/Disabled
Job Type: Full-time
Pay: $145,000.00 - $155,000.00 per year
Benefits:
401(k)
Dental insurance
Employee assistance program
Flexible spending account
Health insurance
Health savings account
Life insurance
Paid time off
Professional development assistance
Referral program
Retirement plan
Tuition reimbursement
Vision insurance
Experience level:
5 years
Schedule:
Monday to Friday
Application Question(s):
What cloud services have you worked with?
Do you have experience with Machine Learning or NLP?
Does the advertised salary align to your expectations?
US citizenship or green card is required. Do you meet this requirement?
This is a remote position (See description for details and requirements). Where are you located?
Are you willing to undergo a federal background check?
Education:
Bachelor's (Required)
Experience:
data scientist or data engineer role: 5 years (Required)
cloud services: 2 years (Required)
Microsoft SQL (development and design)?: 2 years (Required)
optimizing ‘big data’ pipelines, architectures and data sets: 2 years (Required)
AWS: 1 year (Preferred)
Python: 1 year (Required)
Java: 1 year (Required)
Work Location: Remote","$150,000 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,Unknown / Non-Applicable
"Angle Health
4.0",4.0,Remote,Data Engineer,"Changing Healthcare For Good
At Angle Health, we believe the healthcare system should be accessible, transparent, and easy to navigate. As a digital-first, data-driven health plan, we are replacing legacy systems with modern infrastructure to deliver our members the care they need when they need it. If you want to build the future of healthcare, we'd love for you to join us.

The Role
As a Data Engineer on the Strategy team at Angle Health, you will be part of an elite team of problem solvers tackling some of the hardest operational challenges across the business. You will be working closely with Technical Product Managers and other engineers in small teams embedded within functions across the company—from sales to operations to finance and more.

In partnership with operational stakeholders, your job will be to quickly understand business workflows, and design and implement technical, data-driven solutions to achieve the intended business outcomes. That may include architecting backend micro-services, building and maintaining robust data pipelines, standing up health metrics and automated alerts, and developing scalable technical solutions to support Angle Health's day-to-day operations.

Every day will be a new learning experience in this role and may span discussing architecture with fellow engineers, wrangling large-scale data, coding a custom web app or micro-service, or speaking with customers and executives.

This role is ideal for entrepreneurial engineers, applied data scientists, and creative, intellectually curious thinkers that want to solve high-value problems in healthcare.

This position may be based in San Francisco, New York City, Salt Lake City, or Remote.

We are currently trialing various titles for the same role. Please consider the following posted roles as the same position: Deployed Engineer, Software Engineer, Business Operations, and Data Engineer. Please note these are the same, so if you have already applied for one position, there is no need to reapply for the others.
What We Value
A strong engineering background in computer science, software engineering, data science, mathematics, or similar technical field is required for this role
Proficiency in programming languages (e.g. Python, SQL, Java, TypeScript/JavaScript, or similar) and data engineering frameworks
A highly analytical mindset and an eagerness to build technical solutions to complex business problems
High attention to detail and intellectual curiosity—you're not satisfied with surface-level answers. You want to dive into the data, the ""how,"" and the ""why"" because ""the way it's always been done"" is not always the way it should be done
Low ego—the outcome matters more than who gets the credit
Demonstrated ability to collaborate effectively in teams of technical and non-technical individuals
Highly organized with an ability to multitask, problem solve, and balance competing priorities in a rapidly changing environment
Because We Value You:

Competitive compensation and stock options
100% company paid comprehensive health, vision & dental insurance for you and your dependents
Supplemental Life, AD&D and Short Term Disability coverage options
Discretionary time off
Opportunity for rapid career progression
Relocation assistance (if relocation is required)
3 months of paid parental leave and flexible return to work policy (after 10 months of employment)
Work-from-home stipend for remote employees
Company provided lunch for in-office employees
401(k) account
Other benefits coming soon!

Backed by a team of world class investors, we are a healthcare startup on a mission to make our health system more effective, accessible, and affordable to everyone. From running large hospitals and health plans to serving on federal healthcare advisory boards to solving the world's hardest problems at Palantir, our team has done it all. As part of this core group at Angle Health, you will have the right balance of support and autonomy to grow both personally and professionally and the opportunity to own large parts of the business and scale with the company.

Angle Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. Angle Health is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities.",#N/A,1 to 50 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2019,Unknown / Non-Applicable
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
LOVEFOODIES INC,#N/A,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",$30.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ESRI, Inc.
4.0",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1","$98,800 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD)
"Double Line, Inc.
4.2",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w","$85,882 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD)
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fiserv, Inc.
3.2",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.","$108,451 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD)
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
"AgileEngine
5.0",5.0,Remote,Senior/Lead Data Engineer,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Work collaboratively with other engineers, architects, data scientists, analytics teams, and business product owners in an agile environment
Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools
Design and build robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Must haves
Bachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience
6 years of experience with Python or Java
4+ years of experience in building data lake, cloud data platform leveraging cloud (GCP/AWS) cloud native architecture, ETL/ELT, and data integration
3 years of development experience with cloud services ( AWS,GCP,AZURE) utilizing various support tools (e.g. GCS, Dataproc, Cloud Data flow, Airflow(Composer), Kafka , Cloud Pub/Sub)
Expertise in developing distributed data processing and Streaming frameworks and architectures (Apache Spark, Apache Beam, Apache Flink, )
In-depth knowledge of NoSQL database technologies (e.g. MongoDB, BigTable, DynamoDB)
Expertise in build and deployment tools – (Visual Studio, PyCharm, Git/Bitbucket/Bamboo, Maven, Jenkins, Nexus, )
5 years of experience and expertise in database design techniques and philosophies (e.g. RDBMS, Document, Star Schema, Kimball Model)
5 years of experience with integration and service frameworks (e.g API Gateways, Apache Camel, Swagger API, Zookeeper, Kafka, messaging tools, microservices)
Expertise with containerized Microservices and REST/GraphQL based API development
Experience leveraging continuous integration/development tools (e.g. Jenkins, Docker, Containers, OpenShift, Kubernetes, and container automation) in a Ci/CD pipeline
Advanced understanding of software development and research tools
Attention to detail and results-oriented, with a strong customer focus
Ability to work as part of a team and independently
Problem-solving and technical communication skills
Ability to prioritize workload to meet tight deadlines
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
Our team of data experts and engineers has helped create a supply chain management platform used by Forbes 500 enterprises and featured by Gartner. We’ve accompanied the platform from an early prototype to a full-fledged solution based on big data, AI, and machine learning. Data quality is integral to our current engagement with this project as we’re expanding the platform functionality.
AgileEngine has also been working on data and ML solutions in multiple industries and verticals like telecom, FinTech, energy efficiency, and more. Our work with our clients’ data has contributed to patented software products, Nasdaq-grade fraud detection, and products used by Deloitte and T-Mobile.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"Dovenmuehle Mortgage, Inc.
2.6",2.6,"San Francisco, CA",Data Engineer,"Data Engineer
DMI Software, the San Francisco branch of Dovenmuehle Mortgage, Inc, the leading sub-servicer of mortgage loans in the United States, is looking for a talented and enthusiastic data engineer. We work exclusively in Software Development. Our growing office offers the feel of a startup with the backing and security of a long-established company. We aspire to create elegantly scalable products while fostering the continued growth of each team member. The ideal candidate will have 5+ years relevant experience, including Hadoop Ecosystem or similar, and with a scripting language.

Here we believe that the best software is created by an eclectic set of voices, and we strive to nurture an environment rich in differing opinion, belief, and background. Only in this way can we develop revolutionary products capable of meeting the varied needs of an increasingly interconnected world.

What You’ll Be Doing:
Design, implement, automate, and maintain large-scale enterprise ETL processes
Evolve data model and schema based on business and engineering needs
Oversee systems tracking data quality and consistency
Collaborate with data analysts to bridge business goals with data delivery

Requirements:
5+ years data engineering experience
Highly experienced using Python, SQL and and Hadoop
Excellent communication, analytical and problem-solving skills
Keen attention to detail while keeping an eye toward the big picture
You are comfortable with the nuts and bolts of systems programming in the Linux environment (shell/bash scripting)
Experience working in an Agile environment
Excellent presentation and communication skills
Experience profiling, debugging, tracing, and or parallelizing/optimizing Python code
Ideal candidate is one who can adapt and adopt to our existing architectures while also making impactful improvements and suggestions.

Job Type: Full-time","$135,927 /yr (est.)",1001 to 5000 Employees,Company - Private,Financial Services,Banking & Lending,1844,Unknown / Non-Applicable
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
BOTG LLC,#N/A,"Chicago, IL",Data Engineer,"We are looking for a Data Engineer in Chicago, IL (Hybrid) for a direct-hire position.
Job Description:
Position: Data Engineer - Centralized Data Science and Analytics (CDSA)
Location: Chicago, IL (Hybrid)
Duration: Direct-hire position
Client: Direct Client
Note: This is a W2 direct-hire role. Looking for candidates who are open to work independently on W2.
Requirements:
· Experience building and optimizing ""big data"" data pipelines, architectures and data sets.
· Working knowledge of message queuing, stream processing and highly scalable ""big data"" data stores.
· Advanced working SQL knowledge and experience working with cloud and relational databases.
· Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
· Experience building processes supporting data transformation, data structures, metadata, dependency and workload management.
· A successful history of manipulating, processing and extracting value from large, disconnected datasets.
· Experience using the following software/tools:
· Relational SQL and NoSQL databases, including Postgres.
· Data pipeline and workflow management tools.
· Azure cloud services.
· Object-oriented/object function scripting languages: Python, PySpark Java, C++, R/RStudio/RSpark.
· CI/CD systems.
· Strong understanding across cloud and infrastructure components (server, storage, network, data, and applications) and ability to deliver end to end cloud infrastructure, architectures, and designs.
· Knowledge and implementation of enterprise scale cloud security platforms and tooling.
· Experience with enterprise applications, solutions, and data center infrastructures.
· Bachelor's degree in computer science or similar field; master's degree a plus.
· Exceptional product, project and client management skills.
· Azure, AWS or any other cloud/data engineering certifications are preferred.
Job Type: Full-time
Schedule:
8 hour shift
Day shift
Monday to Friday
Experience:
Big data: 5 years (Required)
Advanced SQL: 5 years (Required)
Cloud: 3 years (Required)
CI/CD: 3 years (Preferred)
NoSQL: 2 years (Required)
Work Location: Hybrid remote in Chicago, IL 60606","$85,894 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
TheHive,#N/A,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",$35.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
"CapitalTech Solutions
4.5",4.5,Remote,Data Engineer,"Job Description:
12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
Complete Description:
Require the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals:
(1) Establish a data governance program,
(2) Perform a comprehensive data gap analysis,
(3) Design a master data architecture,
(4) Create a data warehouse for all data assets,
(5) Develop a front-end for program staff to quickly access workforce information and visualize program status,
(6) Create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities
(7) Foster relations with other agencies and improve inter-agency data integration.
The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff.
Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.
Develop and maintain an understanding of the data landscape including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.
Support the Data Management Project team to develop and maintain data quality controls.
Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.
Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.
Support the data stewards to troubleshoot and resolve data issues.
Support business users to obtain requirements for enhancements and/or new analytic assets.
Assist in the Development of data asset training and documentation.
Participate in the development and implementation of a data standard.
Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.
Job Types: Full-time, Contract
Pay: $66.00 - $74.00 per hour
Schedule:
8 hour shift
Experience:
in SQL, Python, R, JavaScript, JSON: 10 years (Preferred)
Agile Testing, Automation Testing, Black-box Testing: 10 years (Preferred)
Windows and Linux: 10 years (Preferred)
of BI tool architecture, Tableau: 9 years (Required)
Work Location: Remote",$70.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,1999,$25 to $100 million (USD)
"etrailer.com
3.9",3.9,Remote,Data Engineer/Data Scientist,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow","$140,000 /yr (est.)",501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
"PrizePicks
4.9",4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1","$101,755 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
Invictus Data,#N/A,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location","$125,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ITExpertUS
2.8",2.8,"Chicago, IL",Data Engineer,"Client: Agilisium
Role: Data Engineer
Location: Chicago, IL – Hybrid role – Need to be onsite once/twice in a month.
Type: Long term contract
Job Description
Title: Data Engineer
We are looking for a savvy Data Engineer to join our growing team. The hire will be responsible for expanding and optimizing our data and pipeline architecture and data flow and collection. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. In addition, the Data Engineer will collaborate and support software developers, implementation architecture, and data engineers on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. In addition, they must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
Responsibilities:
Because we work on a programmatic leading edge of many technologies, we need someone who is a creative problem solver, resourceful in getting things done, and can shift productively to or from working independently and collaboratively. This person would also take on the following responsibilities:
Process unstructured data into a form suitable for analysis.
Support the business with ad hoc data analysis and build reliable data pipelines.
Implementation of best practices and IT operations in mission-critical tighter SLA data
pipelines using Airflow
Query Engine Migration from Dremio to Redshift.
We leverage: Multiple AWS Data & Analytic Services(e.g.,Glue, Kinesis, S3) , SQL (e.g.,
PostgreSQL, Redshift, Athena); NoSQL (e.g., DocumentDB, MongoDB); Kafka, Docker, Spark(AWS EMR and DataBricks), Airflow, Dremio, Qubole, etc.
We use AWS extensively, so experience with AWS cloud and AWS Data & Analytics certification will help you hit the ground running.
Business Skills for Data Engineers:
Creative Problem-Solving: Approaching data organization challenges with a clear eye on what is important; employing the suitable approach/methods to make the maximum use of time and human resources.
Effective Collaboration: Carefully listening to management, data engineers, and architects to establish their needs.
Intellectual Curiosity: Exploring new territories and finding creative and pragmatic ways to solve data problems.
Industry Knowledge: Understanding and perspective of how industry functions and how data can be collected, analyzed, and utilized for value creation; maintaining flexibility in the face of big data developments.
Skills and Qualifications:
3 to 5 years of real-world Data Engineering experience.
Programming experience, ideally in Python and other data engineering languages like Scala
Programming knowledge to clean structure and semi-structure datasets.
Experience processing large amounts of structured and unstructured data. Streaming data experience is a plus.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Background in Linux
Build the infrastructure required for optimal extraction, transformation, and loading of data
from various data sources using SQL and other cloud ‘big data’ technologies like DataBricks, Snowflake, Dremio, and Qubole.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience creating a platform on which complex data pipelines are built using orchestration tools like Airflow, and Astronomer.
Experience with real-time sync between OLTP and OLAP using AWS technologies like real- time sync between AWS Aurora and AWS Redshift.
Job Type: Full-time
Salary: Up to $68.00 per hour
Schedule:
8 hour shift
Work Location: On the road
Speak with the employer
+91 6305048577",$68.00 /hr (est.),501 to 1000 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"E-Logic INC
4.4",4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.","$84,277 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"Fiserv, Inc.
3.2",3.2,"Berkeley Heights, NJ","Software Engineer, Senior – Data and Analytics","What does a Senior Software Engineer – Data and Analytics do?
As an experienced member of our Data Engineering Platform Group you will be responsible for building and taking ownership over the successful design and development of data engineering projects within Fiserv’s Enterprise Data Analytics division. You will be required to apply your depth of knowledge and expertise to all aspects of the data engineering lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. You'll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally. You will Lead large-scale data engineering, integration and warehousing projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL. Additional responsibilities include, but are not limited to Architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years' experience with building java applications
Bachelors in Computer Science or related field
Provide technical leadership in developing data solutions and building frameworks
Hands-on experience with Springboot framework & Java
Java experience with OOPS concepts, multithreading
Hands on experience with leveraging CI/CD to rapidly build & test application code
Conduct code reviews and strive for improvement in software engineering quality
Review data architecture and develop detailed implementation design
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Experience working on SQL Databases
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance","$101,076 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD)
Ascent Solutions,#N/A,Remote,Hadoop Big Data Engineer,"Hadoop Big Data Engineer
Remote
Must Haves • Strong experience in Big Data, Cloudera Distribution 7.x, Cloud migration, RDBMS • Strong Experience with Amazon EMR/Databricks/Cloudera CDP • 4-5 experience building data pipelines using Hadoop components Sqoop, Hive, Solr, MR, Impala, Spark, Spark SQL., HBase. • 4-5 years of programming experience in Python, Java and Scala is must. • Strong experience with REST API development using Python frameworks (Django, Flask etc.) • Micro Services/Web service development experience using Spring framework is highly desirable.
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"IntelliBridge LLC
3.9",3.9,"McLean, VA",Data Engineer,"Title: Data Engineer
Location: Permanent remote role
Clearance: Not required: Start date not contingent on a having or completion of a clearance, however one could be offered upon starting for future programs
Overview:
IntelliBridge is seeking a Data Engineer to collaborate with technical and non-technical data and development team members to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of analytics that provide help ensure national security. You'll be able to gain experience in designing cloud architectures while providing critical support to the client's mission. You will be responsible for designing and building smart data pipelines that are secure, robust, and alerting. You will also create innovative ways to combine disparate data sources and build integrated datasets for advanced analytics.
As a direct employee of IntelliBridge, you would receive a benefit package that includes health/dental/vision insurance coverage, 401K with company match, PTO & paid holidays, and annual tuition/training assistance. For more information, please visit our website.
Responsibilities/Duties:
Build and maintain the infrastructure to support integration, extraction, transformation, and loading (ETL) of data from a wide variety of data sources, such as relational SQL and NoSQL databases, and other platform APIs
Design data pipelines that are robust and secure including pipeline monitoring and alerting mechanisms
Create innovative ways to orchestrate data ETL processes
Guide and support the implementation of new data engineering solutions to enable adoption and growth
Integrate disparate data sources into powerful datasets for advanced analytics
Recommend tools and capabilities based on understanding the current environment and knowledge of various on-premises, cloud based, and hybrid capabilities/technologies
Monitor existing metrics, analyze data, and lead partnership with other Data and Analytics personnel to identify and implement system and process improvements
Develop processes to convert aggregated data from teams, collection tools, and dashboards
Configure and manage data analytic frameworks and pipelines using databases and tools
Develop Python packages to improve application capabilities
Apply distributed systems concepts and principles such as consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms
Administrate cloud computing and CI/CD pipelines to include Amazon Web Service (AWS)
Investigate legacy code to determine areas of improvement and automation
Required Qualifications:
Excellent verbal and written communications
Bachelor’s Degree in a STEM filed or Master’s Degree in Operations Research, Industrial Engineering, Applied Mathematics, Statistics, Physics, Computer Science, or related fields
5+ years of experience with Python, SQL, Unix(Linux), and handling semi-structured data (JSON)
3+ years of experience with Elasticsearch, Logstash, and Kibana (ELK stack)
3+ years of experience with Amazon Web Services (AWS) or other cloud provider
Proficient in Docker
Proficient in Agile Development
Proficient in Git Operations
Experience understanding requirements, analyzing data, discovering opportunities, addressing gaps and communicating them to multiple individuals and stakeholders
Demonstrated expertise in technical data engineering on integrating complex applications, systems, software, and project activities and integrating them into cloud-based resources
General knowledge in machine learning for building efficient and accurate data pipelines that occur for downstream users, such as for data scientists to create the models and analytics that produce insight
Preferred Qualifications:
Organizational skills and a love of documentation
Experienced in Airflow
Experience with demonstrated strength in data lake/warehouse technical architecture, infrastructure components, and ETL/ELT pipelines
Experience with geo-spatial data
Experience with deployments via Kubernetes
Experience with configuring and aggregating logs for data analysis using Splunk or ELK solutions
Experience with developing and managing machine images or templates to automate cloud deployments
About Us:
IntelliBridge delivers IT strategy, cloud, cybersecurity, application, data and analytics, enterprise IT, intelligence analysis, and mission operation support services to accelerate technical performance and efficiency for Defense, Civilian, and National Security & Federal Law Enforcement clients.","$92,898 /yr (est.)",501 to 1000 Employees,Company - Private,Government & Public Administration,National Agencies,#N/A,Unknown / Non-Applicable
"Oddball
4.6",4.6,Remote,Data Engineer,"Oddball believes that the best products are built when teams understand and value the things they are working on. We value learning and growth and the ability to make a big impact at a small company. We believe that we can make big changes happen and improve the daily lives of millions of people by bringing quality software to the federal space.
As a Database Engineer you will be responsible for helping to develop production-quality analytics tools, utilizing existing data sources to empower stakeholders to inform research and development. Act as a liaison between engineers and analysts. Implement and enforce standards and procedures to ensure data is managed consistently and properly integrated.
Experience Requirements
5+ years of proven data and performance engineering experience.
A proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs.
Experience with system analysis, data analysis or programming, using a variety of computer languages and procedures.
Experience working in Agile environments.
Experience in developing, analyzing, and presenting data models.
Knowledge of data management, data standardization, and data governance.
Ability to analyze source data for potential data quality issues.
Expert in SQL and/or SQL based languages.
Create supporting documentation, such as metadata and diagrams of entity relationships, business processes, and process flow.
Create plans, test files, and scripts for data warehouse testing, ranging from unit to integration testing.
This is a Remote position. This is a salaried role. Oddball offers both a tech, and a continuing education stipend.
Must be located within the United States.
Clearances
Ability to obtain low-level federal clearance is required.
Oddball is an Equal Opportunity Employer and does not discriminate against applicants based on race, religion, color, disability, medical condition, legally protected genetic information, national origin, gender, sexual orientation, marital status, gender identity or expression, sex (including pregnancy, childbirth or related medical conditions), age, veteran status or other legally protected characteristics. Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Work Location: Remote",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"MARVEL TECHNOLOGIES INC
3.7",3.7,Remote,Data Engineer,"Primary Skills
SCALA, SPARK, SQL, HADOOP, AWS, HIVE, Spark SQL, HIVE QL, CICD (Continuous Integration/Continuous Delivery), VCS (GIT HUB)
Coding in Scala
Designing in of HADOOP ecosystem
Hands-on experience on AWS tools like EMR, EC2
Hands-on experience of SQL in Big Data: SQL, Spark SQL, Hive QL
Proficient in working with large data sets and pipelines
Proficient with workflow scheduling / orchestration tools
Well versed with CICD process and VCS
Databricks is a big PLUS
Job Types: Full-time, Contract
Pay: $50.00 - $58.00 per hour
Schedule:
8 hour shift
Monday to Friday
Experience:
Spark: 4 years (Required)
Scala: 4 years (Required)
Hadoop: 3 years (Required)
Aws: 3 years (Required)
Hive: 3 years (Required)
CI/CD, VCS: 3 years (Required)
Databricks: 1 year (Required)
Work Location: Remote",$54.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$5 to $25 million (USD)
Grid,#N/A,"Oakland, CA",Data Engineer,"Today’s financial system is built to favor those with money. Grid’s mission is to level that playing field by building financial products that help users better manage their financial future. The Grid app lets users access cash, build credit, spend money, file their taxes and lots, lots more.

Grid is a fast growing team that’s deeply passionate about making a difference in the lives of millions. We’re solving huge problems and believe that a merit driven culture allows every team member to play a big role. Join our growing team in our Bay Area headquarters or additional offices across the US.

Our Data Engineers are responsible for converting the data surrounding a customer's financial life into tools that make our customers smarter and better equipped financially for the future. We value making intelligent decisions backed by good data and tools. We're looking for people who share our values, particularly if you have experience analyzing, processing, and learning from large data sets.

We’re looking for a seasoned data engineer who can help us lay the foundation of an exceptional data engineering practice. The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions. You will be writing code, setting style guides and collaborating cross-functionally with product, engineering and leadership.
Problems we work on:
Analytics: Collect all the data for a user into tools that help our customers
Machine Learning: Using a variety of techniques to reach better insights
Data Processing: Managing data & statistics using scalable and efficient technologies
Visualization: Envision our data as beautiful graphs and tools that allow customers to explore their data & ask their own questions
Risk: Analyze data for anomalous patterns and build tools that allow us to find bad actors quickly
We Practice:
Open collaboration
Code reviews
Testing
Agile development
We Use:
Go
Python
MySQL
Google Cloud Platform
Kubernetes
Kubeflow
Docker
Google Pubsub
BigQuery
Firebase
We’re looking for Engineers to:
Design and implement platform services, frameworks and ecosystems
Build a scalable, reliable, operable and performant big data workflow platform for data scientists/engineers, AI/ML engineers, and product/operation team members
Drive efficiency and reliability improvements through design and automation: performance, scaling, observability, and monitoring
Requirements:
Strong programming skills with Python
Strong programming skills with a typed programming language, such as Java, Scala, Go, etc.
Disciplined approach to development, testing, and quality assurance
Excellent communication skills, capable of explaining highly technical problems in English
Understand data processing and ETL, hands on experience building pipelines and using frameworks such as Hive hdfs, Presto, Spark, etc.
Really strong candidates may have:
Really strong candidates may have:
Actively contribute to open source software
Worked with a strong, lean-based development environment
Previous work experience in a start-up environment
Ability to recognize the right tool for the right situation/problem.
Will have strong programming skill in Go","$112,815 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Agiles Enterprise
3.4",3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road","$136,537 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Less than $1 million (USD)
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Iyka Enterprises, Inc.
3.8",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115","$85,831 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
"Second Wave Delivery Systems, LLC",#N/A,Remote,Data Engineer,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020,Unknown / Non-Applicable
"FlexIT Inc
4.0",4.0,"Beaverton, OR",Data Engineer,"FlexIT client is looking for a Data Engineer 12 months contract in Beaverton, Oregon.
Looking for local candidates to work on site.
Top skills: Python, SQL , AWS, Spark","$106,334 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"PrizePicks
4.9",4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1","$101,755 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"FlexIT Inc
4.0",4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.","$100,714 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Metrohm Spectro,#N/A,"Plainsboro, NJ",Data Engineer,"Metrohm Spectro is an advanced mobile spectroscopic instrumentation leader, developing, manufacturing, and servicing state-of-the-art analytical devices, including portable and handheld Raman analyzers.We provide solutions for the pharmaceutical, biomedical, safety and security, chemical, and academic research industries. We are constantly growing with new products and new opportunities, and are always looking for talented, dedicated employees to join us and grow together as a team.
With the fast growth of our business, we have an immediate vacancy for a full-time Data Engineer for our Plainsboro, NJ location.
Job Description
In this role, you will take responsibility for developing and maintaining databases within software products. You will be required to have hands-on problem solving, from the upkeep and generation of database, to data validation as well as the capability of data processing and analysis, and will be able to perform data processing algorithm validation with the knowledge of data science. To excel in this role, you need to be very organized with a fine eye for detail, and openness to learn new skills to meet growing business needs.
Education
· Bachelor’s of Science degree from an accredited university or college in chemistry, physics, mathematics or computer science.
Experience:
· High-level proficiency in Microsoft Excel or other automated data management tool.
· Experienced in database programming and familiar with all popular database types. Good understanding of MySQL is a plus;
· Knowledge in MATLAB, R, Python or SAS tools for data processing and analysis;
· Knowledge in AI/machine learning and data mining basics;
· Knowledge in C/C++ programming for data processing algorithm;
· High-level proficiency in Microsoft Excel or other automated data management tool;
· Knowledge in Network/Cloud infrastructure will be a plus.
ROLE AND JOB RESPONSIBILITIES
· Develop and maintain database for cross-platform software implementation on all BWTEK spectroscopic products.
· Assist in data process and analysis algorithm design and validation.
· Collaboration with entire software team for product enhancement and new product.
Job Type: Full-time
Application Question(s):
What are your salary expectations?
Work Location: Plainsboro, NJ - on site
Can you commute to this location?
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
5x8
8 hour shift
Monday to Friday
Ability to commute/relocate:
Plainsboro, NJ 08536: Reliably commute or planning to relocate before starting work (Preferred)
Application Question(s):
Will you need sponsorship to work in US?
Work Location: In person",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Manufacturers Bank
3.2",3.2,"Los Angeles, CA",Data Engineer,"GENERAL SUMMARY:
Manufacturers bank is looking for a Data Engineer with strong software engineering backgrounds, a ""platform engineering mindset"" and deep passion for data. This Engineer will help to shape the technical direction of the data domain within the organization. The Engineer will also partner closely with internal business units and external vendors to develop a high performance, reliable and scalable data environment for the bank.

PRINCIPAL DUTIES & RESPONSIBILITIES:
Develop and automate high-performance data processing systems and visualization to ensure reliability and meet critical business requirements.
Design, develop, implement, document and support Data & Analytics systems which adhere to the industry best practices.
Lead data engineering projects, and overall strategy for data governance, security, privacy, quality, and retention.
Triage and resolve issues reported by Risk, Operations, and other business stakeholders.
Mentor junior engineers and continue promoting data engineering and analytics tooling & standards.","$115,908 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,#N/A,$25 to $100 million (USD)
Grid,#N/A,"Oakland, CA",Data Engineer,"Today’s financial system is built to favor those with money. Grid’s mission is to level that playing field by building financial products that help users better manage their financial future. The Grid app lets users access cash, build credit, spend money, file their taxes and lots, lots more.

Grid is a fast growing team that’s deeply passionate about making a difference in the lives of millions. We’re solving huge problems and believe that a merit driven culture allows every team member to play a big role. Join our growing team in our Bay Area headquarters or additional offices across the US.

Our Data Engineers are responsible for converting the data surrounding a customer's financial life into tools that make our customers smarter and better equipped financially for the future. We value making intelligent decisions backed by good data and tools. We're looking for people who share our values, particularly if you have experience analyzing, processing, and learning from large data sets.

We’re looking for a seasoned data engineer who can help us lay the foundation of an exceptional data engineering practice. The ideal candidate will be confident with a programming language of their choice, be able to learn new technologies quickly, have strong software engineering and computer science fundamentals and have extensive experience with common big data workflow frameworks and solutions. You will be writing code, setting style guides and collaborating cross-functionally with product, engineering and leadership.
Problems we work on:
Analytics: Collect all the data for a user into tools that help our customers
Machine Learning: Using a variety of techniques to reach better insights
Data Processing: Managing data & statistics using scalable and efficient technologies
Visualization: Envision our data as beautiful graphs and tools that allow customers to explore their data & ask their own questions
Risk: Analyze data for anomalous patterns and build tools that allow us to find bad actors quickly
We Practice:
Open collaboration
Code reviews
Testing
Agile development
We Use:
Go
Python
MySQL
Google Cloud Platform
Kubernetes
Kubeflow
Docker
Google Pubsub
BigQuery
Firebase
We’re looking for Engineers to:
Design and implement platform services, frameworks and ecosystems
Build a scalable, reliable, operable and performant big data workflow platform for data scientists/engineers, AI/ML engineers, and product/operation team members
Drive efficiency and reliability improvements through design and automation: performance, scaling, observability, and monitoring
Requirements:
Strong programming skills with Python
Strong programming skills with a typed programming language, such as Java, Scala, Go, etc.
Disciplined approach to development, testing, and quality assurance
Excellent communication skills, capable of explaining highly technical problems in English
Understand data processing and ETL, hands on experience building pipelines and using frameworks such as Hive hdfs, Presto, Spark, etc.
Really strong candidates may have:
Really strong candidates may have:
Actively contribute to open source software
Worked with a strong, lean-based development environment
Previous work experience in a start-up environment
Ability to recognize the right tool for the right situation/problem.
Will have strong programming skill in Go","$112,815 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Edrstaffing,#N/A,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker","$120,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Govini
3.7",3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.","$88,151 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable
"etrailer.com
3.9",3.9,Remote,Data Engineer/Data Scientist,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow","$140,000 /yr (est.)",501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
"Airbus Americas
4.0",4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.","$92,046 /yr (est.)",10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD)
"Fiserv, Inc.
3.2",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.","$108,451 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD)
"EMONICS LLC
3.8",3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location","$150,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD)
"Arthur Grand Technologies Inc
4.8",4.8,"Atlanta, GA",AWS Data Engineer,"Role: AWS Data Engineer
Location: Atlanta, GA (Meet customer Monthly once post client selection & onboarding)
JD for AWS Data Engineer
Experience with the core AWS services, plus the specifics mentioned in this job description.
Experience with Docker . Having used ECS , SNS/SQS, Athena, Lambda,S3, RDS and layers in the past is a plus.
Proficiency in at least in Python, Java
Strong notions of security best practices (e.g. using IAM Roles, KMS, etc.).
Experience with monitoring solutions such as CloudWatch, Cloud Trail.
Previous exposure to large-scale systems design.
Knowledge of writing infrastructure as code (IaC) using CloudFormation or Terraform.
Experience with building or maintaining cloud-native applications.
Past experience with the serverless approaches using AWS Lambda is a plus. For example, the Serverless Application Model (AWS SAM).
Job Types: Full-time, Contract
Schedule:
10 hour shift
8 hour shift
Work Location: Remote","$94,994 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable
"Nursa
4.3",4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!","$89,485 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
"Second Wave Delivery Systems, LLC",#N/A,Remote,Data Engineer,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020,Unknown / Non-Applicable
"Oddball
4.6",4.6,Remote,Data Engineer,"Oddball believes that the best products are built when teams understand and value the things they are working on. We value learning and growth and the ability to make a big impact at a small company. We believe that we can make big changes happen and improve the daily lives of millions of people by bringing quality software to the federal space.
As a Database Engineer you will be responsible for helping to develop production-quality analytics tools, utilizing existing data sources to empower stakeholders to inform research and development. Act as a liaison between engineers and analysts. Implement and enforce standards and procedures to ensure data is managed consistently and properly integrated.
Experience Requirements
5+ years of proven data and performance engineering experience.
A proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs.
Experience with system analysis, data analysis or programming, using a variety of computer languages and procedures.
Experience working in Agile environments.
Experience in developing, analyzing, and presenting data models.
Knowledge of data management, data standardization, and data governance.
Ability to analyze source data for potential data quality issues.
Expert in SQL and/or SQL based languages.
Create supporting documentation, such as metadata and diagrams of entity relationships, business processes, and process flow.
Create plans, test files, and scripts for data warehouse testing, ranging from unit to integration testing.
This is a Remote position. This is a salaried role. Oddball offers both a tech, and a continuing education stipend.
Must be located within the United States.
Clearances
Ability to obtain low-level federal clearance is required.
Oddball is an Equal Opportunity Employer and does not discriminate against applicants based on race, religion, color, disability, medical condition, legally protected genetic information, national origin, gender, sexual orientation, marital status, gender identity or expression, sex (including pregnancy, childbirth or related medical conditions), age, veteran status or other legally protected characteristics. Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Work Location: Remote",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"IBR (Imagine Believe Realize)
4.5",4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",#N/A,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable
"Agiles Enterprise
3.4",3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road","$136,537 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Less than $1 million (USD)
"Globaleur
4.5",4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information
Deploy sophisticated analytics programs, machine learning, and statistical methods
Ensure compliance with data governance and security policies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java
Have working experiences in e-commence, travel, marketing domain is a plus
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.","$119,136 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017,Unknown / Non-Applicable
"E-Logic INC
4.4",4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.","$84,277 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
Lendem Solutions,#N/A,"Plano, TX",Data Engineer,"LENDEM Solutions is looking at add a Data Engineer to our business!
CORE COMPETENCIES
Ability to thrive in a dynamic and fast-paced environment, drive change, and collaborate effectively with a variety of individuals and teams
Strong analytical and problem-solving skills
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
PRINCIPAL DUTIES
Shredding and parsing data to extract meaningful information
Preparing historical and live data for data studies to identify trends and patterns
Performing adhoc analysis to answer specific business questions and provide insights
Working with relational databases to model and query complex data relationships
Understanding and working with MySQL data in several different data environments
Mining consumer loan data utilizing tools such as SQL, Python, R, or other comparable data mining tools.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Engineering, or a related field Master’s degree preferred
3+ years of experience as a data engineer or similar role
REQUIRED SKILLS, ABILITIES, SOFT SKILL FACTORS
· Strong experience in ETL processes, data modeling, and data warehousing
Experience working with graph databases, such as Neo4j or Apache Cassandra
Expertise in programming languages such as SQL, Python
Familiarity with big data technologies, such as Hadoop, Spark, or Kafka
Ability to analyze and manipulate large and complex data sets
Strong problem-solving skills and the ability to work independently and as part of a team
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
If you are a data engineer with a passion for problem-solving and a strong background in ETL processes, data modeling, and graph databases, we encourage you to apply for this exciting opportunity. We offer a competitive salary, comprehensive benefits, and a dynamic work environment where you can continue to grow and develop your skills.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Plano, TX 75024: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 5 years (Preferred)
SQL: 5 years (Preferred)
Work Location: Hybrid remote in Plano, TX 75024",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
Okaya Corp,#N/A,"Mahwah, NJ",Azure Data Engineer,"Job Title: Azure Data Engineer
Location: Mahwah, NJ
Duration: Full Time
Skills Required:
Azure data factory, data bricks, data lake, automation, and performance optimization of ETL
Strong Hands-on experience in ADF, data bricks, data lake, power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end-to-end CI/CD implementation or Devops process in Data & Analytics context.
Design, plan, develop and update technical docs & BI solutions.
Understand the requirements and define the data load strategy for data refresh.
Create, debug, troubleshoot and deploy solutions.
Work on ETL design
Designing and optimization of ETL Process using ADF
Implementing end-to-end automated ETL processes and monitoring of those processes using various options using Azure
Experience in Azure data factory, data bricks, data lake, automation, and performance optimization of ETL
Job Type: Full-time
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"FlexIT Inc
4.0",4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.","$100,714 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Fiserv, Inc.
3.2",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.","$108,451 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD)
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
"IBR (Imagine Believe Realize)
4.5",4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",#N/A,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable
"Airbus Americas
4.0",4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.","$92,046 /yr (est.)",10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD)
"PSRTEK
4.6",4.6,"Mount Laurel, NJ",Lead Data Engineer,"AR# 226054
Role: Lead Data Engineer /Databricks (On-site)
Location: Mt. Laurel, NJ
Full-time
Visa status: GC/USC
Must have skills:
Databricks, Python, RDBMS, PowerShell scripting, data warehouse
Detailed JD:
Experience in ETL/Pipeline Development using tools such as Azure Databricks/Apache Spark and Azure
Data Factory with development expertise on batch and real-time data integration
Experience in programming using Python
RDBMS knowledge and experience in writing the Store Procedures
Experience in writing bash and Power shell scripting.
Experience in data ingestion, preparation, integration, and operationalization techniques in optimally addressing the data requirements
Experience in Cloud data warehouse like Azure Synapse, Snowflake analytical warehouse
Experience with Orchestration tools, Azure DevOps, and GitHub
Experience in building end to end architecture for Data Lakes, Data Warehouses and Data Marts
Experience in relational data processing technology like MS SQL, Delta Lake, Spark SQL, SQL Server
Experience to own end-to-end development, including coding, testing, debugging and deployment
Extensive knowledge of ETL and Data Warehousing concepts, strategies, methodologies
Experience working with structured and unstructured data
Familiarity with Azure services like Azure functions, Azure Data Lake Store, Azure Cosmos
Ability to provide solutions that are forward-thinking in data and analytics
Job Type: Full-time
Salary: $120.00 - $130.00 per year
Schedule:
8 hour shift
Experience:
Data Warehouse: 10 years (Required)
Python: 10 years (Required)
PowerShell: 10 years (Required)
Data Bricks: 10 years (Required)
RDBMS: 10 years (Required)
Work Location: On the road
Speak with the employer
+91 609-917-9952","$121,211 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Pomeroy Technologies, LLC.
3.0",3.0,"Cleveland, OH",Data Engineer,"The Data Engineer is responsible for the development, integration, and implementation of ETL Feeds in support of various modern architecture projects. This role requires hands-on knowledge of modern Integrations Platform solutions, file, and database ETL, and XML transformation. The incumbent will act as a technical resource, participating in the successful design, development, unit testing, implementation, and support of new and existing ETL integrations.
Essential Functions
Must be able to acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design.
Working with manager, gathering business requirements for the development of new applications.
Perform programming assignments based on established standards, methods, and best practices.
Develop ETL solutions as assigned in supporting new or updated business solutions and requirements.
Help business users develop functional requirements for integration.
Provide support during user acceptance testing.
Prepare documentation for code changes.
Code ETL feeds, and conduct tests to verify correct program functionality, reliability, and data integrity.
Maintain and modify ETL feeds; make approved changes by amending application documentation, developing detailed programming logic, and coding changes.
Provide support and resolution for application problems and issues.
Provide user and system documentation for operational and technical support.
Assist in the discovery and investigation of production problems as required.
Support ongoing daily use of ETL environments and Integrations platforms.
Provide on-call support for critical application problems and issues.
Analyze program performance and take action to correct deficiencies based on consultation with clients and approval of supervisor.
required.Work outside the standard office 7.5 hour workday may be
Experience Requirements
2+ years of IT experience.
1+ years of experience building ETL feeds using any ETL tool (Talend, Informatica, etc.)
Knowledge of the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment, and post-production support.
Understanding of XML, JSON, XSD, WSDL, REST, and SOAP APIs.
Experience in developing SQL programs.
Job Type: Full-time
Pay: From $80,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Work Location: Hybrid remote in Cleveland, OH 44115","$80,000 /yr (est.)",1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"PrizePicks
4.9",4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1","$101,755 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
"ITExpertUS
2.8",2.8,"Chicago, IL",Data Engineer,"Client: Agilisium
Role: Data Engineer
Location: Chicago, IL – Hybrid role – Need to be onsite once/twice in a month.
Type: Long term contract
Job Description
Title: Data Engineer
We are looking for a savvy Data Engineer to join our growing team. The hire will be responsible for expanding and optimizing our data and pipeline architecture and data flow and collection. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. In addition, the Data Engineer will collaborate and support software developers, implementation architecture, and data engineers on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. In addition, they must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
Responsibilities:
Because we work on a programmatic leading edge of many technologies, we need someone who is a creative problem solver, resourceful in getting things done, and can shift productively to or from working independently and collaboratively. This person would also take on the following responsibilities:
Process unstructured data into a form suitable for analysis.
Support the business with ad hoc data analysis and build reliable data pipelines.
Implementation of best practices and IT operations in mission-critical tighter SLA data
pipelines using Airflow
Query Engine Migration from Dremio to Redshift.
We leverage: Multiple AWS Data & Analytic Services(e.g.,Glue, Kinesis, S3) , SQL (e.g.,
PostgreSQL, Redshift, Athena); NoSQL (e.g., DocumentDB, MongoDB); Kafka, Docker, Spark(AWS EMR and DataBricks), Airflow, Dremio, Qubole, etc.
We use AWS extensively, so experience with AWS cloud and AWS Data & Analytics certification will help you hit the ground running.
Business Skills for Data Engineers:
Creative Problem-Solving: Approaching data organization challenges with a clear eye on what is important; employing the suitable approach/methods to make the maximum use of time and human resources.
Effective Collaboration: Carefully listening to management, data engineers, and architects to establish their needs.
Intellectual Curiosity: Exploring new territories and finding creative and pragmatic ways to solve data problems.
Industry Knowledge: Understanding and perspective of how industry functions and how data can be collected, analyzed, and utilized for value creation; maintaining flexibility in the face of big data developments.
Skills and Qualifications:
3 to 5 years of real-world Data Engineering experience.
Programming experience, ideally in Python and other data engineering languages like Scala
Programming knowledge to clean structure and semi-structure datasets.
Experience processing large amounts of structured and unstructured data. Streaming data experience is a plus.
Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Background in Linux
Build the infrastructure required for optimal extraction, transformation, and loading of data
from various data sources using SQL and other cloud ‘big data’ technologies like DataBricks, Snowflake, Dremio, and Qubole.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of manipulating, processing, and extracting value from large disconnected datasets.
Experience creating a platform on which complex data pipelines are built using orchestration tools like Airflow, and Astronomer.
Experience with real-time sync between OLTP and OLAP using AWS technologies like real- time sync between AWS Aurora and AWS Redshift.
Job Type: Full-time
Salary: Up to $68.00 per hour
Schedule:
8 hour shift
Work Location: On the road
Speak with the employer
+91 6305048577",$68.00 /hr (est.),501 to 1000 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
TY Software,#N/A,"Dallas, TX",Data Engineer,"Job Title Data Engineer,
Location: Dallas, TX
Type of work- Onsite , C2C
Job Description
Experience 3-5 years
At least 3+ years of enterprise experience in working with data bricks and highly proficient in SQL, Spark, Scala/Python.
Skilled in Big Data Technologies like Spark, Spark SQL, PySpark
Experience with one or more of the major cloud platforms & cloud services such as - Azure/AWS/GCP, Databricks
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Strong analytic skills related to working with unstructured datasets
Working knowledge of highly scalable ‘big data’ data stores
A successful history of manipulating, processing and extracting value from large, disconnected datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Experience developing enterprise software products
Experience with at least one of these object-oriented/object function scripting languages: PySpark/Python, Scala, Java
Build monitoring and automated testing to ensure data consistency and availability
Experience supporting and working with cross-functional teams in a dynamic environment
Experience working in an AGILE environment
Job Types: Full-time, Contract
Salary: $42.15 - $60.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75201: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",$51.08 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Nursa
4.3",4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!","$89,485 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
"EMONICS LLC
3.8",3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location","$150,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD)
"AgileEngine
5.0",5.0,Remote,Senior/Lead Data Engineer,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Work collaboratively with other engineers, architects, data scientists, analytics teams, and business product owners in an agile environment
Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools
Design and build robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Must haves
Bachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience
6 years of experience with Python or Java
4+ years of experience in building data lake, cloud data platform leveraging cloud (GCP/AWS) cloud native architecture, ETL/ELT, and data integration
3 years of development experience with cloud services ( AWS,GCP,AZURE) utilizing various support tools (e.g. GCS, Dataproc, Cloud Data flow, Airflow(Composer), Kafka , Cloud Pub/Sub)
Expertise in developing distributed data processing and Streaming frameworks and architectures (Apache Spark, Apache Beam, Apache Flink, )
In-depth knowledge of NoSQL database technologies (e.g. MongoDB, BigTable, DynamoDB)
Expertise in build and deployment tools – (Visual Studio, PyCharm, Git/Bitbucket/Bamboo, Maven, Jenkins, Nexus, )
5 years of experience and expertise in database design techniques and philosophies (e.g. RDBMS, Document, Star Schema, Kimball Model)
5 years of experience with integration and service frameworks (e.g API Gateways, Apache Camel, Swagger API, Zookeeper, Kafka, messaging tools, microservices)
Expertise with containerized Microservices and REST/GraphQL based API development
Experience leveraging continuous integration/development tools (e.g. Jenkins, Docker, Containers, OpenShift, Kubernetes, and container automation) in a Ci/CD pipeline
Advanced understanding of software development and research tools
Attention to detail and results-oriented, with a strong customer focus
Ability to work as part of a team and independently
Problem-solving and technical communication skills
Ability to prioritize workload to meet tight deadlines
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
Our team of data experts and engineers has helped create a supply chain management platform used by Forbes 500 enterprises and featured by Gartner. We’ve accompanied the platform from an early prototype to a full-fledged solution based on big data, AI, and machine learning. Data quality is integral to our current engagement with this project as we’re expanding the platform functionality.
AgileEngine has also been working on data and ML solutions in multiple industries and verticals like telecom, FinTech, energy efficiency, and more. Our work with our clients’ data has contributed to patented software products, Nasdaq-grade fraud detection, and products used by Deloitte and T-Mobile.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
"Agiles Enterprise
3.4",3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road","$136,537 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Less than $1 million (USD)
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
"Globaleur
4.5",4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information
Deploy sophisticated analytics programs, machine learning, and statistical methods
Ensure compliance with data governance and security policies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java
Have working experiences in e-commence, travel, marketing domain is a plus
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.","$119,136 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017,Unknown / Non-Applicable
Sky Consulting Inc,#N/A,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.","$103,312 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Orange County's Credit Union
3.9",3.9,"Santa Ana, CA",Data Engineer,"Great Opportunity At Orange County's Credit Union
Must reside in the state of CA, AZ, NV or TX.
Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk.
Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture.
Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential.
Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY!
More about Orange County's Credit Union:
Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace.
As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service.
Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference!
Overview:
Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization.
Essential Functions:
Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making.
Translate business requirements to technical solutions by applying technical knowledge and strong business acumen.
Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services.
Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform.
Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc).
Technical Must Haves for this Role:
5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment.
3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases).
3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.).
2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores.
2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS).
Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.

The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.

We perform thorough background checks and credit checks. EOE.","$90,612 /yr (est.)",201 to 500 Employees,Nonprofit Organization,Financial Services,Banking & Lending,1938,$25 to $100 million (USD)
"BCVS Group INC
5.0",5.0,"Oakland, CA",Data engineer lead,"Role: Data Engineer Lead
Location: Oakland, CA
Description
Qualitest is looking for a Data Architect to join our team.
Must Have-
Seasoned data professional with experience implementing solutions across Data Quality, Data Assurance,
Data Governance and Test Data Management
Ability to assess current state, recommend solutions and build a rollout strategy
Analyze existing data landscape, business processes and create knowledge articles (architectures, data flows, tools)Design, Build and Deliver quality gates across data lifecycle for automated and continuous data quality assurance.
Integrate data testing solutions with Automation, CI/CD Pipelines for Continuous Testing
Working experience on various database technologies such as Oracle, SQL Server, MongoDB, Snowflake, Teradata
Good understanding about QA Test Management, Defect Management and Testing methodologies. Evaluate tools, build proof of concepts, and implement NextGen solutions
Work with customer executives and cross functional teams for seamless project implementation and delivery
Mentor onshore/offshore teams to deliver projects in time and assist in case of delays/clarifications
Experience working on data quality validation tools such as RightData, Icedq, Informatica, Tosca DI, Datagaps
Experience working on TDM tools such as Informatica TDM , IBM Optim, Delphix, CA TDM/Broadcom TDM
Proficient in AWS Cloud Services such as Glue, Lambda, RedShift, Dynamo DB and Databricks
Experience working on Data Warehouse, Data Lake, Big Data platforms
Requirements
Good to Have-
Knowledge about Enterprise Applications such as Salesforce, SAP, Veeva CRM
AI/ML knowledge on building Intelligent data validations and eliminate data anomalies
Job Types: Full-time, Contract
Salary: From $41.80 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Oakland, CA 94619: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 +12019071788",$41.80 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ASCENDING
4.2",4.2,"Rockville, MD",Data Engineer,"Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.
This role is only available for W2 or individual contracts. Please no C2C.
100% Remote Work.

Responsibilities:
Analyze system requirements and design responsive algorithms and solutions.
Use big data and cloud technologies to produce production quality code.
Engage in performance tuning and scalability engineering.
Work with team, peers and management to identify objectives and set priorities.
Perform related SDLC engineering activities like sprint planning and estimation.
Work effectively in small agile teams.
Provide creative solutions to problems.
Identify opportunities for improvement and execute.

Requirements:
Minimum 5 years of proven professional experience working in the IT industry.
Degree in Computer Science or related domains.
Experience with cloud based Big Data technologies.
Experience with big data technologies like Hadoop, Spark and Hive.
AWS experience is a big plus.
Proficiency in Hive / Spark SQL / SQL. Experience with Spark.
Experience with one or more programming languages like Scala & Python & Java.
Ability to push the frontier of technology and independently pursue better alternatives.
Kubernetes or AWS EKS experience will be a plus.

Thanks for applying!
U3GJMKlbkr","$96,611 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"FalconSmartIT
4.5",4.5,"Dover, DE",Big Data Engineer,"Job Title: Big Data Engineer
Location: Toronto, Ontario, Canada
Job Type: Full time


Job Description:


Qualifications :
8+ years of software development experience in Big Data technologies (Spark/Hive/Hadoop)
Experience in working on Hadoop Distribution, good understanding of core concepts and best practices
Good experience in building/tuning Spark pipelines in Scala/Python
Good experience in writing complex Hive queries to derive business critical insights
Good Programming experience with Java/Python/Scala
Experience with AWS Cloud, exposure to Lambda/EMR/Kinesis will be good to have
Experience in NoSQL Technologies - MongoDB, Dynamo DB
Roles and Responsibilities :
Design and implement solutions for problems arising out of large-scale data processing
Attend/drive various architectural, design and status calls with multiple stakeholders
Ensure end-to-end ownership of all tasks being aligned
Design, build & maintain efficient, reusable & reliable code
Test implementation, troubleshoot & correct problems
Capable of working as an individual contributor and within team too
Ensure high quality software development with complete documentation and traceability
Fulfil organizational responsibilities (sharing knowledge & experience with other teams/ groups)
Conduct technical training(s)/session(s), write whitepapers/case studies/blogs etc.",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Second Wave Delivery Systems, LLC",#N/A,Remote,Data Engineer,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020,Unknown / Non-Applicable
Edrstaffing,#N/A,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker","$120,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Monogram Health Renal Services,#N/A,"Brentwood, TN",Azure Data Engineer,"Position: Azure Data Engineer

The Azure Data Engineer will work with our business operations and technology teams to build out an enterprise-wide Data Warehouse and related systems in the cloud. Will collaborate with our internal teams and 3rd party vendor in building our future state, business process, system interfaces, governance and technology platforms needed to enable a single source of truth for product and customer data, drive a more sophisticated data environment and be positioned to scale for future business growth.

Roles and Responsibilities
In collaboration with our 3rd party vendor, implement strategic roadmap and operational framework for Enterprise Data Management, in partnership with key stakeholders
Collaborate on the strategy on new cloud data stores and migration of existing data in Azure
Work collaboratively with teams to design, document, and ensure continuous improvement of SDLC best practices for data in the cloud
Be an advocate for quality and security through data quality and accuracy validation and auditing of security best practices
Focus on understanding business problems and determine what aspects require optimization; articulate those aspects in a clear and concise manner
Work collaboratively with cross-functional teams to support Data Models in alignment with business needs
Partner with IT, on design and oversight of APIs and other data sharing capabilities in collaboration with short / long term business needs
Partner with IT to integrate new data management technologies and software engineering tools (automation projects, process improvements, etc.) into existing systems to enable single source of truth for product and customer data
Adhere to identified key performance indicators for quality and compliance metrics to ensure data related policies and standards are followed
Acts as a SME to explain all facets of EDM data exchange and processing, business user involvement, and overall architecture
Support Monogram as necessary with any tasks required to deliver excellent personalized kidney care and perform all other duties as assigned
Uphold the mission and values of Monogram Health in all aspects of your role and activities
Position Requirements
Nashville, TN
BS or equivalent experience in crucial duties and responsibilities
3-5 years’ experience in cloud computing
Solid understanding of enterprise data management in Healthcare Data
Solid understanding of data, databases, data tools (e.g. SQL) and data processing experience
Solid understanding of standard ETL tools and techniques, sourcing, maintaining, and updating data, data warehousing, data cleansing, and other analytical techniques required for data usage.
Experience with database systems, Azure cloud storage, and significant exposure to or experience with modern data and BI platforms (such as Databricks, MSSQL Data Warehouse (Synapse), Snowflake, and Tableau).
Previous experience with data migration to cloud-based environment
Interest and passion in designing and implementing data management, querying, and storage, with a particular focus on purpose-built data stores.
Functional knowledge of data visualization tools such as PowerBI and Tableau.
Strong intuition for business and analytical skills
Team player with excellent communication skills

Benefits

Opportunity to work in a dynamic, fast-paced and innovative value-based provider organization that is transforming the delivery of kidney care
Competitive salary and opportunity to participate in the company’s bonus program
Comprehensive medical, dental, vision and life insurance
Flexible paid leave and vacation policy
401(k) plan with matching contributions

About Monogram Health
Monogram Health is a next-generation, value-based chronic condition risk provider serving patients living with chronic kidney and end-stage renal disease and their related metabolic disorders. Monogram seeks to fill systemic gaps and transform the way nephrology, primary care and chronic condition treatment are delivered. Monogram’s innovative, in-home approach utilizes a national nephrology practice powered by a suite of technology-enabled clinical services, including case and disease management, utilization management and review, and medication therapy management services that improve health outcomes while lowering medical costs across the healthcare continuum. By focusing on increasing access to evidence-based care pathways and addressing social determinants of health, Monogram has emerged as an industry leader in championing greater health equity and improving health outcomes for individuals with chronic kidney and end-stage renal disease.

At Monogram Health we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.
Experience
Required
SQL
healthcare data
cloud computing
Azure
Preferred
Tableau
Snowflake
Data Warehouse
Databricks
Power BI",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Savvy Technology Solutions,#N/A,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location","$107,339 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Govini
3.7",3.7,"Pittsburgh, PA",Data Engineer,"Company Description
Govini is the leading Commercial Data company in the Defense Technology space. Built by Govini, Ark.ai is used at scale across the National Security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the Companies, Capabilities, and Capital in National Security to solve challenges pertaining to Acquisition, Foreign Influence & Adversarial Capital, Nuclear Modernization, Procurement, Science & Technology, and Supply Chain. Govini has offices in Arlington, Virginia, San Francisco, California, and Pittsburgh, Pennsylvania.

Job Description
We are seeking an exceptional and experienced data engineer who shares our passion and obsession with quality. You'll be a core member of our product and engineering team dedicated to helping our clients replace time-consuming, manual processes to reach informed real-time decisions about government markets, competitors, and agency relationships.

We need a skilled and dedicated data nerd to join our team to lead us in uncovering truth and meaning in data. You must be a hands-on engineer with a strong understanding of both data management and governance standards. You must also have strong interpersonal skills to work cross-functionally across internal teams as well as directly with end users and Govini platform SMEs.

In order to do this job well, you must be a curious and eager problem solver with a hunger for delivering high-quality data solutions. You have a passion for great work and nothing less than your best will do. You share our intolerance of mediocrity. You’re uber-smart, challenged by figuring things out and producing simple solutions to complex problems. Knowing there are always multiple answers to a problem, you know how to engage in a constructive dialogue to find the best path forward. You’re scrappy. We like scrappy.

This role is a full-time position located in PIttsburgh, PA.

Scope of Responsibilities
Define and lead Govini's data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage.
Ensure key entities within datasets are identified, resolved and linked to existing entities within the current master data repository.
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification.
Improve data sharing, increase data repurposing and improve cost efficiency associated with data management efforts.
Build best practices that help with chain of custody of data so it can be easily traced back to the source for accuracy and consistency.
Work across functional teams to understand advanced statistical, machine learning, and text processing models and incorporate them into Govini’s existing data engineering infrastructure.
Perform exploratory data analyses, generate and test working hypotheses, prepare and analyze historical data and identify patterns.
Work directly with users as well as SMEs to establish, create and populate optimal data architectures and structures, as well as articulate techniques and results using non-technical language.
Qualifications
US Citizenship is required

Required Skills
Bachelor's degree in Computer Science, Mathematics or related technical field
3-5 years experience with programmatically transforming data
Experience with RDBMS
Advanced SQL programming skills
Proficient usage of common data formats such as CSV, XML, and JSON
Requires strong analytical ability and attention to detail
Ability to work independently with little supervision
A burning desire to tackle hard problems and create sustainable solutions

Desired Skills
Experience using Amazon Web Services
Experience in or exposure to the nuances of a startup or other entrepreneurial environment
Working knowledge with large (multiple terabytes) amounts of data
We firmly believe that past performance is the best indicator of future performance. If you thrive in ambiguity, are a self-starter, and care about solving technical problems in the national security domain, we’re eager to hear from you.

Govini is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.","$88,151 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2011,Unknown / Non-Applicable
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"FlexIT Inc
4.0",4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.","$100,714 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
Invictus Data,#N/A,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location","$125,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Fiserv, Inc.
3.2",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.","$108,451 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD)
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"EMONICS LLC
3.8",3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location","$150,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD)
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
"IBR (Imagine Believe Realize)
4.5",4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",#N/A,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable
"AgileEngine
5.0",5.0,Remote,Senior/Lead Data Engineer,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Work collaboratively with other engineers, architects, data scientists, analytics teams, and business product owners in an agile environment
Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools
Design and build robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Must haves
Bachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience
6 years of experience with Python or Java
4+ years of experience in building data lake, cloud data platform leveraging cloud (GCP/AWS) cloud native architecture, ETL/ELT, and data integration
3 years of development experience with cloud services ( AWS,GCP,AZURE) utilizing various support tools (e.g. GCS, Dataproc, Cloud Data flow, Airflow(Composer), Kafka , Cloud Pub/Sub)
Expertise in developing distributed data processing and Streaming frameworks and architectures (Apache Spark, Apache Beam, Apache Flink, )
In-depth knowledge of NoSQL database technologies (e.g. MongoDB, BigTable, DynamoDB)
Expertise in build and deployment tools – (Visual Studio, PyCharm, Git/Bitbucket/Bamboo, Maven, Jenkins, Nexus, )
5 years of experience and expertise in database design techniques and philosophies (e.g. RDBMS, Document, Star Schema, Kimball Model)
5 years of experience with integration and service frameworks (e.g API Gateways, Apache Camel, Swagger API, Zookeeper, Kafka, messaging tools, microservices)
Expertise with containerized Microservices and REST/GraphQL based API development
Experience leveraging continuous integration/development tools (e.g. Jenkins, Docker, Containers, OpenShift, Kubernetes, and container automation) in a Ci/CD pipeline
Advanced understanding of software development and research tools
Attention to detail and results-oriented, with a strong customer focus
Ability to work as part of a team and independently
Problem-solving and technical communication skills
Ability to prioritize workload to meet tight deadlines
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
Our team of data experts and engineers has helped create a supply chain management platform used by Forbes 500 enterprises and featured by Gartner. We’ve accompanied the platform from an early prototype to a full-fledged solution based on big data, AI, and machine learning. Data quality is integral to our current engagement with this project as we’re expanding the platform functionality.
AgileEngine has also been working on data and ML solutions in multiple industries and verticals like telecom, FinTech, energy efficiency, and more. Our work with our clients’ data has contributed to patented software products, Nasdaq-grade fraud detection, and products used by Deloitte and T-Mobile.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
"Manufacturers Bank
3.2",3.2,"Los Angeles, CA",Data Engineer,"GENERAL SUMMARY:
Manufacturers bank is looking for a Data Engineer with strong software engineering backgrounds, a ""platform engineering mindset"" and deep passion for data. This Engineer will help to shape the technical direction of the data domain within the organization. The Engineer will also partner closely with internal business units and external vendors to develop a high performance, reliable and scalable data environment for the bank.

PRINCIPAL DUTIES & RESPONSIBILITIES:
Develop and automate high-performance data processing systems and visualization to ensure reliability and meet critical business requirements.
Design, develop, implement, document and support Data & Analytics systems which adhere to the industry best practices.
Lead data engineering projects, and overall strategy for data governance, security, privacy, quality, and retention.
Triage and resolve issues reported by Risk, Operations, and other business stakeholders.
Mentor junior engineers and continue promoting data engineering and analytics tooling & standards.","$115,908 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,#N/A,$25 to $100 million (USD)
Lendem Solutions,#N/A,"Plano, TX",Data Engineer,"LENDEM Solutions is looking at add a Data Engineer to our business!
CORE COMPETENCIES
Ability to thrive in a dynamic and fast-paced environment, drive change, and collaborate effectively with a variety of individuals and teams
Strong analytical and problem-solving skills
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
PRINCIPAL DUTIES
Shredding and parsing data to extract meaningful information
Preparing historical and live data for data studies to identify trends and patterns
Performing adhoc analysis to answer specific business questions and provide insights
Working with relational databases to model and query complex data relationships
Understanding and working with MySQL data in several different data environments
Mining consumer loan data utilizing tools such as SQL, Python, R, or other comparable data mining tools.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Engineering, or a related field Master’s degree preferred
3+ years of experience as a data engineer or similar role
REQUIRED SKILLS, ABILITIES, SOFT SKILL FACTORS
· Strong experience in ETL processes, data modeling, and data warehousing
Experience working with graph databases, such as Neo4j or Apache Cassandra
Expertise in programming languages such as SQL, Python
Familiarity with big data technologies, such as Hadoop, Spark, or Kafka
Ability to analyze and manipulate large and complex data sets
Strong problem-solving skills and the ability to work independently and as part of a team
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
If you are a data engineer with a passion for problem-solving and a strong background in ETL processes, data modeling, and graph databases, we encourage you to apply for this exciting opportunity. We offer a competitive salary, comprehensive benefits, and a dynamic work environment where you can continue to grow and develop your skills.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Plano, TX 75024: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 5 years (Preferred)
SQL: 5 years (Preferred)
Work Location: Hybrid remote in Plano, TX 75024",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Oddball
4.6",4.6,Remote,Data Engineer,"Oddball believes that the best products are built when teams understand and value the things they are working on. We value learning and growth and the ability to make a big impact at a small company. We believe that we can make big changes happen and improve the daily lives of millions of people by bringing quality software to the federal space.
As a Database Engineer you will be responsible for helping to develop production-quality analytics tools, utilizing existing data sources to empower stakeholders to inform research and development. Act as a liaison between engineers and analysts. Implement and enforce standards and procedures to ensure data is managed consistently and properly integrated.
Experience Requirements
5+ years of proven data and performance engineering experience.
A proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs.
Experience with system analysis, data analysis or programming, using a variety of computer languages and procedures.
Experience working in Agile environments.
Experience in developing, analyzing, and presenting data models.
Knowledge of data management, data standardization, and data governance.
Ability to analyze source data for potential data quality issues.
Expert in SQL and/or SQL based languages.
Create supporting documentation, such as metadata and diagrams of entity relationships, business processes, and process flow.
Create plans, test files, and scripts for data warehouse testing, ranging from unit to integration testing.
This is a Remote position. This is a salaried role. Oddball offers both a tech, and a continuing education stipend.
Must be located within the United States.
Clearances
Ability to obtain low-level federal clearance is required.
Oddball is an Equal Opportunity Employer and does not discriminate against applicants based on race, religion, color, disability, medical condition, legally protected genetic information, national origin, gender, sexual orientation, marital status, gender identity or expression, sex (including pregnancy, childbirth or related medical conditions), age, veteran status or other legally protected characteristics. Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Work Location: Remote",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Nursa
4.3",4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!","$89,485 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
"Pomeroy Technologies, LLC.
3.0",3.0,"Cleveland, OH",Data Engineer,"The Data Engineer is responsible for the development, integration, and implementation of ETL Feeds in support of various modern architecture projects. This role requires hands-on knowledge of modern Integrations Platform solutions, file, and database ETL, and XML transformation. The incumbent will act as a technical resource, participating in the successful design, development, unit testing, implementation, and support of new and existing ETL integrations.
Essential Functions
Must be able to acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design.
Working with manager, gathering business requirements for the development of new applications.
Perform programming assignments based on established standards, methods, and best practices.
Develop ETL solutions as assigned in supporting new or updated business solutions and requirements.
Help business users develop functional requirements for integration.
Provide support during user acceptance testing.
Prepare documentation for code changes.
Code ETL feeds, and conduct tests to verify correct program functionality, reliability, and data integrity.
Maintain and modify ETL feeds; make approved changes by amending application documentation, developing detailed programming logic, and coding changes.
Provide support and resolution for application problems and issues.
Provide user and system documentation for operational and technical support.
Assist in the discovery and investigation of production problems as required.
Support ongoing daily use of ETL environments and Integrations platforms.
Provide on-call support for critical application problems and issues.
Analyze program performance and take action to correct deficiencies based on consultation with clients and approval of supervisor.
required.Work outside the standard office 7.5 hour workday may be
Experience Requirements
2+ years of IT experience.
1+ years of experience building ETL feeds using any ETL tool (Talend, Informatica, etc.)
Knowledge of the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment, and post-production support.
Understanding of XML, JSON, XSD, WSDL, REST, and SOAP APIs.
Experience in developing SQL programs.
Job Type: Full-time
Pay: From $80,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Work Location: Hybrid remote in Cleveland, OH 44115","$80,000 /yr (est.)",1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"Second Wave Delivery Systems, LLC",#N/A,Remote,Data Engineer,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020,Unknown / Non-Applicable
"Airbus Americas
4.0",4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.","$92,046 /yr (est.)",10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD)
"Infinity Quest
4.0",4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote","$105,128 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
"Business Integra Inc
3.5",3.5,"San Francisco, CA",Data Engineer,"Position can be 100% remote but preferred to have candidates who can periodically (2 x month) work at headquarters.
(Data Engineer)
Job Description:
Assigned Personnel to provide data analytic support to the Data Analytics/Data Integration Project for Judicial Branch Statistical Information System (JBSIS) data reporting.
This position will perform high level data engineering and data analytics on a variety of agency data sources, but primarily on the Judicial Branch Statistical Information System (JBSIS).
Partnering with IT staff, this position will reengineer JBSIS to create new technical documentation for JBSIS; create mappings for the Court Statistics Report and other JBSIS products, make policy recommendations, create and/or implement new governance standards, enhance data auditing and data quality controls, and create data visualizations.
These same tasks may be performed with additional agency datasets.
Specific Skills/Qualifications Required
Technical project management and documentation skills.
Ability to analyze issues from system documentation and recommend solutions.
Experience managing technical projects, including conflict resolution, issue escalations, status reporting and resource management.
Experience creating and executing data mappings and scripts to clean, compile and analyze data
Ability to assess and maintain data pipeline, data quality in the database, and address data reporting issues.
Experience developing and implementing testing protocols for data and system quality
Experience in R and Stata.
Experience with data visualization and software such as Tableau and Power BI.
Excellent oral, written, analytical and communication skills with the ability to lead a technical discussion to both technical and non-technical staff.
Excellent analytical, verbal and conflict resolution skills.
Additional Skills/Qualifications Desired:
General:
Understanding of courtroom operations and workflow.
Experience in government (State) setting
Excellent presentation skills for both technical and non-technical audiences, including creating and presenting executive summaries to management and technical committees.
Technical:
Exposure and experience with Cloud computing.
Conceptual understanding of Amazon Web Services, Microsoft Azure, Google Cloud, IBM and Oracle Cloud Platforms.
Prior experience using Snowflake
Experience using Python or other database query languages.
Job Types: Full-time, Contract
Pay: $112,604.02 - $150,000.00 per year
Benefits:
401(k)
401(k) matching
Dental insurance
Employee assistance program
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Professional development assistance
Referral program
Relocation assistance
Retirement plan
Vision insurance
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco, CA 94102: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data science: 9 years (Required)
Work Location: Hybrid remote in San Francisco, CA 94102","$131,302 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2001,$25 to $100 million (USD)
"Softcrylic
4.0",4.0,Remote,Senior Data Engineer,"Who We Are
For more than 20 years, we have been working with organizations large and small to help solve business challenges through technology. We bring a unique combination of engineering and strategy to Make Data Work for organizations.
Our clients range from the travel and leisure industry to publishing, retail and banking. The common thread between our clients is their commitment to making data work as seen through their investment in those efforts.
In our quest to solve data challenges for our clients, we work with large enterprise, cloud based and marketing technology suites. We have a deep understanding of these solutions so we can help our clients make the most of their investment in an efficient way to have a data driven business.
Why Work at Softcrylic?
Softcrylic provides an engaging, team-focused, and rewarding work environment where people are excited about the work they do and passionate about delivering creative solutions to our clients.
We are looking to add a Senior Data Engineer to our team! This is a 100% Remote role and preference will be given to candidates from Atlanta, NJ or Texas regions.
Job Description:
Softcrylic is looking for a Senior Data Engineerwith strong design, development, and team leadership skills. The person should be working with Clients / Customer and with our internal (onshore and offshore) members to design, develop and rollout data projects. The person to be hands on and have strong leadership/communication and interpersonal skills.
Requirement:
· 5 to 7 years of experience in working as a Data Engineer.
· Strong experience in Python.
· Experience in working on GCP.
· Good experience in Airflow.
· Should have good experience in ETL pipeline design and development.
· Very good experience in SQL
· Experience in working on Redshift.
· Excellent designing and documentation (diagrams) and presentation skills.
· Data Quality Concepts are must have.
· Must know design and development of any of industry leading graph databases.
· Good communication skills.
· Independent thinker, good team player with Data Engineering Design skills.
· Work with minimum guidelines.
Plus:
GCP - Big Query
Agile background
Microsoft Power BI
Graph Database
Job Types: Full-time, Contract
Pay: $130,000.00 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Compensation package:
Performance bonus
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
GCP: 3 years (Preferred)
Python: 4 years (Preferred)
Work Location: Remote
Speak with the employer
+91 609.241.9641","$135,000 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$5 to $25 million (USD)
"MOBE, LLC
3.7",3.7,"Minneapolis, MN",Data Engineer (ETL),"MOBE
MOBE guides people to better health and more happiness. Behind our innovative health solutions is uniquely human philosophy. We believe that person-to-person connections and understanding can make a difference in a world where self-care can be complicated, and health care is ever-evolving and complex.
MOBE works with health plans and large employers to identify individuals who are frequent users of health care but aren?t finding resolutions for their underlying health issues. We use a whole-person approach and guidance to impact health outcomes positively.
Supporting people is at the core of our business, employees included. MOBE is a high-growth organization with a culture built on trust and collaboration. Consistent across our teams and offerings is a belief in the power of people doing good together. We genuinely care about people and consider our workforce the most significant asset.
Your Role at MOBE
This is an exciting time at MOBE and we are growing fast. At MOBE, we have a lot of data: eligibility, medical and pharmacy claims, marketing campaign impressions, transcripts from participant interactions, etc.
This position is responsible for providing technical and project expertise to enable MOBE analytics and operations with structured and unstructured data. Responsibilities includes executing and/or leading user story development, data design and architecture, data pipeline development, testing and deployment in the Analytic Data Framework. This role will partner with internal and external business and technology teams to drive project deliverables and ensure high quality delivery of data architecture and integration.
Responsibilities
The Data Engineer ensures the following capabilities and functions:
Translate high level business processes into logical data processing steps
Design data structures and pipelines that are flexible and scalable for MOBE analytics and operational requirements
Support Analytic partners through collaborative and transparent development, information delivery, problem resolution, shared insights, and training
Data processing definition, execution, and documentation, in a time appropriate way, to meet business priorities and requirements
Data quality and maintenance consistent within the Analytic Data Framework
Lead small to moderate sized projects and initiatives, following through on execution of chosen strategies and demonstrating the ability to work through obstacles and changing priorities.
Demonstrate ability and willingness to play multiple roles for different projects (e.g. planning/architecture, project development, hands-on technical resource/support for others, analysis and resolution of data issues)
Identify and constructively communicate the need for improvements or enhancements in MOBE technology assets
All other duties as assigned to help fulfill our Mission and abide by MOBE?s Guiding Principles","$97,357 /yr (est.)",51 to 200 Employees,Company - Private,Personal Consumer Services,Beauty & Wellness,2014,Unknown / Non-Applicable
Sky Consulting Inc,#N/A,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.","$103,312 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Savvy Technology Solutions,#N/A,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location","$107,339 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Monogram Health Renal Services,#N/A,"Brentwood, TN",Azure Data Engineer,"Position: Azure Data Engineer

The Azure Data Engineer will work with our business operations and technology teams to build out an enterprise-wide Data Warehouse and related systems in the cloud. Will collaborate with our internal teams and 3rd party vendor in building our future state, business process, system interfaces, governance and technology platforms needed to enable a single source of truth for product and customer data, drive a more sophisticated data environment and be positioned to scale for future business growth.

Roles and Responsibilities
In collaboration with our 3rd party vendor, implement strategic roadmap and operational framework for Enterprise Data Management, in partnership with key stakeholders
Collaborate on the strategy on new cloud data stores and migration of existing data in Azure
Work collaboratively with teams to design, document, and ensure continuous improvement of SDLC best practices for data in the cloud
Be an advocate for quality and security through data quality and accuracy validation and auditing of security best practices
Focus on understanding business problems and determine what aspects require optimization; articulate those aspects in a clear and concise manner
Work collaboratively with cross-functional teams to support Data Models in alignment with business needs
Partner with IT, on design and oversight of APIs and other data sharing capabilities in collaboration with short / long term business needs
Partner with IT to integrate new data management technologies and software engineering tools (automation projects, process improvements, etc.) into existing systems to enable single source of truth for product and customer data
Adhere to identified key performance indicators for quality and compliance metrics to ensure data related policies and standards are followed
Acts as a SME to explain all facets of EDM data exchange and processing, business user involvement, and overall architecture
Support Monogram as necessary with any tasks required to deliver excellent personalized kidney care and perform all other duties as assigned
Uphold the mission and values of Monogram Health in all aspects of your role and activities
Position Requirements
Nashville, TN
BS or equivalent experience in crucial duties and responsibilities
3-5 years’ experience in cloud computing
Solid understanding of enterprise data management in Healthcare Data
Solid understanding of data, databases, data tools (e.g. SQL) and data processing experience
Solid understanding of standard ETL tools and techniques, sourcing, maintaining, and updating data, data warehousing, data cleansing, and other analytical techniques required for data usage.
Experience with database systems, Azure cloud storage, and significant exposure to or experience with modern data and BI platforms (such as Databricks, MSSQL Data Warehouse (Synapse), Snowflake, and Tableau).
Previous experience with data migration to cloud-based environment
Interest and passion in designing and implementing data management, querying, and storage, with a particular focus on purpose-built data stores.
Functional knowledge of data visualization tools such as PowerBI and Tableau.
Strong intuition for business and analytical skills
Team player with excellent communication skills

Benefits

Opportunity to work in a dynamic, fast-paced and innovative value-based provider organization that is transforming the delivery of kidney care
Competitive salary and opportunity to participate in the company’s bonus program
Comprehensive medical, dental, vision and life insurance
Flexible paid leave and vacation policy
401(k) plan with matching contributions

About Monogram Health
Monogram Health is a next-generation, value-based chronic condition risk provider serving patients living with chronic kidney and end-stage renal disease and their related metabolic disorders. Monogram seeks to fill systemic gaps and transform the way nephrology, primary care and chronic condition treatment are delivered. Monogram’s innovative, in-home approach utilizes a national nephrology practice powered by a suite of technology-enabled clinical services, including case and disease management, utilization management and review, and medication therapy management services that improve health outcomes while lowering medical costs across the healthcare continuum. By focusing on increasing access to evidence-based care pathways and addressing social determinants of health, Monogram has emerged as an industry leader in championing greater health equity and improving health outcomes for individuals with chronic kidney and end-stage renal disease.

At Monogram Health we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.
Experience
Required
SQL
healthcare data
cloud computing
Azure
Preferred
Tableau
Snowflake
Data Warehouse
Databricks
Power BI",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Globaleur
4.5",4.5,"Santa Clara, CA",Sr. Data Engineer,"What will you do:
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automate manual processes
Optimize data retrieval and develop dashboards, reports, and other visualizations for stakeholders
Align architecture with business requirements, develop algorithms to transform data into useful, actionable information
Deploy sophisticated analytics programs, machine learning, and statistical methods
Ensure compliance with data governance and security policies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition
Works closely with a team of frontend and backend engineers, product managers, and external clients.

Requirements:
Have at least 5+ years of relevant experiences
Bachelor or master degree in IT, CS, Engineering, Data Science or related fields
Working knowledge of data tracking/analytics/visualization tools such as Google Analytics, Tag manager, Mixpanel, Tableau, etc. Familiar with CRM integration, A/B testing
Knowledge of SQL, noSQL, MongoDB, AWS, Azure, Python, Java
Have working experiences in e-commence, travel, marketing domain is a plus
Knowledge of best practices and IT operations in an always-up, always-available service
Experience with or knowledge of Agile Software Development methodologies
Excellent problem solving and troubleshooting skills
Process oriented with great documentation skills
Excellent oral and written communication skills with a keen sense of customer service


About Globaleur:

We're a diverse team of thinkers and avid travelers, constantly reimagining how to better help worldwide travelers fully maximize their travel experience. The people who work for our company all share the same passion for innovation that goes into our platform and customer-centric products and services - revolutionizing how global travelers consume travel-related goods and services. Globaleur’s vision is to become the go-to platform that will help users in navigating worldwide destinations with ease, thereby building a more unified world where everyone feels comfortable visiting, communicating and exchanging cultural values with those who are from other parts of the world.","$119,136 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,2017,Unknown / Non-Applicable
"BCVS Group INC
5.0",5.0,"Oakland, CA",Data engineer lead,"Role: Data Engineer Lead
Location: Oakland, CA
Description
Qualitest is looking for a Data Architect to join our team.
Must Have-
Seasoned data professional with experience implementing solutions across Data Quality, Data Assurance,
Data Governance and Test Data Management
Ability to assess current state, recommend solutions and build a rollout strategy
Analyze existing data landscape, business processes and create knowledge articles (architectures, data flows, tools)Design, Build and Deliver quality gates across data lifecycle for automated and continuous data quality assurance.
Integrate data testing solutions with Automation, CI/CD Pipelines for Continuous Testing
Working experience on various database technologies such as Oracle, SQL Server, MongoDB, Snowflake, Teradata
Good understanding about QA Test Management, Defect Management and Testing methodologies. Evaluate tools, build proof of concepts, and implement NextGen solutions
Work with customer executives and cross functional teams for seamless project implementation and delivery
Mentor onshore/offshore teams to deliver projects in time and assist in case of delays/clarifications
Experience working on data quality validation tools such as RightData, Icedq, Informatica, Tosca DI, Datagaps
Experience working on TDM tools such as Informatica TDM , IBM Optim, Delphix, CA TDM/Broadcom TDM
Proficient in AWS Cloud Services such as Glue, Lambda, RedShift, Dynamo DB and Databricks
Experience working on Data Warehouse, Data Lake, Big Data platforms
Requirements
Good to Have-
Knowledge about Enterprise Applications such as Salesforce, SAP, Veeva CRM
AI/ML knowledge on building Intelligent data validations and eliminate data anomalies
Job Types: Full-time, Contract
Salary: From $41.80 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Oakland, CA 94619: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 +12019071788",$41.80 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Boston Globe Media Partners
4.1",4.1,"Boston, MA",Data Engineer,"Boston Globe Media is New England's largest newsgathering organization - and much more. We are committed to being an indispensable, trusted, reliable source of round-the-clock information. Through the powerful journalism from our newsroom, engaging content from our content marketing studio, or through targeted advertising solutions, brands and marketers rely on us to reach highly engaged, educated and influential audiences through a variety of media and experiences.
Responsibilities:
Collect, organize, and document often-used data resources (maps, APIs, etc).
Create scripts to scrape data from websites for stakeholders.
With guidance, start creation of a data style guide.
Technology:
Basic knowledge of HTML, CSS, and JavaScript.
Basic familiarity with PHP, Groovy, or another server side scripting language.
Basic familiarity of build tools such as Grunt, Gulp, or Webpack.
Basic familiarity with version control systems such as SVN or Git.
Qualifications:
Understands and follows the team’s agile process.
Adheres to defined coding standards.
Participates in code reviews.
A willingness to adapt and be audience focused, with a curious mindset and a commitment to creating an inclusive work environment
Vaccination Statement:
We require that all BGMP employees (including temporary employees, co-ops, interns, and independent contractors) be vaccinated from COVID-19, unless an exemption from this policy has been granted as an accommodation or otherwise. All BGMP employees, regardless of vaccination status or work location, must provide proof of vaccination status as instructed by the employee's designated Human Resources contact. Employees may request a reasonable accommodation or other exemption from this policy by contacting their designated Human Resources contact. Failure to comply with or enforce any part of this policy, or misrepresentation of compliance with this policy, may result in discipline, up to and including termination of employment, subject to reasonable accommodation and other requirements of applicable federal, state, and local law.
EEO Statement:
Boston Globe Media Partners is an equal employment opportunity employer, and does not discriminate on the basis of race, color, religion, gender, sexual orientation, gender identity or expression, age, disability, national origin, ancestry, genetic information, military or veteran status, pregnancy or pregnancy-related condition or any other protected characteristic. Boston Globe Media Partners is committed to diversity in its most inclusive sense.
wcZyZ7QvrB","$110,394 /yr (est.)",1001 to 5000 Employees,Company - Private,Media & Communication,Publishing,#N/A,$100 to $500 million (USD)
"NIVID Technologies
3.7",3.7,Remote,Sr. Data Engineer,"We hope this Job meets your skills and expectations. If you are available and interested, please contact me at your earliest convenience. You will be working with a highly skilled team of IT Professionals in a high pace corporate environment. This opportunity will move quickly and candidates will be interviewed in the order they apply. Job Description/ Required Skills: (i) Strong hands-on programming experience in Python (ii) Hands-on experience of API development (from application / software engineering perspective) (iii) AWS Lambda and data streaming ingestion (Kinesis) (iv) AWS tech stack from data engineering stand-point
Job Types: Full-time, Contract, Permanent
Salary: $39.76 - $86.23 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$63.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$5 to $10 billion (USD)
"FlexIT Inc
4.0",4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.","$100,714 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"Fiserv, Inc.
3.2",3.2,"Bridgewater, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.","$108,451 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD)
"AgileEngine
5.0",5.0,Remote,Senior/Lead Data Engineer,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Work collaboratively with other engineers, architects, data scientists, analytics teams, and business product owners in an agile environment
Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools
Design and build robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Must haves
Bachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience
6 years of experience with Python or Java
4+ years of experience in building data lake, cloud data platform leveraging cloud (GCP/AWS) cloud native architecture, ETL/ELT, and data integration
3 years of development experience with cloud services ( AWS,GCP,AZURE) utilizing various support tools (e.g. GCS, Dataproc, Cloud Data flow, Airflow(Composer), Kafka , Cloud Pub/Sub)
Expertise in developing distributed data processing and Streaming frameworks and architectures (Apache Spark, Apache Beam, Apache Flink, )
In-depth knowledge of NoSQL database technologies (e.g. MongoDB, BigTable, DynamoDB)
Expertise in build and deployment tools – (Visual Studio, PyCharm, Git/Bitbucket/Bamboo, Maven, Jenkins, Nexus, )
5 years of experience and expertise in database design techniques and philosophies (e.g. RDBMS, Document, Star Schema, Kimball Model)
5 years of experience with integration and service frameworks (e.g API Gateways, Apache Camel, Swagger API, Zookeeper, Kafka, messaging tools, microservices)
Expertise with containerized Microservices and REST/GraphQL based API development
Experience leveraging continuous integration/development tools (e.g. Jenkins, Docker, Containers, OpenShift, Kubernetes, and container automation) in a Ci/CD pipeline
Advanced understanding of software development and research tools
Attention to detail and results-oriented, with a strong customer focus
Ability to work as part of a team and independently
Problem-solving and technical communication skills
Ability to prioritize workload to meet tight deadlines
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
Our team of data experts and engineers has helped create a supply chain management platform used by Forbes 500 enterprises and featured by Gartner. We’ve accompanied the platform from an early prototype to a full-fledged solution based on big data, AI, and machine learning. Data quality is integral to our current engagement with this project as we’re expanding the platform functionality.
AgileEngine has also been working on data and ML solutions in multiple industries and verticals like telecom, FinTech, energy efficiency, and more. Our work with our clients’ data has contributed to patented software products, Nasdaq-grade fraud detection, and products used by Deloitte and T-Mobile.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"IBR (Imagine Believe Realize)
4.5",4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",#N/A,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
Sky Consulting Inc,#N/A,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.","$103,312 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Plaxonic Technologies
4.6",4.6,"New York, NY",GCP Data Engineer,"Bachelor’s Degree in Computer Science or a related discipline
5+ years of applicable engineering experience
Strong proficiency in Python with an emphasis in building data pipelines
Ability to write complex SQL to perform common types of analysis and aggregations
Experience with Apache Airflow or Google Composer
Detail-oriented and document all the work
Ability to work with others from diverse skill-sets and backgrounds
GCP solution architect - certified
Experience in GCP, Big Query
Working experience in Databricks, Spark is expected
Job Types: Full-time, Contract
Benefits:
401(k)
Health insurance
Paid time off
Schedule:
8 hour shift
Work Location: One location
Speak with the employer
+91 (727) 216-7989","$117,952 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
EZOPs Inc,#N/A,"New York, NY",Python Data Engineer,"Responsibility:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies.
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems.
Utilize programming languages like Python, ReactJs, JavaScript and Open Source RDBMS and Cloud based data warehousing services such as Snowflake.
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community.
Collaborate with product managers and deliver robust cloud-based solutions.
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply).
At least 1 year of experience in data technologies.
Hands on Experience in application development with Python, Pandas, NumPy, SQL, Docker.
Preferred Qualifications:
2+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud).
1+ year experience working on real-time data and streaming applications like Kafka is big plus.
1+ years of data warehousing experience (Redshift or Snowflake)
1+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
Job Type: Full-time
Benefits:
401(k)
Health insurance
Paid time off
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
New York, NY: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location
Speak with the employer
+91 9599382735",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Softcrylic
4.0",4.0,Remote,Senior Data Engineer,"Who We Are
For more than 20 years, we have been working with organizations large and small to help solve business challenges through technology. We bring a unique combination of engineering and strategy to Make Data Work for organizations.
Our clients range from the travel and leisure industry to publishing, retail and banking. The common thread between our clients is their commitment to making data work as seen through their investment in those efforts.
In our quest to solve data challenges for our clients, we work with large enterprise, cloud based and marketing technology suites. We have a deep understanding of these solutions so we can help our clients make the most of their investment in an efficient way to have a data driven business.
Why Work at Softcrylic?
Softcrylic provides an engaging, team-focused, and rewarding work environment where people are excited about the work they do and passionate about delivering creative solutions to our clients.
We are looking to add a Senior Data Engineer to our team! This is a 100% Remote role and preference will be given to candidates from Atlanta, NJ or Texas regions.
Job Description:
Softcrylic is looking for a Senior Data Engineerwith strong design, development, and team leadership skills. The person should be working with Clients / Customer and with our internal (onshore and offshore) members to design, develop and rollout data projects. The person to be hands on and have strong leadership/communication and interpersonal skills.
Requirement:
· 5 to 7 years of experience in working as a Data Engineer.
· Strong experience in Python.
· Experience in working on GCP.
· Good experience in Airflow.
· Should have good experience in ETL pipeline design and development.
· Very good experience in SQL
· Experience in working on Redshift.
· Excellent designing and documentation (diagrams) and presentation skills.
· Data Quality Concepts are must have.
· Must know design and development of any of industry leading graph databases.
· Good communication skills.
· Independent thinker, good team player with Data Engineering Design skills.
· Work with minimum guidelines.
Plus:
GCP - Big Query
Agile background
Microsoft Power BI
Graph Database
Job Types: Full-time, Contract
Pay: $130,000.00 - $140,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Compensation package:
Performance bonus
Experience level:
9 years
Schedule:
Monday to Friday
Experience:
Data Engineer: 5 years (Preferred)
GCP: 3 years (Preferred)
Python: 4 years (Preferred)
Work Location: Remote
Speak with the employer
+91 609.241.9641","$135,000 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2000,$5 to $25 million (USD)
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
"Infinity Quest
4.0",4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote","$105,128 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
"NIVID Technologies
3.7",3.7,Remote,Sr. Data Engineer,"We hope this Job meets your skills and expectations. If you are available and interested, please contact me at your earliest convenience. You will be working with a highly skilled team of IT Professionals in a high pace corporate environment. This opportunity will move quickly and candidates will be interviewed in the order they apply. Job Description/ Required Skills: (i) Strong hands-on programming experience in Python (ii) Hands-on experience of API development (from application / software engineering perspective) (iii) AWS Lambda and data streaming ingestion (Kinesis) (iv) AWS tech stack from data engineering stand-point
Job Types: Full-time, Contract, Permanent
Salary: $39.76 - $86.23 per hour
Experience level:
8 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$63.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2012,$5 to $10 billion (USD)
"Pomeroy Technologies, LLC.
3.0",3.0,"Cleveland, OH",Data Engineer,"The Data Engineer is responsible for the development, integration, and implementation of ETL Feeds in support of various modern architecture projects. This role requires hands-on knowledge of modern Integrations Platform solutions, file, and database ETL, and XML transformation. The incumbent will act as a technical resource, participating in the successful design, development, unit testing, implementation, and support of new and existing ETL integrations.
Essential Functions
Must be able to acquire and interpret business requirements, create technical artifacts, and determine the most efficient/appropriate solution design.
Working with manager, gathering business requirements for the development of new applications.
Perform programming assignments based on established standards, methods, and best practices.
Develop ETL solutions as assigned in supporting new or updated business solutions and requirements.
Help business users develop functional requirements for integration.
Provide support during user acceptance testing.
Prepare documentation for code changes.
Code ETL feeds, and conduct tests to verify correct program functionality, reliability, and data integrity.
Maintain and modify ETL feeds; make approved changes by amending application documentation, developing detailed programming logic, and coding changes.
Provide support and resolution for application problems and issues.
Provide user and system documentation for operational and technical support.
Assist in the discovery and investigation of production problems as required.
Support ongoing daily use of ETL environments and Integrations platforms.
Provide on-call support for critical application problems and issues.
Analyze program performance and take action to correct deficiencies based on consultation with clients and approval of supervisor.
required.Work outside the standard office 7.5 hour workday may be
Experience Requirements
2+ years of IT experience.
1+ years of experience building ETL feeds using any ETL tool (Talend, Informatica, etc.)
Knowledge of the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment, and post-production support.
Understanding of XML, JSON, XSD, WSDL, REST, and SOAP APIs.
Experience in developing SQL programs.
Job Type: Full-time
Pay: From $80,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Supplemental pay types:
Bonus pay
Work Location: Hybrid remote in Cleveland, OH 44115","$80,000 /yr (est.)",1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"BCVS Group INC
5.0",5.0,"Oakland, CA",Data engineer lead,"Role: Data Engineer Lead
Location: Oakland, CA
Description
Qualitest is looking for a Data Architect to join our team.
Must Have-
Seasoned data professional with experience implementing solutions across Data Quality, Data Assurance,
Data Governance and Test Data Management
Ability to assess current state, recommend solutions and build a rollout strategy
Analyze existing data landscape, business processes and create knowledge articles (architectures, data flows, tools)Design, Build and Deliver quality gates across data lifecycle for automated and continuous data quality assurance.
Integrate data testing solutions with Automation, CI/CD Pipelines for Continuous Testing
Working experience on various database technologies such as Oracle, SQL Server, MongoDB, Snowflake, Teradata
Good understanding about QA Test Management, Defect Management and Testing methodologies. Evaluate tools, build proof of concepts, and implement NextGen solutions
Work with customer executives and cross functional teams for seamless project implementation and delivery
Mentor onshore/offshore teams to deliver projects in time and assist in case of delays/clarifications
Experience working on data quality validation tools such as RightData, Icedq, Informatica, Tosca DI, Datagaps
Experience working on TDM tools such as Informatica TDM , IBM Optim, Delphix, CA TDM/Broadcom TDM
Proficient in AWS Cloud Services such as Glue, Lambda, RedShift, Dynamo DB and Databricks
Experience working on Data Warehouse, Data Lake, Big Data platforms
Requirements
Good to Have-
Knowledge about Enterprise Applications such as Salesforce, SAP, Veeva CRM
AI/ML knowledge on building Intelligent data validations and eliminate data anomalies
Job Types: Full-time, Contract
Salary: From $41.80 per hour
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Oakland, CA 94619: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data modeling: 1 year (Preferred)
Work Location: One location
Speak with the employer
+91 +12019071788",$41.80 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"PepsiCo
4.0",4.0,"Plano, TX",Azure Data Engineer,"As a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems.
Responsibilities:
Active contributor to code development in projects and services.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.
Responsible for implementing best practices around systems integration, security, performance and data management.
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.
Develop and optimize procedures to “productionalize” data science models.
Define and manage SLA’s for data products and processes running in production.
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale.
Research in state-of-the-art methodologies.
Create documentation for learnings and knowledge transfer.
Create and audit reusable packages or libraries.
Requirements:
2+ years of overall technology experience that includes at least 2+ years of hands-on software development, data engineering, and systems architecture.
2+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
1+ years in cloud data engineering experience in Azure Certification is a plus.
Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.
Experience with version control systems like Github and deployment & CI tools.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus
Understanding of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI)
Covid-19 vaccination may be a condition of employment dependent on role and location. For specific information, please discuss role requirements with the recruiter
Education
BA/BS in Computer Science, Math, Physics, or other technical fields
Skills, Abilities, Knowledge
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.
Proven track record of leading, mentoring data teams.
Strong change manager. Comfortable with change, especially that which arises through company growth. Able to lead a team effectively through times of change.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.
Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.
Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.
Foster a team culture of accountability, communication, and self-management.
Proactively drives impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals
Ability to lead others without direct authority in a matrixed environment.
Job Type: Full-time
Pay: $85,000.00 - $90,000.00 per year
Schedule:
Monday to Friday
Work Location: Hybrid remote in Plano, TX 75024","$87,500 /yr (est.)",10000+ Employees,Company - Public,Manufacturing,Food & Beverage Manufacturing,1965,$10+ billion (USD)
Lendem Solutions,#N/A,"Plano, TX",Data Engineer,"LENDEM Solutions is looking at add a Data Engineer to our business!
CORE COMPETENCIES
Ability to thrive in a dynamic and fast-paced environment, drive change, and collaborate effectively with a variety of individuals and teams
Strong analytical and problem-solving skills
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
PRINCIPAL DUTIES
Shredding and parsing data to extract meaningful information
Preparing historical and live data for data studies to identify trends and patterns
Performing adhoc analysis to answer specific business questions and provide insights
Working with relational databases to model and query complex data relationships
Understanding and working with MySQL data in several different data environments
Mining consumer loan data utilizing tools such as SQL, Python, R, or other comparable data mining tools.
EDUCATION AND EXPERIENCE
Bachelor's degree in Computer Science, Engineering, or a related field Master’s degree preferred
3+ years of experience as a data engineer or similar role
REQUIRED SKILLS, ABILITIES, SOFT SKILL FACTORS
· Strong experience in ETL processes, data modeling, and data warehousing
Experience working with graph databases, such as Neo4j or Apache Cassandra
Expertise in programming languages such as SQL, Python
Familiarity with big data technologies, such as Hadoop, Spark, or Kafka
Ability to analyze and manipulate large and complex data sets
Strong problem-solving skills and the ability to work independently and as part of a team
Excellent communication skills, both verbal and written
Strong attention to detail and accuracy
Results-oriented and able to meet tight deadlines
If you are a data engineer with a passion for problem-solving and a strong background in ETL processes, data modeling, and graph databases, we encourage you to apply for this exciting opportunity. We offer a competitive salary, comprehensive benefits, and a dynamic work environment where you can continue to grow and develop your skills.
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Life insurance
Paid time off
Retirement plan
Vision insurance
Experience level:
3 years
Schedule:
8 hour shift
Ability to commute/relocate:
Plano, TX 75024: Reliably commute or planning to relocate before starting work (Required)
Experience:
Python: 5 years (Preferred)
SQL: 5 years (Preferred)
Work Location: Hybrid remote in Plano, TX 75024",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Ascendion
4.5",4.5,Remote,Senior Data Engineer,"Responsibilities:
8+ years professional experience as a data engineer
Strong programming skills (some combination of Python, Java, and Scala preferred)
Experience working with Databricks
Analyzing the data sources, building, and scaling Pipelines & reports to meet the needs of the Analytics team and other engineering teams
As a COSMOS Developer the mandatory technical skills include TSQL, SSIS, SSAS COSMOS SCOPE Scripting, Azure data stack (ADF, SQL Azure, ADL)
Experience with creating and monitoring date pipeline with ADF, Azure analytics services
Great problem-solving skills, understanding proposed data models and alignment with business requirements
Knowledge of C# to understand assembly / Custom packages is desirable
Ability to understand vast amounts of data, identify and fix data issues
Knowledge in data modeling is desirable
Knowledge of data warehousing concepts
Experience writing SQL, structuring data, and data storage practices
Experienced building data pipelines
Knowledge of working with microservices
Qualifications:
A passion for building and running continuous integration pipelines
More than 2 years hands on experience on Azure Services like Databricks, Azure Data Factory, Azure Storage Accounts & Azure Data Lakes and 1 year of experience working with Cosmos DB or any other document based noSQL databases (MongoDB)
More than 3 years of experience in programming knowledge on Python, Scala
Experience with SQL and NOSQL Databases preferred
Preferred:
DevOps – CD/CI Implementations
Framework Development and Automation Techniques
Experience in implementation of Data Catalogue and Data Lake Implementations
Experience in Data Management Solution Development with strong experience in SQL and NoSQL data bases
Job Types: Full-time, Contract
Pay: $60.00 - $70.00 per hour
Benefits:
401(k)
Dental insurance
Flexible spending account
Health insurance
Health savings account
Referral program
Vision insurance
Experience level:
8 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
How many years of Cosmos experience do you have?
Work Location: Remote",$65.00 /hr (est.),1001 to 5000 Employees,Company - Private,Information Technology,Software Development,2022,Unknown / Non-Applicable
"Arthur Grand Technologies Inc
4.8",4.8,"Jersey City, NJ",Azure Tech Lead/ Sr Data Engineer,"Role: Azure Tech Lead/ Sr Data Engineer – Onsite role – Preferred locals
Location: Jersey City, New Jersey / Fort Mill, South Carolina.
Full-time
Mandatory Skills: MS Azure using Azure Data Factory, MS Synapse, Scala, Spark, Data Warehousing
Skills:
Over all 12 to 15 years of experience with Data Management, Data Warehousing and Analytics.
At least 4 to 5 years of experience in Architecting and Implementing Data Solutions.
At least 3 years of experience in implementing the data solutions on MS Azure using Azure Data Factory, MS Synapse.
One to two years of experience in Azure Synapse Analytics is plus.
Installing and configuring ADF integration runtimes and linked services.
At least one hands on experience with Big data platform tool selection POC.
Two years of experience in data migrations to Azure by using data box or Data migration Services.
Apache Spark experience using Scala or PySpark or pre-packaged tools like Databrick is must.
Extensive hands-on experience in data warehousing design, tuning and ETL/ELT process development by using cloud native technologies.
At least one year experience with unified data governance solution using MS Purview.
Developing the CICD pipeline for Azure Infrastructure, version control strategy and Integrate source control ( Azure repos)
In-depth understanding of various storage services offered by Azure.
Experience with implementation of data security, encryption, PII/PSI legislation, identity and access management across sources and environments.
Experience with data process Orchestration, end-to-end design and build process of Near-Real Time and Batch Data Pipelines.
Certification in Azure data engineering and solution architecture Azure is must.
Strong client-facing communication and facilitation skills.
Job Type: Full-time
Salary: $81,075.29 - $186,473.81 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Experience:
Azure: 4 years (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road","$133,775 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable
"ConnectiveRx
3.0",3.0,"Hanover, NJ",Sr. Data Engineer,"ConnectiveRx is a leading, technology-enabled healthcare services company. We work strategically with hundreds of biopharmaceutical manufacturers to help commercialize and maximize the benefits of specialty and branded medications. Our mission is to simplify how patients get on and stay on therapy. We fulfill our mission by providing our customers with innovative services such as patient and provider messaging, the design and operation of copay, vouchers and patient affordability programs, and hub services, all of which accelerate speed-to-therapy and help improve outcomes for manufacturers, healthcare providers and patients.

ConnectiveRx was formed in 2015 by bringing together the industry-leading business of PSKW, PDR/LDM, Careform (2017) and The Macaluso Group (2018) to advance our technology-driven expertise in providing state-of-the-art commercialization solutions. To learn more about our company, visit ConnectiveRx.com

Job Description

What you will do:
Looking for a seasoned Senior Data Engineer to help us continue to build out our new Enterprise Data Platform. This person must have a strong understanding and demonstrated experience with data streaming architectures that leverage microservice & message-oriented integration patterns and practices within AWS cloud native technologies. This person will help to scale our data ingestion pipelines which are at the core of our Enterprise Data Platform which supports our client reporting as well as our internal analytics & operational teams.

The successful candidate will:
Work with senior leadership, architects, engineers, data analysts, product managers and cloud infrastructure teams to deliver a new features and capabilities.
Write clean, robust, and well-thought-out code with an emphasis on quality, performance, scalability, and maintainability.
Demonstrate strong end to end ownership & craftsmanship - analysis, design, code, test, debug, and deploy
Your ability to traverse the full stack within AWS server-less technologies will be an asset to us as we evaluate the tradeoffs inherent in software engineering. You have the product driven development mindset and can work closely with BA’s and Product teams to breakdown requirements and translate business workflows into scalable technical solutions.

What we’d like from you:
Strong Python & strong SQL
Extensive relational DB experience (Redshift, SQL Server, PostgresSQL) with exposure to document DBs such as DynamoDB. ElasticSearch.
Experience with designing solutions that run in AWS cloud technologies (Lambda, ECS, DynamoDB etc), docker containers
Message oriented architectures, patterns and tools, CQRS, event streaming, Kafka, SQS
Change data capture concepts, Database Triggers, AWS DMS
Data lake concepts, data catalogs, meta data etc
CICD Pipelines
Event store processing, data validation, operational logging via AWS Cloud Watch
Why work with us?
Excellent company culture, fun events, and volunteer opportunities
Competitive benefits (medical, dental, vision & more)
401k package with dollar-for-dollar match-up
Generous PTO and paid holidays days offered
Opportunities to grow professionally and personally
Team-oriented atmosphere
#LI-BJ1

Equal Opportunity Employer: This employer (hereafter the Company) is an equal opportunity employer and does not discriminate in recruitment, hiring, training, promotion, or other employment policies on the basis of age, race, sex, color, religion, national origin, disability, veteran status, genetic information, or any other basis that is prohibited by federal, state, or local law. No question in this application is intended to secure information to be used for such discrimination. In addition, the Company makes reasonable accommodation to the needs of disabled applicants and employees, so long as this does not create an undue hardship on the Company or threaten the health or safety of others at work. This application will be given every consideration, but its receipt does not imply that the applicant will be employed.","$115,021 /yr (est.)",1001 to 5000 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,2015,Unknown / Non-Applicable
"Airbus Americas
4.0",4.0,"Atlanta, GA",Data Engineer,"Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defense as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.

Position Summary:
We are looking to hire a talented big data engineer to develop and manage our company's Big Data solutions on Airbus Skywise Platform. In this role, you will be required to design and implement ELT data transformation processes on Skywise, collaborate with development teams, build cloud platforms, and maintain the production system.

To ensure success as a big data engineer, you should have prior knowledge of Apache Spark or similar frameworks, excellent project management skills, and high-level problem-solving skills. A top-notch Data Engineer understands the needs of the company and institutes scalable data solutions for its current and future needs.

Primary Responsibilities:
Write code to transform real-world data into high-signal models.
Loading disparate data sets and conducting pre-processing transformation services.
Extract insights from airline and aviation data that will revolutionize our business strategy and communicate insights that resonate with partners.
Collaborate with the software development teams.
Maintaining production systems.
Training staff on data resource management.

Qualified Experience / Skills / Training:
Bachelor's degree in computer engineering or computer science.
Previous experience as a big data engineer.
2-3 years of professional experience with programming languages such as Python, R, Java, or similar languages.
Prior experience working with Apache Spark is also desirable.
Good project management skills.
Good communication skills.
Ability to adapt to a dynamic work environment by consistently revising your approach in response to new information.

Education:
Bachelor's Degree or equivalent work experience

Experience:
Experience with Palantir big data platform.
Data Science expertise.

Eligibility:
Authorized to Work in the US
Clearance:
None

Equal Opportunity: Airbus Group is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus Group is also committed to compliance with all fair employment practices regarding citizenship and immigration status.
As a leader in our field, Airbus Group provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus Group does not offer tenured or guaranteed employment. Employment with Airbus Group is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.","$92,046 /yr (est.)",10000+ Employees,Company - Public,Aerospace & Defense,Aerospace & Defense,1970,$10+ billion (USD)
"Second Wave Delivery Systems, LLC",#N/A,Remote,Data Engineer,"Second Wave Delivery Systems, LLC (SWDS) is determined to improve clinical outcomes in the United States while addressing the rising costs of healthcare. We are developing a technology-enabled, integrated system tailored to optimize value-based arrangements for payors and providers serving Medicare and Medicaid enrollees. Through an increased focus on primary and preventive care, we strive to provide patients with significantly better outcomes and satisfaction in each community we serve.

The Technology organization at SWDS is responsible for building the next generation of value-based healthcare management and optimization through a family of systems and the foundational technologies required to support SWDS’ ambitious mission. Predicated on the latest technologies and built upon a mantra of Engineering and Operational Excellence, we develop mission-critical applications and infrastructure quickly, reliably, and while having fun. We strive to grow, foster, and maintain an entrepreneurial, diverse, collaborative, and flexible team and structure. As a fully remote, fast-paced, and quickly growing organization, we are always looking for talented, passionate, and ambitious individuals who seek to solve exciting and challenging technology problems while having a transformative impact on US healthcare.

Core values and ways of working
We are a tight group of engineers focusing on engineering and operational excellence while delivering high quality solutions to our clients and business partners
We take great pride in our team and the products we deliver, and treat both with utmost respect and care
We are firm believers in engineering high-quality, fault-tolerant systems that scale with business needs with minimal unforeseen disruption
We value great work and great ideas, not ego. We're looking for talented individuals that are accustomed to working within a fast-paced environment and want to ship often to make an impact

Responsibilities
Build and maintain scalable, high-performance data processing systems (batch and/or streaming)
Lead data engineering projects to ensure pipelines are reliable, efficient, testable, & maintainable
Design our data models for optimal storage and retrieval and to meet critical product and business requirements
Contribute to tooling & standards to improve the productivity and quality of output across the company
Work and consult with various non-technical stakeholders
Participate in interviewing and onboarding of new team members

Job Benefits
Health insurance
Vision insurance
Dental insurance
Life insurance
Simple IRA matching
Paid time off

Minimum Qualifications
5+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks
Expert knowledge of relational databases and query authoring (SQL).
Strong experience developing in one of the following - Python, Java, Scala
Experience with building and managing data pipelines
Experience with big data technology, e.g: Airflow, Spark, Google Cloud Composer

Preferred Qualifications
Demonstrated experience developing machine learning algorithms in the healthcare field
Experience extracting and correlating medical findings from unstructured data such as PDF charts
Experience with big data analysis tools such as BigQuery and Looker",#N/A,51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2020,Unknown / Non-Applicable
"BNY Mellon
3.7",3.7,"Pittsburgh, PA",Big Data Sytems Engineer(Hadoop),"Overview
Big Data Engineer
Bring your ideas. Make history.
BNY Mellon offers an exciting array of future-forward careers at the intersection of business, finance, and technology. We are one of the world's top asset management and banking firms that manages trillions of dollars in assets, custody and/or administration. Known as the “bank of banks” - 97% of the world’s top banks work with us as we lead and serve our customers into the new era of digital.
With over 238 years of rich history and industry firsts, BNY Mellon has been built upon our proven ability to evolve, lead, and drive new ideas at every turn. Today, we’re approximately 50,000 employees across 35 countries with a culture that empowers you to grow, take risks, experiment and be yourself. This is what #LifeAtBNYMellon is all about.
We’re seeking a future team member in the role of Big Data Engineer to join our TSG Data Management team. This role is located in Pittsburgh, PA and Nashville, TN .
In this role, you’ll make an impact in the following ways:
Work as a senior system engineer for the Hadoop enterprise data platform platforms and create highly complex big data/Hadoop platforms. You will have expert knowledge in distributed datalake systems.
You’ll configure and tune service components to optimize platform performance. You will design and implement backup and restore strategies and disaster recovery solutions and will support and troubleshoot performance related issues.
You’ll consult with storage and host engineering services to create optimized big data ecosystems and assist in cross-team collaboration and design discussions.
Provide sandbox functionality to users of the datasets to assist application analytics and storage infrastructure system management including capacity planning, performance monitoring and tuning, security management etc.
Responsible for infrastructure Support and projects including system design, planning, installations, configurations, upgrades and planning and Manage Tier-3 support following ITIL practice and incident management
Manage system configurations and solutions, and monitor the full lifecycles of storage area network (SAN) administration, architecture, and support and Data storage tasks such as management, provisioning, troubleshooting and debugging
Lead technical assessments of hardware, software, tools, applications, firmware, and operating systems to support business operations. You’ll develop trusted corporations with application tenants. You’ll advise applications with sounds data layer resiliencies and provide input into the selection of tools and any necessary migration into the company's environment.
To be successful in this role, we’re seeking the following:
Bachelor's degree in computer science or a related discipline, or equivalent work experience required.
Advanced degree preferred10-12 years of related experience required; experience in the securities or financial services industry is a plus.
5-10 years of experience in Hadoop technologies, data lake design, experience in the securities or financial services industry is a plus.
Excellent knowledge with Hadoop components for big data platforms related to data ingestion, storage, transformations and analytics. Excellent DevOps skillsets and SDLC practices. Excellent knowledge of automation tools such as Ansible, Python programming skills, knowledge of Docker containers
8+ years of experience on BigData Platform on Hadoop, Spark, Hive, HBase and 3+ years’ experience on Python and DevOps tools such as Ansible
Strong Agile/Scrum development experience and Working knowledge with PostgreSQL or Mongo DB databases a plus
At BNY Mellon, our inclusive culture speaks for itself. Here’s a few of our awards:
Fortune World’s Most Admired Companies & Top 20 for Diversity and Inclusion
Bloomberg’s Gender Equality Index (GEI)
Best Places to Work for Disability Inclusion , Disability: IN – 100% score
100 Best Workplaces for Innovators, Fast Company
Human Rights Campaign Foundation, 100% score Corporate Equality Index
CDP’s Climate Change ‘A List’
Our Benefits:
BNY Mellon offers highly competitive compensation, benefits, and wellbeing programs rooted in a strong culture of excellence and our pay-for-performance philosophy. We provide access to flexible global resources and tools for your life’s journey. Focus on your health, foster your personal resilience, and reach your financial goals as a valued member of our team, along with generous paid leaves that can support you and your family through moments that matter.
BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer - Underrepresented racial and ethnic groups/Females/Individuals with Disabilities/Protected Veterans.
This person is a subject matter expert in several of the tools/technologies used in the space.Leads the development infrastructure engineering growth strategies and initiatives. Leads initiatives to analyze complex infrastructure problems to be solved with advanced design.Leads the evaluation of the effectiveness of the organization's existing infrastructure technology and tools. Analyzes trends to develop strategy for the implementation of upgrades that will enhance the reliability, Resiliency and efficiency of the IT infrastructure.Provides leadership to execute project plans and performance requirements for all stages/phases through the management of human capital resources.This person is a subject matter expert in at least one of the tools/techologies used in the space.Participates in or leads initiatives to analyze infrastructure problems to be solved with advanced design. Utilizes standard procedures and policies when selecting methods, techniques, and evaluation criteria for obtaining results.Participates in or leads initiatives to analyze infrastructure problems to be solved with advanced design. Utilizes standard procedures and policies when selecting methods, techniques, and evaluation criteria for obtaining results.Manages the processes for ensuring that all systems/applications/software/hardware are compliant with Corporate policy/procedures.Monitors project plans and budgets.Works closely with external vendors, internal partners and busienss teams to provide infrastructure/tool needs.Works with Application Development and Quality Assurance, Testing and Business teams to understand infrastructure needs during the development, testing and production BAU processes. Ensures these needs are taken into account when developing infrastructure.Acts as escalation point for major incidents.Leads strategy to increase automation across the organization.Contributes to the achievement of multiple teams' objectivesBachelor's degree in computer science or a related discipline, or equivalent work experience required; advanced degree preferred10-12 years of related experience required; experience in the securities or financial services industry is a plus. BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer. Minorities/Females/Individuals with Disabilities/Protected Veterans. Our ambition is to build the best global team – one that is representative and inclusive of the diverse talent, clients and communities we work with and serve – and to empower our team to do their best work. We support wellbeing and a balanced life, and offer a range of family-friendly, inclusive employment policies and employee forums.
Employer Description:
For over 230 years, the people of BNY Mellon have been at the forefront of finance, expanding the financial markets while supporting investors throughout the investment lifecycle. BNY Mellon can act as a single point of contact for clients looking to create, trade, hold, manage, service, distribute or restructure investments and safeguards nearly one-fifth of the world's financial assets. BNY Mellon remains one of the safest, most trusted and admired companies. Every day our employees make their mark by helping clients better manage and service their financial assets around the world. Whether providing financial services for institutions, corporations or individual investors, clients count on the people of BNY Mellon across time zones and in 35 countries and more than 100 markets. It's the collective ambition, innovative thinking and exceptionally focused client service paired with a commitment to doing what is right that continues to set us apart. Make your mark: bnymellon.com/careers.
EEO Statement:
BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer. Minorities/Females/Individuals With Disabilities/Protected Veterans. Our ambition is to build the best global team – one that is representative and inclusive of the diverse talent, clients and communities we work with and serve – and to empower our team to do their best work. We support wellbeing and a balanced life, and offer a range of family-friendly, inclusive employment policies and employee forums.
Job Type: Full-time","$80,404 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Investment & Asset Management,1784,$10+ billion (USD)
Monogram Health Renal Services,#N/A,"Brentwood, TN",Azure Data Engineer,"Position: Azure Data Engineer

The Azure Data Engineer will work with our business operations and technology teams to build out an enterprise-wide Data Warehouse and related systems in the cloud. Will collaborate with our internal teams and 3rd party vendor in building our future state, business process, system interfaces, governance and technology platforms needed to enable a single source of truth for product and customer data, drive a more sophisticated data environment and be positioned to scale for future business growth.

Roles and Responsibilities
In collaboration with our 3rd party vendor, implement strategic roadmap and operational framework for Enterprise Data Management, in partnership with key stakeholders
Collaborate on the strategy on new cloud data stores and migration of existing data in Azure
Work collaboratively with teams to design, document, and ensure continuous improvement of SDLC best practices for data in the cloud
Be an advocate for quality and security through data quality and accuracy validation and auditing of security best practices
Focus on understanding business problems and determine what aspects require optimization; articulate those aspects in a clear and concise manner
Work collaboratively with cross-functional teams to support Data Models in alignment with business needs
Partner with IT, on design and oversight of APIs and other data sharing capabilities in collaboration with short / long term business needs
Partner with IT to integrate new data management technologies and software engineering tools (automation projects, process improvements, etc.) into existing systems to enable single source of truth for product and customer data
Adhere to identified key performance indicators for quality and compliance metrics to ensure data related policies and standards are followed
Acts as a SME to explain all facets of EDM data exchange and processing, business user involvement, and overall architecture
Support Monogram as necessary with any tasks required to deliver excellent personalized kidney care and perform all other duties as assigned
Uphold the mission and values of Monogram Health in all aspects of your role and activities
Position Requirements
Nashville, TN
BS or equivalent experience in crucial duties and responsibilities
3-5 years’ experience in cloud computing
Solid understanding of enterprise data management in Healthcare Data
Solid understanding of data, databases, data tools (e.g. SQL) and data processing experience
Solid understanding of standard ETL tools and techniques, sourcing, maintaining, and updating data, data warehousing, data cleansing, and other analytical techniques required for data usage.
Experience with database systems, Azure cloud storage, and significant exposure to or experience with modern data and BI platforms (such as Databricks, MSSQL Data Warehouse (Synapse), Snowflake, and Tableau).
Previous experience with data migration to cloud-based environment
Interest and passion in designing and implementing data management, querying, and storage, with a particular focus on purpose-built data stores.
Functional knowledge of data visualization tools such as PowerBI and Tableau.
Strong intuition for business and analytical skills
Team player with excellent communication skills

Benefits

Opportunity to work in a dynamic, fast-paced and innovative value-based provider organization that is transforming the delivery of kidney care
Competitive salary and opportunity to participate in the company’s bonus program
Comprehensive medical, dental, vision and life insurance
Flexible paid leave and vacation policy
401(k) plan with matching contributions

About Monogram Health
Monogram Health is a next-generation, value-based chronic condition risk provider serving patients living with chronic kidney and end-stage renal disease and their related metabolic disorders. Monogram seeks to fill systemic gaps and transform the way nephrology, primary care and chronic condition treatment are delivered. Monogram’s innovative, in-home approach utilizes a national nephrology practice powered by a suite of technology-enabled clinical services, including case and disease management, utilization management and review, and medication therapy management services that improve health outcomes while lowering medical costs across the healthcare continuum. By focusing on increasing access to evidence-based care pathways and addressing social determinants of health, Monogram has emerged as an industry leader in championing greater health equity and improving health outcomes for individuals with chronic kidney and end-stage renal disease.

At Monogram Health we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.
Experience
Required
SQL
healthcare data
cloud computing
Azure
Preferred
Tableau
Snowflake
Data Warehouse
Databricks
Power BI",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Talent-One,#N/A,"Imperial, CA",Data Engineer,"Responsibilities:
Develop predictive models
Develop optimization models
Develop re-activation and retention models
Advanced analytics to drive incremental revenue
Identify performance metrics definition, algorithm development and automation
Reporting and visualization
Complex data analysis tasks
Data anomaly detection and correction modeling
Conversion of data into stories for internal and external consumption
Cross-team support for CRM and Database Marketing Teams.
Qualifications:
Bachelor’s degree in an Analytical field (Business, Marketing) required.
At least three (3) years casino database experience required, or the equivalent combination of education and experience in data analysis.
SAS programming level 1 or higher certification required.
Ability to use data to solve complex business problems.
Advanced SQL skills
Strong industry experience of Microsoft Office Suite, including Excel, Word, Access, required
Job Type: Full-time
Salary: $1.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Savvy Technology Solutions,#N/A,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location","$107,339 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"BluePath Labs
4.8",4.8,"West Point, NY",Junior Data Scientist or Data Engineer -ONSITE,"Junior Data Scientist or Data Engineer - ONSITE

Location: Army Cyber Institute, West Point, NY - ONSITE
BluePath Labs is looking for a Junior Data Scientist or Data Engineer to support our team at the Army Cyber Institute (ACI) in West Point, NY. This is a critical and exciting opportunity at the intersection of advanced cyber research and the latest cloud technologies. This effort will have direct operational relevance. The successful candidate will demonstrate a track record of successfully working in a collaborative environment. They will be working with leading Army researchers and programs and partners throughout the US Government. This position will start immediately.
Primary Responsibilities
Responsible for designing, developing, testing, and maintaining data pipelines and data models that support advanced research and analysis for the Army Cyber community.
Work both alone and as part of a team with other developers, designers, and stakeholders to deliver high‐quality data‐driven insights, analytics, and models.
Perform exploratory data analysis, cleaning, and labeling on both structured and unstructured data.
Implement and train machine learning models on various data types, including text, tabular, time series and image data.
Comfortable working with on‐premises and cloud‐native environments for the discovery, extraction, and analysis of patterns and trends to gather, process, and derive valuable insights needed to build AI‐enabled capabilities.
Required Qualifications
Minimum of a bachelor's degree in data science (DS), data engineering (DE), computer science (CS), software engineering (SE), computer engineering (CE), electrical engineering (EE), mathematics, information technology (IT), information systems (ISYS) or a related field.
Primary programming proficiency in Python and R.
Additional experience designing and querying with relational and non‐relational databases (SQL, NoSQL, or time‐series).
Familiarity with Debian-based Linux OSes, such as Ubuntu.
Knowledge of computer and network security principles, such as network segmentation and VLANs.
Desired Qualifications
U.S. or Five Eyes national citizenship.
Ability to obtain DoD Secret clearance.
About BluePath Labs
BluePath Labs is a fast-growing research and consulting company committed to solving complex problems for federal, state, and local government clients. We offer a range of professional, scientific, and technology services. Our specific areas of expertise include business consulting, research and data science, and technology integration.
BluePath Labs combines mission and business insights with advanced technologies to deliver measurable performance improvements for our clients. BluePath is dedicated to surpassing client expectations by always living by our core values of integrity, professionalism, and resilience. BluePath's extensive experience in Government, Military, Commercial, and Academic environments is unique among small businesses and a core differentiator of our solutions. Our multidisciplinary background allows us to solve diverse and complex problems. Most importantly, we work closely with our clients to frame problems correctly, optimize processes, leverage technologies, and implement enduring solutions. Labs are where ideas are born, experiments occur, and breakthroughs happen. It is the hallmark of BluePath's culture.
BluePathLabs.com
BluePath Labs is an equal opportunity employer.","$119,189 /yr (est.)",1 to 50 Employees,Company - Private,Government & Public Administration,National Agencies,2016,Unknown / Non-Applicable
"Manufacturers Bank
3.2",3.2,"Los Angeles, CA",Data Engineer,"GENERAL SUMMARY:
Manufacturers bank is looking for a Data Engineer with strong software engineering backgrounds, a ""platform engineering mindset"" and deep passion for data. This Engineer will help to shape the technical direction of the data domain within the organization. The Engineer will also partner closely with internal business units and external vendors to develop a high performance, reliable and scalable data environment for the bank.

PRINCIPAL DUTIES & RESPONSIBILITIES:
Develop and automate high-performance data processing systems and visualization to ensure reliability and meet critical business requirements.
Design, develop, implement, document and support Data & Analytics systems which adhere to the industry best practices.
Lead data engineering projects, and overall strategy for data governance, security, privacy, quality, and retention.
Triage and resolve issues reported by Risk, Operations, and other business stakeholders.
Mentor junior engineers and continue promoting data engineering and analytics tooling & standards.","$115,908 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,#N/A,$25 to $100 million (USD)
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"PLAXONIC
4.6",4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642","$117,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
"Xiar tech inc
3.3",3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$51.90 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"Fiserv, Inc.
3.2",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.","$108,451 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD)
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"ESRI, Inc.
4.0",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1","$98,800 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD)
LOVEFOODIES INC,#N/A,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",$30.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Invictus Data,#N/A,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location","$125,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
"YT Global Network
5.0",5.0,Remote,Data Engineer- Remote,"Data Engineer- Remote
Role: Data and Analytics is an evolving space which includes more software engineering, distributed systems, and cloud skills.
WIll develop, maintain, and enhance the data platform capabilities in an open and collaborative environment to build the central platform.
Will collaborate with internal data customers across IT and the Business to minimize the time from idea inception to analytical insight.
Job responsibilities will include: contributing to data infrastructure design efforts and collaborating with other platforms to integrate infrastructure into the client's systems and testing the feasibility and effectiveness of various technology options; supporting complex tools and solutions to manage orchestration, data pipelines, and infrastructure as code solutions the Data Engineering team builds.
Required skills:
Proven experience in designing, building, and supporting complex data pipelines using a variety of traditional and non-traditional data sources.
Version Control and associated best practices
Advanced programming experience in programming languages used in analytics and data science (e.g. Python, Java, Scala). Comfortable with Linux environments and shell scripting.
Experience with Cloud-based infrastructures (AWS)
Experience working with SQL/NoSQL
Experience utilizing data pipeline orchestration frameworks.
Verbal Communication
Preferred skills and experiences:
Analysis
API Development
CI/CD
Creating Real Time or Streaming Systems
Data Governance
Data Lineage
Data Metadata
Data Testing
Distributed Databases
Domain Knowledge
Schema
Snowflake
Visual Communication
EDUCATION AND/OR EXPERIENCE REQUIRED:
Education and/or experiences listed below are the minimum requirements for job entry.
Bachelor's Degree or higher in an Information Technology discipline or related field of study and minimum of two years of work experience designing, programming, and supporting software programs or applications.
In lieu of degree, minimum of four years related work experience designing, programming, and supporting software programs or applications may be accepted.
Job Types: Full-time, Contract
Pay: $90.00 - $120.00 per hour
Benefits:
Health insurance
Experience level:
6 years
Schedule:
8 hour shift
Monday to Friday
Work Location: Remote",$105.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Edrstaffing,#N/A,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker","$120,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"APLOMB Technologies
4.4",4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$72,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"etrailer.com
3.9",3.9,Remote,Data Engineer/Data Scientist,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow","$140,000 /yr (est.)",501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
"EMONICS LLC
3.8",3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location","$150,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD)
"AgileEngine
5.0",5.0,Remote,Big Data SDET Engineer,"Test with the best! At AgileEngine, you're free to build your own team of A-players and choose out of 70+ available projects the one you'd like to work on. Join the squad of top-3% software experts who make a difference through off-the-wall solutions.
Must haves
Strong Scala and Spark (hands on). If not Scala, strong experience in either Java or Python
Big Data concepts
API testing experience
Understanding qa process to create test cases and test strategies
Independently able to contribute to automation framework as well interact with the stake holders as needed
SQL experience
AWS experience
Strong test mindset
Experience of deploying or using Terraform and Gitlabs would be a bonus
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
The company behind the world’s leading OS virtualization software leverages AgileEngine as a provider of external QA and testing automation expertise. We’ve developed our client’s test strategy complete with test plans, ETAs, analysis, and risk assessment. Our teams have also optimized the company’s test suites, halving the time it takes for the tests to run. Our other deliverables include the implementation of new tests and the development of documentation, how-to guides, and test instructions on an internal Wiki.
Our team is also working to establish a strong QA/AQA process for an industry-leading cybersecurity platform. We’ve developed our client’s test strategy, manual testing cases, and automated tests from scratch. Our deliverables include backend, API, and UI testing, as well as BDD integrated with a streamlined CI process.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Xiar tech inc
3.3",3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$51.90 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Invictus Data,#N/A,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location","$125,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
LOVEFOODIES INC,#N/A,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",$30.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
"Empower Federal Credit Union
4.0",4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000","$99,876 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,#N/A,$25 to $100 million (USD)
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
"Double Line, Inc.
4.2",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w","$85,882 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD)
TheHive,#N/A,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",$35.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"ESRI, Inc.
4.0",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1","$98,800 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD)
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Sky Consulting Inc,#N/A,"Minneapolis, MN",Big Data Engineer,"Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale Experience working in Cloud-based Big Data Infrastructure - GCP Good working experience on Cloud, Delta Lake, ETL processing. Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc. Working knowledge on Python and PySpark Programming.","$103,312 /yr (est.)",1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"APLOMB Technologies
4.4",4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$72,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
Monogram Health Renal Services,#N/A,"Brentwood, TN",Azure Data Engineer,"Position: Azure Data Engineer

The Azure Data Engineer will work with our business operations and technology teams to build out an enterprise-wide Data Warehouse and related systems in the cloud. Will collaborate with our internal teams and 3rd party vendor in building our future state, business process, system interfaces, governance and technology platforms needed to enable a single source of truth for product and customer data, drive a more sophisticated data environment and be positioned to scale for future business growth.

Roles and Responsibilities
In collaboration with our 3rd party vendor, implement strategic roadmap and operational framework for Enterprise Data Management, in partnership with key stakeholders
Collaborate on the strategy on new cloud data stores and migration of existing data in Azure
Work collaboratively with teams to design, document, and ensure continuous improvement of SDLC best practices for data in the cloud
Be an advocate for quality and security through data quality and accuracy validation and auditing of security best practices
Focus on understanding business problems and determine what aspects require optimization; articulate those aspects in a clear and concise manner
Work collaboratively with cross-functional teams to support Data Models in alignment with business needs
Partner with IT, on design and oversight of APIs and other data sharing capabilities in collaboration with short / long term business needs
Partner with IT to integrate new data management technologies and software engineering tools (automation projects, process improvements, etc.) into existing systems to enable single source of truth for product and customer data
Adhere to identified key performance indicators for quality and compliance metrics to ensure data related policies and standards are followed
Acts as a SME to explain all facets of EDM data exchange and processing, business user involvement, and overall architecture
Support Monogram as necessary with any tasks required to deliver excellent personalized kidney care and perform all other duties as assigned
Uphold the mission and values of Monogram Health in all aspects of your role and activities
Position Requirements
Nashville, TN
BS or equivalent experience in crucial duties and responsibilities
3-5 years’ experience in cloud computing
Solid understanding of enterprise data management in Healthcare Data
Solid understanding of data, databases, data tools (e.g. SQL) and data processing experience
Solid understanding of standard ETL tools and techniques, sourcing, maintaining, and updating data, data warehousing, data cleansing, and other analytical techniques required for data usage.
Experience with database systems, Azure cloud storage, and significant exposure to or experience with modern data and BI platforms (such as Databricks, MSSQL Data Warehouse (Synapse), Snowflake, and Tableau).
Previous experience with data migration to cloud-based environment
Interest and passion in designing and implementing data management, querying, and storage, with a particular focus on purpose-built data stores.
Functional knowledge of data visualization tools such as PowerBI and Tableau.
Strong intuition for business and analytical skills
Team player with excellent communication skills

Benefits

Opportunity to work in a dynamic, fast-paced and innovative value-based provider organization that is transforming the delivery of kidney care
Competitive salary and opportunity to participate in the company’s bonus program
Comprehensive medical, dental, vision and life insurance
Flexible paid leave and vacation policy
401(k) plan with matching contributions

About Monogram Health
Monogram Health is a next-generation, value-based chronic condition risk provider serving patients living with chronic kidney and end-stage renal disease and their related metabolic disorders. Monogram seeks to fill systemic gaps and transform the way nephrology, primary care and chronic condition treatment are delivered. Monogram’s innovative, in-home approach utilizes a national nephrology practice powered by a suite of technology-enabled clinical services, including case and disease management, utilization management and review, and medication therapy management services that improve health outcomes while lowering medical costs across the healthcare continuum. By focusing on increasing access to evidence-based care pathways and addressing social determinants of health, Monogram has emerged as an industry leader in championing greater health equity and improving health outcomes for individuals with chronic kidney and end-stage renal disease.

At Monogram Health we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.
Experience
Required
SQL
healthcare data
cloud computing
Azure
Preferred
Tableau
Snowflake
Data Warehouse
Databricks
Power BI",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Infinity Quest
4.0",4.0,"Edison, NJ",Azure Data Engineer,"Job Description:
ROLE: Azure Data Engineer
LOCATION: Edison, NJ
Good knowledge in Azure
Good knowledge on data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, Client Studio, AI/Client, Python
Good knowledge on DevOps in Azure platform
Experience developing and deploying ETL solutions on Azure
Nice to have knowledge in IoT, event-driven, microservices, Containers/Kubernetes in the cloud.
Job Type: Full-time
Salary: $90.00 - $100.00 per year
Schedule:
8 hour shift
Experience:
data factory: 1 year (Required)
SQL: 1 year (Preferred)
Data mining: 1 year (Required)
Work Location: Remote","$105,128 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,2006,Unknown / Non-Applicable
"Teamware Solutions (quantum leap consulting).
4.6",4.6,"South San Francisco, CA",Data Engineer - Onsite,"Hi,
Data Engineer
Bay Area, CA – Onsite(Hybrid)
Client: Decision Minds/PANW
Duration: Contract
Exp Level: 10+ Years
Must have skill: Google cloud exp
Job Responsibilities:
Expert in data engineering and GCP data technologies.
Work with client teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
Work with Agile and DevOps techniques and implementation approaches in the delivery
Key responsibilities: Architecture, Design and Development
Required Skills:
10+ Year experience in BI and Analytics
Hands on and deep experience ( at least 2 years) working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, ]etc.).
Experience in Spark (Scala/Python/Java) and Kafka, Airflow
Data Engineering and Lifecycle (including non-functional requirements and operations) management.
E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.
Experience with SQL and NoSQL modern data stores.
Thanks & Regards
Jagadeesh
Teamware Solutions Inc |2838 E. Long Lake Road,Suite# 210, TROY, MI 48085
Job Type: Full-time
Salary: $60.00 - $65.00 per hour
Experience level:
10 years
11+ years
9 years
Schedule:
On call
Ability to commute/relocate:
South San Francisco, CA 94080: Reliably commute or planning to relocate before starting work (Required)
Experience:
Google Cloud Platform: 4 years (Preferred)
Data Engineer: 9 years (Preferred)
Spark: 4 years (Preferred)
Work Location: One location",$62.50 /hr (est.),1001 to 5000 Employees,Company - Private,Information Technology,Information Technology Support Services,2003,Unknown / Non-Applicable
"MOBE, LLC
3.7",3.7,"Minneapolis, MN",Data Engineer (ETL),"MOBE
MOBE guides people to better health and more happiness. Behind our innovative health solutions is uniquely human philosophy. We believe that person-to-person connections and understanding can make a difference in a world where self-care can be complicated, and health care is ever-evolving and complex.
MOBE works with health plans and large employers to identify individuals who are frequent users of health care but aren?t finding resolutions for their underlying health issues. We use a whole-person approach and guidance to impact health outcomes positively.
Supporting people is at the core of our business, employees included. MOBE is a high-growth organization with a culture built on trust and collaboration. Consistent across our teams and offerings is a belief in the power of people doing good together. We genuinely care about people and consider our workforce the most significant asset.
Your Role at MOBE
This is an exciting time at MOBE and we are growing fast. At MOBE, we have a lot of data: eligibility, medical and pharmacy claims, marketing campaign impressions, transcripts from participant interactions, etc.
This position is responsible for providing technical and project expertise to enable MOBE analytics and operations with structured and unstructured data. Responsibilities includes executing and/or leading user story development, data design and architecture, data pipeline development, testing and deployment in the Analytic Data Framework. This role will partner with internal and external business and technology teams to drive project deliverables and ensure high quality delivery of data architecture and integration.
Responsibilities
The Data Engineer ensures the following capabilities and functions:
Translate high level business processes into logical data processing steps
Design data structures and pipelines that are flexible and scalable for MOBE analytics and operational requirements
Support Analytic partners through collaborative and transparent development, information delivery, problem resolution, shared insights, and training
Data processing definition, execution, and documentation, in a time appropriate way, to meet business priorities and requirements
Data quality and maintenance consistent within the Analytic Data Framework
Lead small to moderate sized projects and initiatives, following through on execution of chosen strategies and demonstrating the ability to work through obstacles and changing priorities.
Demonstrate ability and willingness to play multiple roles for different projects (e.g. planning/architecture, project development, hands-on technical resource/support for others, analysis and resolution of data issues)
Identify and constructively communicate the need for improvements or enhancements in MOBE technology assets
All other duties as assigned to help fulfill our Mission and abide by MOBE?s Guiding Principles","$97,357 /yr (est.)",51 to 200 Employees,Company - Private,Personal Consumer Services,Beauty & Wellness,2014,Unknown / Non-Applicable
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Xiar tech inc
3.3",3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$51.90 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
LOVEFOODIES INC,#N/A,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",$30.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Arthur Grand Technologies Inc
4.8",4.8,"Atlanta, GA",AWS Data Engineer,"Role: AWS Data Engineer
Location: Atlanta, GA (Meet customer Monthly once post client selection & onboarding)
JD for AWS Data Engineer
Experience with the core AWS services, plus the specifics mentioned in this job description.
Experience with Docker . Having used ECS , SNS/SQS, Athena, Lambda,S3, RDS and layers in the past is a plus.
Proficiency in at least in Python, Java
Strong notions of security best practices (e.g. using IAM Roles, KMS, etc.).
Experience with monitoring solutions such as CloudWatch, Cloud Trail.
Previous exposure to large-scale systems design.
Knowledge of writing infrastructure as code (IaC) using CloudFormation or Terraform.
Experience with building or maintaining cloud-native applications.
Past experience with the serverless approaches using AWS Lambda is a plus. For example, the Serverless Application Model (AWS SAM).
Job Types: Full-time, Contract
Schedule:
10 hour shift
8 hour shift
Work Location: Remote","$94,994 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
TheHive,#N/A,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",$35.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
"Empower Federal Credit Union
4.0",4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000","$99,876 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,#N/A,$25 to $100 million (USD)
"ESRI, Inc.
4.0",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1","$98,800 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD)
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"Fiserv, Inc.
3.2",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.","$108,451 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD)
"AgileEngine
5.0",5.0,Remote,Big Data SDET Engineer,"Test with the best! At AgileEngine, you're free to build your own team of A-players and choose out of 70+ available projects the one you'd like to work on. Join the squad of top-3% software experts who make a difference through off-the-wall solutions.
Must haves
Strong Scala and Spark (hands on). If not Scala, strong experience in either Java or Python
Big Data concepts
API testing experience
Understanding qa process to create test cases and test strategies
Independently able to contribute to automation framework as well interact with the stake holders as needed
SQL experience
AWS experience
Strong test mindset
Experience of deploying or using Terraform and Gitlabs would be a bonus
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
The company behind the world’s leading OS virtualization software leverages AgileEngine as a provider of external QA and testing automation expertise. We’ve developed our client’s test strategy complete with test plans, ETAs, analysis, and risk assessment. Our teams have also optimized the company’s test suites, halving the time it takes for the tests to run. Our other deliverables include the implementation of new tests and the development of documentation, how-to guides, and test instructions on an internal Wiki.
Our team is also working to establish a strong QA/AQA process for an industry-leading cybersecurity platform. We’ve developed our client’s test strategy, manual testing cases, and automated tests from scratch. Our deliverables include backend, API, and UI testing, as well as BDD integrated with a streamlined CI process.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"APLOMB Technologies
4.4",4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$72,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
"EMONICS LLC
3.8",3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location","$150,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD)
Edrstaffing,#N/A,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker","$120,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Double Line, Inc.
4.2",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w","$85,882 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD)
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Xiar tech inc
3.3",3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$51.90 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Invictus Data,#N/A,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location","$125,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"APLOMB Technologies
4.4",4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$72,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"Empower Federal Credit Union
4.0",4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000","$99,876 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,#N/A,$25 to $100 million (USD)
"Double Line, Inc.
4.2",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w","$85,882 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD)
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
TheHive,#N/A,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",$35.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"Iyka Enterprises, Inc.
3.8",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115","$85,831 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
"ESRI, Inc.
4.0",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1","$98,800 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD)
"Buchanan & Edwards
3.8",3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location","$138,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD)
"AgileEngine
5.0",5.0,Remote,Big Data SDET Engineer,"Test with the best! At AgileEngine, you're free to build your own team of A-players and choose out of 70+ available projects the one you'd like to work on. Join the squad of top-3% software experts who make a difference through off-the-wall solutions.
Must haves
Strong Scala and Spark (hands on). If not Scala, strong experience in either Java or Python
Big Data concepts
API testing experience
Understanding qa process to create test cases and test strategies
Independently able to contribute to automation framework as well interact with the stake holders as needed
SQL experience
AWS experience
Strong test mindset
Experience of deploying or using Terraform and Gitlabs would be a bonus
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
The company behind the world’s leading OS virtualization software leverages AgileEngine as a provider of external QA and testing automation expertise. We’ve developed our client’s test strategy complete with test plans, ETAs, analysis, and risk assessment. Our teams have also optimized the company’s test suites, halving the time it takes for the tests to run. Our other deliverables include the implementation of new tests and the development of documentation, how-to guides, and test instructions on an internal Wiki.
Our team is also working to establish a strong QA/AQA process for an industry-leading cybersecurity platform. We’ve developed our client’s test strategy, manual testing cases, and automated tests from scratch. Our deliverables include backend, API, and UI testing, as well as BDD integrated with a streamlined CI process.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"EMONICS LLC
3.8",3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location","$150,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD)
Excellerent Solutions Inc,#N/A,"Coppell, TX",Data Integration Engineer,"Role: Data Integration Engineer
Location: Irving, TX(5 days a week in Office, 100% onsite role)
Must have experience working with SnapLogic to perform ELT processes.
Must have good written and verbal skills
Must be capable of knowledge transfer to team of ~10 ETL developers on use of SnapLogic
Must be able to work with a larger team to complete sizable migration of on-prem processes to Snowflake.
Must be able to follow design patterns set out by our architecture team.
Job Types: Full-time, Contract
Pay: $48.18 - $60.00 per hour
Schedule:
8 hour shift
Work Location: In person",$54.09 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Xiar tech inc
3.3",3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$51.90 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"PLAXONIC
4.6",4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642","$117,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
"Arthur Grand Technologies Inc
4.8",4.8,"Mount Laurel, NJ",Lead Informatica / Data Engineer,"Role: Lead Informatica / Data Engineer – On Prem –ETL (Onsite role) / Senior Informatica / Mid-Level Informatica
Location: Mount Laurel, NJ / Charlotte, NC
Duration : FTE
Client :: Hexaware / TD Bank
Key Skills: Informatica Power Centre, Autosys, Unix
Must Have
More than 12+ years of IT experience in Datawarehouse and ETL
Hands-on Experience on ETL Informatica Power Centre
Experience on Autosys, Unix and scripting knowledge on Python, Shell Scripts
Experience on Oracle Database
Ability to understand ETL Design, Source to target mapping (STTM) and create ETL specifications documents
Flexibility to operate from client office locations
Able to mentor and guide junior resources, as needed
Banking experience on RISK & Regulatory OR Commercial OR Credit Cards/Retail
Nice to Have
Any cloud experience on Azure or AWS or Informatica cloud connector
Any relevant certifications
Job Type: Full-time
Salary: $69,919.38 - $166,922.18 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 3 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: On the road","$118,421 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2012,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"APLOMB Technologies
4.4",4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$72,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Invictus Data,#N/A,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location","$125,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"EMONICS LLC
3.8",3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location","$150,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD)
"AgileEngine
5.0",5.0,Remote,Big Data SDET Engineer,"Test with the best! At AgileEngine, you're free to build your own team of A-players and choose out of 70+ available projects the one you'd like to work on. Join the squad of top-3% software experts who make a difference through off-the-wall solutions.
Must haves
Strong Scala and Spark (hands on). If not Scala, strong experience in either Java or Python
Big Data concepts
API testing experience
Understanding qa process to create test cases and test strategies
Independently able to contribute to automation framework as well interact with the stake holders as needed
SQL experience
AWS experience
Strong test mindset
Experience of deploying or using Terraform and Gitlabs would be a bonus
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
The company behind the world’s leading OS virtualization software leverages AgileEngine as a provider of external QA and testing automation expertise. We’ve developed our client’s test strategy complete with test plans, ETAs, analysis, and risk assessment. Our teams have also optimized the company’s test suites, halving the time it takes for the tests to run. Our other deliverables include the implementation of new tests and the development of documentation, how-to guides, and test instructions on an internal Wiki.
Our team is also working to establish a strong QA/AQA process for an industry-leading cybersecurity platform. We’ve developed our client’s test strategy, manual testing cases, and automated tests from scratch. Our deliverables include backend, API, and UI testing, as well as BDD integrated with a streamlined CI process.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"Fiserv, Inc.
3.2",3.2,"Berkeley Heights, NJ",Big Data Engineer,"Data Engineer
What does a successful Data Engineer do at Fiserv?
As an experienced Data Engineer, you will be part of the Data Engineering Platform Group. You will take ownership and build the design and development of projects within Fiserv’s Enterprise Data Analytics division.
What you will do:
Work with product owners and source teams to understand the datasets, process flows and build systems to ingest variety of data to DataHub efficiently.
Develop configuration driven enterprise level batch process to ingest daily batch data and utilized it for ingest TBs of complex type history data.
Develop enterprise level Spark based Kafka streaming consumer application to consume more than 7000TPS
You will Lead large-scale projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, and SQL.
Additionally, you will architect, build, and launch new data models that provide intuitive analytics to the team and Build data expertise and own data quality for the pipelines you create.

What you will need to have:
4+ years of experience analyzing, developing, testing, implementing, and maintaining Big Data and Java/J2EE Web applications.
Hands-on experience in Spark, Hadoop, HDFS, HBASE, Map-Reduce, Hive, Pig, Kafka, Flume, Impala, and Sqoop.
Possess strong knowledge and working experience in Java, J2EE (JSP, Servlets, JDBC), Struts, Hibernate, spring, Java Script, Ajax, XML
Experience with Apache Spark (SQL and streaming) and developed applications to process credit card transaction data to build Near Real Time Dashboard.
Strong knowledge in Hadoop security Kerberos and Sentry and data encryption tools like HP-Voltage.
Technical leadership in developing data solutions and building frameworks
Hands-on experience with Java Development and Springboot framework (MVC architecture, Hibernate, API creation, Restful services, SOAP using Java)
Experience deploying code on containers
Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
What would be great to have:
5+ years' experience building large scale big data applications development
Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure)
Experience working with PCI Data and working with data scientists is a plus
Exposure to Talend Big Data edition and solutions a strong plus
Experience with Cloud platforms (Google Cloud, AWS, Azure)
Experience in Design and architecture review in the Banking, Financial domain.","$108,451 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Financial Transaction Processing,1984,$10+ billion (USD)
Edrstaffing,#N/A,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker","$120,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Iyka Enterprises, Inc.
3.8",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115","$85,831 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
"ESRI, Inc.
4.0",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1","$98,800 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD)
"AgileEngine
5.0",5.0,Remote,Senior/Lead Data Engineer,"Join the squad of experienced software experts and pump up your skills by building off-the-wall software solutions for Fortune 500 and Future 50 brands. We have opportunities in 90+ projects you can contribute to.
What you will do
Work collaboratively with other engineers, architects, data scientists, analytics teams, and business product owners in an agile environment
Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools
Design and build robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Must haves
Bachelor’s degree in computer science, information systems, math, engineering, or other technical field, or equivalent experience
6 years of experience with Python or Java
4+ years of experience in building data lake, cloud data platform leveraging cloud (GCP/AWS) cloud native architecture, ETL/ELT, and data integration
3 years of development experience with cloud services ( AWS,GCP,AZURE) utilizing various support tools (e.g. GCS, Dataproc, Cloud Data flow, Airflow(Composer), Kafka , Cloud Pub/Sub)
Expertise in developing distributed data processing and Streaming frameworks and architectures (Apache Spark, Apache Beam, Apache Flink, )
In-depth knowledge of NoSQL database technologies (e.g. MongoDB, BigTable, DynamoDB)
Expertise in build and deployment tools – (Visual Studio, PyCharm, Git/Bitbucket/Bamboo, Maven, Jenkins, Nexus, )
5 years of experience and expertise in database design techniques and philosophies (e.g. RDBMS, Document, Star Schema, Kimball Model)
5 years of experience with integration and service frameworks (e.g API Gateways, Apache Camel, Swagger API, Zookeeper, Kafka, messaging tools, microservices)
Expertise with containerized Microservices and REST/GraphQL based API development
Experience leveraging continuous integration/development tools (e.g. Jenkins, Docker, Containers, OpenShift, Kubernetes, and container automation) in a Ci/CD pipeline
Advanced understanding of software development and research tools
Attention to detail and results-oriented, with a strong customer focus
Ability to work as part of a team and independently
Problem-solving and technical communication skills
Ability to prioritize workload to meet tight deadlines
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
Our team of data experts and engineers has helped create a supply chain management platform used by Forbes 500 enterprises and featured by Gartner. We’ve accompanied the platform from an early prototype to a full-fledged solution based on big data, AI, and machine learning. Data quality is integral to our current engagement with this project as we’re expanding the platform functionality.
AgileEngine has also been working on data and ML solutions in multiple industries and verticals like telecom, FinTech, energy efficiency, and more. Our work with our clients’ data has contributed to patented software products, Nasdaq-grade fraud detection, and products used by Deloitte and T-Mobile.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"PLAXONIC
4.6",4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642","$117,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Empower Federal Credit Union
4.0",4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000","$99,876 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,#N/A,$25 to $100 million (USD)
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"APLOMB Technologies
4.4",4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$72,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
TheHive,#N/A,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",$35.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Xiar tech inc
3.3",3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$51.90 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
"Buchanan & Edwards
3.8",3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location","$138,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD)
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
Savvy Technology Solutions,#N/A,"Washington, DC",AWS Senior Data Engineer,"Responsibilities
Provide Information and Architecture solutions to ensure efficient manipulation, organization and sharing of information, data and/or master data in data-lake, lake-house and data-mart constructs.
Collaborate with Cloud Enterprise Architect to define the appropriate Cloud Database solution for current and new systems, such as AWS Redshift, Database Migration Service, Glue, EMR, Elastic Compute Cloud (EC2), S3, Relational Database Service (RDS)/Aurora, and Amazon Kinesis.
Define access patterns, storage methods and integration patterns for the movement and maintenance of data into unstructured, relational and noSQL databases
Develop and design the models for complex analytical and data warehouse systems including performing tasks related to database design, data analysis, data quality, metadata management and support.
Work with highly diverse data from a wide variety of sources, in a wide variety of formats, and employed by a wide variety of highly siloed systems.
Apply best practices to design and build complex ETL processes
Support services vendor and third party product vendors in the migration of data to the Cloud
Competences
Yrs.
Bachelor’s degree in computer science or engineering or equivalent work experience
AWS Solutions Architect – Associate certification within past three years
AWS Certified Data Analytics – Specialty or AWS Certified Big Data – Specialty certifications
Solutions architecture experience in an enterprise environment with emphasis on data systems
5
Specific experience designing, executing and supporting AWS data lakes at scale
3
Experience designing, optimizing, and maintaining relational and non-relational databases including MySQL, MS SQL, Redshift, AWS RDS and other NoSQL databases
5
Experience with the design and building of ETL packages, data pipelines and connecting these to BI applications
5
Knowledge of data movement and data presentation tools such as; Informatica, MS SSIS, AWS Glue, Cognos, Tableau
3
Ability to pass a background check and pass the criteria for a CJIS environment
Willingness to research and self-study to keep technical skills relevant in a highly complex environment
Ability to multi-task and prioritize deadlines as needed to deliver results
Ability to work independently or as part of a team
Mentors and coaches colleagues and seeks opportunities for continuous improvement
Excellent verbal and written communication skills with great attention to detail and accuracy
Experience working in an Agile/Scrum environment
Job Types: Full-time, Contract
Pay: $89,677.00 - $125,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
On call
Experience:
AWS Cloud Engineer: 8 years (Required)
AWS Cloud Architecture: 5 years (Required)
Data warehouse: 3 years (Required)
License/Certification:
AWS Certification - Professional (Required)
Work Location: One location","$107,339 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"FlexIT Inc
4.0",4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.","$100,840 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"Iyka Enterprises, Inc.
3.8",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115","$85,831 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
Predict Health,#N/A,"Arlington, VA",Data Engineer,"Overview of the Role
We are looking for an exceptional Data Engineer who is interested in joining a small and rapidly growing Product and Services team at our company. We view our data-driven products and services as strategic assets, and you will be a key part of our team and company. The successful candidate will have 5+ years of experience designing and implementing data solutions and services. This role will report to our VP, Product Engineering and will be an instrumental member of our product development team.
The Data Engineer will participate in the design of new information intensive product and service offerings. They will be responsible for the development and implementation of the data pipelines and services underlying these new products. They will deepen their understanding of the political, policy, financial and business aspects of the health plan industry so as to be able to provide insight into the best manner to collect and manage information related to these businesses.
The Data Engineer will work closely with the Product Management function to understand customer needs and help identify capabilities for new offerings. They will also partner with the Data Science team to understand the analytical needs of the business and assist in the design and implementation of data solutions to support these efforts.
About Predict Health, Inc.
Predict Health is transforming how health plans, providers and their partners increase retention, quality, and lifetime value of their customer or patient relationships. The core competency of the organization is the curation, analysis, and presentation of information to support targeted operational improvements in healthcare enterprises. The company is an early-stage enterprise based in Washington, D.C. which focuses on three solution categories: Member Acquisition, Quality Management, and Retention Management.
Our company, Predict Health was founded in 2019 and is led by an experienced team of healthcare experts, data scientists and healthcare entrepreneurs. We are focused on building services and solutions designed to improve the healthcare experience of our nation’s seniors. We founded our company after seeing first-hand how challenging it is for the seniors in our lives - including our parents, grandparents and friends - to deal with the complexities of Medicare with little or no real help.
Detailed Job Description
The Data Engineer will initially support the deployment of an Azure solution to help collect, aggregate, transform, and distribute information related to Predict Health’s current service offerings. The Data Engineer is responsible for designing the solution, building the data integration and associated pipelines, design and deploying data structures required to support current products and analytical efforts, and automating the operational aspects of the data platform. The Data Engineer responsibilities will include:
· Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as, but not limited to, Azure Synapse, Azure Data Factory and Azure pipelines.
· Translate business requirements from product owners and analysts into technical code.
· Identify, fix and document bugs, bottlenecks in workflows and pipelines.
· Enforce security compliance, fine tune performance and promote code quality standards.
· Identify and curate new data sources required to support business requirements
· Explore, learn the latest Azure data management technologies to add to current competencies.
· Support development of data intensive solutions in a fast-paced, dynamic environment
· Support Business Intelligence Applications (e.g. Power BI) as required.
Qualifications
Requires minimum of five years relevant experience in design, development, implementation, and testing of data services
· Experience in data structures, data modeling, data warehousing and data pipeline development on Azure
· 3+ years of hands-on experience in the design, implementation and performance tuning of Azure Data Factory, Synapse, or other Azure data services
· Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure
· Experience developing software code in one or more programming languages (Java, Python, etc.)
· Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform
· Experience with data migration
· Experience defining system architectures and exploring technical feasibility trade-offs.
· Ability to prototype and evaluate applications and interaction methodologies.
Other Knowledge, Skills, and Abilities Required
· Demonstrated capability for problem solving, decision making, sound judgement and assertiveness.
· Ability to change direction to meet demand in a high-paced work environment
· Ability to focus on details and technicalities while simultaneously retaining a comprehensive vision of organizational goals
· Excellent collaborator and able to influence and direct work contributions from others to meet critical program deliverables and timelines
· Advanced interpersonal, written, and oral communication skills.
· Proficiency in Microsoft Office including Excel and PowerPoint
What we Offer
This is a full-time position and compensation, and benefits will be highly competitive.
Predict Health is committed to bringing together people from different backgrounds and perspectives to deliver real results for our clients and end-users. We recruit, employ, compensate, and promote regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.
This position will be based in our Arlington, VA office. We offer a flexible work schedule with a minimum of three days in the office each week.
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Employee discount
Flexible schedule
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Arlington, VA: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location","$115,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Double Line, Inc.
4.2",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w","$85,882 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD)
"AgileEngine
5.0",5.0,Remote,Big Data SDET Engineer,"Test with the best! At AgileEngine, you're free to build your own team of A-players and choose out of 70+ available projects the one you'd like to work on. Join the squad of top-3% software experts who make a difference through off-the-wall solutions.
Must haves
Strong Scala and Spark (hands on). If not Scala, strong experience in either Java or Python
Big Data concepts
API testing experience
Understanding qa process to create test cases and test strategies
Independently able to contribute to automation framework as well interact with the stake holders as needed
SQL experience
AWS experience
Strong test mindset
Experience of deploying or using Terraform and Gitlabs would be a bonus
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
The company behind the world’s leading OS virtualization software leverages AgileEngine as a provider of external QA and testing automation expertise. We’ve developed our client’s test strategy complete with test plans, ETAs, analysis, and risk assessment. Our teams have also optimized the company’s test suites, halving the time it takes for the tests to run. Our other deliverables include the implementation of new tests and the development of documentation, how-to guides, and test instructions on an internal Wiki.
Our team is also working to establish a strong QA/AQA process for an industry-leading cybersecurity platform. We’ve developed our client’s test strategy, manual testing cases, and automated tests from scratch. Our deliverables include backend, API, and UI testing, as well as BDD integrated with a streamlined CI process.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"IBR (Imagine Believe Realize)
4.5",4.5,"Melbourne, FL",Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote","$118,866 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
"PLAXONIC
4.6",4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642","$117,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Empower Federal Credit Union
4.0",4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000","$99,876 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,#N/A,$25 to $100 million (USD)
Predict Health,#N/A,"Arlington, VA",Data Engineer,"Overview of the Role
We are looking for an exceptional Data Engineer who is interested in joining a small and rapidly growing Product and Services team at our company. We view our data-driven products and services as strategic assets, and you will be a key part of our team and company. The successful candidate will have 5+ years of experience designing and implementing data solutions and services. This role will report to our VP, Product Engineering and will be an instrumental member of our product development team.
The Data Engineer will participate in the design of new information intensive product and service offerings. They will be responsible for the development and implementation of the data pipelines and services underlying these new products. They will deepen their understanding of the political, policy, financial and business aspects of the health plan industry so as to be able to provide insight into the best manner to collect and manage information related to these businesses.
The Data Engineer will work closely with the Product Management function to understand customer needs and help identify capabilities for new offerings. They will also partner with the Data Science team to understand the analytical needs of the business and assist in the design and implementation of data solutions to support these efforts.
About Predict Health, Inc.
Predict Health is transforming how health plans, providers and their partners increase retention, quality, and lifetime value of their customer or patient relationships. The core competency of the organization is the curation, analysis, and presentation of information to support targeted operational improvements in healthcare enterprises. The company is an early-stage enterprise based in Washington, D.C. which focuses on three solution categories: Member Acquisition, Quality Management, and Retention Management.
Our company, Predict Health was founded in 2019 and is led by an experienced team of healthcare experts, data scientists and healthcare entrepreneurs. We are focused on building services and solutions designed to improve the healthcare experience of our nation’s seniors. We founded our company after seeing first-hand how challenging it is for the seniors in our lives - including our parents, grandparents and friends - to deal with the complexities of Medicare with little or no real help.
Detailed Job Description
The Data Engineer will initially support the deployment of an Azure solution to help collect, aggregate, transform, and distribute information related to Predict Health’s current service offerings. The Data Engineer is responsible for designing the solution, building the data integration and associated pipelines, design and deploying data structures required to support current products and analytical efforts, and automating the operational aspects of the data platform. The Data Engineer responsibilities will include:
· Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as, but not limited to, Azure Synapse, Azure Data Factory and Azure pipelines.
· Translate business requirements from product owners and analysts into technical code.
· Identify, fix and document bugs, bottlenecks in workflows and pipelines.
· Enforce security compliance, fine tune performance and promote code quality standards.
· Identify and curate new data sources required to support business requirements
· Explore, learn the latest Azure data management technologies to add to current competencies.
· Support development of data intensive solutions in a fast-paced, dynamic environment
· Support Business Intelligence Applications (e.g. Power BI) as required.
Qualifications
Requires minimum of five years relevant experience in design, development, implementation, and testing of data services
· Experience in data structures, data modeling, data warehousing and data pipeline development on Azure
· 3+ years of hands-on experience in the design, implementation and performance tuning of Azure Data Factory, Synapse, or other Azure data services
· Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure
· Experience developing software code in one or more programming languages (Java, Python, etc.)
· Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform
· Experience with data migration
· Experience defining system architectures and exploring technical feasibility trade-offs.
· Ability to prototype and evaluate applications and interaction methodologies.
Other Knowledge, Skills, and Abilities Required
· Demonstrated capability for problem solving, decision making, sound judgement and assertiveness.
· Ability to change direction to meet demand in a high-paced work environment
· Ability to focus on details and technicalities while simultaneously retaining a comprehensive vision of organizational goals
· Excellent collaborator and able to influence and direct work contributions from others to meet critical program deliverables and timelines
· Advanced interpersonal, written, and oral communication skills.
· Proficiency in Microsoft Office including Excel and PowerPoint
What we Offer
This is a full-time position and compensation, and benefits will be highly competitive.
Predict Health is committed to bringing together people from different backgrounds and perspectives to deliver real results for our clients and end-users. We recruit, employ, compensate, and promote regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.
This position will be based in our Arlington, VA office. We offer a flexible work schedule with a minimum of three days in the office each week.
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Employee discount
Flexible schedule
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Arlington, VA: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location","$115,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
TheHive,#N/A,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",$35.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"APLOMB Technologies
4.4",4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$72,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"FlexIT Inc
4.0",4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.","$100,840 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Iyka Enterprises, Inc.
3.8",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115","$85,831 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
"Buchanan & Edwards
3.8",3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location","$138,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD)
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"PrizePicks
4.9",4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1","$101,755 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"MARVEL TECHNOLOGIES INC
3.7",3.7,Remote,Data Engineer,"Primary Skills
SCALA, SPARK, SQL, HADOOP, AWS, HIVE, Spark SQL, HIVE QL, CICD (Continuous Integration/Continuous Delivery), VCS (GIT HUB)
Coding in Scala
Designing in of HADOOP ecosystem
Hands-on experience on AWS tools like EMR, EC2
Hands-on experience of SQL in Big Data: SQL, Spark SQL, Hive QL
Proficient in working with large data sets and pipelines
Proficient with workflow scheduling / orchestration tools
Well versed with CICD process and VCS
Databricks is a big PLUS
Job Types: Full-time, Contract
Pay: $50.00 - $58.00 per hour
Schedule:
8 hour shift
Monday to Friday
Experience:
Spark: 4 years (Required)
Scala: 4 years (Required)
Hadoop: 3 years (Required)
Aws: 3 years (Required)
Hive: 3 years (Required)
CI/CD, VCS: 3 years (Required)
Databricks: 1 year (Required)
Work Location: Remote",$54.00 /hr (est.),51 to 200 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,$5 to $25 million (USD)
"Double Line, Inc.
4.2",4.2,"Austin, TX",Data Engineer,"(This is a remote position open to candidates residing in North Carolina and Texas. We have an office location in Austin, TX for use at our employees' convenience. We have no plans to return to the office on a mandatory basis.)

Feeling underappreciated? Underutilized? Want to be a part of a specialized team with exposure to a wide variety of data puzzles to solve, while using your skills to improve education? Come join a team where you can Fly the Airplane, not just be a passenger in the back. We're a growing company focused on expanding our Operations team with a solutions-focused Data Engineer. Sound interesting?

If so, we're looking for a motivated and driven person like you who has:
Strength in thinking creatively and collaborating with other data experts in figuring out solutions to really tough data loads or transformation problems
Experience leveraging SQL and/or ETL development, data mapping, and data modeling to manage and organize client data
A passion for continuous improvement in refining the approach and doing it better and faster the next time

Bonus points if you're bringing knowledge of or really want to learn the following:
Consultancy experience with a focus on Agile practices
AWS and Azure Cloud
Python or similar scripting languages
AWS Quicksight, Tableau, Power BI, or other visualization tools

In return, we offer:
A mission-driven company with a long-term focus on helping the world by untangling the technical knots that plague state and local governments, particularly in education, healthcare, and similar fields
A home where your voice matters and you can affect real change
An employer who cares about you, makes sure you're engaged with exciting work, and offers robust benefits, 401k with employer match, and a great culture

We do not want you to make the leap without knowing what we need, so here is how we define success for this position:
Soak up knowledge from the existing team of experts in the first 30 days
Bring fresh eyes to our processes and techniques and bring new ideas to the table in the first 2 months
Mentor a new data engineering hire in your first 90 days

We need to know - can you make this happen? If so, we definitely need to talk to you.

We value diversity at Double Line. We hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.

Double Line does not sponsor applicants for work visas at this time.

Double Line does not currently offer relocation assistance.

iS4dbmGU5w","$85,882 /yr (est.)",51 to 200 Employees,Company - Private,Management & Consulting,Business Consulting,2009,$5 to $25 million (USD)
"Xiar tech inc
3.3",3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$51.90 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Nursa
4.3",4.3,"Murray, UT",Data Engineer,"Nursa is revolutionizing the temporary healthcare staffing industry by delivering an unparalleled level of transparency and accessibility to healthcare providers, and living out our mission to enrich the life of every clinician with innovation and opportunity. We’re committed to helping professionals such as registered nurses (RNs), licensed practical nurses (LPNs), and certified nursing assistants (CNAs) advance their careers by giving them the power to choose from available shifts provided by facilities in their area. In turn, Nursa’s marketplace platform technology empowers healthcare facilities to bring in the talent they need without spending time and resources on traditional nursing recruitment processes. By connecting healthcare clinicians and healthcare facilities to fill per diem and on-call shifts, the Nursa marketplace increases access and transparency to the $17 billion temporary clinician staffing industry. We’re a profitable, venture backed startup founded in March 2020 and are rapidly expanding. Your impact will not only be on the business, but also on the daily lives of healthcare providers and the quality of care everyone experiences.
Role overview:
Our data team is growing and we are expanding the Data Engineering pod within the Data Team. You will function as a Data Engineer and ETL Developer and will report directly to the Data Engineering Manager. You will help to expand our tech stack by implementing processes for ETL, data pipeline, and build monitoring tools. You will help manage our data warehouse, Segment (CDP / session tracking tool), and data integrations for third-party platforms (Salesforce, Iterable, etc). This Data Engineering pod works closely with Business Intelligence, Data Science, Development, as well as many other business teams. The ideal working situation would be hybrid, but highly qualified remote candidates will be considered.
This position requires for someone to be onsite in Murray, UT.
Responsibilities:
Take a proactive role in expanding tech stack and building data engineering team
Develop pipelines between systems using Rest or similar APIs
Integrate and manage ETL for systems such as Salesforce, Iterable, RedShift, NetSuite
Work continually to monitor and improve data accuracy and reliability
Collaborate with and mentor others on data team and across the organization
Qualifications:
3+ year experience in Data Engineering or ETL Development
Proficiency in Python scripting, SQL, and database management (AWS Redshift)
Must have experience creating data pipelines to send and pull data with REST or similar API
Experience using third-party integrations (CRM or CDP)
AWS experience would be beneficial
What you get in return:
Opportunity to revolutionize healthcare industry and build both relationships and teams that make a tangible impact
We empower team members to act intelligently and be owners, believing that execution is everything, and have designed a learning-focused environment where you get ongoing support and regular feedback to help you grow
An opportunity to join an international team with a work culture that is based on trust, flexibility, and curiosity
Competitive salary and benefits
Closing:
Nursa is an equal opportunity employer. We aim to build a workforce of individuals from different backgrounds, with different abilities, identities, and mindsets. Even if you do not meet all of the qualifications listed above, we encourage you to apply!","$89,485 /yr (est.)",51 to 200 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2018,Unknown / Non-Applicable
The Sunwater Institute,#N/A,"Rockville, MD",Data Engineer,"The Sunwater Institute is a nonprofit, nonpartisan public policy organization focused on improving the foundations of our social systems. The Sunwater Institute’s current programs focus on improving congressional performance, preserving and advancing constitutional rights and liberties, strengthening and enlarging property rights, and exploring the foundations of group decision-making.
The Sunwater Institute’s interdisciplinary, long-term orientation and focus on the building blocks of social systems sets it apart as a haven for bold, fresh ideas and free, constructive dialogue. The Sunwater Institute is located in North Bethesda and maintains a presence on Capitol Hill.
POSITION
The Sunwater Institute is seeking a Data Engineer who has a background in engineering or computer science and is highly skilled in SQL and database services technologies. The ideal candidate will possess strong problem-solving skills, excellent communication, and a great technical aptitude. The individual in this position will provide project support and help drive team initiatives. The candidate will be creative, and comfortable working in small teams and coordinating with US and non-US-based developers.
Responsibilities
Develop data models to store and manage large amounts of structured and unstructured data
Design efficient ETL processes to move and integrate data from different sources into the enterprise warehouse
Write complex queries using SQL Server etc. to analyze large datasets
Identify trends, patterns, and correlations within datasets by performing exploratory analysis
Ensure high quality of data by implementing best practices on storage, maintenance, indexing techniques, etc
Coordinate project status updates between teams and communicate deliverables and timelines
REQUIREMENTS
Bachelor’s degree in Computer Science or Engineering related field
4+ years of relevant experience in a similar role
Strong knowledge of relational databases including SQL Server
Proficient skills in writing complex queries using SQL language; developing stored procs, views, etc.
Experience with end-to-end ETL (Extract Transform Load) Processes
Familiarity with Big Data platforms such as AWS etc.
Ability to develop effective reports that help drive business decisions
Must be able to work effectively with teams but be able to work independently on tasks to meet tight deadlines
Exceptional communication skills verbal, written, and active listening
Extreme attention to detail and accuracy
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Health insurance
Life insurance
Paid time off
Parental leave
Vision insurance
Compensation package:
Yearly bonus
Yearly pay
Experience level:
4 years
Schedule:
8 hour shift
Monday to Friday
Ability to commute/relocate:
Rockville, MD 20852: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
"PLAXONIC
4.6",4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642","$117,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
TheHive,#N/A,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",$35.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"APLOMB Technologies
4.4",4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$72,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"Empower Federal Credit Union
4.0",4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000","$99,876 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,#N/A,$25 to $100 million (USD)
"Plaxonic Technologies
4.6",4.6,"Dallas, TX",Data Engineer,"Job Description – Sr. Data Engineer:
Minimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark
Ability to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark
Hands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies
Strong skill in understanding database architecture, data models and writing complex SQL queries/code
Hands on experience with integration of different data sources (Files, DBs, APIs)
Ability to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation
Knowledge of various ETL techniques
Experience with data pipeline and workflow management tools [Azure DevOps]
Strong analytic skill to work with unstructured data
Experience working with Agile teams scrums.
Strong customer handling skills and communication skills
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75287: Reliably commute or planning to relocate before starting work (Required)
Experience:
Microsoft Excel: 1 year (Preferred)
Microsoft Office: 1 year (Preferred)
Compliance management: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: One location
Speak with the employer
+91 7272167989","$98,641 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"Oddball
4.6",4.6,Remote,Data Engineer,"Oddball believes that the best products are built when teams understand and value the things they are working on. We value learning and growth and the ability to make a big impact at a small company. We believe that we can make big changes happen and improve the daily lives of millions of people by bringing quality software to the federal space.
As a Database Engineer you will be responsible for helping to develop production-quality analytics tools, utilizing existing data sources to empower stakeholders to inform research and development. Act as a liaison between engineers and analysts. Implement and enforce standards and procedures to ensure data is managed consistently and properly integrated.
Experience Requirements
5+ years of proven data and performance engineering experience.
A proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs.
Experience with system analysis, data analysis or programming, using a variety of computer languages and procedures.
Experience working in Agile environments.
Experience in developing, analyzing, and presenting data models.
Knowledge of data management, data standardization, and data governance.
Ability to analyze source data for potential data quality issues.
Expert in SQL and/or SQL based languages.
Create supporting documentation, such as metadata and diagrams of entity relationships, business processes, and process flow.
Create plans, test files, and scripts for data warehouse testing, ranging from unit to integration testing.
This is a Remote position. This is a salaried role. Oddball offers both a tech, and a continuing education stipend.
Must be located within the United States.
Clearances
Ability to obtain low-level federal clearance is required.
Oddball is an Equal Opportunity Employer and does not discriminate against applicants based on race, religion, color, disability, medical condition, legally protected genetic information, national origin, gender, sexual orientation, marital status, gender identity or expression, sex (including pregnancy, childbirth or related medical conditions), age, veteran status or other legally protected characteristics. Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible schedule
Flexible spending account
Health insurance
Health savings account
Paid time off
Parental leave
Professional development assistance
Referral program
Retirement plan
Vision insurance
Compensation package:
Bonus pay
Experience level:
5 years
Schedule:
Monday to Friday
Education:
Bachelor's (Preferred)
Work Location: Remote",#N/A,1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Xiar tech inc
3.3",3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$51.90 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Edrstaffing,#N/A,"Boston, MA",Python Data Engineer,"Great opportunity to join a dynamic company on the cutting edge and making a difference in the world we live in!
The role is a unique opportunity to be part of a dynamic, and growing team taking actionable steps to address climate change.
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker
As a Data Engineer , You will deliver on below missions:
Deploy , covering all aspects from data discovery
Work closely with product development team to feed your experience from the frontline back to the
development road map, and with our climate experts to bring leading market advice to our customers
WHAT YOU BRING (EXPERIENCE & QUALIFICATIONS)
We’re looking for exceptional talent, with strong background in data engineer. You would typically have:
2 - 3 years of ETL experience with heavy focus on building robust, repeatable data pipelines
Deep knowledge of Python
Experience with version control, CI/CD, ideally deploying Kubernetes and Docker - based solutions
Experience working in customer facing services, ideally in sustainability & greenhouse gas accounting
company.
Self - driven, problem solving attitude
Proven ability of database management to come up with better ways to wrangle & structure complex
sources of data
An experience in a fast - growing environment , ideally working within SaaS company in critical product scale -
up phase
Experience working in an Agile development environment
We need to see skills, Python, CI/CD, and either Kubernetes and/or Docker","$120,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"PrizePicks
4.9",4.9,"Atlanta, GA",Data Engineer,"PrizePicks
Job Posting
Job Title: Data Engineer
Location:
Hours: Full-time
About Us: PrizePicks is a sports technology start-up based in Atlanta that delivers innovative technologies focused on increasing sports fan engagement. With our flagship game, PrizePicks, we set out to build the most fast & simple fantasy sports game possible.
Job Description: The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments - at the core of these operations is data. By developing, maintaining, and testing data generation infrastructures, you will enable the PrizePicks business to make smarter, better, and faster data-driven decisions.
Key Responsibilities:
Create, maintain and orchestrate optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Qualifications:
Advanced working SQL knowledge and experience working with relational databases
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with object-oriented/object function scripting languages: Python.
Preferred experience with data pipeline and workflow tools: Azkaban, Luigi, Airflow, Prefect, etc.
Experience with AWS or like cloud services: EC2, EMR, RDS, Redshift, GCP

Hiring Process:
Recruiter Resume Screening
Hiring Manager Initial Interview
Technical Assessment (Should take < 4 hours)
Peer Interview
Peer Interview

How You’ll Ramp:
30 Days: Familiar with the data tech stack - comfortable querying basic datasets and requests for the team
60 Days: Contributing to internal conversations on data organization and structure
90 Days: Rolled out your first data pipeline to support analytics teams w/ a new data source
Want to Learn More?
PrizePicks agrees DFS partnership with Atlanta Falcons (SBC Americas)
PrizePicks Continues to Post Record Months to Begin 2021, Expands Leadership Team With an Eye Toward Category Growth (Yahoo Finance)
PrizePicks Stacks Its Chips for the Future, Closes Round of Strategic Investors (AP News) ? CSL Esports joins forces with PrizePicks (Esports Insider)
#LI-remote
#LI-DM1","$101,755 /yr (est.)",1 to 50 Employees,Company - Private,"Arts, Entertainment & Recreation",Sports & Recreation,2015,Unknown / Non-Applicable
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Predict Health,#N/A,"Arlington, VA",Data Engineer,"Overview of the Role
We are looking for an exceptional Data Engineer who is interested in joining a small and rapidly growing Product and Services team at our company. We view our data-driven products and services as strategic assets, and you will be a key part of our team and company. The successful candidate will have 5+ years of experience designing and implementing data solutions and services. This role will report to our VP, Product Engineering and will be an instrumental member of our product development team.
The Data Engineer will participate in the design of new information intensive product and service offerings. They will be responsible for the development and implementation of the data pipelines and services underlying these new products. They will deepen their understanding of the political, policy, financial and business aspects of the health plan industry so as to be able to provide insight into the best manner to collect and manage information related to these businesses.
The Data Engineer will work closely with the Product Management function to understand customer needs and help identify capabilities for new offerings. They will also partner with the Data Science team to understand the analytical needs of the business and assist in the design and implementation of data solutions to support these efforts.
About Predict Health, Inc.
Predict Health is transforming how health plans, providers and their partners increase retention, quality, and lifetime value of their customer or patient relationships. The core competency of the organization is the curation, analysis, and presentation of information to support targeted operational improvements in healthcare enterprises. The company is an early-stage enterprise based in Washington, D.C. which focuses on three solution categories: Member Acquisition, Quality Management, and Retention Management.
Our company, Predict Health was founded in 2019 and is led by an experienced team of healthcare experts, data scientists and healthcare entrepreneurs. We are focused on building services and solutions designed to improve the healthcare experience of our nation’s seniors. We founded our company after seeing first-hand how challenging it is for the seniors in our lives - including our parents, grandparents and friends - to deal with the complexities of Medicare with little or no real help.
Detailed Job Description
The Data Engineer will initially support the deployment of an Azure solution to help collect, aggregate, transform, and distribute information related to Predict Health’s current service offerings. The Data Engineer is responsible for designing the solution, building the data integration and associated pipelines, design and deploying data structures required to support current products and analytical efforts, and automating the operational aspects of the data platform. The Data Engineer responsibilities will include:
· Data acquisition and ingestion - Identify data sources and build pipelines using various ETL tools such as, but not limited to, Azure Synapse, Azure Data Factory and Azure pipelines.
· Translate business requirements from product owners and analysts into technical code.
· Identify, fix and document bugs, bottlenecks in workflows and pipelines.
· Enforce security compliance, fine tune performance and promote code quality standards.
· Identify and curate new data sources required to support business requirements
· Explore, learn the latest Azure data management technologies to add to current competencies.
· Support development of data intensive solutions in a fast-paced, dynamic environment
· Support Business Intelligence Applications (e.g. Power BI) as required.
Qualifications
Requires minimum of five years relevant experience in design, development, implementation, and testing of data services
· Experience in data structures, data modeling, data warehousing and data pipeline development on Azure
· 3+ years of hands-on experience in the design, implementation and performance tuning of Azure Data Factory, Synapse, or other Azure data services
· Hands on experience in orchestrating data flows for scalable, highly available data engineering and data science solutions in Azure
· Experience developing software code in one or more programming languages (Java, Python, etc.)
· Hands-on experience in DevOps and Infrastructure as Code preferably using Azure DevOps (ADO) and Terraform
· Experience with data migration
· Experience defining system architectures and exploring technical feasibility trade-offs.
· Ability to prototype and evaluate applications and interaction methodologies.
Other Knowledge, Skills, and Abilities Required
· Demonstrated capability for problem solving, decision making, sound judgement and assertiveness.
· Ability to change direction to meet demand in a high-paced work environment
· Ability to focus on details and technicalities while simultaneously retaining a comprehensive vision of organizational goals
· Excellent collaborator and able to influence and direct work contributions from others to meet critical program deliverables and timelines
· Advanced interpersonal, written, and oral communication skills.
· Proficiency in Microsoft Office including Excel and PowerPoint
What we Offer
This is a full-time position and compensation, and benefits will be highly competitive.
Predict Health is committed to bringing together people from different backgrounds and perspectives to deliver real results for our clients and end-users. We recruit, employ, compensate, and promote regardless of race, religion, color, national origin, gender identity, disability, age, veteran status, and other protected status as required by applicable law and as a matter of our company values.
This position will be based in our Arlington, VA office. We offer a flexible work schedule with a minimum of three days in the office each week.
Job Type: Full-time
Pay: $100,000.00 - $130,000.00 per year
Benefits:
Dental insurance
Employee discount
Flexible schedule
Health insurance
Life insurance
Paid time off
Vision insurance
Schedule:
Monday to Friday
Supplemental pay types:
Bonus pay
Ability to commute/relocate:
Arlington, VA: Reliably commute or planning to relocate before starting work (Required)
Work Location: One location","$115,000 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Iyka Enterprises, Inc.
3.8",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115","$85,831 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
Ascent Solutions,#N/A,Remote,Hadoop Big Data Engineer,"Hadoop Big Data Engineer
Remote
Must Haves • Strong experience in Big Data, Cloudera Distribution 7.x, Cloud migration, RDBMS • Strong Experience with Amazon EMR/Databricks/Cloudera CDP • 4-5 experience building data pipelines using Hadoop components Sqoop, Hive, Solr, MR, Impala, Spark, Spark SQL., HBase. • 4-5 years of programming experience in Python, Java and Scala is must. • Strong experience with REST API development using Python frameworks (Django, Flask etc.) • Micro Services/Web service development experience using Spring framework is highly desirable.
Job Types: Full-time, Contract
Pay: $55.00 - $65.00 per hour
Schedule:
Monday to Friday
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"PLAXONIC
4.6",4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642","$117,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"APLOMB Technologies
4.4",4.4,"Princeton, NJ",Data Engineer,"We are hiring for Training and placement opportunity for our client requirements.
These are W2 roles.
Location: Plainsboro, NJ
Below are the benefits that you will get from us
- Training and mock interview support
- Placement assistant
- H1 Sponsorship
- STEM extension support
- Accommodation support for in class training
- Will provide you on job support
Job Types: Full-time, Contract
Salary: $70,000.00 - $75,000.00 per year
Experience level:
8 years
Schedule:
8 hour shift
Ability to commute/relocate:
Princeton, NJ 08540: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$72,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,$1 to $5 million (USD)
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Empower Federal Credit Union
4.0",4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000","$99,876 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,#N/A,$25 to $100 million (USD)
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"IBR (Imagine Believe Realize)
4.5",4.5,"Melbourne, FL",Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote","$118,866 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Iyka Enterprises, Inc.
3.8",3.8,"Dekalb, IL",Data Engineer,"(Submit Resume and All Inclusive hourly rate or Salary).
The selected candidate will coordinate the maintenance and ongoing development of data systems for two closely related projects as the primary data engineer on a small (2-5) technical services team. The existing system has been developed in Microsoft SQL hosted via Microsoft Azure Government Cloud. The candidate does NOT need to be Department of Defense system qualified. Work is primarily remote but the candidate should reside in Illinois or the region; work teams meet once per month for strategic planning in DeKalb, Illinois.
o Preferred skill set:
§ Strong background in Microsoft SQL Server, both installing/configuring and administering.
§ Experience with Microsoft Azure.
§ Experience designing and implementing ETL processes.
§ Experience with event-driven automated systems preferred.
§ Some familiarity with Tableau Server and Office 365 preferred.
§ Strong communication skills
Job Types: Full-time, Contract
Schedule:
Monday to Friday
Work Location: Hybrid remote in DeKalb, IL 60115","$85,831 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Computer Hardware Development,#N/A,Less than $1 million (USD)
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"BNY Mellon
3.7",3.7,"Pittsburgh, PA",Big Data Sytems Engineer(Hadoop),"Overview
Big Data Engineer
Bring your ideas. Make history.
BNY Mellon offers an exciting array of future-forward careers at the intersection of business, finance, and technology. We are one of the world's top asset management and banking firms that manages trillions of dollars in assets, custody and/or administration. Known as the “bank of banks” - 97% of the world’s top banks work with us as we lead and serve our customers into the new era of digital.
With over 238 years of rich history and industry firsts, BNY Mellon has been built upon our proven ability to evolve, lead, and drive new ideas at every turn. Today, we’re approximately 50,000 employees across 35 countries with a culture that empowers you to grow, take risks, experiment and be yourself. This is what #LifeAtBNYMellon is all about.
We’re seeking a future team member in the role of Big Data Engineer to join our TSG Data Management team. This role is located in Pittsburgh, PA and Nashville, TN .
In this role, you’ll make an impact in the following ways:
Work as a senior system engineer for the Hadoop enterprise data platform platforms and create highly complex big data/Hadoop platforms. You will have expert knowledge in distributed datalake systems.
You’ll configure and tune service components to optimize platform performance. You will design and implement backup and restore strategies and disaster recovery solutions and will support and troubleshoot performance related issues.
You’ll consult with storage and host engineering services to create optimized big data ecosystems and assist in cross-team collaboration and design discussions.
Provide sandbox functionality to users of the datasets to assist application analytics and storage infrastructure system management including capacity planning, performance monitoring and tuning, security management etc.
Responsible for infrastructure Support and projects including system design, planning, installations, configurations, upgrades and planning and Manage Tier-3 support following ITIL practice and incident management
Manage system configurations and solutions, and monitor the full lifecycles of storage area network (SAN) administration, architecture, and support and Data storage tasks such as management, provisioning, troubleshooting and debugging
Lead technical assessments of hardware, software, tools, applications, firmware, and operating systems to support business operations. You’ll develop trusted corporations with application tenants. You’ll advise applications with sounds data layer resiliencies and provide input into the selection of tools and any necessary migration into the company's environment.
To be successful in this role, we’re seeking the following:
Bachelor's degree in computer science or a related discipline, or equivalent work experience required.
Advanced degree preferred10-12 years of related experience required; experience in the securities or financial services industry is a plus.
5-10 years of experience in Hadoop technologies, data lake design, experience in the securities or financial services industry is a plus.
Excellent knowledge with Hadoop components for big data platforms related to data ingestion, storage, transformations and analytics. Excellent DevOps skillsets and SDLC practices. Excellent knowledge of automation tools such as Ansible, Python programming skills, knowledge of Docker containers
8+ years of experience on BigData Platform on Hadoop, Spark, Hive, HBase and 3+ years’ experience on Python and DevOps tools such as Ansible
Strong Agile/Scrum development experience and Working knowledge with PostgreSQL or Mongo DB databases a plus
At BNY Mellon, our inclusive culture speaks for itself. Here’s a few of our awards:
Fortune World’s Most Admired Companies & Top 20 for Diversity and Inclusion
Bloomberg’s Gender Equality Index (GEI)
Best Places to Work for Disability Inclusion , Disability: IN – 100% score
100 Best Workplaces for Innovators, Fast Company
Human Rights Campaign Foundation, 100% score Corporate Equality Index
CDP’s Climate Change ‘A List’
Our Benefits:
BNY Mellon offers highly competitive compensation, benefits, and wellbeing programs rooted in a strong culture of excellence and our pay-for-performance philosophy. We provide access to flexible global resources and tools for your life’s journey. Focus on your health, foster your personal resilience, and reach your financial goals as a valued member of our team, along with generous paid leaves that can support you and your family through moments that matter.
BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer - Underrepresented racial and ethnic groups/Females/Individuals with Disabilities/Protected Veterans.
This person is a subject matter expert in several of the tools/technologies used in the space.Leads the development infrastructure engineering growth strategies and initiatives. Leads initiatives to analyze complex infrastructure problems to be solved with advanced design.Leads the evaluation of the effectiveness of the organization's existing infrastructure technology and tools. Analyzes trends to develop strategy for the implementation of upgrades that will enhance the reliability, Resiliency and efficiency of the IT infrastructure.Provides leadership to execute project plans and performance requirements for all stages/phases through the management of human capital resources.This person is a subject matter expert in at least one of the tools/techologies used in the space.Participates in or leads initiatives to analyze infrastructure problems to be solved with advanced design. Utilizes standard procedures and policies when selecting methods, techniques, and evaluation criteria for obtaining results.Participates in or leads initiatives to analyze infrastructure problems to be solved with advanced design. Utilizes standard procedures and policies when selecting methods, techniques, and evaluation criteria for obtaining results.Manages the processes for ensuring that all systems/applications/software/hardware are compliant with Corporate policy/procedures.Monitors project plans and budgets.Works closely with external vendors, internal partners and busienss teams to provide infrastructure/tool needs.Works with Application Development and Quality Assurance, Testing and Business teams to understand infrastructure needs during the development, testing and production BAU processes. Ensures these needs are taken into account when developing infrastructure.Acts as escalation point for major incidents.Leads strategy to increase automation across the organization.Contributes to the achievement of multiple teams' objectivesBachelor's degree in computer science or a related discipline, or equivalent work experience required; advanced degree preferred10-12 years of related experience required; experience in the securities or financial services industry is a plus. BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer. Minorities/Females/Individuals with Disabilities/Protected Veterans. Our ambition is to build the best global team – one that is representative and inclusive of the diverse talent, clients and communities we work with and serve – and to empower our team to do their best work. We support wellbeing and a balanced life, and offer a range of family-friendly, inclusive employment policies and employee forums.
Employer Description:
For over 230 years, the people of BNY Mellon have been at the forefront of finance, expanding the financial markets while supporting investors throughout the investment lifecycle. BNY Mellon can act as a single point of contact for clients looking to create, trade, hold, manage, service, distribute or restructure investments and safeguards nearly one-fifth of the world's financial assets. BNY Mellon remains one of the safest, most trusted and admired companies. Every day our employees make their mark by helping clients better manage and service their financial assets around the world. Whether providing financial services for institutions, corporations or individual investors, clients count on the people of BNY Mellon across time zones and in 35 countries and more than 100 markets. It's the collective ambition, innovative thinking and exceptionally focused client service paired with a commitment to doing what is right that continues to set us apart. Make your mark: bnymellon.com/careers.
EEO Statement:
BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer. Minorities/Females/Individuals With Disabilities/Protected Veterans. Our ambition is to build the best global team – one that is representative and inclusive of the diverse talent, clients and communities we work with and serve – and to empower our team to do their best work. We support wellbeing and a balanced life, and offer a range of family-friendly, inclusive employment policies and employee forums.
Job Type: Full-time","$80,404 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Investment & Asset Management,1784,$10+ billion (USD)
TheHive,#N/A,"Richville, MN",Data Engineer,"Top skills:
Confluent or Apache Kafka (ability to ingest data into snowflake)
Snowflake is preferred, almost a must
Python for automation
SQL server or other DB knowledge
Preferred:
Public cloud knowledge, specifically Azure
ETL tools
Responsibilities:
Build constructive partnerships with business leadership to proactively recommend solutions to business issues through practical, elegant, and effective business intelligence and reporting vehicles.
Direct deployment of enterprise data strategy vision through successful program delivery
Collaborate with global and contractor leaders to develop partnerships that allow for capacity flexibility and offer equivalent and or superior quality delivery capability
Effectively manages a balanced scorecard metric set that would include key measures such as quality & timely delivery.
Incorporate best practices and standards in metadata and report/dashboard development
Translates highly complex concepts in ways that can be understood by a variety of audiences
Function as a mentor for other team members, providing assistance where needed
Job Types: Full-time, Contract
Salary: $20.00 - $50.00 per hour
Schedule:
Monday to Friday
Work Location: Hybrid remote in Richville, MN 56576",$35.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Plaxonic Technologies
4.6",4.6,"Dallas, TX",Data Engineer,"Job Description – Sr. Data Engineer:
Minimum experience of 5-8 years working as a Bigdata developer/engineer in PySpark
Ability to develop data integration and transformation code/pipelines in object oriented/scripting language: PySpark
Hands on experience in Azure Cloud components such as Azure Databricks, Azure Data Factory, Azure Logic apps service, AKS, Azure Devops or any Cloud technologies
Strong skill in understanding database architecture, data models and writing complex SQL queries/code
Hands on experience with integration of different data sources (Files, DBs, APIs)
Ability to work with large volume of data sets in performing data ingestion, loading, transformation and aggregation
Knowledge of various ETL techniques
Experience with data pipeline and workflow management tools [Azure DevOps]
Strong analytic skill to work with unstructured data
Experience working with Agile teams scrums.
Strong customer handling skills and communication skills
Job Type: Full-time
Benefits:
401(k)
Dental insurance
Health insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Dallas, TX 75287: Reliably commute or planning to relocate before starting work (Required)
Experience:
Microsoft Excel: 1 year (Preferred)
Microsoft Office: 1 year (Preferred)
Compliance management: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: One location
Speak with the employer
+91 7272167989","$98,641 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Invictus Data,#N/A,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location","$125,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Ascent Technologies,#N/A,Remote,Data Engineer,"Job Description: Involved in resource / volume ramp-up plan/ backfilling resources planned for rotation and resignation cases Tracking Performance of individual Projects within the delivery/SIPs. Participate / presenting SD performance during client visit Publish Daily, Weekly and Monthly reports on team productivity Reviewing the weekly / monthly dashboards Publishing dashboards to customer People Management Drive the team to reach the defined targets Ensure training and development of the team members Mentor team members on technical and communication aspects. onsite anni data enginneer-pheonix,az python,charlottee,nc ba-houston tx
Job Types: Full-time, Contract, Permanent
Salary: $39.87 - $86.24 per hour
Benefits:
Dental insurance
Compensation package:
1099 contract
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91 7842516676",$63.05 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Buchanan & Edwards
3.8",3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location","$138,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD)
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
"PLAXONIC
4.6",4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642","$117,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"E-Logic INC
4.4",4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.","$84,277 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
Invictus Data,#N/A,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location","$125,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Xiar tech inc
3.3",3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$51.90 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"FlexIT Inc
4.0",4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.","$100,840 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Excellerent Solutions Inc,#N/A,"Coppell, TX",Data Integration Engineer,"Role: Data Integration Engineer
Location: Irving, TX(5 days a week in Office, 100% onsite role)
Must have experience working with SnapLogic to perform ELT processes.
Must have good written and verbal skills
Must be capable of knowledge transfer to team of ~10 ETL developers on use of SnapLogic
Must be able to work with a larger team to complete sizable migration of on-prem processes to Snowflake.
Must be able to follow design patterns set out by our architecture team.
Job Types: Full-time, Contract
Pay: $48.18 - $60.00 per hour
Schedule:
8 hour shift
Work Location: In person",$54.09 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"Empower Federal Credit Union
4.0",4.0,"Syracuse, NY",Data Engineer,"Empower Federal Credit Union offers excellent benefits including:
Medical and Dental Insurance, 401K with Employer Match, Holiday Pay, Paid Time Off and more!

Please note: All candidates will be subject to a credit check to determine employment eligibility.

Role:
Do you enjoy building and innovating in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and ever-changing delivery environment? With Empower Federal Credit Union, you'll be part of a group of trailblazing, creative, and not to mention smart teams, who solve real problems and meet credit union needs. We are seeking a Data Engineer who is passionate about data and emerging technologies. As an Empower FCU Data Engineer, you'll have the opportunity to be on the forefront of driving a transformation within the credit union from a Business Intelligence and Technology standpoint.

The Data Engineer will join a team purpose-driven team members in a friendly, focused, fast-paced entrepreneurial credit-union environment.

The Data Engineer role is responsible for the creative and strategic design, implementation, and maintenance processes of mission-critical customized data modeling and application integration solutions. They are also responsible for implementing methods to improve data reliability, quality, and governance while laying the foundation to allow for data-driven decision making, and effectively able to communicate technical opportunities to various levels within the organization.

The Data Engineer will utilize strong analytical skills and the ability to combine data from different sources to build and support architectures suitable for data extraction and transformation for predictive or prescriptive modeling by analyzing raw data, developing and maintaining datasets, and improving data quality and efficiency.

The Data Engineer works alongside domain experts, business groups, BI Solutions Developers and Analysts to frame business problems, integrate the needed data and determine the best way to provision that data on demand. The role requires flexibility and adaptability to handle multiple and changing priorities.

Familiarity with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.

Reporting directly to the Vice President, Business Intelligence, the Data Engineer will be responsible for data capabilities as a part of the data landscape.

This is a role with hybrid and/or remote options, depending on your location. We are headquartered in Syracuse, New York.

Essential Functions & Responsibilities:
E
50%

Build, manage and optimize data pipelines that facilitate data movement. Develop processes to store, manage and deliver data for analytics and application integration. Drive Automation by employing innovative tools, techniques, and architectures to automate common, repetitive, and tedious data preparation and integrations tasks. Promote best practices in data governance and change management. Create documentation including requirements, design documentation, technical specifications, procedures, and testing documentation to support existing and new development.

E
20%

Collaborate across agile Business and IT to develop and implement scalable BI data model strategies that promote data democratization and administrative use of data to further the goals of the organization. Support Solutions Developers and Analysts in training, developing and operationalizing data science models.

E 20% Develops, delivers, and manages business intelligence and analysis data models and tools to be used organization wide.
E
5%

Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the Empower team.

E 5% Other duties, as assigned.

Performance Measurements:
1. See Dayforce (HRIS) for Performance Goals upon hire.

Knowledge and Skills:
Experience
Eight to ten years of similar or related experience including:
Proven experience in building/operating/maintaining fault tolerant and scalable data processing integration.
Well versed with distributed computing principles, proven experience building high scale high-performance cloud platforms and services
Familiar & Comfortable with Oracle, MS SQL, SSRS, SSIS, Azure, Power BI, Agile preferred.
Experience using SQL server reporting services, PowerBI, analysis services, or any other data visualization tools.
Education
(1) A bachelor's degree, or (2) achievement of formal certifications recognized in the industry as equivalent to a bachelor's degree (e.g. information technology certifications in lieu of a degree).
(2) Preferred Degrees: Computer Science, Economics, Information Systems, Statistics, Applied Math, Business Administration, Data Science, or any other related field.

Interpersonal Skills
This role is a technical leader, as such needs to be a commander of BI technology within the organization to further business objectives. The Data Engineer will interface with developers, 3rd party vendors, contractors, and internal stakeholders across all levels of the organization. Effective communication and collaboration skills are essential. Work involves much personal contact with others inside and/or outside the organization for the purpose of integration support, requirement and specification gathering, building relationships, and soliciting cooperation. Discussions involve a higher degree of confidentiality and discretion, requiring diplomacy and tact in communication.

Other Skills
A passion for details, planning, workflow, systems, and process.
Highly efficient and strong prioritization skills and the ability to manage many projects concurrently.
Extremely organized and thrives in an environment where you get to utilize your detail-oriented nature to the max.
Problem Solving: You enjoy putting the puzzle pieces together to create successful data solutions and tools.
Partnership & Encouragement: Ability to encourage the team to think out-of-the-box and overcome engineering obstacles while incorporating new innovative design principles.
Desire for innovation with the ability to initiate ideas and lead initiatives from inception.
Excellent verbal and written communicator.
Adept at performing effectively in a fast-paced environment, thinking strategically, and meeting deadlines in a timely manner while maintaining professionalism and product quality.
Ability to hold a strong point of view, combined with a flexible and open mind that accepts input and adapts gracefully
Financial Institution experience a plus.
Experience in previous regulated industry a plus.
Physical Requirements
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of the job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is required to sit and stand for long periods at a time, use hand to finger, handle, or feel, and to talk or hear. The employee is occasionally required to walk; reach with hands and arms; and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 35 pounds.

Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. The noise level in the work environment is usually moderate.

This Job Description is not a complete statement of all duties and responsibilities comprising the position.

Empower Federal Credit Union (EFCU) and its affiliated companies know employees are our greatest asset. We take pride in our commitment to member service, teamwork, and excellence. We offer employees a work environment designed to encourage personal and professional development.

Empower Federal Credit Union is an equal opportunity employer and does not discriminate on the basis of any legally protected status or characteristic including race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Our equal employment opportunity policy statement, the EEO is the Law Poster and Supplement, and Pay Transparency Nondiscrimination Provision reaffirm our commitment.

Empower Federal Credit Union is committed to equal opportunity to qualified persons without regard to basis of race, religion, ethnicity, color, national or ethnic origin, age, sex, sexual orientation, gender identity or expressions, transgender status, sexual or other reproductive health decisions, marital status, ancestry, citizenship status, physical or mental disability, genetic information/ predisposition or carrier status, status in the uniformed services of the United States (including veteran status) or any other basis protected by applicable national, federal, state, provincial or local law. Empower Federal Credit Union will make reasonable accommodations for known physical or mental limitations of otherwise qualified employees and applicants with disabilities unless the accommodation would impose an undue hardship on the operation of our business. If you are interested in applying for an employment opportunity and feel you need a reasonable accommodation pursuant to the ADA, please contact us through one of the methods listed below:

a) Fax: 315-455-5423
b) US Mail: 1 Member Way Syracuse, NY 13212
c) Phone: 800-462-5000","$99,876 /yr (est.)",201 to 500 Employees,Company - Private,Financial Services,Investment & Asset Management,#N/A,$25 to $100 million (USD)
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
"AgileEngine
5.0",5.0,Remote,Big Data SDET Engineer,"Test with the best! At AgileEngine, you're free to build your own team of A-players and choose out of 70+ available projects the one you'd like to work on. Join the squad of top-3% software experts who make a difference through off-the-wall solutions.
Must haves
Strong Scala and Spark (hands on). If not Scala, strong experience in either Java or Python
Big Data concepts
API testing experience
Understanding qa process to create test cases and test strategies
Independently able to contribute to automation framework as well interact with the stake holders as needed
SQL experience
AWS experience
Strong test mindset
Experience of deploying or using Terraform and Gitlabs would be a bonus
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
The company behind the world’s leading OS virtualization software leverages AgileEngine as a provider of external QA and testing automation expertise. We’ve developed our client’s test strategy complete with test plans, ETAs, analysis, and risk assessment. Our teams have also optimized the company’s test suites, halving the time it takes for the tests to run. Our other deliverables include the implementation of new tests and the development of documentation, how-to guides, and test instructions on an internal Wiki.
Our team is also working to establish a strong QA/AQA process for an industry-leading cybersecurity platform. We’ve developed our client’s test strategy, manual testing cases, and automated tests from scratch. Our deliverables include backend, API, and UI testing, as well as BDD integrated with a streamlined CI process.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"Buchanan & Edwards
3.8",3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location","$138,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD)
"Enterprise Knowledge LLC
4.0",4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”","$89,080 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD)
LOVEFOODIES INC,#N/A,"Menlo Park, CA",Sr. Data Engineer,"Role: Data Engineer
Essential Skills:
SQL. SQL, or Structured Query Language, is the ubiquitous industry-standard database language
Data pipelines in python is required
Python–Statistical Programming
Data Visualization
Strong Python programming.
Strong SQL programming.
Strong Data Modeling.
Excellent communication and presentation skills.
Basic knowledge of Tableau.
Should have interest in creating BI reports.
Job Types: Full-time, Contract
Pay: From $30.00 per hour
Benefits:
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
8 hour shift
Ability to commute/relocate:
Menlo Park, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
SQL: 3 years (Required)
Work Location: One location",$30.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Agiles Enterprise
3.4",3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road","$136,537 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Less than $1 million (USD)
"BNY Mellon
3.7",3.7,"Pittsburgh, PA",Big Data Sytems Engineer(Hadoop),"Overview
Big Data Engineer
Bring your ideas. Make history.
BNY Mellon offers an exciting array of future-forward careers at the intersection of business, finance, and technology. We are one of the world's top asset management and banking firms that manages trillions of dollars in assets, custody and/or administration. Known as the “bank of banks” - 97% of the world’s top banks work with us as we lead and serve our customers into the new era of digital.
With over 238 years of rich history and industry firsts, BNY Mellon has been built upon our proven ability to evolve, lead, and drive new ideas at every turn. Today, we’re approximately 50,000 employees across 35 countries with a culture that empowers you to grow, take risks, experiment and be yourself. This is what #LifeAtBNYMellon is all about.
We’re seeking a future team member in the role of Big Data Engineer to join our TSG Data Management team. This role is located in Pittsburgh, PA and Nashville, TN .
In this role, you’ll make an impact in the following ways:
Work as a senior system engineer for the Hadoop enterprise data platform platforms and create highly complex big data/Hadoop platforms. You will have expert knowledge in distributed datalake systems.
You’ll configure and tune service components to optimize platform performance. You will design and implement backup and restore strategies and disaster recovery solutions and will support and troubleshoot performance related issues.
You’ll consult with storage and host engineering services to create optimized big data ecosystems and assist in cross-team collaboration and design discussions.
Provide sandbox functionality to users of the datasets to assist application analytics and storage infrastructure system management including capacity planning, performance monitoring and tuning, security management etc.
Responsible for infrastructure Support and projects including system design, planning, installations, configurations, upgrades and planning and Manage Tier-3 support following ITIL practice and incident management
Manage system configurations and solutions, and monitor the full lifecycles of storage area network (SAN) administration, architecture, and support and Data storage tasks such as management, provisioning, troubleshooting and debugging
Lead technical assessments of hardware, software, tools, applications, firmware, and operating systems to support business operations. You’ll develop trusted corporations with application tenants. You’ll advise applications with sounds data layer resiliencies and provide input into the selection of tools and any necessary migration into the company's environment.
To be successful in this role, we’re seeking the following:
Bachelor's degree in computer science or a related discipline, or equivalent work experience required.
Advanced degree preferred10-12 years of related experience required; experience in the securities or financial services industry is a plus.
5-10 years of experience in Hadoop technologies, data lake design, experience in the securities or financial services industry is a plus.
Excellent knowledge with Hadoop components for big data platforms related to data ingestion, storage, transformations and analytics. Excellent DevOps skillsets and SDLC practices. Excellent knowledge of automation tools such as Ansible, Python programming skills, knowledge of Docker containers
8+ years of experience on BigData Platform on Hadoop, Spark, Hive, HBase and 3+ years’ experience on Python and DevOps tools such as Ansible
Strong Agile/Scrum development experience and Working knowledge with PostgreSQL or Mongo DB databases a plus
At BNY Mellon, our inclusive culture speaks for itself. Here’s a few of our awards:
Fortune World’s Most Admired Companies & Top 20 for Diversity and Inclusion
Bloomberg’s Gender Equality Index (GEI)
Best Places to Work for Disability Inclusion , Disability: IN – 100% score
100 Best Workplaces for Innovators, Fast Company
Human Rights Campaign Foundation, 100% score Corporate Equality Index
CDP’s Climate Change ‘A List’
Our Benefits:
BNY Mellon offers highly competitive compensation, benefits, and wellbeing programs rooted in a strong culture of excellence and our pay-for-performance philosophy. We provide access to flexible global resources and tools for your life’s journey. Focus on your health, foster your personal resilience, and reach your financial goals as a valued member of our team, along with generous paid leaves that can support you and your family through moments that matter.
BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer - Underrepresented racial and ethnic groups/Females/Individuals with Disabilities/Protected Veterans.
This person is a subject matter expert in several of the tools/technologies used in the space.Leads the development infrastructure engineering growth strategies and initiatives. Leads initiatives to analyze complex infrastructure problems to be solved with advanced design.Leads the evaluation of the effectiveness of the organization's existing infrastructure technology and tools. Analyzes trends to develop strategy for the implementation of upgrades that will enhance the reliability, Resiliency and efficiency of the IT infrastructure.Provides leadership to execute project plans and performance requirements for all stages/phases through the management of human capital resources.This person is a subject matter expert in at least one of the tools/techologies used in the space.Participates in or leads initiatives to analyze infrastructure problems to be solved with advanced design. Utilizes standard procedures and policies when selecting methods, techniques, and evaluation criteria for obtaining results.Participates in or leads initiatives to analyze infrastructure problems to be solved with advanced design. Utilizes standard procedures and policies when selecting methods, techniques, and evaluation criteria for obtaining results.Manages the processes for ensuring that all systems/applications/software/hardware are compliant with Corporate policy/procedures.Monitors project plans and budgets.Works closely with external vendors, internal partners and busienss teams to provide infrastructure/tool needs.Works with Application Development and Quality Assurance, Testing and Business teams to understand infrastructure needs during the development, testing and production BAU processes. Ensures these needs are taken into account when developing infrastructure.Acts as escalation point for major incidents.Leads strategy to increase automation across the organization.Contributes to the achievement of multiple teams' objectivesBachelor's degree in computer science or a related discipline, or equivalent work experience required; advanced degree preferred10-12 years of related experience required; experience in the securities or financial services industry is a plus. BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer. Minorities/Females/Individuals with Disabilities/Protected Veterans. Our ambition is to build the best global team – one that is representative and inclusive of the diverse talent, clients and communities we work with and serve – and to empower our team to do their best work. We support wellbeing and a balanced life, and offer a range of family-friendly, inclusive employment policies and employee forums.
Employer Description:
For over 230 years, the people of BNY Mellon have been at the forefront of finance, expanding the financial markets while supporting investors throughout the investment lifecycle. BNY Mellon can act as a single point of contact for clients looking to create, trade, hold, manage, service, distribute or restructure investments and safeguards nearly one-fifth of the world's financial assets. BNY Mellon remains one of the safest, most trusted and admired companies. Every day our employees make their mark by helping clients better manage and service their financial assets around the world. Whether providing financial services for institutions, corporations or individual investors, clients count on the people of BNY Mellon across time zones and in 35 countries and more than 100 markets. It's the collective ambition, innovative thinking and exceptionally focused client service paired with a commitment to doing what is right that continues to set us apart. Make your mark: bnymellon.com/careers.
EEO Statement:
BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer. Minorities/Females/Individuals With Disabilities/Protected Veterans. Our ambition is to build the best global team – one that is representative and inclusive of the diverse talent, clients and communities we work with and serve – and to empower our team to do their best work. We support wellbeing and a balanced life, and offer a range of family-friendly, inclusive employment policies and employee forums.
Job Type: Full-time","$80,404 /yr (est.)",10000+ Employees,Company - Public,Financial Services,Investment & Asset Management,1784,$10+ billion (USD)
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
"PLAXONIC
4.6",4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642","$117,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
Excellerent Solutions Inc,#N/A,"Coppell, TX",Data Integration Engineer,"Role: Data Integration Engineer
Location: Irving, TX(5 days a week in Office, 100% onsite role)
Must have experience working with SnapLogic to perform ELT processes.
Must have good written and verbal skills
Must be capable of knowledge transfer to team of ~10 ETL developers on use of SnapLogic
Must be able to work with a larger team to complete sizable migration of on-prem processes to Snowflake.
Must be able to follow design patterns set out by our architecture team.
Job Types: Full-time, Contract
Pay: $48.18 - $60.00 per hour
Schedule:
8 hour shift
Work Location: In person",$54.09 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"E-Logic INC
4.4",4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.","$84,277 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Agiles Enterprise
3.4",3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road","$136,537 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Less than $1 million (USD)
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"AgileEngine
5.0",5.0,Remote,Big Data SDET Engineer,"Test with the best! At AgileEngine, you're free to build your own team of A-players and choose out of 70+ available projects the one you'd like to work on. Join the squad of top-3% software experts who make a difference through off-the-wall solutions.
Must haves
Strong Scala and Spark (hands on). If not Scala, strong experience in either Java or Python
Big Data concepts
API testing experience
Understanding qa process to create test cases and test strategies
Independently able to contribute to automation framework as well interact with the stake holders as needed
SQL experience
AWS experience
Strong test mindset
Experience of deploying or using Terraform and Gitlabs would be a bonus
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
The company behind the world’s leading OS virtualization software leverages AgileEngine as a provider of external QA and testing automation expertise. We’ve developed our client’s test strategy complete with test plans, ETAs, analysis, and risk assessment. Our teams have also optimized the company’s test suites, halving the time it takes for the tests to run. Our other deliverables include the implementation of new tests and the development of documentation, how-to guides, and test instructions on an internal Wiki.
Our team is also working to establish a strong QA/AQA process for an industry-leading cybersecurity platform. We’ve developed our client’s test strategy, manual testing cases, and automated tests from scratch. Our deliverables include backend, API, and UI testing, as well as BDD integrated with a streamlined CI process.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Enterprise Knowledge LLC
4.0",4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”","$89,080 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD)
"Buchanan & Edwards
3.8",3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location","$138,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD)
"etrailer.com
3.9",3.9,Remote,Data Engineer/Data Scientist,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow","$140,000 /yr (est.)",501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
"IBR (Imagine Believe Realize)
4.5",4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",#N/A,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable
Invictus Data,#N/A,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location","$125,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
"PLAXONIC
4.6",4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642","$117,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"IBR (Imagine Believe Realize)
4.5",4.5,"Melbourne, FL",Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote","$118,866 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
Excellerent Solutions Inc,#N/A,"Coppell, TX",Data Integration Engineer,"Role: Data Integration Engineer
Location: Irving, TX(5 days a week in Office, 100% onsite role)
Must have experience working with SnapLogic to perform ELT processes.
Must have good written and verbal skills
Must be capable of knowledge transfer to team of ~10 ETL developers on use of SnapLogic
Must be able to work with a larger team to complete sizable migration of on-prem processes to Snowflake.
Must be able to follow design patterns set out by our architecture team.
Job Types: Full-time, Contract
Pay: $48.18 - $60.00 per hour
Schedule:
8 hour shift
Work Location: In person",$54.09 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"E-Logic INC
4.4",4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.","$84,277 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
"AgileEngine
5.0",5.0,Remote,Big Data SDET Engineer,"Test with the best! At AgileEngine, you're free to build your own team of A-players and choose out of 70+ available projects the one you'd like to work on. Join the squad of top-3% software experts who make a difference through off-the-wall solutions.
Must haves
Strong Scala and Spark (hands on). If not Scala, strong experience in either Java or Python
Big Data concepts
API testing experience
Understanding qa process to create test cases and test strategies
Independently able to contribute to automation framework as well interact with the stake holders as needed
SQL experience
AWS experience
Strong test mindset
Experience of deploying or using Terraform and Gitlabs would be a bonus
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
The company behind the world’s leading OS virtualization software leverages AgileEngine as a provider of external QA and testing automation expertise. We’ve developed our client’s test strategy complete with test plans, ETAs, analysis, and risk assessment. Our teams have also optimized the company’s test suites, halving the time it takes for the tests to run. Our other deliverables include the implementation of new tests and the development of documentation, how-to guides, and test instructions on an internal Wiki.
Our team is also working to establish a strong QA/AQA process for an industry-leading cybersecurity platform. We’ve developed our client’s test strategy, manual testing cases, and automated tests from scratch. Our deliverables include backend, API, and UI testing, as well as BDD integrated with a streamlined CI process.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"Agiles Enterprise
3.4",3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road","$136,537 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Less than $1 million (USD)
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Enterprise Knowledge LLC
4.0",4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”","$89,080 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD)
Invictus Data,#N/A,"San Francisco, CA",Sr. Data Engineer,"Are you an experienced Data Engineer looking for the chance to make a real impact? We're hiring! We help organizations create robust technical architecture, develop complex systems and build out their infrastructures. Check out our opening here.
Role and Responsibilities:
Design and Develop scalable industry standard ETL pipelines using big data technologies.
Loading from disparate data sets by leveraging various big data technologies EMR, Hive, Spark. Presto etc.
Ingest, Extract, Move, Transform, Cleanse, and Load massive structured and unstructured data in Hadoop environment both in batch and real-time.
Build and maintain robust data pipelines for multiple use cases, and ensure high quality data i.e., accurate, complete and timely.
High-speed querying using in-memory technologies such as Spark.
Design and implement data modeling
Translate complex functional and technical requirements into detailed design
Code and query Optimization.
Skills & Qualifications
3-5 years of experience in Hadoop (Hive and Impala) & Spark (Spark/Scala)
Proficient in a scripting language and object-oriented language – Python/Scala preferred
Proficient with a public cloud (AWS, Azure, GCP)
Expert in SQL development
Proficient in using Big Data stack (Hadoop, Hive, Spark, Kafka, Kerberos, OOZIE, impala, etc.)
Adept in multi-threading and concurrency concepts.
Job Type: Full-time
Pay: $100,000.00 - $151,000.00 per year
Experience level:
3 years
Schedule:
Monday to Friday
Ability to commute/relocate:
San Francisco Bay Area, CA: Reliably commute or planning to relocate before starting work (Required)
Experience:
Scripting: 2 years (Required)
Hadoop: 2 years (Preferred)
Python: 2 years (Required)
Work Location: One location","$125,500 /yr (est.)",#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Kaizen Dynamics,#N/A,"Washington, DC",Data Engineer ETL,"12+ years of experience needed to serve as the subject matter expert on data architecture for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse.
The DC Department of Employment Services (DOES) operates the District’s Workforce Development System; maintains oversight of the Unemployment Insurance and Paid Family Leave programs; and administers, plans, and develops various employment-related services to all segments of the Washington, DC metropolitan population. DOES achieves its mission through empowering and sustaining a diverse workforce, which enables all sectors of the community to achieve economic and social stability. DOES requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration. The Data Management Project Data Engineer serves as an ETL developer and Catalog support specialist for the agency’s multi-year Data Management Project to modernize the agency’s data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: include: Specific Duties: 1. Develop, test, and maintain extraction, transformation, and load (ETL) processes. 2. Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software. 3. Support the Data Management Project team to develop and maintain data quality controls. 4. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. 5. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. 6. Support the data stewards to troubleshoot and resolve data issues. 7. Support business users to obtain requirements for enhancements and/or new analytic assets. 8. Assist in the Development of data asset training and documentation. 9. Participate in the development and implementation of a DOES data standard. 10. Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices. Required Skills and Experience (ensure this matches the CAI required skills and background) • Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts. ? Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. • Operating Systems: Windows and Linux. • Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. • Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). • Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. • Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. Bachelor’s Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.
Job Types: Full-time, Contract
Pay: $80.00 - $90.00 per hour
Schedule:
8 hour shift
Experience:
Linux-based Operating Systems: 8 years (Preferred)
ETL: 10 years (Preferred)
Work Location: Hybrid remote in Washington, DC 20001",$85.00 /hr (est.),Unknown,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Buchanan & Edwards
3.8",3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location","$138,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD)
"Xiar tech inc
3.3",3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$51.90 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
"PLAXONIC
4.6",4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642","$117,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Agiles Enterprise
3.4",3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road","$136,537 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Less than $1 million (USD)
"E-Logic INC
4.4",4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.","$84,277 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"AgileEngine
5.0",5.0,Remote,Big Data SDET Engineer,"Test with the best! At AgileEngine, you're free to build your own team of A-players and choose out of 70+ available projects the one you'd like to work on. Join the squad of top-3% software experts who make a difference through off-the-wall solutions.
Must haves
Strong Scala and Spark (hands on). If not Scala, strong experience in either Java or Python
Big Data concepts
API testing experience
Understanding qa process to create test cases and test strategies
Independently able to contribute to automation framework as well interact with the stake holders as needed
SQL experience
AWS experience
Strong test mindset
Experience of deploying or using Terraform and Gitlabs would be a bonus
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
The company behind the world’s leading OS virtualization software leverages AgileEngine as a provider of external QA and testing automation expertise. We’ve developed our client’s test strategy complete with test plans, ETAs, analysis, and risk assessment. Our teams have also optimized the company’s test suites, halving the time it takes for the tests to run. Our other deliverables include the implementation of new tests and the development of documentation, how-to guides, and test instructions on an internal Wiki.
Our team is also working to establish a strong QA/AQA process for an industry-leading cybersecurity platform. We’ve developed our client’s test strategy, manual testing cases, and automated tests from scratch. Our deliverables include backend, API, and UI testing, as well as BDD integrated with a streamlined CI process.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Enterprise Knowledge LLC
4.0",4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”","$89,080 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD)
"Certec Consulting
5.0",5.0,"Madison, WI",Data Engineer 925,"Data Engineer 925
Visa : W2(USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO C2C
Visa : ONLY W2/1099 (USC/GC/H4/L2/GC-EAD/H1-B) ONLY NO subcontractors
Candidates must be located in the United States
Duration: 6 month with potential to go long term or permanent
Location: 100% remote position
Interview: Phone and Teams
Required:
The primary tech skills we're looking for are Databricks, Python, Snowflake, AWS/cloud infrastructure services, and experience building data integrations/ETL's and analytic data structures. Solid SQL experience is absolutely required for all data engineering levels.
Candidates must have at least 5 yrs experience for each of these below. Ideally 7-10 yrs but open to 5yrs. The Data Engineer will be pulling data from EPIC and other sources and adding into Snowflake.
Databricks (required) Python (required) Snowflake (required) Work will be in a fast-paced, agile, matrixed environment. (required) Self-starters we are looking for people who will be able to hit the ground running. (required) AWS/Cloud ETL's- building data integrations/ETL's and analytic data structures (required) SQL (***must be expert level**) Epic Cogito/Clarity/Caboodle (required) Apache Spark (required) **Must have great communication skills and be able to work in a team environment. This is NOT a lead role so you must be able to work with the team and follow through on instructions.
DATABRICKS - this is what they are using to bring data into Snowflake (central repository) ETL Must understand architecture concepts and de-normalizing data Python since we'll be building new ETL in Databricks, etc, some basic Python knowledge seems like a good pre-req, specifically around the Pandas library and how to move data around. Snowflake must be familiar with and have worked with in the past
SSIS would be useful to have some SSIS experience as we're still building some of our extracts from here
Thanks,
Jay Kernes
Certec Consulting, inc
847-253-8968, Fax 888-523-7832
We are certified as a Women's Business Enterprise (WBE)","$92,845 /yr (est.)",Unknown,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
Excellerent Solutions Inc,#N/A,"Coppell, TX",Data Integration Engineer,"Role: Data Integration Engineer
Location: Irving, TX(5 days a week in Office, 100% onsite role)
Must have experience working with SnapLogic to perform ELT processes.
Must have good written and verbal skills
Must be capable of knowledge transfer to team of ~10 ETL developers on use of SnapLogic
Must be able to work with a larger team to complete sizable migration of on-prem processes to Snowflake.
Must be able to follow design patterns set out by our architecture team.
Job Types: Full-time, Contract
Pay: $48.18 - $60.00 per hour
Schedule:
8 hour shift
Work Location: In person",$54.09 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"Buchanan & Edwards
3.8",3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location","$138,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD)
"Xiar tech inc
3.3",3.3,"Dallas, TX",Senior Data Engineer,"· 5+ Years’ experience in Data Engineering
· 1 year experience with Nexla or similar data ingestion & engineering tools
· 4+ years’ experience in SQL/SnowSQL
· 2+ Years’ thorough experience in Python coding and packages
· 4+ years of overall Data Integration experience with 2+ experience in Data Engineering.
· 2+ years’ experience in large volume File processing with different types and formats
· Experience in Orchestration Tools - Airflow
· Understanding of dimensional and relational data modeling.
· Understanding of BI/DW development methodologies.
Job Types: Full-time, Contract
Salary: $42.96 - $60.84 per hour
Schedule:
8 hour shift
Day shift
Ability to commute/relocate:
Dallas, TX: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location",$51.90 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"IBR (Imagine Believe Realize)
4.5",4.5,Remote,Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote",#N/A,51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable
"etrailer.com
3.9",3.9,Remote,Data Engineer/Data Scientist,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow","$140,000 /yr (est.)",501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
Ascent Technologies,#N/A,Remote,Data Engineer,"Job Description: Involved in resource / volume ramp-up plan/ backfilling resources planned for rotation and resignation cases Tracking Performance of individual Projects within the delivery/SIPs. Participate / presenting SD performance during client visit Publish Daily, Weekly and Monthly reports on team productivity Reviewing the weekly / monthly dashboards Publishing dashboards to customer People Management Drive the team to reach the defined targets Ensure training and development of the team members Mentor team members on technical and communication aspects. onsite anni data enginneer-pheonix,az python,charlottee,nc ba-houston tx
Job Types: Full-time, Contract, Permanent
Salary: $39.87 - $86.24 per hour
Benefits:
Dental insurance
Compensation package:
1099 contract
Experience level:
10 years
11+ years
8 years
9 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote
Speak with the employer
+91 7842516676",$63.05 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"ESRI, Inc.
4.0",4.0,"Redlands, CA",ETL Data Engineer,"Overview:
Esri’s ArcGIS World Geocoding solutions deliver global search and geocoding capabilities to organizations across the world. Search and geocoding are used in a wide range of applications across the ArcGIS platform such as where to open your retail store, how to best allocate resources for delivering packages, or where is, and how do I get to, my favorite coffee shop?

As an ETL Data Engineer on the geocoding team, you will work with massive spatial data from both commercial and open sources. You’ll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If you are passionate about data and geospatial technology, join the geocoding team because you can’t have “The Science of Where®” if you don’t know where to start!
Responsibilities:
Analyze, enhance, prepare and load spatial data into normalized databases on regular intervals
Write and maintain Python and SQL scripts for data processing and loading
Evaluate new data sources for quality and attribution to support product requirements
Review data specifications and changes, adjust scripts and processes where necessary
Requirements:
2+ years of experience developing Python scripts or tools
Experience writing scripts to perform spatial operations, advanced queries, and joins in SQL
Intermediate to advanced experience working with large, normalized databases
Experience transforming data from flat to normalized databases
Prior experience with ETL workflows
Ability to read data product specifications and translate into database models and tables
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's in computer science, information systems, GIS, or related field, depending on position level
Recommended Qualifications:
Experience working with commercial data from data vendors such as Here, TomTom, or Infutor
Experience working with open-source spatial data such as OpenStreetMap, or Tiger/Line
Experience creating stored procedures for optimizing common tasks and operations
Experience working with ArcMap, ArcGIS Pro, and ArcGIS geoprocessing
Prior experience with geocoding, geocoding data, or address data
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
Master’s or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
The Company:
Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.

Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.

If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.

Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.

A reasonable estimate of the base salary range is $72,800.00 - $124,800.00.

#LI-EL1
#IND1","$98,800 /yr (est.)",1001 to 5000 Employees,Company - Private,Information Technology,Computer Hardware Development,1969,$1 to $5 billion (USD)
"NN Tech
5.0",5.0,"Richmond, VA",Senior Data Engineer,"FULL STACK DATA ENGINEER
US CITIZEN OR GREEN CARD ONLY
5+ YEARS OF EXPERIENCE REQUIRED
Job Type: Full-time
Pay: $55.00 - $75.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
Monday to Friday
Application Question(s):
what is your citizenship status?
what is your location?
please send your resume to nicayla@nntechus.com for evaluation
what is your contact number and email?
Work Location: Remote",$65.00 /hr (est.),1 to 50 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,Unknown / Non-Applicable
"Clairvoyant
4.4",4.4,Remote,Data Engineer (MDM),"Required Skills:
Must have 5-8+ Years of experience as Data Engineer
Solid experience in pyspark & SQL
Experience working with AWS services (S3, EMR) and Databricks platform
Following skills are important to have:
SQL
Python
Spark
MDM (Master Data Management)
QA/ UAT
Cloud
Job Types: Full-time, Contract
Salary: $65.00 - $70.00 per hour
Benefits:
Dental insurance
Health insurance
Compensation package:
Performance bonus
Experience level:
8 years
Schedule:
Monday to Friday
Experience:
Data engineer: 8 years (Required)
MDM (Master Data Management): 1 year (Required)
QA/UAT: 1 year (Required)
AWS: 1 year (Required)
Python: 1 year (Required)
Work Location: Remote",$67.50 /hr (est.),51 to 200 Employees,Company - Private,Pharmaceutical & Biotechnology,Biotech & Pharmaceuticals,#N/A,Unknown / Non-Applicable
"AGM Tech Solutions
4.8",4.8,"Alpharetta, GA",Sr. Data Engineer,"Sr. Data Engineer - Alpharetta GA
Location: Must be local to Atlanta Metropolitan area, will not consider relocations at this time.
Hybrid Remote, a few days in office in Alpharetta, GA.
Contract Length: 6-12 month contract to full time.
Summary
Our Direct Client, a Global top Payroll technology company, is looking for a Senior Data Engineer. You will design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Lambda, Glue Design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, and Python. You will also design and implement data engineering, ingestion, and curation functions on AWS cloud using AWS native or custom programming.
Required Experience
Experience with ETL, Data Modeling, and Data Architecture. Experience with Big Data technologies such as Hadoop/Hive/Spark.
Skilled in writing and optimizing SQL. Experience operating very large data warehouses or data lakes.
This candidate must be skilled in : AWS, Python, (Hadoop, Hive, or Spark), ETL, data modeling, SQL scripting.
Preferred Experience
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Integrating end-to-end data pipeline to take data from source systems to target data repositories ensures the quality and consistency of data is always maintained.
Excellence using standard methodologies
Comfortable using PySpark APIs to perform advanced data transformations
Familiarity with implementing classes with Python.
Job Types: Full-time, Contract
Pay: $70.00 - $84.00 per hour
Schedule:
8 hour shift
Evening shift
Ability to commute/relocate:
Alpharetta, GA 30005: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 8 years (Preferred)
SQL: 8 years (Preferred)
Data warehouse: 8 years (Preferred)
Work Location: Hybrid remote in Alpharetta, GA 30005",$77.00 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2018,$5 to $25 million (USD)
Data Crunch Corp,#N/A,Remote,Senior Data Engineer - Remote,"Who we are:
We (https://datacrunchcorp.com) are a data visualization and data engineering consultancy serving clients of all sizes and in all industries. We help businesses transform their data into meaningful insights that drives measurable financial improvements for them.
We're a close-knit, small team and believe in providing tangible, practical insights and value for our clients. We move fast and we help each other. We are deeply analytical and creative. We all work remotely.
Job Responsibilities
Client & Project Management:
Scoping new projects. Coming up with data driven-estimates and task breakdowns. Breaking a project down into manageable tasks for more junior members of the team.
Communicating with clients. Communicating technical details to clients, client progress reviews, etc.
Data wrangling, data pipelining, and data modeling:
Working with REST APIs to assemble data, SQL to pull or insert data, using Pandas library in Python to clean and transform data, automating data transformation process in AWS, writing web scrapers with beautifulsoup and selenium
Will also use Tableau Prep for proof-of-concept work as this tool is faster than Python (Training provided if needed)
SQL Database management and performance tuning
Automation of data processing:
Using AWS Lambda or other cloud equivalents to deploy and run completed data pipelines.
Using Docker containers to automate deployment.
Setting up error logging, tracking, and fault tolerance for deployed data pipelines.
Machine learning or predictive analytics
Be able to code, deploy, and monitor models for accuracy and business results.
We work a lot with OpenAI, experience here is a plus
Communicate verbally and written to internal team and clients
Excellent English skills are a must
Meeting with clients to gather abstract business requirements and turn them into a technical project plan
Create, manage, and execute plans according to client timelines (some project mgmt. training will be provided)
Manage and communicate client expectations, deadlines, and deliverables
Assist in developing detailed requirements for scoping of projects
Reviewing code, Dashboards, and data to ensure quality work and valid data
Please be prepared to answer the following questions before the interview:
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Desired Skills:
Strong internal team and external client communication and management
Strong coding knowledge (Python)
Strong in SQL, Database Architecture, Database Optimization
Strong automation in the cloud (AWS Lambda and EC2 or Microsoft/Google equivalent) and docker containers
Excellent communication abilities, both written and verbal
Ability to learn and adapt quickly to new technologies
Excellent organizational skills with strong attention to detail
Experience with Tableau or PowerBI (Nice to have)
Great things about our company:
This job is 100% remote.
Hours are flexible, but you should generally be available during working hours to interface with the team and clients.
You’ll receive world-class training to help you in your job, as well as ongoing mentorship.
You’ll be part of a fun, collaborative team.
You’ll work on a variety of projects and be exposed to a lot of different areas of our company and other companies, giving you an excellent foundation for anything else you want to do in your career.
Health Benefit options and 401k are available
Job Type: Full-time
Job Type: Full-time
Pay: $50.00 - $70.00 per hour
Benefits:
401(k)
Flexible schedule
Health insurance
Health savings account
Schedule:
Monday to Friday
Application Question(s):
Please tell us about your experience building data pipelines, data schemas, cleaning and understanding data with SQL, python, and other tools.
Please tell us how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget, manage changing requirements, and produce results.
Work Location: Remote",$60.00 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
Futuretech Consultants LLC,#N/A,"Newton, MS",Snowflake Data Engineer,"My name is Dileep and I am a recruiter at Futuretech Consultants LLC. Futuretech Consultants is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients.
Snowflake Data Engineer
Experience – 6 to 8 Years
Work Location: Peachtree Corners GA ( our office)
Work model: Initially Onsite and then remote
Client: Goldman Sachs/Appridat Solutions, LLC
Duration: On going project – Long term
Job description for Data Engineer (Snowflake Data Engineer)
Position Title: Mid-level to Sr. Data Engineer
Role Overview
We are looking to hire a skilled Snowflake Data Engineer to fill a data warehousing environment in our Company. This role requires the individual to be able to have an end-to-end vision, and to see how a logical design will translate into one or more physical Databases.
To ensure success as a Snowflake data engineer, you should have extensive knowledge of database design, implementation, and optimizing of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Your duties will include implementing ETL pipelines within and outside of a data warehouse using Snowflakes Snow SQL, Querying Snowflake using SQL, Development of scripts using Talend, etc. for loading, extracting, and transforming data. Assisting with production issues in Data Warehouses like reloading data, transformations, and translations. Developing a Reporting Design based on Business Intelligence and Reporting requirements.
Qualifications/Requirements
Bachelor’s Degree in Computer Science, IT, or related field.
Essential Skills
Technical:
o 5+ years of ANSI SQL programming skills including writing, debugging, and tuning SQL scripts, stored procedures/functions etc.
o 1+ years of experience working on Snowflake including Snow SQL, Tasks, Procs, functions, Query tuning etc.
o 1+ years of experience with ETL and CDC tools including Talend, SSIS and HVR.
o Experience in advanced snowflake features such as data sharing, events, and lake-house patterns.
o Minimum of 2 years of experience with Database design (preferably in a in banking or financial services Industry).
o Work experience on JIRA and /or other Quality Management software.
Functional:
Develop and maintain datasets for business use.
Build data systems and pipelines.
Improving data quality and efficiency.
Combined raw information from different sources.
Client facing with excellent communication, presentation, and documentation skills – ability to communicate at all levels within business from C-level to end users, with ability to communicate technical matters in non-technical business terms.
A team player who works collaboratively and provides coaching and support to transfer technical & data knowledge
Demonstrated success in managing multiple deliverables concurrently often within aggressive timeframes; ability to cope under time pressure.
Experience in partnering with a diverse team in multiple locations.
Troubleshooting any issues that may arise.
Providing maintenance support.
Thanks, and Regards
G Dileep Kumar
Sr. Technical Recruiter
Futuretech Consultants, LLC
912-623-4772
Job Types: Full-time, Contract
Salary: $40.00 - $45.00 per hour
Schedule:
8 hour shift
Ability to commute/relocate:
Newton, MS 39345: Reliably commute or planning to relocate before starting work (Required)
Application Question(s):
share only who are willing to work On W2
Experience:
Informatica: 4 years (Preferred)
SQL: 4 years (Preferred)
Data warehouse: 4 years (Preferred)
Work Location: One location
Speak with the employer
+91 912-623-4772",$42.50 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"PLAXONIC
4.6",4.6,United States,Snowflake Data Engineer,"Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem.
This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving.
Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.
Job Responsibilities:
Solve various technical problems in the project in a collaborative manner under specified timeline with minimal guidance and review
Responsible for implementing diverse engineering tasks including gap, data, and impact analysis
Independently perform low-level design configuration, code customization, refactoring and review
Gain expertise and demonstrate high levels of skills in design, review, development, testing, build and deployment stages of the software life cycle
Ensure adequate documentation and provide assistance as a reliable engineer to all stakeholders
Specialization:
we work on modernizing the on-premises legacy data warehouses to cloud based data warehouse solutions like AWS RedShift, Azure Synapse, Google BigQuery, and Snowflake.
We work on migrating on-premises MPP systems like Teradata, Greenplum, Netezza, and Exadata to cloud data warehouse and scale them by using distributed computing solutions on the cloud.
We have dedicated Center of Excellences CoEs on key technology platforms including Business Objects, IBM, Oracle, Microsoft BI, Informatica, MicroStrategy, SAS, MDM, Big Data and Opensource.
Job Type: Full-time
Salary: $110,000.00 - $125,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data Engineer: 10 years (Preferred)
Snowflake: 7 years (Preferred)
Work Location: On the road
Speak with the employer
+91 727 216 7642","$117,500 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$1 to $5 million (USD)
"Amazee Global Ventures Inc
5.0",5.0,"Irving, TX",Big Data Engineer,"Title: Big Data Engineer
Location: Irving TX (Hybrid)
Experience : 10 +
Duration: Long Term
Responsibilities:
Design application code, implement technical solutions, and configure applications in various environments in response to business problems in close collaboration with Architects, Business Analysts and Change Partners.
Manage Hadoop, NoSQL, and/or MPP infrastructure supporting data.
Write applications to solve analytical problems.
Code, test, release, and support Big Data.
Help in the design and build of the data platform over Big Data technologies.
Solve big data engineering problems.
Analyze, recommend, and implement data technologies for the platform.
Involved in case studies about Big Data.
Responsible for efficient deliveries.
Work with others to propose the best technical solutions.
Help design the best backend data warehouse platform to support the capacity and performance.
Participate in the proof of concept application.
MUST HAVE SKILLS (Most Important): Experience working in a cross platform data environment, working in both NoSQL and RDBMS databases concurrently Experience managing data workflows with Airflow Experience with AWS and GCP
DESIRED SKILLS: Experience developing pure ETL workflows Experience developing data analytics workflows
Amazee Global Ventures Inc, is an equal opportunity employer. We will not discriminate and will follow all measures to ensure no discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression. We are committed to providing an inclusive and welcoming environment for all members of our staff, clients, volunteers, subcontractors, vendors, and clients.
Job Types: Full-time, Contract
Salary: $60.00 - $65.00 per hour
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Experience level:
10 years
Schedule:
8 hour shift
Experience:
RDBMS: 3 years (Preferred)
NoSQL: 1 year (Preferred)
AWS: 1 year (Preferred)
Hadoop: 1 year (Preferred)
Willingness to travel:
25% (Preferred)
Work Location: On the road",$62.50 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,2019,Less than $1 million (USD)
"PCS Global Tech
4.7",4.7,"Riverside, CA",Data Engineer | PAID BOOTCAMP,"Responsibilities
· Analyze and organize raw data
· Build data systems and pipelines
· Evaluate business needs and objectives
· Interpret trends and patterns
· Conduct complex data analysis and report on results
· Prepare data for prescriptive and predictive modeling
· Build algorithms and prototypes
· Combine raw information from different sources
· Explore ways to enhance data quality and reliability
· Identify opportunities for data acquisition
· Develop analytical tools and programs
· Collaborate with data scientists and architects on several projects
Requirements
· Previous experience as a data engineer or in a similar role
· Technical expertise with data models, data mining, and segmentation techniques
· Knowledge of programming languages (e.g. Java and Python)
· Hands-on experience with SQL database design
· Great numerical and analytical skills
· Degree in Computer Science, IT, or similar field (STEM)
Job Types: Full-time, Contract
Salary: $60,000.00 - $80,000.00 per year
Benefits:
Dental insurance
Employee assistance program
Life insurance
Professional development assistance
Compensation package:
Bonus pay
Experience level:
1 year
Under 1 year
Schedule:
8 hour shift
Monday to Friday
No nights
No weekends
Experience:
SQL: 1 year (Preferred)
Work Location: On the road","$70,000 /yr (est.)",501 to 1000 Employees,Company - Private,Information Technology,Information Technology Support Services,#N/A,Unknown / Non-Applicable
"Phasorsoft LLC
4.0",4.0,"Austin, TX",Cloud Data Engineer (Azure),"Responsibilities
Function as the solution lead for building the data pipelines to support the development/enablement of Information Supply Chains within our client organizations – this could include building (1) data provisioning frameworks, (2) data integration into the data warehouse, data marts, and other analytical repositories (3) integration of analytical results into operational systems, (4) development of data lakes and other data archival stores.
Optimally leverage the data integration tool components for developing efficient solutions for data management, data wrangling, data packaging, and integration. Develop overall design and determine the division of labor across various architectural components
Deploy and customize Daman Standard Architecture components
Mentor client personnel. Train clients on the Daman Integration Methodology and related supplemental solutions
Provide feedback and enhance Daman intellectual property related to data management technology deployments
Assist in the development of task plans including schedule and effort estimation
Qualifications
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
1+ year experience working with Azure analytical stack
Deep experience with building analytical solutions on Azure SQL DW (or Azure Synapse)
Experience with Delta Lake is nice to have but experience with Azure Data Lake is required
Experience with Databricks is nice to have
Experience building high-performance, and scalable distributed systems
Continuous Data Movement/ Streaming/ Messaging:
Experience with related technologies ex Spark streaming or other message brokers like MQ is a PLUS
3+ years experience developing, deploying, and supporting scalable and high-performance data pipelines (leveraging distributed, data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing)
3+ years experience in software engineering, leveraging Java, Python, Scala, etc.
Bachelor’s Degree or foreign equivalent in Computer Science, Electrical Engineering, Mathematics, Computer Applications, Information Systems or Engineering is required
Job Type: Full-time
Pay: $72,727.90 - $158,843.83 per year
Schedule:
8 hour shift
Monday to Friday
Work Location: In person","$115,786 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"urpan technology
4.5",4.5,Remote,Senior Data Engineer,"Job Description:
Job Title: Senior Data Engineer
Duration: 2 Months ()
Location: New York, NY 10004 (100% Remote)
""The Test and Trace Corps is looking for a Senior Data Engineer to join the Data, Analytics, and Product Development Team, which is dedicated to organizing, analyzing, and communicating data as well as building technology solutions, in order to support and inform programmatic and operational efforts of the initiative.
SUMMARY OF DUTIES AND RESPONSIBILITIES:
· Reporting to the Data and Analytics Unit Director in the Data, Analytics, and Product Development Team, the Senior Data Engineer within the Data and Analytics Unit will be responsible for designing, evaluating, and testing data structures to support the work of the Data, Analytics, and Product Development Team and its ability to inform the planning, reporting and development initiatives under the Test and Trace Corps.
The Senior Data Engineer will also be responsible for:
· Building and maintaining the internal data infrastructure to be used by data analytics staff and developing and ensuring standardization in data cleaning, programming, and analyses across large, complex data sets
· Structuring and transforming datasets for multiple complex use cases, user stories, and related requirements within the Data Analytics and Product Team and across the Test and Trace Corps organization
· Auditing current data practices being employed by the Data, Analytics, and Product Development Team and re-engineering tools for improved data flow
· Building back-end data migration tools and infrastructure to support data analytics work
· Performing in-depth investigation and analysis to identify and resolve complex processing problems associated with the systems, programs, and datasets utilized by the Test and Trace Corps organization
· Recommending programs, practices, and standards to facilitate uniform application of electronic data methods, code versioning, and review, ticket management
· Supervising and overseeing the Data Engineer, providing assistance and insights on complex and difficult data sets and ensuring the accuracy of work products.
· Providing advice to front-end developers and field staff on overall data intake and integrations SPAN OF CONTROL (# and Type of Staff Directly Supervised)
· Data Engineer, Data and Analytics Unit (1)""
COMPUTER PROGRAMS/SOFTWARE OPERATED:
· Expertise in Python or equivalent programming language for automation
· Advanced knowledge of SQL
· Experience with API interfacing in a data engineering and analytics environment
· YEARS OF EXPERIENCE:
· 5+ years in a data engineering role in a large organization
PREFERRED SKILLS:
· Ability to work autonomously, think analytically, and anticipate data issues to solve before they arise
· Excellent written and verbal communication skills, with the ability to explain data systems to non-technical teams
· Strong quality control abilities and exceptional attention to detail
· Ability to manage multiple complex projects at a time, prioritize, and execute on tight timelines
Job Types: Contract, Full-time
Salary: $45.00 - $50.00 per hour
Schedule:
8 hour shift
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: Remote",$47.50 /hr (est.),1001 to 5000 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Skytech Consultancy Services
5.0",5.0,"Baltimore, MD",Data Engineer,"Description of Work:
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI-analytics.
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications: Minimum knowledge, skills, abilities needed.
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development with Tableau and other BI tools
Strong working experience with ETL development
Strong oral and written communication skills and ability to communicate with all levels within the organization
Strong data analysis and problem-solving skills
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Must be able to obtain public trust clearance.
Preferred Qualifications: Candidates with these skills will be given preferential consideration.
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)",$65.00 /hr (est.),1 to 50 Employees,Company - Public,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Pomeroy
3.0",3.0,"East Hartford, CT",Data Software Engineer,"Pomeroy is looking for a skilled Manufacturing Data Software Engineer for a 3 month Contract-to-Hire. This is a fully on-site role in Hartford, CT.
The Manufacturing Systems Software Engineer will be responsible for the day-to-day development, support, and troubleshooting of custom and off-the-shelf manufacturing applications. This will include but is not limited to manufacturing supervisory/SCADA systems, shop floor device network connectivity, reporting or visualization tools, such as “dashboards” and statistical process control tools (SPC).
Requirements:
Experience in a Manufacturing Operation is highly desired. Knowledge of basic manufacturing principles is required.
Proficiency in any language used to build desktop applications considered.
Proficient in .net, C#, java or python programming languages.
Experience with Oracle and SQL Server is a plus, including in writing SQL queries and stored procedures in Oracle or SQL Server.
Experience with the Ignition SCADA system is a plus.
Understanding of networking concepts, architecture and tools is required.
Must hold US Citizenship and be able to pass a background check.
Strong interpersonal and communication skills (verbal and presentation). Able to work in a Team Environment.
Results driven and self-motivated.
Must work onsite in East Hartford, CT
Job Duties:
Interface with vendors as required for shop floor device network connectivity, application implementation and operational support.
Collaborate with internal “customers” to discuss and document their business reporting and visualization requirements.
Design and create custom business applications to collect, analyze and report data critical to plant operations.
Analyze application performance and debug and/or repair as necessary.
Modify custom and vendor applications to accommodate new functionality needed to satisfy compliance or business requirements.
Develop reporting solutions by interfacing to existing data repositories, extracting data, manipulating data, and displaying it for the computer users in a dashboard format.
Convert legacy software applications to current technologies. (Ex. Visual Basic to VB.net)
Job Types: Full-time, Contract
Pay: $45.00 - $51.00 per hour
Benefits:
401(k)
Schedule:
Monday to Friday
Ability to commute/relocate:
East Hartford, CT 06108: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Software Engineering: 3 years (Required)
Work Location: One location",$48.00 /hr (est.),1001 to 5000 Employees,Contract,Information Technology,Information Technology Support Services,1982,Unknown / Non-Applicable
"ERPMark Inc
4.0",4.0,"Santa Clara, CA",Azure Data Engineer,"Role: Azure Data Engineer (with Azure Data Bricks)
Job Location: Santa Clara, CA
Job Type: Fulltime ($140K/Annum + Benefits) / Contract ($75/hr on C2C)
Interview: 1st Level – Video Interview and 2nd Level – In-Person
Selected Candidates need to work On-Site (NO Remote Work)
Only local candidates to CA will be considered (Or candidates within CA who are willing to Relocate).
Prefer Fulltime
Experience: Minimum 10 Years (Total IT exp) with 8 years in Data.
Candidate should have knowledge for Databricks and Lakehouse and understands the architecture.
Job Description:
At least 8+ years’ experience, ideally within a Data Engineer role.
CI/CD for software deployment in Windows/Linux/Kubernetes cluster, Airflow, Argo.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
Excellent experience working Python, Pandas, Flask/Fast/Django API, Middleware, Scheduler, SQL, Databases.
Prior experience with Python frameworks such as Django or Flask and a strong knowledge of SQL (queries, joins, etc.).
Good to have some experience in AWS/Azure.
Capability of developing highly-scalable RESTful APIs.
Excellent team player and can work well in an individual capacity as well.
Detail-oriented and possess strong analytical skills.
Pays strong attention to detail and deliver work that is of a high standard.
Highly goal-driven and work well in fast-paced environments. Good to have data science background.
Job Types: Full-time, Contract
Pay: $121,702.09 - $140,000.00 per year
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Santa Clara, CA 95050: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 10 years (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$130,851 /yr (est.)",1 to 50 Employees,Company - Private,Management & Consulting,Business Consulting,#N/A,Less than $1 million (USD)
"Peach IT Professionals
3.1",3.1,"Braham, MN",Data Engineer,"Required Skills:
5+ years of hands-on data analysis & SQL
Must have experience within Data Lineage / Data Visualization / Data Cleaning / Data Modeling
ETL/SSIS
Will be working with product owners to build acceptance criteria for user stories, assist with unit testing and integration testing.
Will participate in daily stand up calls and other scrum ceremonies
Will work with data modelers and analysts to build comprehensive data warehouses and assist with peer code review.
*
Nice to Haves: (NOT REQUIRED)
AI/ML
-Hadoop
R, Python
Linear Algebra and Calculus
Job Types: Full-time, Contract
Pay: $40.00 - $70.00 per hour
Schedule:
8 hour shift
Work Location: Hybrid remote in Braham, MN 55006",$55.00 /hr (est.),51 to 200 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"Rocket Travel, Inc.
4.5",4.5,"Chicago, IL",Senior Data Engineer,"About the Role
Rocket Travel is looking for a Senior Data Engineer with experience in Python/SQL to join our growing Data Science Team.
The Data Science Team oversees building internal-facing decision making tools and product-facing production models that aid in scaling our growing business.
Rocket Travel is a place where you:
Work with a group of intrinsically motivated people with a track record for building successful new businesses from scratch.
Embody curiosity, community, and accountability. We live and build products by these values every day.
Own decisions and take action that can be implemented in a matter of days (or hours).
Get inspired and encouraged to vacation faster, with an annual vacation stipend.
Receive a competitive compensation package, including bonus, 401k with match, flexible vacation time, maternity and paternity benefits, health, and dental insurance.
Total Compensation is based on experience - Salary: $125k-170k, Bonus: up to 20%
Can have the option to work remotely. Though we have great offices in Chicago's West Loop and in NYC's Empire State building, we have a hybrid team, a flexible work environment, and we welcome full-time remote applicants.
Share your passion for travel with equally adventurous teammates.
Work within the largest online travel company in the world. Rocket Travel creates B2C and B2B2C travel products and is part of Booking Holdings (BKNG). We have many worldwide partners and a diversified business. Despite the world's current situation, Booking Holdings has been rated the healthiest company in travel, and Rocket itself is already seeing travel demand surpass pre-pandemic levels
As a Senior Data Engineer at Rocket Travel, you will:
Build SQL and python data pipelines to address different business needs, which includes: updating pipelines for generating financial reports, modeling data for front-end analytics
Own projects end-to-end, which includes creating data pipelines for models and setting up the proper data schema and naming conventions for the data
Evolve the Rocket data science toolkit by identifying and recommending best tools for each task. Examples of technologies that we experimented and adopted in the past include: MLflow, Sagemaker, DBT, Jenkins
Work closely with a variety of stakeholders across the company including data science, bizops, partnerships, engineering, and finance
Improve and expand our suite of automated tests using pytest and great expectations
Continuously improve your skill set by learning new technologies, and eventually grow to researching, piloting, and pitching new tools for the team to adopt
Collaborate with the data team to deliver projects end to end including ingesting new data to our data warehouse, creating ETLs, and serving our ML models
Maintain a sense of empathy for our customers and move quickly where users are most acutely affected
About you:
Must Haves:
You have worked with Python/SQL for at least 3 years in a professional setting
You have worked with Python common data related libraries (pandas, numpy)
You have experience working with MPP databases (e.g. Redshift, Vertica, Big Query, etc.)
You have hands-on experience with SQL and can write complex queries with ease. You have experience building batch and real-time data pipelines. Bonus points if you have worked with Airflow and dbt
You care deeply about the quality of your code, but are also aware of timelines and don't spend countless hours trying to bring things to perfection
You are intellectually curious and self-directed problem solver, keen to work on a variety of data projects and independently search for answers
You communicate clearly and effectively

Other experience you may have
This is not meant to be a comprehensive list, but more so serve as examples of experience you may have
Pipeline orchestration (Airflow, DBT)
DevOps (Git, Jenkins, GitLab)
AWS data related tools (kinesis, glue, S3, sagemaker)
Big data technologies (Hadoop, Spark)
Containerization technologies (kubernetes, ECS, docker)

About Rocket Travel
We make travel more rewarding than anyone else
Rocket Travel awards customers for booking travel through our products, allowing people to earn or use loyalty benefits from their favorite loyalty programs.

Our journey
We began in 2013 with the same goal of making travel even more rewarding. This started with our bespoke Rocketmiles website, earning travelers their favorite airline loyalty miles for hotel bookings, and every year since, we've advanced our goal.

We now partner with over 60 loyalty programs that travelers can choose from, we build unique partner-branded travel sites, we offer the ability to earn and redeem loyalty rewards, we sit within Booking Holdings—the most experienced Group of travel companies in the world—and we continually grow the number of ways that people can book travel through us, from stays to cars and more.

Travel constantly evolves, opening additional opportunities to create rewarding experiences, and we intend to be at the forefront of building and innovating on those for travelers worldwide.

A diverse and global team
Our teammates work across the globe, in person and remotely. We have offices in Chicago, NYC, Bangkok, Bogota, Kuala Lumpur, Manila, and Cebu. No matter where people work, our main team-building goal is to create a diverse, equitable, and inclusive environment. We do that with a Diversity and Inclusion Committee, setting DEI hiring goals, investing in employee retention, and conducting regular team training that fosters collaboration and morale.
All of these efforts help ensure that we're promoting a supportive workplace, where people are motivated to grow professionally and build rewarding travel experiences together.

Note on general employment requirements
Candidates should be authorized to accept employment in the US from any employer, should be willing to start within three weeks of accepting an offer, and should be able to work the same daily working hours as our Chicago office.

Equal Opportunity Employer
At Rocket Travel and Agoda, we pride ourselves on being companies represented by people of all different backgrounds and orientations. We prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and innovation. Employment at Rocket Travel and Agoda is based solely on a person's merit and qualifications. We are committed to providing equal employment opportunity regardless of sex, age, race, color, national origin, religion, marital status, pregnancy, sexual orientation, gender identity, disability, citizenship, veteran or military status, and other legally protected characteristics. We will keep your application on file so that we can consider you for future vacancies and you can always ask to have your details removed from the file. For more details, please read Agoda's privacy policy.

A Final Word:
To all recruitment agencies: Rocket Travel does not accept third party resumes. Please do not send resumes to our jobs alias, employees or any other organization location. Rocket Travel is not responsible for any fees related to unsolicited resumes.","$147,500 /yr (est.)",51 to 200 Employees,Company - Public,Hotels & Travel Accommodation,Travel Agencies,2012,Unknown / Non-Applicable
"E-Logic INC
4.4",4.4,"Washington, DC",Data Engineer (ETL & Data Catalogue Support),"Specific Duties:
Develop, test, and maintain extraction, transformation, and load (ETL) processes.

Develop and maintain an understanding of the data landscape at DOES including the data catalog, master data, data sources, data lakes, data warehouses, data assets, analytic products, and associated tools and software.

Support the Data Management Project team to develop and maintain data quality controls.

Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations.

Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation.

Support the data stewards to troubleshoot and resolve data issues.

Support business users to obtain requirements for enhancements and/or new analytic assets.

Assist in the Development of data asset training and documentation.

Participate in the development and implementation of a DOES data standard.

Participate in the development and maintenance of agency data security, privacy, policies, procedures, and best practices.","$84,277 /yr (est.)",1 to 50 Employees,Company - Private,Information Technology,Information Technology Support Services,2007,$5 to $25 million (USD)
"PRO IT
5.0",5.0,"Newark, NJ",AWS Data Engineer,"Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
Advanced skills in data model design and SQL development experience in RDBMS databases such as DB2, Oracle, Teradata
Extensive knowledge of coding and scripting languages, SQL, Python, etc.
Strong knowledge in ETL/ELT processes and tools. Informatica tech stack experience is a big plus
Experience implementing metadata solutions for configurable codebase
Ability to solve business problems by designing and building reports and queries, enhancing the ETL and databases, or identifying root cause issues
Experience implementing, supporting data lakes, data warehouses and data applications on AWS for large enterprises
Hands on experience of AWS services such as CloudFormation, S3, Athena , Glue, EMR/Spark, RDS, Redshift, DynamoDB, Lambda, Step Functions, IAM, KMS, SM etc.
Solid experience implementing solutions on AWS based data lakes
Experience in system analysis, design, development, and implementation of data ingestion pipeline in AWS
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS
Implement high velocity streaming solutions using Amazon Kinesis, SQS, and Kafka (preferred)
Migrate data from traditional relational database systems, file systems, NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift
Migrate data from APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift
AWS Solutions Architect or AWS Developer or AWS Big Data Specialty Certification preferred
Job Type: Full-time
Pay: $111,544.00 - $131,964.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Schedule:
5x8
8 hour shift
Day shift
Monday to Friday
Ability to commute/relocate:
Newark, NJ: Reliably commute or planning to relocate before starting work (Required)
Experience:
Informatica: 1 year (Preferred)
SQL: 1 year (Preferred)
Data warehouse: 1 year (Preferred)
Work Location: One location","$121,754 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"AgileEngine
5.0",5.0,Remote,Big Data SDET Engineer,"Test with the best! At AgileEngine, you're free to build your own team of A-players and choose out of 70+ available projects the one you'd like to work on. Join the squad of top-3% software experts who make a difference through off-the-wall solutions.
Must haves
Strong Scala and Spark (hands on). If not Scala, strong experience in either Java or Python
Big Data concepts
API testing experience
Understanding qa process to create test cases and test strategies
Independently able to contribute to automation framework as well interact with the stake holders as needed
SQL experience
AWS experience
Strong test mindset
Experience of deploying or using Terraform and Gitlabs would be a bonus
The benefits of joining us
Professional growth: accelerate your professional journey with mentorship, TechTalks, and personalized growth roadmaps
Competitive compensation: we match your ever-growing skills, talent, and contributions with competitive USD-based compensation and budgets for education, fitness, and team activities
A selection of exciting projects: join projects with modern solutions development and top-tier clients that include Fortune 500 enterprises and leading product brands
Flextime: tailor your schedule for an optimal work-life balance, by having the options of working from home and going to the office – whatever makes you the happiest and most productive.
About our projects
The company behind the world’s leading OS virtualization software leverages AgileEngine as a provider of external QA and testing automation expertise. We’ve developed our client’s test strategy complete with test plans, ETAs, analysis, and risk assessment. Our teams have also optimized the company’s test suites, halving the time it takes for the tests to run. Our other deliverables include the implementation of new tests and the development of documentation, how-to guides, and test instructions on an internal Wiki.
Our team is also working to establish a strong QA/AQA process for an industry-leading cybersecurity platform. We’ve developed our client’s test strategy, manual testing cases, and automated tests from scratch. Our deliverables include backend, API, and UI testing, as well as BDD integrated with a streamlined CI process.
Job Type: Full-time
Experience level:
4 years
Work Location: Remote",#N/A,1001 to 5000 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2010,Unknown / Non-Applicable
"EMONICS LLC
3.8",3.8,"Mahwah, NJ",Azure Data Engineer,"Azure data factory, data bricks, data lake, automation and performance optimization of ETL
power BI, automation, Azure AD, Azure monitor, Azure Purview, Azure Analysis services, Synapse, Predictive analytics & machine learning
Prior Hands-on experience in end to end CI/CD implementation or DevOps process in Data & Analytics context.
Job Type: Full-time
Salary: From $150,000.00 per year
Schedule:
8 hour shift
Ability to commute/relocate:
Mahwah, NJ 07430: Reliably commute or planning to relocate before starting work (Required)
Experience:
Data Engineer: 8 years (Preferred)
Azure Data factory: 8 years (Preferred)
ETL: 8 years (Preferred)
DataLakes / Databricks: 6 years (Preferred)
CI/CD: 7 years (Preferred)
Work Location: One location","$150,000 /yr (est.)",1001 to 5000 Employees,Company - Private,Human Resources & Staffing,HR Consulting,2016,$100 to $500 million (USD)
"Cognira
4.3",4.3,"Atlanta, GA",Senior Data Engineer,"About us:
At Cognira, we strongly believe that people are the biggest asset of our company.
Our hand-picked team consists of passionate, collaborative, and forward-thinking individuals from all over the globe.
Our industry-leading PromoAI Solution leverages data science and AI to effectively manage the entire end-to-end promotion lifecycle in a single tool, allowing retailers to effectively plan, optimize, and analyze all types of promotions.
For the last three years in a row, Cognira has been recognized as one of the fastest-growing companies in North America. We are proud to have a growing team of domain experts and data scientists, as well as a culture that fosters strong and long-lasting relationships with our clients.

Are you ready to grow with us?
To find out more about Cognira, please visit our website at www.cognira.com

About this role
Cognira is seeking a motivated and passionate Senior Data Engineer to join our fast-growing development teams. We are leveraging the latest cloud-native technologies and data science techniques to design and deploy scalable SaaS solutions for retailers.

What you will do
Create innovative, scalable, and fault-tolerant solutions
Design configurable and reusable components for a multi-tenant environment
Work closely with project and product management, data scientists, and QA in a fast-paced team environment
Investigate and resolve technical and performance issues
Experiment, fail-fast, and learn, as you build skills and experience
Ensure high quality with test automation

We would love to hear from you if you have
You have experience with Java, Scala, Python, and other programming languages
You have experience with scalable and resilient microservice orchestration with Kubernetes, Docker, and Helm
You are conversant in asynchronous programming techniques
You are familiar with database design techniques in a distributed NoSQL cloud environment
You are familiar with Apache Spark and big data analytics
You are aware of data science tools and techniques, including Tensorflow and Keras
You are conversant in agile team development using tools such as Git, Jenkins, Jira, and more
You have excellent communication and presentation skills

Perks
In addition to joining us on our journey to build a state-of-the-art, AI-enabled software we also get a ton of perks!

You get the choice to work on a Mac or a PC.
Casual dress code, social events, and after-works.
Flexible, diverse work environment.
Respectful, innovative team.
But it’s not all about the fun. You get a competitive salary and a progressive bonus while getting startup experience at a company with an awesome culture.
You get 21 days of PTO, and major national holidays.","$122,989 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Enterprise Software & Network Solutions,2015,Unknown / Non-Applicable
"IBR (Imagine Believe Realize)
4.5",4.5,"Melbourne, FL",Senior Data Engineer,"The Data Engineer must be able to meet the key criteria below:
Location: 100% telework
Years' Experience: Mid and Senior level openings
Education: Bachelor’s in IT related field
Clearance: Must be able to obtain and maintain a Public Trust clearance
Work Authorization: Must be a US Citizen
Employment Type: Full-Time, W-2
Key Skills:
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling required
ETL experience required
Experience modeling solutions and deploying production enterprise applications in AWS required
AI/ML knowledge a bonus
Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities
Design, implement and maintain data architecture and performing data analytics
Implement and validate predictive and prescriptive models, create and maintain statistical models with a focus on big data
Incorporate a variety of statistical and machine learning techniques in your projects
Write programs to cleanse and integrate data in an efficient and reusable manner
Use leading edge and open-source tools such as Python and our AI application suites
Work in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors
Communicate with internal and external clients to understand and define business needs and appropriate modeling techniques to provide analytical solutions
Evaluate modeling results and communicate the results to technical and non-technical audiences
Build reusable code and libraries for future use
Ensure the technical feasibility of UI/UX designs
Create work estimates and meet project deadlines
Participate end to end in the product life cycle: requirements gathering, solution design, development, testing, and implementation
Translate application storyboards, requirements docs and use cases into functional applications
Work both independently and collaboratively with business and development team to create great user experiences for our customers
Troubleshoot and resolve any bugs assigned.
Qualifications
Enterprise data architecture and management experience
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience modeling solutions in AWS.AI/ML experience a bonus
Experience deploying production enterprise applications in AWS.
Experience with large scale, high performance enterprise big data application deployment and solution architecture in AWS environment
Must be able to define and maintain BI/Data Warehouse methodologies, standards and industry best practices
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Experience working with large scale database requirements supporting diverse data types
Experience developing ML applications using Cloudera Machine Learning or Alteryx tools, object-oriented design, and software engineering best practices
Debug, troubleshoot, design and implement solutions to complex technical issues
Strong technical and analytical abilities
Basic understanding of statistical programming in a language such as R, Python, SAS, SPSS, Hadoop, Spark, Redis, Elastic, or Tableau
Proficiency in at least one computer programming language is a plus such as Java, C++, JavaScript, Node, JSs
Basic understanding of Cloud (AWS, Azure, etc.)
Ability to thrive in a team-based environment
Hands-on experience in JavaScript, JQuery, HTML, CSS, Node.JS, Angular.js;
Excellent work experience with R/Python scripting language;
Experience with UNIX; and text manipulation using vi or similar text editors.
Knowledge of simulation and analysis tools including editors, compilers, linkers, debuggers, code analyzers, version control systems (GIT/SVN), software testing tools, etc.
Expertise in Agile and DevSecOps approaches
Experience with AWS Cloud (ie. EC2, S3, Redshift)
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About IBR
Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:
Nationwide medical, dental, and vision insurance
3 weeks of paid time off and 11 paid federal holidays
401k matching
Life insurance, short term disability and long term disability at no cost to our employees
Health Care Flex Spending and Dependent Care Flex Spending accounts
Training and education assistance opportunities
IBR is an Equal Opportunity and Affirmative Action Employer. It is our policy to offer employment opportunity to all persons without regard to race, color, age, national origin, religion, sex, gender identity/transgender status, veteran status, disability, genetic information, pregnancy, childbirth or related medical conditions, or any other status protected under applicable federal, state, or local law. IBR does not discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant.
Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional information has been provided below:
407.459.1830.
Job Type: Full-time
Benefits:
401(k)
401(k) matching
Dental insurance
Flexible spending account
Health insurance
Life insurance
Paid time off
Tuition reimbursement
Vision insurance
Schedule:
8 hour shift
Work Location: Remote","$118,866 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Software Development,2007,Unknown / Non-Applicable
"Hired by Matrix
4.5",4.5,"South Milwaukee, WI",Data Engineer,"Data Engineer (Hybrid):
Work with the engagement team to understand priorities, build backlogs, prioritize tasks that will address issues and changes, and present proposals for improving the client’s environments. Ability to understand the end game and create iterative, agile outputs that build momentum and maintain the flexibility to stay in line with maximizing business value.
Qualifications:
Change agent that has a thirst and commitment to the perpetual search of new ways of leveraging cutting-edge capabilities to deliver world-class solutions.
Bachelor’s degree in Computer Science, MIS, IS, or Engineering preferred
Emotional Intelligence and ability to meet the customer where they are and provide collaborative consultative solutions.
3-5 years of experience in engineering solutions for Cloud applications leveraging Microsoft Azure, Google Cloud, or ServiceNow services.
Knowledge of and the ability to perform basic statistical analysis such as measures of central tendency, normal distribution, variance, standard deviation, basic tests, correlation, and regression techniques.
Cloud-based and data-oriented certifications in Azure, Google, ServiceNow, or AWS.
Experience with modern storage technologies (Azure Data Lake, Azure Synapse, GCP Cloud, BigQuery)
Experience with Hadoop, AutoSys or Control M, Docker, Kubernetes, public cloud services, modern SDLC processes & tools, and ML model management.
Experience with data migration, transformation, and refactoring to a cloud-based environment
Experienced in the use of standard ETL and ELT tools and techniques.
3-5 years developing solutions that consist of data integration, data warehouse, data marts, data transformation, data modeling, and data presentation on Azure, GCP, and ServiceNow platforms.
3-5 years of experience with IoT, unstructured data capture, and transformation techniques leveraging Azure and Google Cloud capabilities.
Expertise in Python, SQL, .Net, Scala, Spark, and other advanced programming capabilities
Working knowledge of PaaS services relating to Data, Analytics and end-point API interfaces – EMR/Dataproc/Databricks etc.
Knowledge of security controls around authentication, authorizations, encryptions, and certificates
Versed in data visualization tools such as Microsoft Power BI, Tableau.
Has a working knowledge of various data structures and the ability to extract data from various data sources.
Strong verbal and written communication skills.
Understanding of cloud scaling technologies and the economic impact of doing so.
Versed in on-prem and cloud-based deployments and impact on network resources to effectively deploy solutions seamlessly and assisting in triage where needed.
Ability to work in a fast-paced, rapidly changing environment.
Job Types: Contract, Permanent
Salary: $50.00 - $65.00 per hour
Benefits:
401(k)
401(k) matching
Health insurance
Tuition reimbursement
Compensation package:
Performance bonus
Signing bonus
Experience level:
2 years
3 years
4 years
5 years
Schedule:
8 hour shift
Monday to Friday
Application Question(s):
Are you willing to relocate at Milwaukee, WI?
Are you comfortable working on W2, full time role?
Work Location: One location
Speak with the employer
+91 8135358173",$57.50 /hr (est.),51 to 200 Employees,Company - Private,Human Resources & Staffing,Staffing & Subcontracting,1986,$5 to $25 million (USD)
"etrailer.com
3.9",3.9,Remote,Data Engineer/Data Scientist,"Mid-to-Senior Level Data Engineer/Data Scientist
Salary:100,000-180,000 yearly

Get to know etrailer.com
etrailer.com is an industry leader in helping people lead fulfilling lives by empowering them to complete work, go on vacations, and experience the outdoors. We achieve this through unmatched industry expertise, putting the customer first, and expert service. We want to supercharge our customer interactions to provide individualized, incredible customer experiences.

We are looking for...
etrailer.com is looking for experienced Data Engineers/Data Scientists that will be key players in propelling the company beyond its current capabilities. Preferred candidates will be able to suggest and implement solutions with new toolsets, methodologies, and industry standards to help improve the etrailer data ecosystem.

Required Qualifications
Bachelor's degree in Computer Science, Electrical/Computer Engineering, Statistics, Mathematics, or other related fields with 5 years of experience OR Master's degree in Data Science or Data Engineering with 3 years of experience
Strong Python experience
SQL experience
Nice to Have Qualifications
Azure experience
Splunk experience
C# experience
Experienced in designing, implementing, and maintaining data capture and data processing solutions. Some example technologies include:
Kafka
SQL
Splunk
Python
C#
Experienced in designing, implementing, and maintaining monitoring, alerting, and visualization solutions. Some example technologies include:
Splunk
Power BI
Tableau
Experienced in designing, implementing, and maintaining machine learning solutions through the entire model life cycle. Some example technologies include:
Python
Azure cloud
Data bricks
ML Flow","$140,000 /yr (est.)",501 to 1000 Employees,Company - Private,Retail & Wholesale,Automotive Parts & Accessories Stores,1946,$100 to $500 million (USD)
"Angle Health
4.0",4.0,Remote,Data Engineer,"Changing Healthcare For Good
At Angle Health, we believe the healthcare system should be accessible, transparent, and easy to navigate. As a digital-first, data-driven health plan, we are replacing legacy systems with modern infrastructure to deliver our members the care they need when they need it. If you want to build the future of healthcare, we'd love for you to join us.

The Role
As a Data Engineer on the Strategy team at Angle Health, you will be part of an elite team of problem solvers tackling some of the hardest operational challenges across the business. You will be working closely with Technical Product Managers and other engineers in small teams embedded within functions across the company—from sales to operations to finance and more.

In partnership with operational stakeholders, your job will be to quickly understand business workflows, and design and implement technical, data-driven solutions to achieve the intended business outcomes. That may include architecting backend micro-services, building and maintaining robust data pipelines, standing up health metrics and automated alerts, and developing scalable technical solutions to support Angle Health's day-to-day operations.

Every day will be a new learning experience in this role and may span discussing architecture with fellow engineers, wrangling large-scale data, coding a custom web app or micro-service, or speaking with customers and executives.

This role is ideal for entrepreneurial engineers, applied data scientists, and creative, intellectually curious thinkers that want to solve high-value problems in healthcare.

This position may be based in San Francisco, New York City, Salt Lake City, or Remote.

We are currently trialing various titles for the same role. Please consider the following posted roles as the same position: Deployed Engineer, Software Engineer, Business Operations, and Data Engineer. Please note these are the same, so if you have already applied for one position, there is no need to reapply for the others.
What We Value
A strong engineering background in computer science, software engineering, data science, mathematics, or similar technical field is required for this role
Proficiency in programming languages (e.g. Python, SQL, Java, TypeScript/JavaScript, or similar) and data engineering frameworks
A highly analytical mindset and an eagerness to build technical solutions to complex business problems
High attention to detail and intellectual curiosity—you're not satisfied with surface-level answers. You want to dive into the data, the ""how,"" and the ""why"" because ""the way it's always been done"" is not always the way it should be done
Low ego—the outcome matters more than who gets the credit
Demonstrated ability to collaborate effectively in teams of technical and non-technical individuals
Highly organized with an ability to multitask, problem solve, and balance competing priorities in a rapidly changing environment
Because We Value You:

Competitive compensation and stock options
100% company paid comprehensive health, vision & dental insurance for you and your dependents
Supplemental Life, AD&D and Short Term Disability coverage options
Discretionary time off
Opportunity for rapid career progression
Relocation assistance (if relocation is required)
3 months of paid parental leave and flexible return to work policy (after 10 months of employment)
Work-from-home stipend for remote employees
Company provided lunch for in-office employees
401(k) account
Other benefits coming soon!

Backed by a team of world class investors, we are a healthcare startup on a mission to make our health system more effective, accessible, and affordable to everyone. From running large hospitals and health plans to serving on federal healthcare advisory boards to solving the world's hardest problems at Palantir, our team has done it all. As part of this core group at Angle Health, you will have the right balance of support and autonomy to grow both personally and professionally and the opportunity to own large parts of the business and scale with the company.

Angle Health is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. Angle Health is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities.",#N/A,1 to 50 Employees,Company - Private,Healthcare,Health Care Services & Hospitals,2019,Unknown / Non-Applicable
"Enterprise Knowledge LLC
4.0",4.0,"Arlington, VA",Data Engineer,"Enterprise Knowledge (EK) is hiring for a full-time Data Engineer to join our growing Data and Information Management Practice. This individual will support dynamic and fast-paced technical delivery projects for a range of commercial and federal clients that provide the opportunity to learn fast and grow quickly. We are seeking a well-organized, curious, and quick learner who will tackle diverse challenges with creative solutions. The right candidate will have a passion for working with diverse data types and applying new methods and approaches to data challenges including managing data at scale using popular data manipulation frameworks backed by scalable cloud architecture.
The data engineer will be part of a team working on cutting-edge projects developing an orchestrated data solution to integrate and transform source data to build a Knowledge Graph that powers advanced search and Artificial Intelligence (AI) solutions.
As an EK team member, you will join a fast-growing company that is committed to diversity and inclusion, have the opportunity to work in a collaborative workplace, take advantage of our unique benefits, and help build our innovative culture. Don’t just take our word for it – we were recently featured as one of Inc. Magazine’s 2022 Best Workplaces!
Required Skills and Qualifications:
Experience in development using leading data manipulation frameworks, especially Python or R, and associated libraries such as pandas and dplyr.
Able to carefully identify and understand client needs in order to design practical solutions that surpass their expectations.
Minimum of 3 years of experience developing data pipelines or data products and services such as microservices or APIs.
Experience implementing data transformation pipelines using data orchestration or ETL workflows such as with tools like Apache Airflow or AWS Step Functions.
Experience with multiple data structures and tools such as relational databases, graph databases, document stores, search indexes, etc., and multiple data serialization formats such as CSV, JSON, Parquet, or HDFS.
Proficiency in at least one database query language such as SQL, SPARQL, or Gremlin.
Proven experience working directly with clients, providing briefings, facilitating meetings, and presenting work products.
All of our employees are required to be fully vaccinated against COVID-19 regardless of the employee's location or work arrangement (e.g., telework, remote work, etc.), subject to such exceptions as required by law. If you are hired, we will require you to prove that you have received the COVID-19 vaccine, unless you have received a medical or religious exemption.
Preferred Skills and Qualifications:
Experience with the graph data structure such as Knowledge Graphs or other graph analytics use cases.
Working knowledge or experience with implementing Machine Learning workflows including Exploratory Data Analysis, Feature Engineering, Model Training, and Cross-Validation.
Implementation of distributed data processing frameworks such as Apache Spark, Kolas, or Dask.
Experience building, deploying and supporting cloud-based infrastructure such as AWS and understanding of key cloud/IT principles such as networking, permissions, basic server management, and Infrastructure as Code (IaC) templating.
“We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.”","$89,080 /yr (est.)",51 to 200 Employees,Company - Private,Information Technology,Information Technology Support Services,2013,$5 to $25 million (USD)
Excellerent Solutions Inc,#N/A,"Coppell, TX",Data Integration Engineer,"Role: Data Integration Engineer
Location: Irving, TX(5 days a week in Office, 100% onsite role)
Must have experience working with SnapLogic to perform ELT processes.
Must have good written and verbal skills
Must be capable of knowledge transfer to team of ~10 ETL developers on use of SnapLogic
Must be able to work with a larger team to complete sizable migration of on-prem processes to Snowflake.
Must be able to follow design patterns set out by our architecture team.
Job Types: Full-time, Contract
Pay: $48.18 - $60.00 per hour
Schedule:
8 hour shift
Work Location: In person",$54.09 /hr (est.),#N/A,#N/A,#N/A,#N/A,#N/A,#N/A
"Agiles Enterprise
3.4",3.4,"Glen Allen, VA",Data Engineer,"· Databricks (s.Python, sql)
· Candidates should be really strong in Python and SQL
· Azure Data Factory
· Synapse Analytics
· Parque and Delta table
Job Type: Full-time
Salary: $84,050.65 - $189,023.89 per year
Schedule:
8 hour shift
Work Location: On the road","$136,537 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Less than $1 million (USD)
"Buchanan & Edwards
3.8",3.8,"Washington, DC",Data Engineer,"Overview
Growth requires a strong foundation, and at Buchanan & Edwards, our people are our strength.
They are the foundation for building the next generation of innovative IT solutions, revolutionizing the industry, and solving the nation’s most critical challenges.
Let Buchanan & Edwards help you unleash your potential and reach your goals.
Are you looking for an opportunity to join a growing company that is innovative, cutting edge, and mission centered? Our prestigious technical and management consultants work closely with our customers in a dynamic, solution-oriented environment, and we are inviting you to apply for our recent opening as an Data Engineer.
The success of our mission relies on you!
Clearance
TS clearance
Responsibilities
Responsible for:
Understanding of programming and data engineering concepts and best practices.
Experience with Python, SQL, and/or Spark.
Experience working with both structured, semi-structured, and unstructured data to include data parsing, transformation, schema definition, and query/analysis.
Ability to manage and organize data while identifying trends and inconsistencies that will impact downstream analytics
Ability to work both independently and collaboratively.
Have experience with data pipelines or be willing to learn a pipeline from bottom to top
Will be able to trouble shoot files against an architecture to see where the upload process is failing.
Will be able to understand unit tests and add to them to increase stability to the entire pipeline.
Familiar with GIT, JIRA, Confluence, Anaconda, Spyder, and Microsoft tools.
Qualifications
Bachelors Degree
10 years of experience with programming and software development to include analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
5 years of experience architecting solutions based on customer requirements.
5 years of experience leading technical teams.
3 years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
Job Type: Full-time
Pay: $120,000.00 - $157,000.00 per year
Benefits:
401(k)
Dental insurance
Health insurance
Paid time off
Vision insurance
Compensation package:
Yearly pay
Experience level:
10 years
Schedule:
8 hour shift
Ability to commute/relocate:
Washington, DC 20535: Reliably commute or planning to relocate before starting work (Required)
Education:
Bachelor's (Required)
Experience:
programming and software development: 10 years (Required)
Python, SQL, and/or Spark: 10 years (Required)
Security clearance:
Top Secret (Required)
Work Location: One location","$138,500 /yr (est.)",201 to 500 Employees,Company - Private,Information Technology,Information Technology Support Services,1998,$25 to $100 million (USD)
"FlexIT Inc
4.0",4.0,"Beaverton, OR",Big Data Engineer,"FlexIT client is looking for a Big Data Engineer, 12 months contract role in Beaverton, Oregon.
Key Skills: Big Data, Spark, Hiva, Python, Scala, Java, RESTful APIs, OpenAPI, NoSQL/DynamoDB/Cassandra, Apache/Kafka
Here are some tasks that you could do day to day:
Design and implement distributed data processing pipelines using Spark, Hive, Python, and other tools and languages prevalent in the Hadoop ecosystem. You will be given the opportunity to own the design and implementation. You will collaborate with Product managers, Data Scientists, Engineering folks to accomplish your tasks.
Publish RESTful APIs to enable real-time data consumption using OpenAPI specifications. This will enable many teams to consume the data that is being produced.
Explore and build proof of concepts using open source NOSQL technologies such as HBase, DynamoDB, Cassandra and Distributed Stream Processing frameworks like ApacheSpark, Flink, Kafka stream.
Take part in DevOps by building utilities, user defined functions and frameworks to better enable data flow patterns.
Work with architecture/engineering leads and other teammates to ensure high quality solutions through code reviews, engineering best practices documentation.
Experience in Business Rule management systems like Drools will also come in handy.","$100,840 /yr (est.)",1 to 50 Employees,Company - Private,#N/A,#N/A,#N/A,Unknown / Non-Applicable
"ATCS Inc.
3.6",3.6,"Atlanta, GA","Azure Data Engineer (Azure Data Factory, Databricks)","Role: Azure Data Engineer (Azure Data Factory, Databricks)
Location: Atlanta, GA
Duration: Fulltime

Job Description for Data Engineer:
Candidate needs to work on the Azure Cloud using core cloud data warehouse tools (Azure Data Factory, Azure Databricks, Azure SQL DW and other Big Data related technologies).
Must be able to analyze data and develop strategies for populating data lakes.
Candidate must know complex coding using SQLs, Pyspark or Python as Data bricks developer.
Responsibilities Work as on Cloud Data and Analytics solutions.
Participate in development of cloud data warehouses, data as a service, business intelligence solutions.
Databricks development usings SQLs, Spark (Scala or Python)- Minimum E1 (E2 is good).
Developing Modern Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data Factory, Azure Databricks)

Skills/Qualifications
Expertise in ETL tools such as Azure Databricks and who understands key Azure cloud concepts.
Familiarity with implementing Data warehousing Solutions.
Experience as Data Engineer in Azure Big Data Environment is nice to have.
Programming experience in Pyspark or Python, SQLs
Hands-on experience in Azure stack Azure Data Lake, Azure Data Factory, Azure Databricks.
Good understanding of other Azure services like Azure Data Lake Analytics U-SQL, Azure SQL DW.
Good understanding of Azure Databricks platform and can build data analytics solutions to support the required performance scale.
Good analytical and problem-solving skills.
I1dfOh43IR","$74,927 /yr (est.)",501 to 1000 Employees,Company - Private,Transportation & Logistics,"Airlines, Airports & Air Transportation",1986,Unknown / Non-Applicable